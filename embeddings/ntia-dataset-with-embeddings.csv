ID,text,Commenter_alias,n_tokens
NTIA-2023-0009-0155," 
 
 
National Telecommunications and Information Administration Request for Comments 
on Dual Use Foundation Models and Widely Available Model Weights 
March 26, 2024 
 
BSA appreciates the opportunity to provide comments on the National Telecommunications 
and Information Administration’s (NTIA) Request for Comments (RFC) on Dual Use 
Foundation Models and Widely Available Model Weights.  
 
BSA is the leading advocate for the global software industry.1 BSA members are at the 
forefront of developing cutting-edge services — including AI — and their products are used 
by businesses across every sector of the economy.2 For example, BSA members provide 
tools including cloud storage and data processing services, customer relationship 
management software, human resource management programs, identity management 
services, cybersecurity services, and collaboration software. BSA members are on the 
leading edge of providing AI-enabled products and services. As a result, they have unique 
insights into the technology’s tremendous potential to spur digital transformation and the 
policies that can best support the responsible use of AI. 
  
BSA’s views are informed by our experience working with member companies to develop 
the BSA Framework to Build Trust in AI,3 a risk management framework we published 
almost three years ago to help companies mitigate the potential for unintended bias in AI 
systems. Built on a vast body of research and informed by the experience of leading AI 
developers, the BSA Framework outlines a lifecycle-based approach for performing impact 
assessments and highlights corresponding best practices.4 Our experience on these issues 
informs our recommendations below. 
 
 
 
 
 
 
1 BSA’s members include: Adobe, Alteryx, Asana, Atlassian, Autodesk, Bentley Systems, Box, Cisco, 
CNC/Mastercam, Databricks, DocuSign, Dropbox, Elastic, Graphisoft, Hubspot, IBM, Informatica, 
Kyndryl, MathWorks, Microsoft, Okta, Oracle, PagerDuty, Palo Alto Networks, Prokon, Rubrik, 
Salesforce, SAP, ServiceNow, Shopify Inc., Siemens Industry Software Inc., Splunk, Trend Micro, 
Trimble Solutions Corporation, TriNet, Twilio, Workday, Zendesk, and Zoom Video Communications, 
Inc.  
2 See BSA | The Software Alliance, Artificial Intelligence in Every Sector, available at  
https://www.bsa.org/files/policy-filings/06132022bsaaieverysector.pdf. 
3 See BSA | The Software Alliance, Confronting Bias: BSA’s Framework to Build Trust in AI, available 
at https://www.bsa.org/reports/confronting-bias-bsas-framework-to-build-trust-in-ai.   
4 BSA has testified before the United States Congress and the European Parliament on the 
Framework and its approach to mitigating AI-related risks. See, e.g., Testimony of Victoria Espinel, 
Public Hearing on AI & Bias, Special Committee on Artificial Intelligence in a Digital Age, European 
Parliament, Nov. 30, 2021, available at 
https://www.europarl.europa.eu/cmsdata/244265/AIDA_Verbatim_30_November_2021_EN.pdf; 
Testimony of Victoria Espinel, The Need for Transparency in Artificial Intelligence, Before the Senate 
Committee on Commerce, Science, and Transportation Subcommittee on Consumer Protection, 
Product Safety, and Data Security, available at https://www.bsa.org/files/policy-
filings/09122023aitestimonyoral.pdf.   
 
AI systems with widely available model weights are an increasingly important part of the AI 
supply chain. Many companies increasingly incorporate ‘fine-tuned’ (retrained) open large 
language models (LLMs) in a wide variety of business applications. Proprietary foundation 
models also continue to spur innovation, and the US government should support both 
distribution models.  
 
To the degree that AI creates risks that companies should guard against, many of these 
risks are attributable to AI models broadly, rather than open foundation models specifically. 
Because the development of open foundation models does not occur in the United States 
alone, restrictions on open foundation models could cut off regulators’ access to essential 
information about how systems operate in practice and ultimately fail to mitigate potential 
risks.  
 
The RFC asks about a range of issues related to open foundation models, including their 
benefits, their risks, potential safeguards, and the role of government in imposing 
restrictions on the availability of model weights. We address these issues and recommend 
the US government: 
 
• 
Recognize the substantial benefits that open foundation models provide to both 
consumers and businesses;  
• 
Avoid restricting the availability of open foundation models; 
• 
Ground policies that address risks of open foundation models on empirical 
evidence; and 
• 
Encourage the implementation of safeguards to enhance the safety of open 
foundation models. 
 
I. 
Open foundation models provide substantial benefits to businesses and 
consumers. 
 
AI provides immeasurable economic and societal benefits, and open foundation models 
play an important role in fueling this innovation. Indeed, the benefits that open foundation 
models provide are substantial and include: 
 
• 
Lower Cost, More Competition, and Democratization of AI. LLMs are expensive 
to operate, often 10 cents or more per query, and using an open LLM brings the 
operating cost down considerably. Open foundation models also lower barriers to 
entry, improve competition, decrease the cost of ownership, and increase the range 
of alternative options, thus democratizing the use and control of AI. Availability of 
open foundation models also enables companies that don’t have access to 
significant levels of compute to bridge the gap and offer foundation model 
capabilities that match the outputs of proprietary models. For example, a company 
may start with an open foundation model, fine tune that model, and integrate the 
revised model into the products and services it provides to consumers and 
businesses. Open AI models also reduce vendor lock-in, giving users greater 
flexibility and ability to control their AI system. 
 
• 
Increased Transparency and Quality Control. Open foundation models allow 
organizations to understand exactly what is happening in AI models because the 
source code — and typically the model weights and training data — are freely 
available for access and inspection to any third party that may wish to analyze 
them, including regulators and researchers. This accessibility and transparency can 
enable rapid identification and mitigation of problems that may arise, permit greater 
understanding of how an AI system works, and generally enhance trust in AI. 
 
 
• 
Safety and Resiliency. Access to open foundation models is a driving force behind 
research on increasing safety and reducing AI risks, including alignment methods. 
Researchers and other experts use open foundation models to develop solutions to 
a range of issues relevant to both proprietary and open foundation models, 
including interpretability, security, and safety. A decentralized open AI ecosystem 
also increases the availability of different foundation models, which enhances 
resiliency because there is no single point of failure in the event that a particular 
model becomes impaired or unavailable. Indeed, many enterprise firms leverage 
multiple open and proprietary models to power products and features within their 
ecosystems precisely as a means of ensuring resiliency. The availability of open AI 
models can make the AI ecosystem more, not less, safe. 
 
• 
Increased Customization. With open foundation models, organizations have an 
opportunity to create customized applications by fine-tuning or otherwise adapting 
the model for particular purposes, which expands the number of potential 
applications and increases innovation. For example, enterprise customers of video 
communications providers can, within their own closed data ecosystem, use open 
LLMs to power features that provide tailored summaries of online meetings. In 
addition to customizing how the model operates, customizable model sizing also 
provides companies with an alternative to operating a larger model through an API 
where a smaller model may be perfectly capable for a given use case. In addition to 
making available AI tools to companies with less computational power, using 
smaller models helps companies of all sizes decrease compute workloads and 
impact on the environment. 
 
• 
Advanced Scientific Research. The broader access provided by open foundation 
models accelerates scientific research in a range of fields, and it is essential for 
reproducibility of research. Notably, open AI models have enhanced healthcare 
research in medical imaging analysis, which could lead to more accurate diagnoses 
and better treatment decisions. 
 
• 
Promoting Equity and Inclusion. The availability of open foundation models 
increases access to cutting-edge AI tools. As a result, it lowers barriers to obtaining 
skills that are necessary to have careers in AI, expanding opportunities for diverse 
communities.  
 
 
II. 
The US government should avoid restricting the availability of open 
foundation models. 
 
The RFC solicits information on the role the US government should play in supporting or 
restricting the availability of model weights, and on legal or other measures that could be 
employed to prevent widespread availability of open foundation model weights. We 
recommend the US government not restrict the availability of open foundation models. 
Instead, it should support the further development of a robust AI ecosystem, including open 
foundation models. Any specific policy options for open foundation models should be 
considered only as any marginal risk posed by such models are better understood.   
 
Restricting foundation models with widely available model weights would significantly curtail 
the benefits of open foundation models. Restrictions are also unlikely to mitigate perceived 
risks, because the development of open foundation models does not occur in the United 
 
States alone. Indeed, such restrictions may instead cut off an essential pipeline of 
information for researchers, including those focused on advancing AI safety solutions. 
Restricting the availability of open foundation models in the United States could also 
frustrate the US government’s regulatory efforts by limiting access to key information that 
provides an understanding of how particular AI models operate. Internationally, the 
development and use of open foundation models would continue to be available to global 
partners and adversaries, while US organizations would be stymied, further hampering US 
economic and security interests.  
 
The RFC also inquires about circumstances in which the government contracts with 
companies that use open foundation models. The Administration should not limit the 
government’s ability to contract with companies that use open foundation models, nor 
should it require disclosures regarding the use of these models. Open-source components 
are ubiquitous in software and are a key part of the technological landscape that fuels 
innovation. Companies should be incentivized, not discouraged, from leveraging these 
resources to expand the diversity and capability of available applications. The government 
should also benefit from the innovation created through open foundation models, which 
aligns with the Administration’s IT modernization goals. 
 
Finally, the US government should support a robust AI ecosystem with open models that is  
government-wide and does not vary by sector. The US AI Safety Institute could be 
particularly helpful in advancing the responsible development of open foundation models, 
including supporting more research on credible risks and safety safeguards. Global 
interoperability is also critical to effective AI policies, and the United States should work 
with its allies to ensure that the global policy landscape promotes the development and use 
of responsible AI, including open foundation models. 
 
 
III. 
Policies aimed at addressing the risks of open foundation models should be 
grounded in empirical evidence. 
 
AI provides immense benefits to consumers and businesses, including stimulating 
economic growth and solving complex societal challenges. In some contexts, however, 
certain uses of AI can exacerbate existing risks, including risks of biased outputs and 
disinformation. BSA members are committed to taking steps to mitigate AI risks as they 
develop and use AI responsibly. That is why BSA worked with member companies to 
develop the BSA Framework to Build Trust in AI, which was released in 2021 and is 
designed to help organizations mitigate the potential for unintended bias in AI systems by 
adopting a lifecycle-based approach to risk management.  
 
The RFC acknowledges that open foundation models create benefits, but it also indicates 
that open foundation models could “engender substantial harm,” such as risks to security 
and equity. Although we agree that AI has the potential to create risks that companies 
should guard against, there are important questions about how much such risks can be 
attributed to the availability of model weights. Leading researchers from Stanford, 
Princeton, and other notable organizations recently released research highlighting 
commonly cited risks of open foundation models, including biosecurity and cybersecurity.5 
They emphasized that other types of technology already present similar risks, and that 
more empirical evidence is necessary to quantify the marginal risk created by open 
foundation models — i.e., the amount of new risk that goes beyond those already posed by 
 
5 Sayash Kapoor et al., On the Societal Impact of Open Foundation Models, Feb. 27, 2024, at 2, 
available at https://crfm.stanford.edu/open-fms/paper.pdf.  
 
proprietary foundation models or other pre-existing technologies.6 As one example, the 
paper highlights concerns about open foundation models generating accurate information 
about pandemic-causing pathogens — and notes that similar information is available on 
public internet search engines.7 
 
As the US government develops policies governing open foundation models, it should 
ground those policies in credible evidence of the incremental risks presented by the 
availability of open models as compared to proprietary models or other technologies, like 
search engines. To the extent that evidence remains unavailable, the government should 
take care to avoid premature action. Even in areas where more impact of open foundation 
models has been observed — such as facilitating disinformation — the US government 
should weigh these outcomes against the considerable benefits that open foundation 
models provide and create more targeted policy solutions that address actual harms. The 
government also may want to consider ways to encourage solutions that use models with 
widely available model weights to address such risks, to avoid diminishing the benefits of 
open models in other contexts.  
 
 
IV. 
AI developers and deployers should be encouraged to implement safeguards 
to advance responsible AI in both open and proprietary AI systems. 
 
There are a range of safeguards that developers and deployers of AI can implement to 
identify, mitigate, and ultimately reduce potential risks associated with AI systems. We 
focus on five particularly important safeguards: risk management programs, impact 
assessments, information-sharing, model evaluations, and safety measures. In many 
cases, these safeguards can help to identify and address risks across AI systems, including 
for both open and proprietary AI systems.   
 
Risk management programs. Companies can adopt risk management programs to 
identify the personnel, policies, and processes necessary to manage AI risks. A strong risk 
management program can benefit companies that develop or use either proprietary or open 
AI systems; companies that use both open and proprietary systems can also benefit from a 
risk management program that helps them holistically identify risks across multiple AI 
systems. To implement a risk management program, a company can adopt a range of 
important corporate governance elements, including clearly assigning roles and 
responsibilities to key personnel, establishing formal policies on their development and/or 
use of AI, identifying their evaluation mechanisms, ensuring executive oversight, performing 
impact assessments for high-risk AI, and creating internal independent review mechanisms, 
such as interdepartmental governance or ethics committees, to evaluate and address AI 
issues that pose high risks. These steps align with the AI Risk Management Framework 
(RMF) developed by the National Institute of Standards and Technology (NIST).  
 
Impact assessments. Impact assessments are important accountability tools that help 
developers and deployers identify and mitigate risks associated with open or proprietary 
high-risk AI systems. Impact assessments should focus on high-risk AI systems, to ensure 
that organizations devote resources to addressing systems that pose the greatest potential 
risks. Importantly, there is no “one-size-fits-all” approach to evaluating and mitigating risks 
of AI; impact assessments should be tailored to address the nature of the system at issue, 
the type of harms the system may pose, and the role of the actor along the AI value chain. 
For example, a company developing an AI system based on an open foundation model may 
 
6 Id. at 2, 8. 
7 Id. 
 
be well situated to assess whether additional safeguards are necessary to protect privacy 
or security in the context of that system, determine performance metrics, and improve 
representativeness of the data used to fine-tune the model. 
 
Information sharing. Developers of foundation models should ensure that they provide to 
downstream providers or otherwise make available transparent information documenting 
key aspects of the model, including its design features, capabilities, a summary of the type 
of data used in training, known limitations, and factors relating to safety and security 
features. This information is consistent with model cards provided for some existing open AI 
models and will be helpful to organizations fine-tuning the model and integrating it into 
other products and services. 
 
Model evaluations. Model evaluations can be an important mechanism for surfacing 
problems with an AI model. Testing is a key component of these evaluations and can 
identify safety and bias issues. Techniques can include adversarial testing (i.e., red-
teaming) and vulnerability scanning. Cross-disciplinary review will also be helpful to detect 
and address a wider array of issues that may arise.   
 
Safety. Developers of open foundation models should diligently follow industry standard 
safety procedures in developing open foundation models. There are a range of options 
available, and research continues to explore new technical mitigation strategies.  
 
These safeguards can be applied in different ways by different companies, depending on 
their role in developing or deploying an AI system. For example, the developer of an open 
foundation model may adopt a risk management program, to ensure that the company has 
personnel and policies that govern its development of the AI model. A company developing 
its own AI systems based on the open foundation model may focus on additional 
safeguards appropriate to the context in which that system will be used, such as conducting 
an impact assessment that identifies the specific risks likely to arise from using that AI 
system in the company’s products and services. A deployer may also conduct an impact 
assessment, focusing on the risks presented by its context of use, and create feedback 
mechanisms for addressing issues that arise after deployment.  
 
                                 *    
 
 
*  
 
* 
 
Thank you for the opportunity to provide comments.  We look forward to serving as a          
resource as you continue to consider AI policy issues. 
 
Respectfully submitted, 
 
 
Shaundra Watson 
Senior Director, Policy 
BSA | The Software Alliance 
",BSA,3874
NTIA-2023-0009-0223,"8605 Santa Monica Blvd PMB 63639
West Hollywood | CA | 90069-4109 | United States
opensource.org
March 27, 2024
Mr. Bertram Lee
National Telecommunications and Information Administration (NTIA)
U.S. Department of Commerce
1401 Constitution Avenue NW
Washington, DC 20230
RE: [Docket Number 240216-0052] Dual Use Foundation Artificial Intelligence
Models with Widely Available Model Weights
Dear Mr. Lee:
The Open Source Initiative (“OSI”) appreciates the opportunity to provide our views on the
above referenced matter. As steward of the Open Source Definition,1 the OSI sets the
foundation for Open Source software, a global public good that plays a vital role in the
economy and is foundational for most technology we use today. As the leading voice on the
policies and principles of Open Source, the OSI helps build a world where the freedoms and
opportunities of Open Source software can be enjoyed by all and supports institutions and
individuals working together to create communities of practice in which the healthy Open
Source ecosystem thrives. One of the most important activities of the OSI, a California
public benefit 501(c)(3) organization founded in 1998, is to maintain the Open Source
Definition for the good of the community.2
The OSI is encouraged by the work of NTIA to bring stakeholders together to understand
the lessons from the Open Source software experience in having a recognized, unified
Open Source Definition that enables an ecosystem whose value is estimated to be worth
$8.8 trillion.3 As provided below in more detail, it is essential that federal policymakers
encourage Open Source AI models to the greatest extent possible, and work with
3 Hoffmann, Manuel and Nagle, Frank and Zhou, Yanuo, The Value of Open Source Software (January
1, 2024). Harvard Business School Strategy Unit Working Paper No. 24-038, Available at SSRN:
https://ssrn.com/abstract=4693148 or http://dx.doi.org/10.2139/ssrn.4693148
2 The OSI is a public charity with no beneficial owners and no corporate controllers, but with 80+
civil society Affiliates working on Open Source. We are incorporated in California, USA, but with a
global membership and mission of promoting and defending Open Source software on behalf of
the general public and of building bridges within its global community.
1 The Open Source Definition found at https://opensource.org/osd.
1
organizations like the OSI which is endeavoring to create a unified, recognized definition of
Open Source AI.
The Power of Open Source
Open Source delivers autonomy and personal agency to software users which enables a
development method for software that harnesses the power of distributed peer review and
transparency of process. The promise of Open Source is higher quality, better reliability,
greater flexibility, lower cost, and an end to proprietary lock-in.
Open Source software is widely used across the federal government and in every critical
infrastructure sector. “The Federal Government recognizes the immense benefits of Open
Source software, which enables software development at an incredible pace and fosters
significant innovation and collaboration.”4 For the last two decades, authoritative direction
and educational resources have been given to agencies on the use, management and
benefits of Open Source software.5
Moreover, Open Source software has direct economic and societal benefits. Open Source
software empowers companies to develop, test and deploy services, thereby substantiating
market demand and economic viability. Furthermore, it reduces costs for essential
applications like databases within these services (Hoffmann, Nagle and Zhou found that
firms would need to spend 3.5 times more on software than they currently do if OSS did
not exist).6 By leveraging Open Source, companies can accelerate their progress and focus
on innovation. Many of the essential services and technologies of our society and economy
are powered by Open Source software, including, e.g., the Internet.7
7 BIND is the most commonly used DNS server software on the Internet. See
https://www.isc.org/bind/. See Chinmayi Sharma, “Tragedy of the Digital Commons”, North Carolina
6 “The Value of Open Source Software”, Working Paper 24-038, Harvard Business Review, January
2024, found at:
https://www.hbs.edu/ris/Publication%20Files/24-038_51f8444f-502c-4139-8bf2-56eb4b65c58a.pdf.
5 The earliest and most comprehensive such guidance is the “DoD Open Source Software FAQ”,
Office of the DoD CIO, dated 2021-10-28, found at:
https://dodcio.defense.gov/open-source-software-faq/#frequently-asked-questions-regarding-open-
source-software-oss-and-the-department-of-defense-dod. See also, generally, U.S. Digital Services,
“Digital Services Playbook” (2014) which encourages a ‘default to open’ policy for federal IT, found at:
https://playbook.cio.gov/.
4 Office of the National Cyber Director, Executive Office of the President, Cybersecurity and
Infrastructure Security Agency, DHS, National Science Foundation, Defense Advanced Research
Projects Agency, and Office of Management and Budget, Executive Office of the President, “Request
for Information on Open-Source Software Security: Areas of Long-Term Focus and Prioritization”,
August 10,2023, found at:
https://www.federalregister.gov/documents/2023/08/10/2023-17239/request-for-information-on-op
en-source-software-security-areas-of-long-term-focus-and-prioritization.
2
The Open Source Definition has demonstrated that massive social benefits accrue when
the barriers to learning, using, sharing and improving software systems are removed. The
core criteria of the Open Source Definition – free redistribution; source code; derived
works; integrity of the author's source code; no discrimination against persons or groups;
no discrimination against fields of endeavor; distribution of license; license must not be
specific to a product; license must not restrict other software; license must be
technology-neutral – have given users agency, control and self-sovereignty of their
technical choices and a dynamic ecosystem based on permissionless innovation.
A recent study published by the European Commission estimated that companies located
in the European Union invested around €1 billion in Open Source Software in 2018, which
brought about a positive impact on the European economy of between €65 and €95
billion.8
This success and the potency of Open Source software has for the last three decades relied
upon the recognized unified definition of Open Source software and the list of Approved
Licenses that the Open Source Initiative maintains.9
OSI believes this “open” analog is highly relevant to Open Source AI as an emerging
technology domain with tremendous potential for public benefit.
Distinguishing the Open Source Definition
The OSI Approved License® trademark and program creates a nexus of trust around which
developers, users, corporations and governments can organize cooperation on Open
Source software. However, it is generally agreed that the Open Source Definition, drafted
26 years ago and maintained by the OSI, does not cover this new era of AI systems.
AI models are not just code; they are trained on massive datasets, deployed on intricate
computing infrastructure, and accessed through diverse interfaces and modalities. With
traditional software, there was a very clear separation between the code one wrote, the
compiler one used, the binary it produced, and what license they had. However, for AI
models, many components collectively influence the functioning of the system, including
the algorithms, code, hardware, and datasets used for training and testing. The very notion
9 See https://opensource.org/licenses for a full list.
8 PRESS RELEASE, “Commission publishes study on the impact of Open Source on the European
economy”, 06 September 2021, found at:
https://digital-strategy.ec.europa.eu/en/news/commission-publishes-study-impact-open-source-eur
opean-economy#:~:text=It%20is%20estimated%20that%20companies,65%20and%20%E2%82%AC95
%20billion.
Law Review, October 2022, found at:
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4245266. See also,
3
of modifying the source code (which is important in the [Open Source Definition]) becomes
fuzzy. For example, there is the key question of whether the training dataset, the model
weights, or other key elements should be considered independently or collectively as the
source code for the model/weights that have been trained.10
AI (specifically the Models that it manifests) include a variety of technologies, each is a vital
element to all Models.11
This challenge is not new. In its guidance on use of Open Source software, the US
Department of Defense distinguished open systems from open standards, that while
“different from Open Source software, they are complementary and can work well
together”12:
“Open standards make it easier for users to (later) adopt an Open Source software
program, because users of open standards aren’t locked into a particular
implementation. Instead, users who are careful to use open standards can easily
switch to a different implementation, including an OSS implementation. … Open
standards also make it easier for OSS developers to create their projects, because
the standard itself helps developers know what to do. Creating any interface is an
effort, and having a predefined standard helps reduce that effort greatly.
“OSS implementations can help create and keep open standards open. An OSS
implementation can be read and modified by anyone; such implementations can
quickly become a working reference model (a “sample implementation” or an
“executable specification”) that demonstrates what the specification means
(clarifying the specification) and demonstrating how to actually implement it.
Perhaps more importantly, by forcing there to be an implementation that others can
examine in detail, resulting in better specifications that are more likely to be used.
“OSS implementations can help rapidly increase adoption/use of the open standard.
OSS programs can typically be simply downloaded and tried out, making it much
easier for people to try it out and encouraging widespread use. This also pressures
proprietary implementations to limit their prices, and such lower prices for
proprietary software also encourages use of the standard.
12 Office of the Chief Information Officer, U.S. Department of Defense, “DoD Open Source Software
FAQ”, found at: https://dodcio.defense.gov/open-source-software-faq/.
11 See NIST, ‘Artificial Intelligence: Overview” at https://www.nist.gov/artificial-intelligence.
10 The OSI was a participant in the “Columbia Convening on “Openness and AI”, a recent gathering of
over 40 experts and stakeholders in the field of Artificial Intelligence (AI), sponsored by Mozilla and
the Columbia Institute of Global Politics. See
https://blog.mozilla.org/en/mozilla/ai/introducing-columbia-convening-openness-and-ai/#:~:text=Ou
r%20shared%20hope%20is%20that,shared%20understandings%20and%20next%20steps. A
Technical Readout (was well as a companion Policy Readout), to be published shortly, explores these
and related issues.
4
“With practically no exceptions, successful open standards for software have OSS
implementations.”13
Towards a Unified Vision of what is ‘Open Source AI’
With these essential differentiating elements in mind, last summer, the OSI kicked off a
multi-stakeholder process to define the characteristics of an AI system that can be
confidently and generally understood to be considered as “Open Source”.
This collaboration utilizes the latest definition of AI system adopted by the Organization for
Economic Cooperation and Development (OECD), and which has been the foundation for
NIST’s “AI Risk Management Framework”14 as well as the European Union’s AI Act:15
“An AI system is a machine-based system that, for explicit or implicit objectives,
infers, from the input it receives, how to generate outputs such as predictions,
content, recommendations, or decisions that can influence physical or virtual
environments. Different AI systems vary in their levels of autonomy and
adaptiveness after deployment.” 16
Since its announcement last summer, the OSI has had an open call for papers and held
open webinars in order to collect ideas from the community describing precise problem
areas in AI and collect suggestions for solutions.17 More than 6 community reviews – in
Europe, Africa, and various locations in the US – have taken place in 2023, coinciding with a
first draft of the Open Source AI Definition.18 This year, the OSI has coordinated working
groups to analyze various foundation models, released three more drafts of the Definition,
18 The Open Source AI Definition – draft v. 0.0.6
https://opensource.org/deepdive/drafts/the-open-source-ai-definition-draft-v-0-0-6.
17 A chronology of relevant activity can be followed at:
https://opensource.org/deepdive#see_all_we_achieved_in_2023.
16 ""OECD Recommendation of the Council on Artificial Intelligence
https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449.
15 “OECD updates definition of Artificial Intelligence ‘to inform EU’s AI Act’”, Euractive.com, November
9, 2023 (updated: November 14, 2023), found at:
https://www.euractiv.com/section/artificial-intelligence/news/oecd-updates-definition-of-artificial-int
elligence-to-inform-eus-ai-act/.
14 Found at: https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf. January 2023 by (Adapted from:
OECD Recommendation on AI:2019; ISO/IEC 22989:2022)”.
13 Ibid. See “Q: How does Open Source software work with open systems/open standards?”, found
at:
https://dodcio.defense.gov/open-source-software-faq/#q-how-does-open-source-software-work-wit
h-open-systemsopen-standards.
5
hosted bi-weekly public town halls to review and continues to get feedback from a wide
variety of stakeholders, including:
●
System Creators (makes AI system and/or component that will be studied,
used, modified, or shared through an Open Source license;
●
License Creators (writes or edits the Open Source license to be applied to the
AI system or component; includes compliance;
●
Regulators (writes or edits rules governing licenses and systems (e.g.
government policy-maker); Licensees (seeks to study, use modify, or share an
Open Source AI system (e.g. AI engineer, health researcher, education
researcher);
●
End Users (consumes a system output, but does not seek to study, use,
modify, or share the system (e.g., student using a chatbot to write a report,
artist creating an image);
●
Subjects (affected upstream or downstream by a system output without
interacting with it intentionally; includes advocates for this group (e.g. people
with loan denied, or content creators.
What is Open Source AI?
An Open Source AI is an AI system made available to the public under terms that grant the
freedoms to:
●
Use the system for any purpose and without having to ask for permission.
●
Study how the system works and inspect its components.
●
Modify the system for any purpose, including to change its output.
●
Share the system for others to use with or without modifications, for any
purpose.
Precondition to exercise these freedoms is to have access to the preferred form to make
modifications to the system.19
The OSI expects to wrap up and report the outcome of in-person and online meetings and
anticipates having the draft endorsed by at least 5 reps for each of the stakeholder groups
with a formal announcement of the results in late October.
To address the need to define rules for maintenance and review of this new Open Source
AI Definition, the OSI Board of Directors20 approved the creation of a new committee to
20 The OSI is managed by a member-elected board of directors that is the ultimate authority
responsible for the organization. More information on the OSI board and governance can be found
at: https://opensource.org/board.
19 The latest drafts can always be found on opensource.org/deepdive/drafts.
6
oversee the development of the Open Source AI Definition, approve version 1.0, and set
rules for the maintenance of Definition.
Some preliminary observations based on these efforts to date:
●
It is generally recognized, as indicated above, that the Open Source Definition
as created for software does not completely cover this new era of Open
Source AI. This is not a software-only issue and is not something that can be
solved by applying the same exact terms in the new territory of defining
Open Source AI. The Open Source AI definition will start from the core
motivation of the need to ensure users of AI systems retain their autonomy
and personal agency.
●
To the greatest degree practical, Open Source AI should not be limited in
scope, allowing users the right to adopt the technology for any purpose. One
of the key lessons and underlying successes of the Open Source Definition is
that field-of-use restrictions deprive creators of software to utilize tools in a
way to affect positive outcomes in society.
●
Reflecting on the past 20-to-30 years of learning about what has gone well
and what hasn’t in terms of the open community and the progress it has
made, it’s important to understand that openness does not automatically
mean ethical, right or just. Other factors such as privacy concerns and safety
when developing open systems come into play, and in each element of an AI
model – and when put together as a system — there is an ongoing tension
between something being open and being safe, or potentially harmful.
●
Open Source AI systems lower the barrier for stakeholders outside of large
tech companies to shape the future of AI, enabling more AI services to be
built by and for diverse communities with different needs that big companies
may not always address.
●
Similarly, Open Source AI systems make it easier for regulators and civil
society to assess AI systems for compliance with laws protecting civil rights,
privacy, consumers, and workers. They increase transparency, education,
testing and trust around the use of AI, enabling researchers and journalists
to audit and write about AI systems’ impacts on society.
●
Open source AI systems advance safety and security by accelerating the
understanding of their capabilities, risks and harms through independent
research, collaboration, and knowledge sharing.
●
Open source AI systems promote economic growth by lowering the
barrier for innovators, startups, and small businesses from more diverse
communities to build and use AI. Open models also help accelerate
scientific research because they can be less expensive, easier to
fine-tune, and supportive of reproducible research.
7
**************
The OSI looks forward to working with NTIA as it considers the comments to this RFI, and
stands ready to participate in any follow on discussions to this or the general topic of ‘Dual
Use Foundation Artificial Intelligence Models With Widely Available Model Weights’. As
shared above, it is essential that federal policymakers encourage Open Source AI models to
the greatest extent possible,21 and work with organizations like the OSI and others who are
endeavoring to create a unified, recognized definition of Open Source AI.
Respectfully submitted,
THE OPEN SOURCE INITIATIVE
For more information, contact:
Stefano Maffulli, Executive Director
stefano@opensource.org
Deb Bryant, US Policy Director
deb.bryant@opensource.org
21 “Openness and Transparency in AI Provide Significant Benefits for Society”, Joint Letter from civil
society organizations to Secretary Gina Raimundo (U.S. Department of Commerce), March 25,2024,
found at:
https://cdt.org/wp-content/uploads/2024/03/Civil-Society-Letter-on-Openness-for-NTIA-Process-Mar
ch-25-2024.pdf.
8
",Open Source Initiative,4309
NTIA-2023-0009-0180," 
 
 
 
 
600 14th St. NW, Suite 300 
Washington, D.C. 20005 
March 27, 2024 
Department of Commerce 
NaƟonal TelecommunicaƟons and InformaƟon AdministraƟon 
1401 ConsƟtuƟon Ave, NW 
Washington, DC 20230 
Subject: NTIA–2023–0009 - Dual Use FoundaƟon ArƟﬁcial Intelligence Models with Widely Available 
Model Weights 
Dear Administrator Davidson,  
On behalf of InternaƟonal Business Machines CorporaƟon (IBM), we welcome the opportunity to 
respond to the NaƟonal TelecommunicaƟons and InformaƟon AdministraƟon’s (NTIA) request for 
comment on dual use foundaƟon arƟﬁcial intelligence models with widely available model weights. The 
topic is a criƟcal one, and NTIA’s recommendaƟons will have an impact on the U.S.’s conƟnued 
leadership in this emerging technology. As the agency evaluates these challenging quesƟons, we 
recommend that NTIA:  
• 
Ensure that any regulaƟons designed to miƟgate AI safety risks posed by widely available model 
weights follow the science, rather than pre-empt it; 
• 
Work with NIST and other stakeholders to co-develop and test precise regulatory intervenƟons 
that miƟgate speciﬁc AI safety risks; and 
• 
Preserve and prioriƟze the criƟcal beneﬁts of open innovaƟon ecosystems for AI for increasing AI 
safety, advancing naƟonal compeƟƟveness, and promoƟng democraƟzaƟon and transparency of 
this technology.   
IBM commends NTIA for its thorough approach to invesƟgaƟng these challenging quesƟons around the 
potenƟal risks and beneﬁts of dual-use foundaƟon models with widely available model weights. IBM 
strongly believes that while not every AI model can or will be “open,” protecƟng an open innovaƟon 
ecosystem for AI in which model weights are widely available should be policymakers’ highest priority for 
advancing U.S. leadership in AI. AddiƟonally, IBM recommends NTIA acknowledge the lack of robust 
scienƟﬁc evidence that would be suﬃcient to jusƟfy any restricƟons on an open innovaƟon ecosystem 
for AI.  
IBM has been a global leader in invesƟgaƟng and advancing the potenƟal of AI since the term “arƟﬁcial 
intelligence” was coined at the seminal conference at Dartmouth University which IBM cosponsored in 
1956.1 We look forward to sharing our experƟse with policymakers to maximize the beneﬁts AI will oﬀer, 
so long as it is deployed in safe and trustworthy manner.  
Respecƞully,  
 
Darío Gil  
Senior Vice President and Director, IBM Research 
 
 
1 htps://home.dartmouth.edu/about/arƟﬁcial-intelligence-ai-coined-dartmouth 
 
 
 
 
 
 
IBM urges policymakers and all AI stakeholders to embrace a degree of epistemic humility when 
evaluaƟng the potenƟal risks of dual-use foundaƟon models with widely evaluable model weights. 
Policymakers are right to want to minimize the risk of harm that AI could pose. However, there is a 
glaring absence of evidence suggesƟng that many AI safety risks – parƟcularly existenƟal risks – are 
imminent or even likely, or that widely available model weights could increase them. For example, a 
recent study led by researchers at Stanford University and Princeton University found litle to no 
evidence supporƟng the argument that widely available model weights posed any marginal increase in 
societal risk across key categories, such as biosecurity and disinformaƟon.2  
This is not to say that safety risks of dual-use foundaƟon models, including those potenƟally exacerbated 
by the widely available model weights, will not or cannot emerge as this technology matures. But 
policymakers should recognize three criƟcal factors that jusƟfy a more prudent approach than taking 
early acƟon to restrict access to model weights: 
• 
The science of evaluaƟng AI models for potenƟal safety risks is extremely immature. The U.S., 
United Kingdom, and Japan have taken the laudable step to launch AI Safety InsƟtutes to 
speciﬁcally develop “science-based and empirically backed guidelines and standards for AI 
measurement and policy, laying the foundaƟon for AI safety across the world.”3 UnƟl this 
science is more mature and such guidelines actually exist, decisions about how to address 
potenƟal risks will be necessarily under-informed.  
• 
Open access to model weights will be a key driver of miƟgaƟons to AI safety risks. 
Undermining an open ecosystem around AI, in which a wide and diverse group of stakeholders 
has the freedom to scruƟnize, break down, and improve upon these technologies, would 
signiﬁcantly increase the U.S.’ risk exposure to AI by sacriﬁcing one of our most eﬀecƟve 
resources for combaƫng risk.  
• 
The broad economic and social beneﬁts of openness are overwhelming. Any restricƟons on 
access to model weights will have signiﬁcant downstream eﬀects that would erode open 
ecosystems in the US and globally. Such restricƟons would hurt U.S. naƟonal security and 
economic leadership, enable anƟcompeƟƟve market dynamics, and create signiﬁcant barriers to 
the democraƟzaƟon and transparency of this criƟcal technology.  
NTIA also asks for input on potenƟal regulatory models that could increase the beneﬁts and/or decrease 
the risk of dual use foundaƟon models with widely available model weights. One model worth exploring 
is partnering with the NIST AI Safety InsƟtute (AISI) to develop a “regulatory sandbox for regulaƟon,” 
rather than a sandbox for technology development. Similar iniƟaƟves have proven successful in other 
countries, such as the IMDA AI Verify FoundaƟon in Singapore, which houses AI Verify – “an AI 
governance tesƟng framework and soŌware toolkit that validates the performance of AI systems against 
a set of internaƟonally recognised principles through standardised tests.”4 Eﬀorts such as AI Verify and 
other regulatory sandboxes are vital tools for promoƟng iteraƟve developments to regulatory  
 
2 htps://crfm.stanford.edu/open-fms/ 
3 htps://www.nist.gov/arƟﬁcial-intelligence/arƟﬁcial-intelligence-safety-insƟtute 
4 htps://aiverifyfoundaƟon.sg/what-is-ai-verify/ 
 
 
 
 
 
 
frameworks, and oﬀer the signiﬁcant beneﬁt of being ﬂexible, adapƟve, and growth- and evoluƟon-
oriented.  
Novel regulatory approaches may be warranted at some point to address potenƟal risks of foundaƟon 
models, but the consequences of an imprecise approach to regulaƟon are too great to risk untested 
soluƟons. As NIST works to develop the scienƟﬁc understanding of idenƟfying and evaluaƟng AI safety 
risks, an NTIA-led pilot that works with NIST to test and iterate on regulatory responses to novel risks not 
already addressed by exisƟng regulatory frameworks would be extremely beneﬁcial.  
Another regulatory model worth exploring as part of this sandbox approach is creaƟng a toolbox of 
regulatory soluƟons that could be leveraged if and when certain milestones in the science of AI safety 
evaluaƟons are achieved. This “condiƟonal regulaƟon” approach should focus on domains of AI that 
many believe to be intrinsically higher risk, such as bioengineering.5 While it is intuiƟve that an AI model 
capable of generaƟng novel molecular structures for pharmaceuƟcal research might be able to also 
design dangerous biological agents, there are no widely-agreed upon, standardized processes for 
empirically detecƟng this capability in a model, nor evaluaƟng whether access to such a model could 
increase marginal societal risk of harm. As NIST’s AI Safety InsƟtute develops these processes and 
evaluaƟons, NTIA could propose relevant regulatory agencies work with the InsƟtute to co-develop and 
test regulatory intervenƟons precisely targeted to miƟgate such risks that would only go into eﬀect aŌer 
this scienƟﬁcally-grounded understanding is established.  
Just as IBM is working with NIST through the AI Safety InsƟtute ConsorƟum to advance the science of 
evaluaƟng and miƟgaƟng AI safety risks, IBM is eager to work with NTIA and other policymakers and 
stakeholders to apply that science to risk- and evidence-based regulatory intervenƟons. 
What, if any, are the risks associated with widely available model weights? 
The main risk commonly associated with widely available model weights is that it can enable bad actors 
to modify or remove built in guardrails intended to prevent misuse. This concern does not acknowledge 
the fact that future innovaƟons – themselves enabled by access to widely available model weights – 
could reduce this risk.  
 
It is also important to recognize that these risks do not exist in a vacuum. While a bad actor could 
theoreƟcally modify an open source model to perform malicious acƟvity, such as generaƟng malware 
that can steal conﬁdenƟal informaƟon, such acƟviƟes are sƟll criminal oﬀenses, regardless of whether 
the malware was AI-generated or not. Of course, malicious hacking sƟll occurs even though it is illegal. 
Nonetheless, legal and regulatory frameworks that impose penalƟes for harmful applicaƟons of a 
technology can sƟll address both the risks and the harms associated with such acƟviƟes, regardless of 
the tools bad actors choose to employ in such criminal enterprises.  
 
 
5 htps://www.gov.uk/government/publicaƟons/ai-safety-summit-introducƟon/ai-safety-summit-introducƟon-
html#scope-of-the-ai-safety-summit-what-is-fronƟer-ai 
 
 
 
 
 
 
CriƟcally, the NIST AISI is tasked with idenƟfying these risks. While responses to this Request for 
Comment may help shape and advance this work, NTIA should defer to NIST’s determinaƟons when 
completed to ensure that any potenƟal downstream regulaƟon is truly evidence-based. 
How do these risks compare to those associated with closed models? 
It is naïve to assume that bad actors – state actors or otherwise – will be unable to leverage AI to cause 
harm just because they do not have access to model weights. RestricƟng access to model weights by 
requiring all interacƟon with an AI model to go through an API, for example, has not proven eﬀecƟve at 
reducing risk. A harassment campaign devoted to generaƟng and disseminaƟng deepfake pornography 
of Taylor SwiŌ and other celebriƟes leveraged consumer-facing image generaƟon tools that processed 
user requests through an API to generate this imagery.6 While closed models may someƟmes introduce 
fricƟon into the process of using AI for undesirable purposes, it has not proven to be nearly as durable or 
robust an approach to prevenƟng AI safety risks as many advocates of restricƟng access to model 
weights claim it to be.  
AddiƟonally, as Arvind Narayanan and Sayash Kapoor of Princeton University note, “AI safety” is not a 
property of a model, but a contextually dependent evaluaƟon based on mulƟple factors.7 How a model 
is released, including whether model weights are made widely available, can be a relevant factor in 
evaluaƟng safety, but there is simply not suﬃcient evidence to indicate that open release strategies 
marginally increase risks.8 Thus, it is diﬃcult to draw any meaningful contrast between simply “open” 
and “closed” models in terms of safety risk.  
This highlights the signiﬁcant need for addiƟonal clarity and nuance in how stakeholders talk about “AI 
safety” and “AI risk.” Just as “safety” is not a model property, many of the risks of AI are not unique to AI, 
yet are discussed as AI-speciﬁc challenges that can be addressed at the technology layer, rather than the 
applicaƟon layer, with AI-speciﬁc regulatory soluƟons.9 We know this is not the case for many of the 
most prominent risks of AI, such as malicious use of deepfakes – while it is true generaƟve AI can 
exacerbate these harms, many soluƟons are straighƞorward and are technology neutral, centered on 
holding bad actors liable for the harm they cause.10 The NIST AI Safety InsƟtute is tasked with addressing 
many of these quesƟons surrounding taxonomy, and we believe that other agencies and stakeholders 
can and should play a meaningful role in that process. 
What, if any, risks could result from diﬀerences in access to widely available models across diﬀerent 
jurisdicƟons? 
RestricƟons on access to model weights, which if imposed will vary across jurisdicƟons, are unlikely to 
meaningfully address AI safety risks and could pose signiﬁcant unintended consequences.  
 
6 htps://www.nyƟmes.com/2024/02/05/business/media/taylor-swiŌ-ai-fake-images.html 
7 htps://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property 
8 htps://crfm.stanford.edu/open-fms/ 
9 htps://newsroom.ibm.com/Whitepaper-A-Policymakers-Guide-to-FoundaƟon-Models 
10 htps://newsroom.ibm.com/Blog-Heres-What-Policymakers-Can-Do-About-Deepfakes 
 
 
 
 
 
 
First, model weights are easily portable digital ﬁles that can be disseminated undetected with minimal 
cost on consumer hardware and communicaƟon plaƞorms. Thus, a bad actor could easily disseminate 
model weights across jurisdicƟons.  
Second, restricƟons on access to model weights in a parƟcular jurisdicƟon will only deter good actors 
from accessing them in that jurisdicƟon. As described in response to other quesƟons, these “good 
actors” in open ecosystems are criƟcal drivers of innovaƟon, using AI to deliver greater economic and 
social beneﬁts and advancing AI safety research. Thus, one of the most likely outcomes of jurisdicƟonal 
variaƟons in access to model weights is that the most restricƟve jurisdicƟons will also be the most 
limited in their ability to beneﬁt from AI.  
What beneﬁts do open model weights oﬀer for compeƟƟon and innovaƟon, both in the AI 
marketplace and in other areas of the economy? In what ways can open dual-use foundaƟon models 
enable or enhance scienƟﬁc research, as well as educaƟon/training in computer science and related 
ﬁelds? 
Open innovaƟon ecosystems in which model weights are widely available can oﬀer signiﬁcant economic 
and social beneﬁts for the AI ecosystem and beyond.11  
The most obvious beneﬁt of an open ecosystem is that it lowers the barrier to entry for compeƟƟon and 
innovaƟon. By making many of the technical resources necessary to develop and deploy AI more readily 
available, including model weights, open ecosystems enable small and large ﬁrms alike, as well as 
research insƟtuƟons, to develop new and compeƟƟve products and services without steep, and 
potenƟally prohibiƟve, upfront costs. And there is tremendous demand from businesses for these kinds 
of resources. Not only does IBM rely on collaboraƟve technologies like open source, we also make them 
available through IBM’s watsonx plaƞorm, as our clients value access to a variety of powerful tools and 
models from the open innovaƟon community like Meta’s Llama2-chat 70 billion parameter model 
alongside our proprietary oﬀerings for specialized enterprise use cases.12 
Openness and innovaƟon go hand-in-hand, and open ecosystems can drive AI advancements in three key 
ways.  
First, more compeƟƟon means everyone must raise the bar. Companies should compete based on how 
well they can tailor and deploy AI in valuable ways, rather than on how eﬀecƟvely they can hoard the 
resources and enact barriers of entry to develop AI.  
Second, more access to AI means more stakeholders can idenƟfy opportuniƟes to improve AI 
technologies and more easily pursue novel and valuable applicaƟons for AI. Combined, an open AI 
ecosystem is dramaƟcally more innovaƟve, inclusive, and compeƟƟve than a closed one. 
 
 
11 htps://www.ibm.com/policy/why-we-must-protect-an-open-innovaƟon-ecosystem-for-ai/ 
12 htps://developer.ibm.com/arƟcles/awb-the-open-source-ecosystem-of-watsonx/; 
htps://newsroom.ibm.com/2023-08-09-IBM-Plans-to-Make-Llama-2-Available-within-its-Watsonx-AI-and-Data-
Plaƞorm  
 
 
 
 
 
 
Third, just as openness drives compeƟƟon, it also drives democraƟzaƟon. Open ecosystems mean more 
opportuniƟes for anyone to explore, test, modify, study, and deploy AI, which can dramaƟcally lower the 
bar to deploying AI for socially beneﬁcial applicaƟons. This is why IBM partnered with NASA to develop a 
geospaƟal foundaƟon model and make it available under an open source license.13 The model can 
analyze NASA’s geospaƟal data up to four Ɵmes faster than state-of-the-art deep-learning models, with 
half as much labeled data, making it a powerful tool to accelerate climate-related discoveries. Since it is 
openly available, anyone – including non-governmental organizaƟons, governments, and other 
stakeholders – can leverage this model freely to drive eﬀorts to build resilience to climate change. 
It is much easier to learn about a subject when you can access the materials for free. An open innovaƟon 
ecosystem unleashes a signiﬁcantly broader pool of AI talent, as students, academics, and exisƟng 
members of the workforce can more easily access the resources necessary to acquire AI skills. This is 
part of the reason IBM contributes hundreds of open models and datasets to Hugging Face, the leading 
open source collaboraƟon plaƞorm for the machine learning community building the future of AI.14 IBM 
has also commited to training 2 million people in AI by 2026, with a focus on underrepresented 
communiƟes, by partnering with universiƟes around the world to provide AI training resources and by 
making AI coursework available for free on our SkillsBuild plaƞorm.15 
How can making model weights widely available improve the safety, security, and trustworthiness of 
AI and the robustness of public preparedness against potenƟal AI risks? 
As with open source soŌware, open models can enable a higher level of scruƟny from the community, 
greatly increasing the likelihood that vulnerabiliƟes are idenƟﬁed and patched. This means that open AI 
models can be as trusted, secure, and ﬁt for use in diﬀerent contexts as proprietary models. 
Openness can also drive innovaƟons that can improve safety. For example, Stanford researchers 
developed a novel technique called “meta-learned adversarial censoring (MLAC)” that can greatly 
impede eﬀorts to manipulate a model to perform harmful acƟvity, even with access to model weights.16 
These researchers themselves relied on open model weights for a language model to test and evaluate 
their technique – a feat that would not have been possible were model weights restricted.  
In some contexts, AI safety can also depend on the ability for diverse stakeholders to scruƟnize and 
evaluate models to idenƟfy any vulnerabiliƟes, idenƟfy undesirable behaviors, and ensure they are 
funcƟoning properly. However, without “deep access,” which includes access to model weights, these 
evaluaƟons will be severely limited in their eﬀecƟveness.17  
At a high level, wide access to model weights means more good actors, including those in governments, 
industry, civil society, academia, and the general public, can leverage AI to miƟgate the potenƟal impacts 
of harmful applicaƟons of AI. RestricƟons on access to open model weights will mean suﬃciently-
 
13 htps://research.ibm.com/blog/nasa-hugging-face-ibm 
14 htps://huggingface.co/ibm 
15 htps://newsroom.ibm.com/2023-09-18-IBM-Commits-to-Train-2-Million-in-ArƟﬁcial-Intelligence-in-Three-Years,-
with-a-Focus-on-Underrepresented-CommuniƟes 
16  htps://arxiv.org/pdf/2211.14946.pdf 
17 htps://bpb-us-e1.wpmucdn.com/sites.mit.edu/dist/6/336/ﬁles/2024/03/Safe-Harbor-0e192065dccf6d83.pdf 
 
 
 
 
 
moƟvated bad actors will sƟll be able to leverage AI to cause harm, whereas good actors, who respect 
the rule of law, will not be able to suﬃciently build resilience to these harms.  
Are there eﬀecƟve ways to create safeguards around foundaƟon models, either to ensure that model 
weights do not become available, or to protect system integrity or human well-being (including 
privacy) and reduce security risks in those cases where weights are widely available? 
There are emerging techniques to create safeguards for foundaƟon models when weights are widely 
available. In addiƟon to the Stanford University researchers’ MLAC technique described earlier, Meta has 
developed a watermarking technique called Stable Signature that embeds a watermark in generated 
images.18 Watermarking is currently an unreliable method for detecƟng syntheƟc content as watermarks 
can typically be easily edited out by end users. Stable Signature applies a watermark, which could 
indicate the model version and user, at the model level in a way that is not aﬀected by addiƟonal ﬁne-
tuning or image manipulaƟon.19  
What are the prospects for developing eﬀecƟve safeguards in the future? 
FoundaƟon models are sƟll an emerging technology and there will be enormous amounts of innovaƟon 
over the coming decade. While it is impossible to predict exactly what kinds of technologies will be 
developed, novel safeguards, new types of algorithm architectures, methods for governance, 
watermarking and detecƟon techniques, and more will emerge that will have signiﬁcant posiƟve impact 
for safety.  
 
Fostering the development of these safeguards should be a top priority for the NIST AISI. And criƟcally, 
the AISI and the enƟre AI ecosystem will be signiﬁcantly more successful in developing these safeguards 
if model weights are widely available.   
In which ways is open-source soŌware policy analogous (or not) to the availability of model weights? 
Are there lessons we can learn from the history and ecosystem of open-source soŌware, open data, 
and other “open” iniƟaƟves for open foundaƟon models, parƟcularly the availability of model 
weights? 
There are many lessons to be learned from past debates about the safety and security of open-source 
soŌware (OSS) that can add clarity to debates around the perceived safety risks of open foundaƟon 
models. Of these, the most important is that openness has always been a force for improving safety and 
security. For decades, increased transparency and access to source code has enabled a broad and diverse 
stakeholder community to idenƟfy and ﬁx vulnerabiliƟes, increase performance and resilience, and raise 
the bar for security overall.  
For example, in 2000, Red Hat and the U.S. NaƟonal Security Agency (NSA) developed Security-Enhanced 
Linux (SELinux), a Linux kernel module that can modify Linux distribuƟons to provide robust security 
features.20 SELinux is both made available as OSS and is built on top of exisƟng OSS – representaƟve of 
the feedback loop for improving security that openness can enable.  
 
18 htps://ai.meta.com/blog/stable-signature-watermarking-generaƟve-ai/ 
19 htps://ai.meta.com/blog/stable-signature-watermarking-generaƟve-ai/ 
20 htps://www.redhat.com/en/topics/linux/what-is-selinux 
 
 
 
 
 
Present-day arguments that access to model weights increase safety risks share parallels with debunked 
past arguments that providing broad access to source code could make it easier for bad actors to exploit 
it. While policymakers are right to be asking the quesƟon of whether potenƟally dangerous misuse of AI 
will be enabled by access to model weights, they should be careful to acknowledge that similar concerns 
about OSS have rouƟnely been proven wrong, and that openness was a key driver in miƟgaƟng safety 
and security concerns.  
What security, legal, or other measures can reasonably be employed to reliably prevent wide 
availability of access to a foundaƟon model’s weights, or limit their end use? 
There are unlikely any legal measures that can reliably or sustainably limit access to model weights for 
highly capable models on a global scale. Two of the most commonly proposed soluƟons – government-
issued licenses to develop or operate a highly-capable model and export controls on model weights -- 
would both come with extreme consequences for U.S. compeƟƟveness. And as described elsewhere, not 
only would such eﬀorts do litle to advance AI safety in the short term, but they risk undermining it in 
the longer term by limiƟng the ability for good actors to contribute to AI safety soluƟons and to take 
advantage of AI for beneﬁcial applicaƟons.  
What role, if any, should the U.S. government take in seƫng metrics for risk, creaƟng standards for 
best pracƟces, and/or supporƟng or restricƟng the availability of foundaƟon model weights? Should 
other government or non-government bodies, currently exisƟng or not, support the government in 
this role? Should this vary by sector? 
Standards-seƫng and scienƟﬁc agencies, parƟcularly NIST through its AI Safety InsƟtute, have a criƟcally 
important role to play in developing methods to evaluate AI models, measure risk, and develop 
safeguards. This work should be conducted in close collaboraƟon with other agencies and stakeholders 
outside of government. The AI Safety InsƟtute ConsorƟum is a worthwhile model which, while sƟll new, 
shows promise to ensure that these standards are informed by robust scienƟﬁc evidence and community 
consensus.  
U.S. agencies should work with their counterparts around the world to share best pracƟces, avoid 
pursuing redundant lines of research, and facilitate the development and mutual adopƟon of consensus 
standards around AI risk. Governments should not create a new internaƟonal body, as some have 
proposed, to oversee and steer this work globally. Proposed models such as the InternaƟonal Atomic 
Energy Agency (IAEA), which promotes responsible use of nuclear technology globally, are simply not 
useful to adopt for AI, and an internaƟonal body would quickly become heavily poliƟcized.  
NoƟng that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the threshold, is the 
amount of computaƟonal resources required to build a model, such as the cutoﬀ of 10^26 integer or 
ﬂoaƟng-point operaƟons used in the ExecuƟve Order, a useful metric for thresholds to miƟgate risk in 
the long-term, parƟcularly for risks associated with wide availability of model weights? 
Compute requirements are not a useful metric for idenƟfying safety risks. First, as described above, AI 
safety is a context-dependent evaluaƟon of mulƟple factors, not a disƟnct property of a model. Second, 
compute requirements do not necessarily indicate dangerous capabiliƟes. As the ﬁeld matures, greater 
levels of performance are being achieved with smaller and smaller models. Any compute threshold  
 
 
 
 
 
 
intended to diﬀerenƟate between low and high capability models will quickly become obsolete for this 
reason.  
Conclusion 
NTIA should not consider any restricƟons on how model weights can be distributed or accessed unƟl it 
has the necessary scienƟﬁc evidence to understand if such restricƟons would be necessary, eﬀecƟve, 
and worth the tradeoﬀs that would come with them. NTIA should acknowledge that it does not yet have 
this evidence and should work with the NIST AISI and other stakeholders to help develop it. At present, 
any restricƟons on the open innovaƟon ecosystem for AI would signiﬁcantly hinder U.S. leadership in AI 
without meaningfully addressing AI safety risks.  
",IBM,7042
NTIA-2023-0009-0142," 
 
 
 
 
 
 
Comment on “NTIA AI Open Model Weights RFC” [Docket No. 
NTIA–2023–0009] 
Date: 
 
March 25, 2024 
 
 
 
By Electronic Delivery To: 
 
Information Technology Laboratory 
ATTN: AI E.O. RFI Comments, 
National Institute of Standards and 
Technology 
100 Bureau Drive, Mail Stop 8900 
Gaithersburg, MD 20899-8900 
 
 
 
 
303 2nd Street, Ste N460 
San Francisco, CA 94107 
info@unlearn.ai 
415 455 3005 
 
To Whom It May Concern: 
Unlearn.AI, Inc. (Unlearn) is submitting these comments in response to the 
February 25, 2024 National Telecommunications and Information Administration 
(NTIA) AI Open Model Weights Request for Comment. Unlearn is innovating 
advanced machine learning (ML) methods to leverage generative artificial 
intelligence (AI) in forecasting patient health outcomes, starting with the domain 
of randomized clinical trials (RCTs). We produce a distribution of longitudinal 
forecasts for individual trial participants (e.g., their ”digital twins”), enabling 
smaller and more efficient clinical trials to bring effective medicines to patients 
sooner. As part of our shared goal to improve patient health outcomes through 
more efficient clinical trials, Unlearn has developed an EMA-qualified method for 
prognostic covariate adjustment (PROCOVA) that leverages AI to enable increases 
in power in RCTs with continuous outcomes. 
We applaud NTIA for taking action to understand the benefits and risks of open 
foundation models and agree that these models have the potential to transform 
research and advance scientific knowledge. While there are important considerations 
for mitigating the risks of rights- and safety-impacting AI systems, such as in 
healthcare, the evaluation of AI models should not be approached in a one-size-fits-
all manner. Rather, AI technologies should be regulated according to risk level, which 
can only be determined by the specific context of use. As existing regulatory bodies 
would have the best understanding of associated risks in specific AI use cases, 
methods involving AI should be evaluated in a sector-dependent manner. For 
example, AI models used in drug development should be regulated by the U.S. Food 
and Drug Administration (FDA), where a risk-based framework should be developed 
to include requirements that are commensurate with the potential risk associated with 
the specific context of use.  
The following addresses the questions put forth by the NTIA on dual use foundation 
AI models with widely available model weights: 
1a. Is there evidence or historical examples suggesting that weights of 
models similar to currently-closed AI systems will, or will not, likely 
become widely available? If so, what are they? 
While it is difficult to predict whether the weights of all currently-closed models will 
become widely available, there are some historical examples to consider. Several AI 
systems that were initially closed or proprietary have eventually been made available 
to the public, either in full or in part. These examples include Google’s BERT 
(Bidirectional Encoder Representations from Transformers), LLaMA by Meta, as well 
as GPT-2 by OpenAI. The growing movement towards open science and open source 
has encouraged developers to enhance collaboration and shared knowledge, and 
advocate for the transparency and accessibility of scientific knowledge. However, 
many models have remained closed and proprietary, such as OpenAI’s more 
advanced GPT models, Anthropic AI’s Claude, and Google’s Gemini.  
 
 
unlearn.ai
 
 
 
 
There are various reasons that cause developers to keep their AI systems closed, such as commercial interests to maintain a 
competitive advantage, privacy and security, and to control the model's usage. 
The history of AI model performance has driven the innovation of new AI models and the evolution of existing ones. In fact, 
comparisons between models of the same size category (see: LMSYS Chatbot Arena) have shown favorable ranking of open 
source models that are consistently improving over time. While the scale of capital has created a gap in performance 
between truly open-source and closed-source foundation models, improvements in algorithmic and computational efficiency, 
advances in hardware, and better quality of training data may make it possible for open-source models to reach the 
performance level of current closed-source models rapidly. Once open-source models are able to match the capabilities of 
closed models, the developers of closed models may be encouraged to open the weights of these foundational models. 
1b. Is it possible to generally estimate the timeframe between the deployment of a closed model and the 
deployment of an open foundation model of similar performance on relevant tasks? How do you expect that 
timeframe to change? Based on what variables? How do you expect those variables to change in the coming 
months and years? 
Estimating the timeframe between the deployment of a closed model and the deployment of an open foundation model of 
similar performance on relevant tasks is possible by looking at the gaps in human-evaluated performance between open 
foundation models and closed counterparts. While this is highly dependent on the specific AI model and its application 
domain, we can look towards a few examples. At the moment, it takes about 6 months to 1 year for similarly performing 
open models to be successfully deployed after the deployment of OpenAI’s closed models. The time gap between 
proprietary image recognition models and high-quality open-source alternatives has narrowed relatively quickly due to 
robust community engagement and significant public interest. In contrast, more niche or complex applications, such as 
those requiring extensive domain-specific knowledge or data, might see longer timeframes before competitive open models 
emerge. 
With continued research into making the training of foundation models more efficient, both in terms of training data 
requirements and overall compute cost and time, this timeframe will gradually decrease. Because of the significant utility of 
these models, and their great cost, there is huge market incentive to further optimize the efficiency of their production. The 
discovery of more data-efficient architectural approaches to building foundation models, the further development of task-
specific hardware, advances in the field of resource efficient training (e.g. quantized or single bit model parameters), and 
focused improvements to firmware and software engineering for training libraries will contribute to making the production of 
foundation models with today’s capabilities comparatively cheap over the next few years. 
1c. Should “wide availability” of model weights be defined by level of distribution? If so, at what level of 
distribution (e.g., 10,000 entities; 1 million entities; open publication; etc.) should model weights be 
presumed to be “widely available”? If not, how should NTIA define “wide availability?” 
 
There is no effective way to assess how “widely available” a given set of model weights are. The existence of a single public 
link to a set of model weights would be sufficient to ensure their continued distribution in public or private. In the case of 
Mistral AI, a public torrent link was sufficient to distribute their foundation models, even without their source code for 
inference. Open-source contributors were able to produce that code independently within hours.  
 
Furthermore, once a foundation model is widely available, a vast number of progenitor models can be developed from them 
via fine tuning, instruction tuning, reinforcement learning from human feedback (RLHF), or direct preference optimization 
(DPO) to adapt the models and improve their performance on any number of specific use cases. Each of these subsequent 
models may have their own history of distribution and further refinement in the open-source community. These patterns of 
distribution have been observed for both Mistral AI and LLaMA model releases. The tools to modify and finetune open 
foundation models have become very refined over the past six months and will continue to have their effectiveness and 
accessibility improved. 
 
1d. Do certain forms of access to an open foundation model (web applications, Application Programming 
Interfaces (API), local hosting, edge deployment) provide more or less benefit or more or less risk than 
others? Are these risks dependent on other details of the system or application enabling access? 
 
Providing models through an API offers the possibility of rejecting certain prompts or user requests based on the risk 
tolerance of the hosting organization. However, it risks centralization and singular control of downstream applications. A 
recent example of this is the backlash that Google faced after its release of their Gemma and Gemini models, which were 
widely criticized for reinforcing the views of their designers rather than aiding their users. Furthermore, privacy risks 
associated with the content of user inputs are significant for centralized models. Users inputting personally identifiable 
information or classified materials into the model is a risk for system maintainers.  
 
 
 
 
While the burden is on the user to make sure that they are using a system appropriately, the operator must comply with 
government regulations with respect to such information and any subsequent audits.  
 
Local hosting, on the other hand, offers the maximum amount of freedom of use – even if a model has been fine-tuned to a 
designer’s policy, holding the model locally allows users to alter the model to better suit their needs or use case. 
Furthermore, all inputs remain local, which obviates the privacy and security risks of information sharing with a second 
party. Of course, one may be concerned about how such models are being used, as there is no central authority on the 
nature of the local hosted models. However, the U.S. government should recognize that policing the ownership of local 
models is a losing battle. They already exist in the open and will continue to exist in the open, whether shared publicly or 
privately.  
 
The focus should not be on policing or preventing the creation and use of foundation models, but rather on ensuring that we 
have the tools to react quickly to their misuse. This includes having criminal statutes up to date with the modern 
technological landscape, properly funding and equipping state and national cybercrime divisions, and prosecuting individuals 
who misuse technology to harm or defraud others.  
 
1d.i. Are there promising prospective forms or modes of access that could strike a more favorable benefit-risk 
balance? If so, what are they? 
 
Because capable models are already widely accessible and currently in use, the government should focus more on how they 
can apply foundation models to practical problems and understand how to detect and respond to their potential misuse.  
 
2a. What, if any, are the risks associated with widely available model weights? How do these risks change, if 
at all, when the training data or source code associated with fine tuning, pretraining, or deploying a model is 
simultaneously widely available? 
 
The most significant risk associated with widely available model weights is that one cannot police the use of the model. 
While this could potentially amplify the capabilities of bad actors to operate on a larger scale, the context of use remains 
critical to determining the risk level associated with a model.  
 
A lack of source code for model training does not significantly slow down the use of a model, as the open-source ML 
community has shown that re-engineering equivalent model training from limited information is possible on the time-scale of 
weeks. If there are risks in the mere production of a certain foundation model, restricting access to the source code will not 
materially prevent those risks. 
 
A lack of training data, however, does impede the progress of external development of model capabilities. If the intended, 
risk-bearing task has a limited or somehow unique dataset which is not widely available (e.g., genomic data, clinical records, 
medical imaging), then the reproduction of a dual-use foundation model may not be possible without large efforts being 
applied to acquire such datasets, whether in private or publicly. However, if one considers dual-use models that are able to 
develop capabilities based on widely available text, image, or video data from the internet, then access to such datasets is 
not a significant impediment to the reproduction of the dual-use foundation model, or the risks it represents. 
 
2b. Could open foundation models reduce equity in rights and safety-impacting AI systems (e.g., healthcare, 
education, criminal justice, housing, online platforms, etc.)? 
 
Open foundation models would only reduce equity in rights and safety-impacting AI systems if the models were misused, 
either intentionally or unintentionally, by their operators. In the example cases listed, the risks associated with AI or dual-
use foundation models in particular are a consequence of a lack of the operators' understanding of how to apply such 
systems equitably. Or, in the case of an intentional misuse, the operator wittingly uses an AI model to “launder” 
responsibility from the operator of the model in order to justify a biased or improper decision. 
 
These cases highlight why it is important for regulators to focus on the use of a model rather than the model in general. 
Ultimately, it is impossible to enumerate all the ways an operator may improperly use a technology, but the responsibility 
lies with the operator to take reasonable actions to ensure that the service or good they are providing does not cause public 
harm. 
 
Whether the model is open or closed, the risks remain the same. A closed dual-use foundation model accessed via API is as 
much capable of misuse in specific situations as an open one which is operated by a third-party. A closed approach simply 
puts an unreasonable and unmaintainable burden on the central operator to foresee such cases and misplaces responsibility 
for improper use on the tool maker rather than on the operator who is misusing the tool.  
 
 
 
 
2c. What, if any, risks related to privacy could result from the wide availability of model weights? 
 
Making model weights widely available comes with the risk of leaked information from the dataset used to train the model. 
This is known as a membership inference attack, which is a common concern cited in the literature on differential privacy. 
However, in order for a member of the public to have privileged information leaked from a membership inference attack on 
the model weights, some third party would have already had access to their privileged information in order to put it into the 
training set. Since the individual’s personal information would have already been accessible on the internet prior to the 
model’s production, the responsibility for an information leak would be on other third parties rather than the foundation 
model developer. 
 
These risks are mostly proprietary – the contents of training datasets are often considered intellectual property in the form 
of trade secrets. Leaking information about data mix is more harmful to the producer of the foundation model in a 
commercial sense than it is for the public or government at large. A possible scenario arises in which a model owner and 
operator continuously retrains their foundation model using user input they have collected as an operator. If this information 
was provided under an expectation of privacy from the user, and if the operator later releases a model trained using that 
information, then there could be a concern that the model leaks the information at a later point. Model operators must be 
forthright with their users about how their inputs may be used in the future for model training and should be informed if 
model weights are released to the public using their data.  
 
7a. What security, legal, or other measures can reasonably be employed to reliably prevent wide availability 
of access to a foundation model's weights, or limit their end use? 
 
There are no security, legal, or other measures that can reasonably be employed to reliably prevent wide availability of 
access to a foundation model's weights or limit their end use once they become publicly available. Please refer to the 
response in 1c for clarification. 
 
7b. How might the wide availability of open foundation model weights facilitate, or else frustrate, 
government action in AI regulation? 
 
The wide availability of open foundation model weights would frustrate government action in AI regulation for several 
reasons. Once foundation model weights are openly available, they can be downloaded and used by anyone with the 
necessary computational resources, regardless of their intent. This makes it difficult for governments to control who is using 
these models and for what purposes, potentially leading to misuse. Additionally, foundational models can be developed and 
hosted in one country but used globally, making it challenging for individual governments to enforce regulations since 
different countries have varying levels of regulatory frameworks and enforcement capabilities. This can lead to a patchwork 
of regulation that is hard to navigate and enforce effectively. Finally, the field of AI is evolving at an unprecedented rate, 
and government regulatory processes (which often involve extensive deliberations, public consultations, and legal 
frameworks) can struggle to keep pace with the speed of innovation, making it difficult to create timely and relevant 
regulations. 
 
7d. What role, if any, should the U.S. government take in setting metrics for risk, creating standards for best 
practices, and/or supporting or restricting the availability of foundation model weights? 
 
AI systems should be regulated in a sector-specific manner with a risk-based framework dependent on the model’s context 
of use. Rather than the U.S. government implementing overarching standards for best practices of all AI systems, a sector-
specific approach would address the unique challenges and risks each industry presents, ensuring that safety and innovation 
are balanced effectively. 
 
8b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the threshold, is the 
amount of computational resources required to build a model, such as the cutoff of 1026 integer or floating-
point operations used in the Executive order, a useful metric for thresholds to mitigate risk in the long-term, 
particularly for risks associated with wide availability of model weights? 
 
Metrics related to raw computing power, like floating-point operations (FLOPs), present challenges for accurate and 
enforceable regulation. They aim to gauge risks associated with AI capabilities, with FLOPs serving as a measure of compute 
used in model development. However, the efficiency of compute use varies—inefficient use may not accurately reflect actual 
risk, while highly efficient methods can signify strong capabilities with less compute. Additionally, emerging hardware for 
model training might lack a direct equivalent to FLOPs for comparing training scales. 
 
 
 
 
 
 
 
 
Auditing and validating FLOP reports pose significant challenges for regulators, given that estimation methods vary by 
organization. This variance could lead to underreporting or inaccurate claims about computational resources used, 
complicating regulatory efforts and potentially undermining the effectiveness of such regulations. 
 
Moreover, imposing a computational threshold might inadvertently encourage the development of more efficient AI 
technologies to avoid regulatory scrutiny, potentially benefiting environmentally but also enabling less resourced entities to 
develop advanced AI capabilities. This could counteract government efforts to manage AI risks by diminishing any 
competitive advantage established through regulatory measures. 
Thank you for taking the time to review our comment.  
Best regards, 
Jess Ross 
Senior Government Affairs Lead, Unlearn 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
unlearn.ai
",Unlearn.AI,3962
NTIA-2023-0009-0156,"Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 1 
 
Convening stakeholders across industries to craft principles and concrete codes of practice for the 
development and use of artificial intelligence. 
 
March 26, 2024 
RE: NTIA, Request for Comments (RFC) on Dual Use Foundation Artificial Intelligence 
Models with Widely Available Model Weights 
These comments are submitted on behalf of the Alliance for Trust in AI (the Alliance), a 
nonprofit association of companies using artificial intelligence (AI) representing diverse 
sectors. Members of the Alliance seek to ensure that AI can be a trusted tool by promoting 
effective policy and clear codes of practice for AI. We appreciate the opportunity to respond to 
the National Telecommunications and Information Administration’s (NTIA) Request for 
Comments on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights. 
The Alliance is appreciative of the work that NTIA, and the rest of the U.S. government, is 
engaging in to make access to and development of AI systems safe, equitable, and innovative. 
In these comments, we will discuss different models for delivery of AI components, systems, 
tools, and products, including aspects of openness; the benefits and risks associated with this 
kind of access; and the role that the U.S. government should play. We hope that NTIA’s report 
will recognize the transformative importance of open source and openness, the differences 
between different kinds of openness, and find more effective ways to evaluate risk for dual-use 
foundation models with widely available model weights. 
Part of our mission is to make AI widely, and safely, available to organizations of all sizes. 
Considering and supporting different release models and operating models will enable more 
organizations to adopt and deploy AI, and it is important to ensure trust in AI across the many 
different technologies and delivery methods available. We encourage NTIA to keep in mind the 
broad diversity of technologies, contexts, and uses of AI, and to take a risk-based approach to 
developing guidelines and regulations for AI, including dual-use AI models with publicly 
available model weights. Finding nuanced, context-driven approaches to defining risk will be 
critical to these efforts since the risks cannot be captured by simply describing the size of the 
model or what elements are widely available, usable, or understood. 
Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 2 
About the Alliance 
The Alliance for Trust in AI brings together companies using advanced AI in many sectors to 
advocate for ways that we can build trust in all the kinds of AI that empower companies across 
the country and world. The Alliance works with companies developing foundational AI models, 
creating AI systems, and implementing these systems and models in their own work across 
industries.  
We aim to give organizations concrete guidance around how to build AI responsibly, implement 
AI principles, support learning and information sharing across sectors, and establish a shared 
voice for the many users of AI now and in the future. The Alliance is building on work done by 
technologists, policymakers, and academics to create a shared understanding of how to 
develop and use AI responsibly. Through multi-stakeholder partnership with members across 
industries and sectors, the Alliance is developing definitions, principles, and codes of practice 
that ensure that AI is available, and trusted, to everyone. 
Open Source, Public Access, and AI 
Open Source AI, and AI openness, can deliver transformative benefits 
Technology and the digital sector have been driven forward by a long history of openness and 
collaboration, and openness has the potential to underpin a similar set of advances – both in AI 
technologies and how they are used. The incredible acceleration in work around AI across 
sectors, building on decades of more retrained advances, has been spurred by the 
democratization of AI: making advanced generative language and image models accessible to 
the public has spurred acceleration in AI adoption, investment, and advancement. 
Openness in AI can be as transformative as openness has been for other emerging 
technologies. Online and digital tools are traditionally created using many components, 
including open-source libraries and software (OSS). This open-source model – where anyone 
can inspect, modify, and reuse the code and other elements according to certain terms and 
conditions – have made creation and deployment of software much more accessible across all 
sectors, especially to small companies.1 The vast majority of programming languages in use 
today have a freely available implementation available, including compilers and interpreters, and 
there are many freely and openly available libraries that developers can use to kickstart their 
own development. There are many different varieties of OSS, including open-source platforms, 
open-core, and licenses of all kinds. The key is that the code, libraries, systems, and software 
 
1“The Value of Open Source Software - Working Paper,” Harvard Business School, 
https://www.hbs.edu/faculty/Pages/item.aspx?num=65230 
Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 3 
have been available as components for anyone to take and use in their own development, 
potentially making their own changes or commercializing them. 
Releasing open source software is not the same as making model weights available, and 
conflating the two is counterproductive. Policies around OSS should not be directly applied or 
translated to openness in AI given these differences. More clearly defining these terms will be 
useful, and thoughtfully translating recommendations, such as those in NIST’s “Software 
Security in Supply Chains: Open Source Software Controls” under Executive Order 14028, 
Improving the Nation’s Cybersecurity.2 The underlying themes of this guidance – including 
ensuring trustworthy sources, guardrails to mitigate vulnerabilities, due-diligence, and secure 
deployment – should help inform guidance on open foundation models. However, while these 
themes serve as a good basis for commonality, direct translation of technical specifics should 
be avoided given the differences in how open source is demonstrated in OSS and AI.  
NTIA should use established definitions of “open source” in relation to AI, while 
acknowledging the benefits of openness  
It is important to note that the term “open source” as it is applied to AI models is not the same 
as “open source” as has been applied to code and OSS. Open source in the context of AI is used 
colloquially as a much broader term than it has been traditionally in OSS, and applied to many 
situations where people can customize or build on the core models. We will discuss the benefits 
of other kinds of AI openness in these comments, but we urge NTIA to use the term “open 
source” precisely, referring to established definitions around the availability of code that can be 
used, modified, and redistributed. This generally should include being able to reproduce the 
weights and contribute back, as well as other considerations. There are a number of 
constructive definitions of open source AI, including from the Open Source Initiative3 and 
Stanford University’s Institute on Human-Centered Artificial Intelligence.4 
Foundation models, or elements such as model weights, training data, and other assets that can 
be used to create or modify foundation models, can be made available in similar ways to OSS. 
And similarly, the code and methods used to train and test AI models can also be released for 
broad or public use. Creating sophisticated or foundation models with these tools is still quite 
challenging and resource intensive, beyond the reach of many individuals and organizations – 
and likely will be for some time. 
 
2 “Software Security in Supply Chains: Open Source Software Controls,” NIST, 
https://www.nist.gov/itl/executive-order-14028-improving-nations-cybersecurity/software-security-
supply-chains-open 
3 “The Open Source AI Definition – draft v. 0.0.6,” Open Source Initiative, 
https://opensource.org/deepdive/drafts/the-open-source-ai-definition-draft-v-0-0-6 
4 “Considerations for Governing Open Foundation Models,” Stanford University Institute on Human-
Centered Artificial Intelligence, https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-
Foundation-Models.pdf 
Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 4 
There are also other approaches to making AI development more accessible, which often take 
the form of delivery methods such as web applications, edge deployment, and others. Each of 
these approaches will have their own advantages and challenges, and the risk associated with 
each must be evaluated in context. 
The Benefits and Risks of AI Delivery and Distribution Models 
Regulation should protect the many benefits of increasing access to AI models and tools 
Industries across the country have already benefited from access to advanced AI. These include 
healthcare, marketing, education, manufacturing, agriculture – it’s difficult to think of a sector 
that isn’t using advanced AI. Additional access to AI, especially open models that they may not 
be able to create themselves, will improve efficiency and competition in these sectors. 
Openness in foundation models, specifically, can promote competition, improve cooperation, 
and accelerate innovation by allowing small companies and researchers greater access to AI 
tools, and the elements needed to build, modify, and commercialize their own AI systems. 
Self-governance and diligence should always be a part of the designing, developing, deploying, 
and using AI. To help users of open-source foundation models, the NIST U.S. Artificial 
Intelligence Safety Institute should develop resources, guidelines, and tools should that allow 
users to assess and address potential risks before using a specific model. Such resources can 
help users engage in due-diligence before working with either intentional or unintentional faulty 
models. This allows deployers and end users to benefit from expert guidance, and to use that 
guidance within context of their specific use case – which they know best. 
Guidance and regulation should not artificially restrict open source AI, public access to AI, 
and foundational AI models  
Openness should not be artificially restricted based on a misplaced belief that this will decrease 
risk. Each form of access to open foundation models carries different risks and has different 
benefits. The focus should be on ensuring that comprehensive risk management is built in by 
developers and deployers. Taking a more technology-agnostic approach, including relying on 
existing technology agnostic standards, will allow greater applicability to different instances 
without creating overly prescriptive standards.  
Risk of AI models can be effectively managed when developers and deployers engage in due 
diligence and testing. This is true for all attributes of trustworthy AI and are not unique to open 
source or dual-use foundation models. The kinds of diligence that are necessary will change 
within the AI development lifecycle, and many kinds of risk mitigation will depend on the 
deployment use case and implementation. In many cases, only the deployers will be able to fully 
assess this impact.  
Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 5 
Mitigating malicious and harmful use of available model weights are also context 
dependent 
Availability of model weights are one element to consider in assessing risk by availability of AI - 
but it is not determinative. A model with widely available model weights determines the kinds of 
fine-tuning that can be done with it. In discussing these risks, it is important to distinguish 
between the risk of creation of new, malicious foundation models; the use of existing, open 
foundation models towards malicious or harmful ends; and the use of open foundation models 
in other contexts. Each of these will require different kinds of efforts to address and are context 
dependent. 
Open (or widely available) model weights may be adapted by bad actors in ways that remove 
safeguards built in by the original developer. This is a risk for any model where the developer is 
not in control of the software, model, or product. While available model weights may make it 
easier to develop advanced AI, there are still significant barriers to run and modify large or 
advanced models. It is not clear whether the model weights themselves provide enough 
information to end users to significantly change what they can do or develop themselves. 
Any restrictions on openness, or on delivery models, must recognize their own risks 
Calls for increased regulation and restrictions around AI development are understandable given 
concerns around risks stemming from advanced AI. However, limitations on openness and 
transparency create their own risks in turn, and may not appropriately address the concerns at 
hand, as discussed above. Openness, and broad access to AI components, models, and 
weights, facilitates the ability of regulators, academics, and researchers to understand the risks 
and operations of AI models. A lack of access to model architectures, training data, and other 
core components obscures how these models truly operate, hampering efforts by researchers 
and watchdogs to analyze their safety and identify potential risks or biases.  
Onerous restrictions also raise barriers to entry, putting cutting-edge AI tools out of reach for 
smaller organizations, stifling innovation. An open ecosystem allows scrutiny of powerful AI and 
fosters a diversity of approaches – critical for developing AI that is robust, accountable, and 
aligned with human values. While ensuring responsible governance is wise, diminishing 
openness could make AI less transparent and its societal impact more fragmented and 
unpredictable. 
Guidelines for Open Foundation Models Must Be Risk-Based, Like Other 
AI Standards 
Deploying AI, regardless of the deployment, carries risks. It is therefore important to have 
shared high-level principles for deployers to use as they evaluate, mitigate, and manage risks. 
This applies to AI of all kinds, not just open foundation models. Organizations across sectors 
Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 6 
have established their own AI governance with shared goals and common approaches, and the 
Alliance’s members have brought together a set of high-level principles around development, 
implementation, and use of AI based on these experiences. The principles are centered around 
governance, data, society, safety, and implementation. The NIST AI Risk Management 
Framework5 and the work of the U.S. AI Safety Institute Consortium,6 are also examples of 
constructive risk-based efforts to address risk from AI models. 
We do have existing tools to regulate and govern AI, including advanced AI and foundation 
models. Technology-neutral laws and regulations apply, and many sectors (e.g., financial, 
health, and employment) have advanced programs to ensure compliance and manage potential 
unintended bias. As NTIA works to develop this report around the risks and benefits of open 
source AI and foundation AI models, the Alliance hopes that you will look to the sectors that 
have already developed similar practices for risk governance by necessity. While each sector 
approaches these questions from different perspectives, there are many commonalities that 
can help inform NTIA’s work, and the broader work around risk management across the U.S. 
government. 
Guidance should not be based on delivery mechanisms; instead, risk-based standards and 
regulations can help ensure that undue burdens are not placed on developers, stifling the open 
source foundation model ecosystem. Finding nuanced, context-driven approaches to defining 
risk will be critical to efforts to manage risks going forward. The risks cannot be captured by 
simply describing the size of the model or what elements are widely available, usable, or 
understood. Instead, risk must be understood as the likelihood and impact of the use (or non-
use) of a given kind of AI, within a given context.  
Policy, guidance, and standards for AI should not use simple proxies for risk 
Any risk management for AI will necessarily include open foundation models. When possible, 
the guidelines, risk measurement, and standards for AI should be applicable to all types of AI. 
Assessing the risk of open foundation models, dual-use foundation models, and publicly 
available model weights must go beyond simply assessing their openness, and instead address 
risk in the context of the use of the AI model. Policy, guidance, and standards should not 
demonize particular development, distribution, or business strategies. Instead, guidance should 
be broadly applicable and achievable by organizations of many sizes and with diverse 
approaches to AI to support existing diversities in business models. 
Rules for dual-use foundation models, including open source models or models with widely 
available model weights, should be carefully scoped and targeted to address concrete risks, 
 
5 “Artificial Intelligence Risk Management 
Framework (AI RMF 1.0),” U.S. National Institute of Standards and Technology, 
https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf 
6 U.S. AI Safety Institute, U.S. National Institute of Standards and Technology, 
https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute 
Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 7 
rather than hypothetical ones. Standards should not create undue or asymmetric burdens on the 
developers of these open foundation models. Most risks from open foundation models occur 
through downstream uses by third-parties, which developers cannot control; therefore, heavily 
regulating the developers and their models will increase regulatory burdens without 
meaningfully reducing risk. This in turn will likely reduce the number of open-source foundation 
models available to companies and researchers, slowing the pace of innovation and 
development of AI for good.  
In assessing concrete risks, neither the size of a foundation model nor the number or people or 
organizations with access to a particular kind of AI is a good proxy for risk. While we appreciate 
that the AI Executive Order put a threshold for dual-use foundation models in place, it will be 
important to find risk measurements that are more meaningful than the size of a model. These 
factors may contribute to the scope of the risk but are not inherent risks themselves. A small 
model accessed by ten people could carry more risk than a large model accessed by ten 
thousand people depending on how it is implemented, for what purpose, and under what 
context. The Alliance urges NTIA to not exclusively rely on these factors as a determinant of 
risk. 
Additionally, we caution against any overly simplistic or prescriptive definitions as we 
differentiate between risks and benefits from different approaches. Terms like “open” and 
“openness” have been the topic of significant, animated discussion within the OSS community 
for decades. Where fine distinctions in how AI is created or delivered are not meaningful to the 
kinds of risk that should be considered, avoiding these will be helpful to achieving the goals 
within the AI Executive Order.  
International Collaboration 
In developing best practices and guidelines for foundation model safety and security, the U.S. 
should collaborate broadly with national and international standards organizations. 
International alignment on risk management will help ensure that all jurisdictions take the same 
kind of care and create guidance that works together, instead of fragmenting the ecosystem. 
This in turn will ensure a greater secure openness of AI. Given broad multistakeholder support 
for NIST’s AI RMF, we suggest that this forms the basis of these international collaborations. 
The work of the U.S. AI Safety Institute will be to provide international policymakers with 
information and technical tools for regulations.  
In international collaborations, it will be important to recognize the global and decentralized 
nature of open source. This makes it difficult to enforce requirements on open source projects. 
Instead, it will be more effective to ensure that open source projects have the necessary 
guidance and use rules on the deployment of AI systems to create the necessary incentives and 
pressures within the ecosystem to encourage developers and deployers to adopt that guidance. 
Alliance for Trust in AI, comments on NTIA Openness in AI Request for Comment | March 26, 2024 | page 8 
Conclusion 
Democratizing AI and ensuring wider availability is beneficial for everyone. When researchers 
and organizations have greater access to different AI models, they are more likely to adopt and 
innovate - and researchers can test and evaluate these same models. Making sure that AI is 
trusted is a core element of getting more organizations comfortable with using powerful AI. 
Thank you for the opportunity to comment on these questions. If you have questions, or believe 
that we can be helpful to your work in any way, please contact the Alliance’s coordinator 
Heather West at hewest@venable.com. 
 
",Alliance for Trust in AI,4374
NTIA-2023-0009-0181,"1 
 
 
 
 
 
 
 
 
Response to Request for Comment on 
“Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights” 
Docket No. NTIA–2023–0009 
March 27, 2024 
 
Executive Summary 
 
President Biden’s “Executive Order on the Safe, Secure, and Trustworthy Development and Use of 
Artificial Intelligence” (Executive Order 14110) directs the U.S. National Telecommunications and 
Information Administration (NTIA) to submit a report to the president on the “potential benefits, risks, 
and implications of dual-use foundation models for which the model weights are widely available, as 
well as policy and regulatory recommendations pertaining to those models.” NTIA seeks public input 
informing its report recommendations (NTIA–2023–0009).  
 
NTIA’s report to the president should find that a thriving ecosystem of widely available foundation 
model weights is necessary to promote: 
 
A. Innovation, by offering unique privacy and security benefits for AI technologies that will catalyze 
scientific discovery, free expression, and free enterprise. 
B. Economic growth, by democratizing access to productivity-enhancing AI technologies to 
entrepreneurs and small- and medium-sized enterprises, to underrepresented regions, and by 
promoting competition; and 
C. AI safety, by enabling open, community-driven AI risk management frameworks for building AI 
safety solutions. 
 
NTIA should implement Executive Order 14110 by partnering with the U.S. National Institute for 
Standards and Technology (NIST) AI Safety Institute to ensure that any future regulation is shaped by 
independent, evidence-based research on: 
 
A. Reliable methods of assessing the marginal risks posed by open foundation models; 
B. Effective risk management frameworks for the responsible development of open foundation 
models; and 
C. Balancing regulation with the benefits that open foundation models offer for expanding access 
to the technology and catalyzing economic growth. 
 
 
 
2 
Table of Contents 
 
I. Introduction ................................................................................................................................................. 2 
A. Defining “Open” AI. 
.............................................................................................................................. 2 
II. Many of the foundation models that will serve as the next medium for scientific discovery, free 
expression, and free enterprise will be open. ................................................................................................ 3 
A. Open foundation models are advancing scientific discovery and free enterprise. ............................ 3 
B. Openness has unique privacy and security benefits. 
.......................................................................... 4 
C. Openness protects freedom of expression. ........................................................................................ 5 
III. The economic benefits of AI will likely accrue most where open foundation models are most widely 
available. ......................................................................................................................................................... 5 
A. Foundation models will catalyze economic growth. ........................................................................... 5 
B. Openness brings foundation models to entrepreneurs and small- and medium-businesses. ......... 6 
C. Open foundation models promote competition and choice. .............................................................. 6 
D. Openness brings foundation models to underrepresented regions. 
.................................................. 7 
IV. Community-driven risk management frameworks are addressing foundation model risks. 
.................. 7 
A. Community-driven, multi-stakeholder efforts are advancing foundation model safety. .................. 7 
B. Community-driven risk management frameworks are keeping pace with AI advancements. 
.......... 8 
V. Conclusion 
................................................................................................................................................... 8 
 
 
 
I. 
Introduction 
 
The AI Alliance is an international community of researchers, developers, and organizational leaders 
committed to supporting and enhancing open innovation across the artificial intelligence (AI) technology 
landscape. We are enabling developers and researchers to accelerate responsible, open innovation in AI 
while ensuring scientific rigor, trust, safety, security, diversity, and economic competitiveness. By 
bringing together leading developers, scientists, academic institutions, companies, and other 
innovators, the AI Alliance pools resources and knowledge to address safety concerns and provides a 
platform for sharing and developing solutions that fit researchers, developers, and adopters around the 
world. 
 
The content in this filing is provided by the AI Alliance and is not intended to reflect the views of any 
particular member organization. 
 
We value the opportunity to provide feedback to NTIA about the benefits of widely available foundation 
model weights. Our comment begins in Section Two by showing how open foundation model weights 
will play a critical role in advancing scientific discovery, free expression, and free enterprise. Section 
Three shows how the economic benefits of foundation models will likely accrue most where open 
foundation model weights are most widely adopted. Both of these sections are most responsive to RFC 
question #3 (“What are the benefits of foundation models with model weights that are widely available 
 
3 
as compared to fully closed models?”) Section Four describes how community-driven risk management 
frameworks are addressing foundation model risks. This section is most responsive to RFC question #5 
(“What are the safety related or broader technical issues involved in managing risks and amplifying 
benefits of dual-use foundation models with widely available weights?”). 
 
A. Defining “Open” AI. 
 
The open-source community broadly agrees that software-focused definitions of “open-source” 
licensing need modification to be effectively applied to AI models or systems.1 Because dialogue about 
defining “open-source AI” remains ongoing, we believe it is premature to adopt a formal definition of 
“open-source AI” here.2 
 
In lieu of a formal definition, we use the words “open” and “openness” to refer to conditions in which 
foundation model weights are publicly available under a permissive license that allows for research and 
commercial use. This definition encompasses the AI models most relevant to NTIA as it drafts its 
recommendations to the president. 
 
II. 
Many of the foundation models that will serve as the next medium for scientific discovery, 
free expression, and free enterprise will be open. 
 
A. Open foundation models are advancing scientific discovery and free enterprise. 
 
Across multiple scientific disciplines, open foundation models are accelerating fundamental research 
and showing promise for enabling future breakthroughs. One open foundation model, GHP-
MOFassemble, is accelerating research into improving the effectiveness of carbon capture technology.3 
Another model, the AI Foundation Model for Earth Observations, promises to speed up the analysis of 
satellite images and boost climate-related discoveries.4 Another model, ESM-Fold, has been used to 
accelerate research on increasing the effectiveness of COVID-19 antibodies.5 A breakthrough open 
foundation model capable of universal representation of living cells, Universal Cell Embeddings, has 
 
1 E.g., Open Source Initiative, Why do we need a new Definition of Open Source just for AI?, https://opensource.org/deepdive (“The 
traditional view of Open Source code and licenses when applied to AI components are not sufficient to guarantee the freedoms to 
use, study, share and modify the systems.”). 
2 See Megan Morrone, ""Open"" software needs an AI rethink, Axois, Feb. 15, 2024 https://www.axios.com/2024/02/15/open-
source-ai-definition-openai-meta (“the people and companies creating today's most advanced AI models don't even agree on 
what ‘open’ AI means.”) 
3 See Rob Mitchum, Researchers generate a carbon capture breakthrough using AI, physics and supercomputers, UIC Today, Feb. 
14, 2024, https://today.uic.edu/researchers-generate-a-carbon-capture-breakthrough-using-ai-physics-and-supercomputers/; 
Park, H., Yan, X., Zhu, R. et al. A generative artificial intelligence framework based on a molecular diffusion model for the design of 
metal-organic frameworks for carbon capture. Commun Chem 7, 21 (2024). https://doi.org/10.1038/s42004-023-01090-2. The 
GHP-MOFassemble framework relies on an open foundation model called DiffLinker available at 
https://github.com/igashov/DiffLinker. 
4 See IBM and NASA open source the largest geospatial AI foundation model on Hugging Face, IBM, Aug. 3, 2023, 
https://research.ibm.com/blog/nasa-hugging-face-ibm (“IBM is now making its foundation model public … It’s the largest 
geospatial model to be hosted on Hugging Face and the first open-source AI foundation model NASA has collaborated to build … it 
can analyze geospatial data up to four times faster than state-of-the-art deep-learning models, with half as much labeled data, 
IBM has estimated.”). 
5 See Hie, B.L., Shanker, V.R., Xu, D. et al. Efficient evolution of human antibodies from general protein language models. Nat 
Biotechnol 42, 275–283 (2024). https://doi.org/10.1038/s41587-023-01763-2 
 
4 
discovered new cell types and functions.6 Dr. Eric Topol, the director of the Scripps Research 
Translational Institute, told the New York Times in reaction to Universal Cell Embeddings that a “vital 
discovery about biology that otherwise would not have been made by the biologists — I think we’re 
going to see that at some point.”7 More than 90 leading scientists recently signed a statement affirming 
the belief that “the benefits of current AI technologies for protein design far outweigh the potential for 
harm,” noting that “many researchers in our community benefit from open-source scientific software, 
which has enabled rapid innovation and broad collaboration.”8 
 
Open foundation models are beginning to drive the next wave of innovation for business. Furniture 
company Wayfair is using an open foundation model, Stable Diffusion, to help customers generate new 
images and visualize redecorated rooms.9 Cloud computing company VMWare is using an open 
foundation model, StarCoder, for code generation assistance.10 Shopify is using Llama 2 to create 
product descriptions and marketing content.11 Forbes recently surveyed 600 business owners using or 
planning to use AI and found that many were planning on using AI for customer relationship 
management, digital personal assistants, inventory management, content production, product 
recommendations, accounting, supply chain operations, recruitment and talent sourcing, and audience 
segmentation.12  
 
B. Openness has unique privacy and security benefits. 
 
Community-driven development of open foundation models can drive state-of-the-art privacy and safety 
improvements.13 One research paper, MasterKey: Automated Jailbreak Across Multiple Large Language 
Model Chatbots, shows how widely available model weights enable scrutiny of model vulnerabilities and 
drive security improvements for closed and open models alike.14 Another paper shows similar benefits 
for data privacy.15 This is because, as one Brookings Institution report notes, open models “can be 
cross-examined and interrogated for bugs or possible improvements,” resulting in “collaborative 
development and an engaged community” that can create “accessible, robust, and high-quality code.”16 
 
6 See Universal Cell Embeddings: A Foundation Model for Cell Biology Yanay Rosen, Yusuf Roohani, Ayush Agarwal, Leon 
Samotorčan, Tabula Sapiens Consortium, Stephen R. Quake, Jure Leskovec bioRxiv 2023.11.28.568918; doi: 
https://doi.org/10.1101/2023.11.28.568918 (preprint). Model weights available at https://github.com/snap-stanford/UCE. 
7 Carl Zimmer, A.I. Is Learning What It Means to Be Alive, The New York Times, Mar. 10, 2024, 
https://www.nytimes.com/2024/03/10/science/ai-learning-biology.htm 
8 Responsible AI x Biodesign, Community Values, Guiding Principles, and Commitments for the Responsible Development of AI for 
Protein Design, Mar. 8, 2024, https://responsiblebiodesign.ai/. 
9 See Belle Lin, How Did Companies Use Generative AI in 2023? Here’s a Look at Five Early Adopters, The Wall Street Journal, Dec. 
29, 2023, https://www.wsj.com/articles/how-did-companies-use-generative-ai-in-2023-heres-a-look-at-five-early-adopters-
6e09c6b3. 
10 See Matt Marshall, How enterprises are using open source LLMs: 16 examples, VentureBeat, Jan 29, 2024, 
https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/  
11 Id. 
12 Katherine Haan, How Businesses Are Using Artificial Intelligence In 2024, Forbes, Apr. 24, 2023, 
https://www.forbes.com/advisor/business/software/ai-in-business/. 
13 Evidence about the brittleness of closed model security casts some doubt on the argument that closed models are necessarily 
more resistant to malicious misuse than open models. See Arvind Narayanan and Says Kapoor, AI safety is not a model property, AI 
Snake Oil, Mar. 12, 2024, https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property. 
14 See https://arxiv.org/abs/2307.08715. 
15 See https://arxiv.org/abs/2403.04801.  
16 See Alex Engler, How open-source software shapes AI policy, Brookings Institution, Aug. 10, 2021, 
https://www.brookings.edu/articles/how-open-source-software-shapes-ai-policy/ (“OSS enables and increases AI adoption by 
 
5 
Twenty-five leading AI experts across industry, academia, and civil society agree that “model weights 
are essential for several forms of research across AI interpretability, security, and safety” and that 
“model weights enable external researchers, auditors, and journalists to investigate and scrutinize 
foundation models more deeply.”17  
 
To put it succinctly, as more than 1,200 people across the field affirmed in a Joint Statement on AI 
Safety and Openness, if “our objectives are safety, security and accountability, then openness and 
transparency are essential ingredients to get us there.”18  
 
C. Openness protects freedom of expression. 
 
Access to foundation model weights decentralizes control over core model features that can affect 
access to accurate information and freedom of expression. Freedom House found in its recent report, 
The Repressive Power of Artificial Intelligence, that authoritarian governments are building centralized 
foundation models that limit access to accurate information and embed censorship.19 Open model 
weights, by contrast, decentralize control over a foundation model’s knowledge and better guarantees 
that each developer can moderate content for the developer’s customers or community.20 This approach 
is comparable to the decentralized, open approach taken for the technical infrastructure behind the 
internet.21 And as the U.S. Department of State, Bureau of Democracy, Human Rights and Labor recently 
highlighted, the internet’s open technical infrastructure plays a crucial role in thwarting authoritarian 
governments seeking to advance centralized control over the Internet for censorship and surveillance.22 
 
III. 
The economic benefits of AI will likely accrue most where open foundation models are most 
widely available. 
 
A. Foundation models will catalyze economic growth. 
 
Foundation models will increase worker productivity and grow economies. According to Goldman Sachs 
Research, the widespread adoption of foundation models could increase U.S. productivity by 1.5% on an 
annual basis.23 Another report, from the International Monetary Fund, The Macroeconomics of Artificial 
 
reducing the level of mathematical and technical knowledge necessary to use AI. … Since OSS code is all public, it can be cross-
examined and interrogated for bugs or possible improvements. With collaborative development and an engaged community, as 
often arises around popular OSS, this collaborative-competitive environment can frequently result in accessible, robust, and high-
quality code.”) 
17 See https://arxiv.org/abs/2403.07918. 
18 See Joint Statement on AI Safety and Openness, Mozilla, Oct. 31, 2023, https://open.mozilla.org/letter/  
19 See Allie Funk, Adrian Shahbaz, and Kian Vesteinsson, Freedom on the Net 2023: The Repressive Power of Artificial Intelligence, 
Freedom House, Oct. 3, 2023, https://freedomhouse.org/sites/default/files/2023-11/FOTN2023Final.pdf.  
20 See Mark Gimein, AI’s Spicy-Mayo Problem, The Atlantic, Nov. 24, 2023, 
https://www.theatlantic.com/ideas/archive/2023/11/ai-safety-regulations-uncensored-models/676076/. 
21 See Steven Vaughan-Nichols, Open source is actually the cradle of artificial intelligence. Here's why, ZDNet, 
https://www.zdnet.com/article/why-open-source-is-the-cradle-of-artificial-intelligence/. 
22 See Bureau of Democracy, Human Rights, and Labor, Funding Opportunity Announcement: Supporting Critical Open Source 
Technologies That Enable a Free and Open Internet, U.S. Department of State, Feb. 21, 2023, https://www.state.gov/supporting-
critical-open-source-technologies-that-enable-a-free-and-open-internet/.  
23 Goldman Sachs, Research, AI may start to boost US GDP in 2027, Nov. 7, 2023, 
https://www.goldmansachs.com/intelligence/pages/ai-may-start-to-boost-us-gdp-in-2027.html. (“Generative artificial 
 
6 
Intelligence, acknowledges that AI may “end up being less promising or less ready to bring to market 
than initially hoped,” but also forecasts that “AI might be applied to a substantial share of the tasks 
done by most workers … and massively boost productivity in those tasks.”24 That is no small outcome. 
As the IMF notes, productivity is the single largest determinant of “the wealth of nations and the living 
standards of their people.”25 Overall, according to an estimate by the consulting firm McKinsey, 
foundation models may generate between $2.6 trillion to $4.4 trillion in economic growth across the 
global economy.26 
 
B. Openness brings foundation models to entrepreneurs and small- and medium-
businesses. 
 
Open foundation models are often the most affordable, cost-effective option for entrepreneurs and 
small- and medium-businesses. Building a new, enterprise-specific foundation model often requires 
prohibitively expensive investments in model training.27 Access to open foundation models, which are 
typically free to procure and more affordable to customize than starting from scratch, substantially 
lowers the barrier to entry.28 These models also enable a thriving ecosystem of foundation model 
development support and cloud service providers serving open foundation models to enterprise 
customers. This makes it possible for more businesses to build foundation models at lower cost, 
ensuring that corporate resources are not the sole determinant of whether a company can realize the 
benefits of a bespoke foundation model.29 This spreads the productivity benefits of foundation models 
to more sectors of the economy. 
 
C. Open foundation models promote competition and choice. 
 
Openness creates increased competition in the foundation model marketplace by enabling downstream 
developers to build innovative, custom products.30 Growing the number of foundation model-based 
products reduces overall market concentration and increases options for enterprise customers and end 
 
intelligence has the potential to automate many work tasks and eventually boost global economic growth … In the baseline 
scenario, the Goldman Sachs Research economists estimate AI could increase US productivity growth by 1.5 percentage points 
annually assuming widespread adoption over a 10-year period.”) 
24 Erik Brynjolfsson and Gabriel Unger, The Macroeconomics of Artificial Intelligence, International Monetary Fund, Dec. 2023, 
https://www.imf.org/-/media/Files/Publications/Fandd/Article/2023/December/20-25-Brynjolfsson-final.ashx. 
25 Id. 
26 See McKinsey & Company, The Economic Potential of Generative AI: The Next Productivity Frontier, Jun. 2023, 
https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-
productivity-frontier. 
27 See https://arxiv.org/abs/2403.07918. 
28 One exception is that certain use cases, like customer service chatbots, are amenable to the fine-tuning options made available 
by closed model providers. But realizing many of the business benefits of foundation models depends on access to pretrained 
model weights. E.g., MosaicML, Stardog: Customer Spotlight, Dec. 19, 2023 https://www.mosaicml.com/blog/stardog-customer-
spotlight (“we quickly learned that if you have a smaller model that you fine-tune for a specific task, you can match or exceed the 
quality of [a closed model] and you have more control over data security and privacy. … Fine-tuning is not really a nice-to-have for 
us. It's a necessity.”). 
29 See Oguz A. Acar and Andrés Gvirtz, GenAI Can Help Small Companies Level the Playing Field, Harvard Business Review, Feb. 1, 
2024, https://hbr.org/2024/02/genai-can-help-small-companies-level-the-playing-field.  
30 See Will Douglas Heaven, The open-source AI boom is built on Big Tech’s handouts. How long will it last?, MIT Technology 
Review, May 12, 2023, https://www.technologyreview.com/2023/05/12/1072950/open-source-ai-google-openai-eleuther-meta/ 
(“if the trend toward closing down access continues, then not only will the open-source crowd be cut adrift—but the next 
generation of AI breakthroughs will be entirely back in the hands of the biggest, richest AI labs in the world.”). 
 
7 
users.31 The availability of open foundation models is also likely applying market pressure on closed 
developers to lower prices and “compete against free.”32 This has wide benefits; in general, as the White 
House has noted, “when firms have to compete for customers, it leads to lower prices, higher quality 
goods and services, greater variety, and more innovation.”33 
 
D. Openness brings foundation models to underrepresented regions. 
 
Openness can bring foundation model development to underrepresented regions, including the Global 
South. By publication index (h-index), a widely-used measure of research impact, Africa and Latin 
America trail the Global North on AI research and development.34 According to one analysis, none of the 
top 100 most-cited companies or universities for AI research were based in Africa or Latin America.35 
Open models can help close the gap. Unlike closed models, which support select languages and are 
tailored to select audiences, open models empower members of the Global South to build and localize 
models that speak their own language and understand their own culture.36 As one study found, Africa-
based “technologists note that their solutions often perform better than tools from large multinational 
companies, simply because they, the technologists, can speak the language.”37 
 
IV. 
 Community-driven risk management frameworks are addressing foundation model risks. 
 
A. Community-driven, multi-stakeholder efforts are advancing foundation model safety.  
 
Openness is not a one-size-fits-all solution and risk management is an important aspect of the 
responsible development and release of a foundation model.38 That’s why stakeholders across industry, 
academia, civil society, and government are working together to build AI risk assessment and mitigation 
 
31 See https://arxiv.org/abs/2403.07918 (access to model weights promotes innovation in downstream markets by “helping to 
reduce market concentration at the foundation model level from vertical cascading”). 
32 See Tyler Cowen, Open-Source Software Is Worth a Lot More Than You Pay for It, Bloomberg, Feb. 26, 2024 
https://www.bloomberg.com/opinion/articles/2024-02-26/open-source-software-is-worth-a-lot-more-than-you-pay-for-it. 
33 See Heather Boushey and Helen Knudsen, The Importance of Competition for the American Economy, U.S White House Blog, Jul. 
9, 2021, https://www.whitehouse.gov/cea/written-materials/2021/07/09/the-importance-of-competition-for-the-american-
economy/. 
34 Many factors may explain this divide, including historical inequities, higher costs of internet access, and underrepresentation in 
available training data for model training. See https://arxiv.org/abs/2102.01265. 
35 See https://arxiv.org/abs/2102.01265 
36 For example, with adequate training data, the BigScience Large Open-science Open-access Multilingual Language Mode 
(BLOOM) model can enable developers to more affordably fine-tune for a new language. See https://arxiv.org/abs/2212.09535 
(“we adapt BLOOM models to support eight new languages (German, Russian, Bulgarian, Thai, Turkish, Greek, Korean, and 
Guarani) in the resource-constrained settings…”). See also MakerereNLP, Text & Speech for East Africa, 
https://www.masakhane.io/ongoing-projects/makererenlp-text-speech-for-east-africa (“The project aims to deliver open, 
accessible and high quality text and speech datasets for low resourced East African languages from Uganda, Tanzania and 
Kenya.”). 
37 See Andrew Paul, AI programs often exclude African languages. These researchers have a plan to fix that, Popular Science, Aug. 
11, 2023, https://www.popsci.com/technology/african-language-ai-bias/; Kathleen Siminyu et. al.,. Consultative engagement of 
stakeholders toward a roadmap for African language technologies, Patterns, https://doi.org/10.1016/j.patter.2023.100820. 
38 Seger, Dreksler, Moulange, Dardaman, Schuett, Wei, et al, Open-Sourcing Highly Capable Foundation Models: An Evaluation of 
Risks, Benefits, and Alternative Methods for Pursuing Open-Source Objectives, Centre for the Governance of AI, 2023 (“If models 
are determined to pose significant threats, and those risks are determined to outweigh the potential benefits of open-sourcing, 
then those models should not be open-sourced. … This is not to say that a given highly capable model should never be open-
sourced. Expected model impacts are likely to change with increasing societal resilience and development of new defensive 
techniques. … The need to conduct risk assessments prior to model release seems to be generally accepted”). 
 
8 
frameworks. Our multi-stakeholder organization, the AI Alliance, is already collaborating on establishing 
improved, community-driven model evaluation testing frameworks and mitigations. Our members are 
also leading other important safety efforts. For example, ML Commons has established an AI Safety 
Working Group focused on the development of safety benchmarks for certain foundation models.39 The 
Partnership on AI has been growing the list of signatories to its Responsible Practices for Synthetic 
Media, a framework for how to responsibly develop, create, and share AI-generated media.40 The 
Partnership on AI has also been maturing its Guidance for Safe Foundation Model Deployment, a 
framework for model providers to responsibly develop and deploy AI models.41 Hugging Face is leading 
a multi stakeholder collaboration on building a framework for assessing the social impact of foundation 
models.42 
 
B. Community-driven risk management frameworks are keeping pace with AI 
advancements. 
 
Community-driven risk management frameworks are supplementing government approaches with 
solutioning that more closely track the pace of foundation model development. For example, the UC 
Berkeley AI Risk-Management Standards Profile for General-Purpose AI Systems (GPAIS) and 
Foundation Models proposed a comprehensive adaptation of the U.S. National Institute for Standards 
and Technology (NIST) AI Risk Management Framework (RMF) to foundation models after the NIST AI 
RMF did not address it.43  
Open foundation models inform and validate community-driven risk assessment and risk management 
solutions. For example, the non-profit Humane Intelligence and the United Kingdom's national academy 
of sciences, the Royal Society, organized an event with 40 experts in climate science and disease that 
depended on using the Llama 2 model for a risk assessment exercise.44 
V. 
Conclusion 
 
The AI Alliance values this opportunity to highlight the benefits of widely available foundation model 
weights. We look forward to additional opportunities to show how open science and open innovation is 
important to realizing many of the benefits of AI advancements. 
 
39 See MLCommons Announces the Formation of AI Safety Working Group, ML Commons, Oct. 26, 2023, 
https://mlcommons.org/2023/10/mlcommons-announces-the-formation-of-ai-safety-working-group/. 
40 See Responsible Practices for Synthetic Media, Partnership on AI, https://syntheticmedia.partnershiponai.org/. 
41 See Guidance for Safe Foundation Model Deployment, Partnership on AI, https://partnershiponai.org/modeldeployment/. 
42 See https://arxiv.org/abs/2306.05949. 
43 See Center for Long-Term Cybersecurity, AI Risk-Management Standards Profile for General-Purpose AI Systems (GPAIS) and 
Foundation Models, University of California, Berkeley, Nov. 8, 2023, https://cltc.berkeley.edu/seeking-input-and-feedback-ai-risk-
management-standards-profile-for-increasingly-multi-purpose-or-general-purpose-ai/  
44 See Billy Perrigo, The Scientists Breaking AI to Make It Safer, Time, Oct. 26, 2023, https://time.com/6328851/scientists-
training-ai-safety/. 
",AI Alliance,6331
NTIA-2023-0009-0224,"To: National Telecommunications and Information Administration
Author: Thomas Larsen, Center for AI Policy
RE: Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights
Executive Summary
There has been substantial debate over whether open-sourcing artificial intelligence (AI) is good
for the world. As is often the case in AI, we think the answer depends on which AI you’re talking
about. For the majority of AI models, we believe that open development and release is
beneficial for society. However, releasing highly capable AI model weights would
substantially elevate national security risks and hence should not be allowed.
We think that there are two types of systems that should be considered high-risk and therefore
should not be allowed to be open-sourced:
1.
WMD-like AI: AI systems that enable bad actors to develop WMDs much more
effectively.
2.
Artificial General Intelligence (AGI): AI systems that are as generally competent as
humans across a wide range of tasks.
Open weight high risk AI systems are dangerous because a large number of actors would gain
access to very destructive technology. Fortunately, current AI systems are probably not high
risk1. There remains a great deal of uncertainty about the length of time until we develop high
risk AI systems: Shane Legg, the Co-Founder of DeepMind, predicts that a 50/50 chance of AGI
by 2028, but many other experts think that AGI timelines are substantially longer. Dario Amodei,
the CEO of Anthropic, predicted that AIs will be able to build highly dangerous bioweapons
within 2 years, suggesting that WMD-like AI may be just around the corner.
In this response, we first outline key benefits and risks of widely available model weights. Then,
we discuss policy recommendations for mitigating threats. Our main recommendations are to
develop better monitoring and forecasting to predict when advanced AI might be developed, and
to prevent high-risk AI models from being open sourced. Finally, we respond to selected
questions from the NTIA Request For Comment.
1 Unfortunately, our capabilities evaluation methodology is imperfect, and so we cannot be completely
certain that the current models are not high risk – this is discussed in the capabilities elicitation section.
Benefits of widely available model weights
This section addresses the NTIA RFC Question 3: What are the benefits of foundation models
with model weights that are widely available as compared to fully closed models?
Accelerate AI safety research. Open model weights accelerate a large number of AI safety
research agendas. Most interpretability research is done on large language models with widely
available model weights. This especially helps smaller labs and academics have access to
models that are closer to the cutting edge of capabilities.
Accelerate the adoption of positive AI use cases. There are many positive uses of AI
systems – AI can be used to: increase productivity, improve decision making, and accelerate
economic growth. Increasing access to model weights accelerates that research.
Enable Transparency. Widely available model weights enable more thorough understanding of
those models, and enables red teaming efforts to identify potential vulnerabilities. Historically,
open source software has been more secure, as the broader community can identify and
propose mitigations for risks. This carries over somewhat to model weights because, as with
other software, friendly actors can identify risks and propose mitigations.
Risks of widely available model weights
This section addresses the NTIA RFC Question 2(a): What, if any, are the risks associated with
widely available model weights? How do these risks change, if at all, when the training data or
source code associated with fine tuning, pretraining, or deploying a model is simultaneously
widely available?
We identify two classes of risks: direct risks, where an individual AI model poses some harm to
society, and indirect risks, where an AI doesn’t directly cause harm to society, but does lead to
harm down the road.
Direct Risks
Weaponization. AI technology could be weaponized to cause massive harm. If an AI system
has the capability of a nuclear engineer and is widely avialible, this could dramatically
accelerate the development of nuclear weapons. Even narrow AI systems may provide
substantial weaponization concerns, particularly in CBRN domains. There are substantial
concerns both in designing more dangerous weapons (e.g. developing a supervirus that is
substantially deadlier than naturally occurring viruses) and in reducing the barrier to entry (e.g.
reducing the number of scientists to build a WMD from a team of 50 to a team of 5).
Loss of Control. If we develop sufficiently capable autonomous AI systems, we may face
widespread loss control of these AIs. This is a particular concern when developing extremely
intelligent AI systems that exceed human capabilities across a wide range of tasks. If AI
systems which can autonomously do AI capabilities research become widely available, it
will become extremely difficult to prevent the development of substantially
smarter-than-human AI systems. Widely releasing these AI model weights give a large
number of actors access to these models, and each actor will have a large financial incentive to
build more capable AI systems. Even if there are substantial safety concerns, it is unlikely that
all of these actors will be able to coordinate to avoid building the extremely capable systems,
which are also the most dangerous.
Open sourcing AI models that pose severe direct risks such as weaponization or loss of control
is a mistake for several reasons:
1.
We allow widespread access to highly dangerous technology. For both AGI systems
and WMD-like AI, giving widespread access dramatically increases the likelihood that
someone will use them irresponsibly. Particularly concerning applications include the
development of extremely competent agentic AGI systems, AI-engineered pandemics,
and large scale AI powered deception.
2.
AI safety safeguards are rendered obsolete. It’s extremely easy to remove safety
features from an open-weights AI model. We can expect any safety techniques that are
applied to be removed by people wishing to do unsafe things with their AI system that
are limited by the safety techniques.
3.
We lose the ability to turn the AI off. If an AI is released throughout society, there’s no
clear way to turn it off: you can’t get people to un-download the piece of software. In an
emergency, an individual company can take down a dangerous product, but a
decentralized AI would be impossible to shut down. This strongly limits our ability to
respond to threats.
Indirect Risks
AI capabilities acceleration. Many experts are concerned that building and empowering highly
capable AI systems could lead to very bad outcomes, including extinction. We agree that society
is not sufficiently prepared to build highly capable AI systems. We lack both technical safety
solutions for aligning an AI system to its developer’s interests and government response
capacity to react to the drastic changes in the rate of technological development such as
widespread job loss and problems associated with the obsolescence of human workers. Open
weight models can accelerate the development of extremely powerful models, giving society
less time to prepare.
Reducing the American lead. The duration of time between when the first actor who can build
AGI and when adversaries can build AGI is critical, as this is a period when urgent technical and
governance measures must be taken to improve the safety of AI systems. Releasing models
that push the open-weight frontier reduces this gap.
Policy Recommendations
Our two main policy recommendations around open-weight AI systems are:
1.
Improve monitoring and forecasting capacity to understand the timeline until AI
systems pose direct risk.
2.
Secure future high-risk AI models by creating capability based safety standards that
involve preventing the model weights of high risk AI models from becoming widely
available.
Improve AI monitoring and forecasting capacity
AI development is highly unpredictable, and so rapid policy responses may be necessary.
Monitoring and forecasting AI development is critical for making informed policy decisions.
Ideally, we would develop precise measurements that could evaluate the extent to which
individual models pose risks.
Model Evaluations are tests which attempt to measure certain properties about an AI model.
The simplest form of model evaluation involves simple benchmarking to check performance on
relevant datasets. For example, benchmarks for highly capable models include:
1.
GPQA (A Graduate Level Google Proof Q&A Benchmark) is a dataset containing expert
level multiple choice questions in chemistry, biology and physics.
2.
SWE Bench contains unresolved software issues from GitHub, and checks whether AI
models can resolve them.
3.
GAIA is a benchmark containing tasks that involve tool use and many step reasoning.
This tests models capability to solve complicated problems.
4.
Weapons of Mass Destruction Proxy (WMDP) is a benchmark that tests AI models'
ability to answer questions relating to the development of WMDs, including
cybersecurity, bioweapons, and chemical weapons.
One can also conduct more sophisticated evaluations that test models in more realistic settings.
For example, METR (formerly ARC Evals) made an evaluation that tested AI's capability to
autonomously set up new instances of itself on new servers. While model evaluations are
fallible, they provide useful information about the level of risk from AI models that are being
developed.
Model evaluations attempt to address direct risk from specific AI systems. However, it is difficult
for model evaluations to capture structural effects like speeding up AI capabilities or reducing
the lead time of the leading actor. In order to estimate these effects, it is important to forecast
the danger coming from future models. If there is a significant chance that the next generation
of AI systems will be high-risk, we must take steps to contain them before they are widely
released. To do this successfully we need accurate forecasts of future model capabilities. This
forecasting should involve consulting with professional risk assessors and AI experts in order to
estimate critical strategic variables.
Capability Based Safety Standards
To address risks, the government could require that model developers follow safety procedures
to evaluate and counteract risks. These could either be written into voluntary standards such as
the NIST Risk Management Framework, or ideally, used within required safety standards
enforced by an AI regulator. Before releasing models, these safety standards could specify
which types of evaluations to run for estimating certain types of harms.
One part of these safety standards could involve evaluating the effects of giving widespread
access to model weights. In this section, we sketch how this evaluation could work. Because the
risks of AI models depend on the capabilities of the underlying system, we propose three tiers
that could be used in safety standards: low risk, medium risk, and high risk. The development of
complete standards should involve setting specific thresholds using the AI capability evaluations
discussed above along with setting up safety and security requirements for AI models at each
capability level.
Low Risk
Medium Risk
High Risk
AI systems that are less
capable than the most
capable open weight models
Narrow AI systems, e.g.
recommender systems
AI systems that accelerate
the development of high-risk
AI systems
AI systems that could be
enhanced to become
high-risk
Artificial General Intelligence
(AGI)
WMD-like AI
Low Risk. Given that models with higher capabilities are already widely accessible, the risks of
widely releasing models below the current open-weight frontier are very low. Given that there
are benefits and few risks, AI safety standards should allow model weights to be released freely.
Medium Risk. Releasing model weights that are more capable than existing open-weight
models pose some risk, even if they appear safe during the evaluations testing for specific
dangerous capabilities. Medium risk models pose:
●
Direct risk because the tests are fallible: a model that appears safe during initial testing
might turn out to pose dangers later (after it is too late to recall the model) based on
post-training enhancements, a wider availability of plug-ins, and a decreased cost of
compute.
●
Indirect risk because any model that pushes the capability frontier will have effects on
other AI development, including reducing the American lead time on AI because of
accelerating other AI projects.
Current AI models on the open weight frontier are probably not sufficiently capable that they
pose significant risk. We therefore recommend thorough monitoring of these systems for risks
that may emerge, but do not yet recommend enforcement actions taken against models in this
category.
High Risk. High risk models are models that pose substantial risk because they have
dangerous capabilities or are soon projected to have highly dangerous capabilities due to post
training enhancements. The threshold for being a high risk model should include tests for both
weaponization capabilities and loss of control capabilities. The model weights for high risk
models should not be widely released because of the substantial risks described above.
Fortunately, no current AI systems are in this high risk category.
Selected Questions from the NTIA Request for Comment.
Is it possible to generally estimate the timeframe between the deployment
of a closed model and the deployment of an open foundation model of
similar performance on relevant tasks? How do you expect that timeframe
to change? Based on what variables? How do you expect those variables
to change in the coming months and years?
We find that the timeframe between closed and open models right now is around 1.5 years. We
can arrive at this conclusion by analyzing benchmark performance between current leading
open weight AI models and the best closed source AI models.
One of the best open weight AI models is Meta’s Llama-2 70B, which has comparable
benchmark performance to GPT-3.5.2 Llama 2 finished training in July 20233, right before it was
released. GPT 3.5 was released after a period of red teaming on March 15 2022, and so likely
finished training at least several months before, likely Jan 1 2022. This gives us a performance
gap of 1.5 years.
There are multiple competing effects that determine how this timeframe will change over time.
Here, we survey some of the major considerations: the cost of training runs, ideological
commitments, feedback loops, and the proliferation of information.
The cost of training runs. Large dollar costs will incentivize LLM developers to avoid widely
releasing their model weights. GPT-4, trained in 2022, cost over 100M. The training costs for the
largest AI model has been increasing at a rate of 3.1x/year. If we extrapolate this trend, we get
very large dollar costs for frontier AI models over the next few years. Quantitatively, we can
graph the compute used for existing models over time, and then extrapolate the growth using
the trendlines. However, naive extrapolation is probably incorrect because the rate of growth is
unsustainable: AI developers will run out of capital to spend on training runs. Large tech
companies will probably be unable to raise $1T to spend on individual training runs – they do
3 See https://github.com/microsoft/Llama-2-Onnx/blob/main/MODEL-CARD-META-LLAMA-2.md
2 Llama-2 has a performance of 1082 on LLM-Leaderboard and 30.4 on GPQA (extended set), which is
slightly better than GPT-3.5, which has 1068 on LLM-Leaderboard and 28.2 on GPQA (extended set).
not have that much cash on hand4 and there are few funding sources which could be that large.
Additionally, at that point they will run out of hardware to buy.5
This chart shows some existing large language models, as well as lines showing the amount of
compute6 that can be bought for $1 million, $1 billion, and $1 trillion, adjusted as compute gets
cheaper over time. The unlabeled x’s represent our all-things-considered best estimate for the
compute used in the largest closed training run over the next several years. Over the next two
years, this is a linear extrapolation from the past, and after that we adjust downward to account
for running out of hardware.
Some AI developers have strong commitments to open source. Mark Zuckerburg laid out a
vision of Meta open sourcing the models all the way to AGI. This means that some AI
developers may widely release AI model weights despite financial cost or substantial risk to
public safety. This underscores the need for regulation: model developers may not take
appropriate safeguards on their own.
Feedback loops from closed source AI R&D could extend lead times.
6 Compute is measured in floating-point operations (FLOPs), assuming that training is done using a bit-length of 16. If
training is done in a different bitlength, one must adjust the FLOP availability proportionally, e.g. for 8-bit training,
there are twice as many FLOPs available.
5 A rough calculation is that Nvidia produced 1.5e6 H100 GPUs during 2023, each of which produced 1e15 FLOP/s in
FP16, which together make up 1/10th of the global compute supply. Assuming that GPU utilization is 50% the entire
compute supply could support a training run of 6e28 FLOP. Given that GPT-4 used 2e25 FLOP, it used about 3e-4, or
.03% of the world’s compute supply. This limits the amount that increased investment could result in increased
compute without increasing global chip production to around 1000x without additional chip capacity. Chip supply is
highly inelastic given the 3-5 year lead time to construct a new fabrication plant.
4 Google had 110B cash on hand in December 2023.
AI models can accelerate AI research – there are many existing examples for AI models
providing value to accelerate research. Model developers that keep their models closed may be
able to extend their lead by accelerating AI research. This will become a larger and larger effect
over time as AI models become increasingly capable. When AI systems are able to provide
useful research contributions without humans in the loop, there will be a substantial increase in
the rate of research progress because of the large increase in the number of researchers able
to tackle specific problems that arise.
Diffusion of algorithmic insights. AI labs historically have had weak information security –
there have been a plethora of leaks of algorithms, as well as plenty of personnel flow between
labs. In the runup to AGI, poor information security will probably prevent the OS frontier from
falling too far behind the closed source labs.
What security, legal, or other measures can reasonably be employed to
reliably prevent wide availability of access to a foundation model's weights,
or limit their end use?
To reliably prevent wide availability, it is necessary to (a) not widely release model weights and
(b) implement good information security practices to prevent model weights from being
exfiltrated by adversarial actors.
If the model is widely available, it is difficult to limit the end use of models. On specific tasks that
require many inference steps (e.g. if the user is trying to automate certain jobs on an ongoing
basis), limiting those tasks is possible by restricting hardware usage. However, most usages of
models can be done with very small amounts of hardware. To give an idea of the scope, to use
GPT-4, OpenAI charges 10$ per 1M tokens. The total amount of computation in a single forward
pass is simply quite cheap to create, and therefore does not require all that much compute.
As an alternative to releasing model weights publically, there are partially open measures that
obtain some of the benefits of open weight models while mitigating some risk. For example, one
could use the NAIRR as a secure compute cluster to give access to some specific users who
are doing research on an AI model with dangerous capabilities without making the weights
widely available to arbitrary actors. In particular, this can allow us to continue accelerating AI
safety research and positive AI use cases, while preventing malicious or reckless actors from
obtaining the model weights.
Noting that E.O. 14110 grants the Secretary of Commerce the capacity to
adapt the threshold, is the amount of computational resources required to
build a model, such as the cutoff of 1026 integer or floating-point operations
used in the Executive order, a useful metric for thresholds to mitigate risk in
the long-term, particularly for risks associated with wide availability of
model weights?
Floating point operation (FLOP) thresholds are useful thresholds in the toolbox, but are limited
in some important ways. FLOP thresholds are clear and easy to measure. Moreover, they are a
reasonable proxy for capabilities: models that increase in scale tend to also increase in
capabilities.
However, AI algorithms are improving every year. With a given amount of compute, developers
will be able to build more and more capable models over time. Additionally, the physical bounds
on AI capabilities suggests that it is physically possible to build AI systems that have advanced
capabilities using significantly less than 10^26 FLOPs.7
Another option is to use a capability-based threshold instead of a compute based threshold.
The key advantage of this approach is that the capabilities of an AI system are the true source
of risks, and so this can be a much better proxy.
Unfortunately evaluating model capabilities has proved challenging, and so this is less clear as
a regulatory threshold than FLOP. AI capabilities are rapidly increasing, and so benchmarks are
quickly becoming saturated. A capability based evaluation will need to be updated often.
7 As a rough calculation, a human brain uses around 10^15 FLOP per second, though various methods
find different numbers. 10^9 seconds is around 30 years, and so the total amount of computation done by
a human brain running for 30 years is 10^24 FLOP.
",CAIP,4575
NTIA-2023-0009-0230," 
 
 
 
 
 
March 27, 2024 
 
The Honorable Alan Davidson   
Administrator  
National Telecommunications and Information Administration (NTIA)  
1401 Constitution Ave, NW  
Washington, DC  20230  
 
Re: Request for Comment, National Telecommunications and Information Administration, 
Department of Commerce; Dual Use Foundation Artificial Intelligence Models Widely 
Available Model Weights (89 Fed. Reg. 14,059-14,063, February 26, 2024)  
Dear Administrator Davidson: 
 
The U.S. Chamber of Commerce (“Chamber”) appreciates the opportunity to respond 
to the National Telecommunications and Information Administration (“NTIA”) request for 
comment (“RFC”) on “Dual Use Foundation Artificial Intelligence Models with Widely Available 
Model Weights.” We agree with NTIA’s statement that “Artificial Intelligence (AI) has had, and 
will have, a significant effect on society, the economy, and scientific progress.1” Furthermore, 
we appreciate the RFC’s recognition of how open-source models “could play a key role in 
fostering growth among less resourced actors, helping to widely share access to AI’s 
benefits.”2  
 
The Chamber continues to be a strong advocate for using technology to assist small 
and medium businesses. A Chamber report released last year highlighted that 87%” of small 
businesses believe that technology platforms have helped their business operate more 
efficiently3 and that 71% of them plan to adopt the latest technology, including AI.4  
We remain concerned with the short comment period provided. Given the strategic 
importance of AI and its many associated technological and policy complexities, agencies 
must receive meaningful and robust public input to prevent ill-informed policy developments 
that could hinder the ability of Americans to reap the benefits of AI and maintain global 
leadership of this groundbreaking technology. Accordingly, the Chamber, with other 
associations, corresponded requesting a 60-day extension to allow stakeholders to provide 
the necessary and thoughtful comments.5  
 
 
1 89 Fed. Reg. 14060 available at https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-
foundation-artificial-intelligence-models-with-widely-available-model-weights#print.  
2 Id. 
3 Empowering Small Business: The Impact of Technology on U.S. Small Business at 3 (September 2023) available at  
https://www.uschamber.com/assets/documents/The-Impact-of-Technology-on-Small-Business-Report-2023-
Edition.pdf.  
4 Id.  
5Letter to NTIA (March 14, 2014) available at U.S. Chamber Multi-Association Comment Extension Request on 
NTIA's Dual Use Open Model RFI | U.S. Chamber of Commerce (uschamber.com).  
Unfortunately, that request for an extension was denied, and therefore, the business 
community can only provide limited feedback and comments. This denial will not provide the 
agency the benefit of robust feedback, and the denial will hamper the ability of the agency to 
assess and weigh the comments received. Accordingly, the agency is in danger of ignoring 
issues or data raised in the comment process. 
 
I. 
General Feedback  
  
Many benefits of open-source technology exist. Open-source technology allows 
developers to build, create, and innovate in various areas that will drive future economic 
growth. We already see innovation in marketing, communication, cybersecurity, and medicine, 
among other fields. Access model weights can be a boon to driving safety and security 
improvements to artificial intelligence by providing greater transparency, allowing flaws to be 
quickly identified and patched.6  
 
We would further highlight the need for NTIA to make decisions based on sound 
science and not unsubstantiated concerns that open models pose an increased risk to 
society.7  
 
Trust in technology is fundamental for society to embrace and use technology. For this 
reason, it is key to educate and prepare the American public through public-private 
partnerships. NTIA can help prepare the American public for a future in which AI is widely 
integrated throughout society and ensure that the United States has the necessary skilled 
workforce to benefit from the technology fully. We also further emphasize that the specific 
policy questions within the RFC are not just being asked within the United States but globally 
as well. NTIA must be mindful of the current geopolitical dialogue on this topic.  
 
II. 
Definitions .  
 
The Chamber believes it is critically important for NTIA not to deviate from the 
definition of the “dual-use foundation model” defined within Executive Order 14110.8 Potential 
changes in the definition could impact the scope of NTIA’s work, which could put 
unnecessary burdens on researchers, academia, and developers who are not developing 
technology that hits the level of “dual use foundation models with widely available, and the 
deployers of systems build on these models.” NTIA should also look to harmonize terminology 
with other industry efforts, such as the G7, AI Alliance, Frontier Model Forum, and academic 
and non-governmental standard development organizations. A consent lexicon is vital in 
creating an environment where all AI value chain stakeholders can understand and become 
familiar with the terminology.  
 
 
 
 
6 https://arxiv.org/pdf/2211.14946.pdf 
7 https://crfm.stanford.edu/open-fms/ 
8 88 Fed. Reg. 75194 available at https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-
secure-and-trustworthy-development-and-use-of-artificial-intelligence.  
III. 
Risk & Risk Management  
 
The Chamber emphasizes that risk-management processes are still evolving, which is 
why we support the National Institutes of Standards and Technologies (NIST) work around the 
Risk Management Framework (RMF) and subsequent companion efforts to develop common 
standards to address and mitigate potential concerns. Additionally, we note that “risk” is 
contextual. As indicated in the NIST RMF 1.0, “Risk tolerance and the level of risk acceptable 
to organizations or society are highly contextual and application and use-case specific.”9 This 
is why we believe it is essential for NTIA to focus on the marginal risk, which is context-
specific.  
 
IV. 
Other Issues:  
 
The Chamber is concerned with NTIA asking if “risk” should be associated with 
floating-point operations per second (aka “FLOPs”), as using such criteria does not use the 
appropriate quantitative metrics based on benchmarks and evaluations.  
Finally, it is essential to emphasize that there can be valid reasons for some “artifacts,” 
such as code, datasets, and model weight, not to be released, as many corresponding factors 
may make releasing such information challenging.   
V. 
Conclusion  
 
Although we reiterate our concerns about this RFC’s short comment period, we look 
forward to working with NTIA and other stakeholders to address these matters and encourage 
NTIA to look for other opportunities to receive necessary feedback on the current request.  
 
Sincerely, 
 
 
 
Michael Richards 
Senior Director  
Chamber Technology Engagement Center 
 
U.S. Chamber of Commerce 
 
9 NIST Risk Management Framework at 7 (January 2023) available at 
https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf  
",Chamber Commerce,1546
NTIA-2023-0009-0231," 
 
1 
March 27, 2024 
 
Mr. Bertram Lee 
National Telecommunications and Information Administration 
U.S. Department of Commerce 
1401 Constitution Avenue NW 
Washington, DC 20230 
 
Re: Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights 
 
Dear Mr. Lee, 
 
On behalf of the Center for Data Innovation (datainnovation.org), I am pleased to submit this response to 
the National Telecommunications and Information Administration’s (NTIA) request for comment on the 
potential benefits, risks, and implications of dual-use foundation models for which the model weights are 
widely available, as well as policy and regulatory recommendations pertaining to those models.1 
 
The Center for Data Innovation studies the intersection of data, technology, and public policy, and 
formulates and promotes pragmatic public policies designed to maximize the benefits of data-driven 
innovation in the public and private sectors. It educates policymakers and the public about the opportunities 
and challenges associated with data, as well as technology trends such as open data, artificial intelligence, and 
the Internet of Things. The Center is part of the Information Technology and Innovation Foundation (ITIF), 
a nonprofit, nonpartisan think tank. 
 
In this submission, we encourage U.S. policymakers to learn lessons from past debates about dual-use 
technologies, such as encryption, and refrain from imposing restrictions on foundation models with widely 
available model weights (i.e. “open models”) because such policies would not only be ultimately ineffective at 
addressing risk, but they would slow innovation, reduce competition, and decrease U.S. competitiveness. 
NTIA should use an evidence-based approach to addressing AI risks and avoid broad rules that would 
negatively impact the ability to develop open models. Moreover, U.S. policymakers should defend open AI 
models at the international level as part of its continued embrace of the global free flow of data. 
 
Sincerely, 
 
Daniel Castro 
Vice President, Information Technology and Innovation Foundation (ITIF) 
Director, ITIF’s Center for Data Innovation 
 
1 “Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights,” Federal Register, 
February 26, 2024, https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-foundation-
artificial-intelligence-models-with-widely-available-model-weights. 
 
 
2 
1. HOW SHOULD NTIA DEFINE “OPEN” OR “WIDELY AVAILABLE” WHEN THINKING ABOUT FOUNDATION 
MODELS AND MODEL WEIGHTS? 
 
“Open” should refer to models and model weights that are made freely available, for any purpose, for 
anyone. In other words, anyone may use, distribute, customize, or improve the model and model weights at 
no cost. Some models and model weights may be partially open, meaning that they have some restrictions. 
For example, a developer may release a model under license terms that only allow for non-commercial use or 
require that any derivative works retain the same license. These restrictions may also prohibit certain harmful 
uses. For example, Llama 2’s acceptable use policy explicitly prohibits using the model for “illegal or unlawful 
activity or content,” “activities that present a risk of death or bodily harm to individuals,” and to 
“intentionally deceive or mislead others.”2 
 
“Widely available” should refer to models and model weights that are freely accessible on the Internet. This 
definition mirrors how the IRS uses the term.3 According to IRS regulations, a document is “widely 
available” when an organization 1) informs users that the document is available and provides download 
instructions; 2) provides the document in a format that allows exact reproduction of the original; and 3) 
allows any user to access the document without special hardware or software and without payment of any 
fee.4 NTIA should incorporate similar definitions in its work. 
 
NTIA should not base the widely available definition on the number of downloads or the number of entities 
making the model or model weights available for download. Instead, as in the IRS definition, it should be 
based on meeting certain specific criteria related to its potential discoverability and accessibility on the 
Internet. For example, NTIA should consider a model to be widely available if a developer makes its model 
available on its own website or publishes it to a third-party website, such as a publicly accessible GitHub or 
HuggingFace repository.  
 
Some repositories like HuggingFace allow developers to provide gated models, meaning that users must 
request permission before they can access them. In some cases, the developer can authorize the platform to 
automatically approve these requests. In other cases, the developer may review requests and only grant them 
to certain individuals. NTIA should consider gated models widely available if they have automatic approval. 
 
2 “Acceptable Use Policy,” n.d., Meta, https://ai.meta.com/llama/use-policy/. 
3 “Public Disclosure and Availability of Exempt Organizations Returns and Applications: Exemption Where 
Organization Makes Documents ‘Widely Available’,” IRS, December 4, 2023, https://www.irs.gov/charities-non-
profits/public-disclosure-and-availability-of-exempt-organizations-returns-and-applications-exemption-where-
organization-makes-documents-widely-available. 
4 “26 CFR § 301.6104(d)-2 - Making applications and returns widely available,” Cornell Law School, n.d., 
https://www.law.cornell.edu/cfr/text/26/301.6104(d)-2. 
 
 
3 
2. HOW DO THE RISKS ASSOCIATED WITH MAKING MODEL WEIGHTS WIDELY AVAILABLE COMPARE TO 
THE RISKS ASSOCIATED WITH NON-PUBLIC MODEL WEIGHTS? 
 
NTIA's focus is on foundation models with widely accessible weights that pose significant risks. However, 
determining whether a model presents such risks can be challenging. While some risks may be apparent from 
the outset, many are not, and even for those risks that are apparent, it may not be clear if they are significant. 
There is no standardized benchmark for assessing whether a specific AI model poses a significant risk. 
Indeed, opinions vary widely on what constitutes a serious threat. For instance, a terrorist constructing a 
bomb unquestionably poses a national security risk. By this definition, a large language model (LLM) offering 
guidance on bomb-making falls into this category. Yet, similar information is readily available on the Internet. 
Treating a search engine that can uncover this data differently from an LLM that does the same would be 
inconsistent and ineffective policy. 
 
Therefore, instead of focusing solely on the absolute level of risk, NTIA should consider the relative increase 
in risk resulting from open AI models. These risks exist along a spectrum. On one end are scenarios where 
open AI models introduce entirely new risks that would not exist without them. On the other end, open 
models merely reduce barriers to existing risks. However, many instances exist where open AI models merely 
lower barriers to risk in relatively inconsequential ways. Thus, NTIA should prioritize oversight and research 
on cases where open AI models significantly enhance performance in tasks posing serious security risks 
compared to non-open AI models. 
 
Additionally, NTIA should recognize that even when AI models introduce new risks, the response is not 
necessarily to restrict the technology. Consider the invention of the automobile: crime existed before personal 
vehicles became widespread, but they also facilitated criminal activities, as exemplified by John Dillinger's  
infamous use of getaway cars in bank robberies. However, the government’s response was not to ban vehicles 
but rather to rethink approaches to crime prevention, leading to the establishment of the FBI and the 
allocation of new resources to law enforcement. 
3. WHAT ARE THE BENEFITS OF FOUNDATION MODELS WITH MODEL WEIGHTS THAT ARE WIDELY 
AVAILABLE AS COMPARED TO FULLY CLOSED MODELS? 
 
Foundation models with model weights that are widely available, i.e., open models, provide an important 
pathway for innovation since anyone can freely use and modify the model for research and commercial 
purposes. This collaborative process accelerates development, cuts costs, and democratizes access to AI 
technology. For example, developers can fine-tune open models for specific applications thereby facilitating 
adoption. In some cases, commercial providers of closed models may be unwilling or unable to address the 
AI needs of a specific sector or community, but if members of that sector or community have access to open 
models, they can customize them for their own purposes. For example, a non-profit organization may adapt 
an AI model for a specific healthcare application or educational use case. 
 
 
4 
 
Open models not only enable research and development on more capable AI models, but they also allow 
progress on other goals such as explainability, bias, safety, and efficiency. Here again having open models is 
useful because commercial providers cannot always focus on improving every useful feature. Allowing others 
to build on open models allows developers and researchers focused on specific improvements to contribute 
to advancements in the field, including critical ones like security and safety. These advancements can benefit 
both open and closed models, as developers making closed models can still integrate the ideas and techniques 
developed for open models.  
 
Open models also provide opportunities for those learning about AI to better understand how these systems 
work by being able to “look under the hood” and experiment in ways that are not usually possible with fully 
closed models. This type of accessibility means more people can understand exactly how AI works, which 
helps to demystify the technology and increase public acceptance. For example, concerns about fairness and 
bias could impede adoption of AI in fields such as criminal justice or public administration, but using open 
models could mitigate some concerns because anyone would have the opportunity to conduct their own 
independent third-party testing. In addition, having access to not just open models, but open-source code and 
training data, allows for greater understanding of AI by students and practitioners in the field, as well as 
increased opportunities for customization and collaboration.  
 
Competition between open and closed models provides more user choice. History shows that there is a 
market for both open and closed approaches, such as in operating systems, software, and mobile device 
ecosystems, for both consumers and enterprises. Competition between open and closed models encourages 
innovation that benefits users. Policymakers should provide a level playing field, and neither penalize nor 
favor open models over closed models. 
 
There are open foundation models today, and there will almost certainly be open foundation models in the 
future. The question is whether these models will be created and led by U.S. developers and reflect U.S. 
values.5 Imposing restrictions on open models for U.S. firms and developers will hurt U.S. competitiveness as 
other nations, including China, would fill that void and U.S.-based firms interested in developing open AI 
would likely consider relocating abroad. Moreover, if foreign firms are creating the state-of-the-art open 
foundation models, the U.S. government will have less influence over its development and oversight over 
potential risks.  
 
5 “A Global Declaration on Free and Open AI,” Information Technology and Innovation Foundation, September 13, 
2023, https://itif.org/publications/2023/09/13/global-declaration-on-free-and-open-ai/. 
 
 
5 
5. WHAT ARE THE SAFETY-RELATED OR BROADER TECHNICAL ISSUES INVOLVED IN MANAGING RISKS 
AND AMPLIFYING BENEFITS OF DUAL-USE FOUNDATION MODELS WITH WIDELY AVAILABLE MODEL 
WEIGHTS? 
 
The challenges of dual-use, open-source technology are not new. For example, in the 1990s there was 
widespread debate over how policymakers should address the risks of strong encryption.6 At the time, U.S. 
policy restricted the export of advanced encryption technology. In 1991, Phil Zimmermann created Pretty 
Good Privacy (PGP), an open-source public-key encryption program to enable anyone to sign and encrypt 
files and messages. After Zimmerman published his software, the U.S. government investigated him for 
violating export controls (PGP used a minimum of 128-bit keys, enabling significantly stronger encryption 
than was allowed to be exported at the time), but eventually dropped its case. Policymakers came to realize 
that restricting encryption technology was both impractical (because limiting its lawful distribution had no 
impact on bad actors getting access to the technology) and counterproductive (because it would limit U.S. 
users from using encryption to secure their data and communications and hurt U.S. businesses that could not 
integrate advanced encryption technology into their products and services). 
 
Policymakers face a similar scenario today with dual-use foundation models. Just as there were some people 
pushing for restrictions on encryption in the 1990s, primarily in the interest of national security, so too are 
there some today advocating for similar restrictions on AI models, especially open AI models, for similar 
reasons. While bad actors might make use of this technology for harmful activities, there are almost always 
better ways to mitigate these risks than restricting AI models, especially when such restrictions would 
negatively impact many beneficial uses of the technology. Therefore, rather than focusing on general 
restrictions aimed at foundation models for a broad set of risks, policymakers should instead focus on specific 
countermeasures aimed at specific risks. In many cases, the most effective countermeasures may not be 
related to imposing restrictions on AI models, but instead addressing the problem somewhere closer to the 
problematic behavior. Moreover, U.S. policymakers should recognize that there is little they can do to stop 
open models developed and released abroad. 
 
The research by the National Institute of Standards and Technology (NIST) on AI test, evaluation, validation, 
and verification (TEVV) will support AI safety in both closed and open AI models. As part of this effort, 
NTIA should direct the NIST AI Safety Institute to include a workstream on assessing AI memorization—or 
the condition where AI models output training data verbatim. In many cases, AI memorization is a feature, 
such as to ensure that a model produces accurate, factual output. However, in other cases, AI memorization 
can be a problem, such as if it reveals private information from training data. Developing protocols for 
efficiently and effectively assessing memorization in AI models will help developers identify and address 
potential risks. 
 
6 Daniel Castro, “Why New Calls to Subvert Commercial Encryption Are Unjustified,” July 13, 2020, Information 
Technology and Innovation Foundation, https://itif.org/publications/2020/07/13/why-new-calls-subvert-commercial-
encryption-are-unjustified/. 
 
 
6 
7. WHAT ARE CURRENT OR POTENTIAL VOLUNTARY, DOMESTIC REGULATORY, AND INTERNATIONAL 
MECHANISMS TO MANAGE THE RISKS AND MAXIMIZE THE BENEFITS OF FOUNDATION MODELS WITH 
WIDELY AVAILABLE WEIGHTS? WHAT KIND OF ENTITIES SHOULD TAKE A LEADERSHIP ROLE ACROSS 
WHICH FEATURES OF GOVERNANCE? 
 
The U.S. government has historically supported the global free flow of data because cross-border data flows 
are essential for the digital economy. Restricting data flows, such as with data localization requirements, can 
slow innovation, raise costs, and hurt economic growth. U.S. policymakers should continue to uphold this 
position for AI, specifically by opposing restrictions on the global free flow of AI model weights (which are a 
form of data). For example, cross-border restrictions on widely available model weights could limit access to 
AI models developed and used by U.S. companies thereby restricting market access for U.S. firms. In 
addition, some governments could impose restrictions on sharing AI model weights to limit free speech.  
 
The U.S. government should also oppose regulations that focus on the development and sharing of AI 
models, including those with widely available weights, rather than the use of AI models. Imposing restrictions 
on the development and sharing of AI models can make open-source development impractical, especially 
when there is no central developer managing the project. Instead, policymakers should focus any regulations 
on specific high-risk uses of AI, so that any rules and requirements relate to the actual context in which an 
entity uses the AI system. For example, it would be better for the Department of Transportation to create 
specific regulations about the safety of autonomous vehicles rather than have broad regulations about the 
development of AI-based image recognition systems that have thousands of different potential uses. 
8. IN THE FACE OF CONTINUALLY CHANGING TECHNOLOGY, AND GIVEN UNFORESEEN RISKS AND 
BENEFITS, HOW CAN GOVERNMENTS, COMPANIES, AND INDIVIDUALS MAKE DECISIONS OR PLANS 
TODAY ABOUT OPEN FOUNDATION MODELS THAT WILL BE USEFUL IN THE FUTURE? 
 
The Department of Commerce should create evidence-based policy. In this case, there is no evidence that AI 
models that exceed certain computational thresholds or require a certain amount of computational resources 
present novel risks that require extraordinary treatment. In addition, there is new evidence that the risk of 
sudden emergent abilities in AI models is less likely than previous thought and that better evaluation can help 
assess the capabilities of AI models.7 
 
7 Rylan Schaeffer, Brando Miranda and Sanmi Koyejo, “Are Emergent Abilities of Large Language Models a Mirage?” 
arxiv.org, May 22, 2023, https://arxiv.org/abs/2304.15004. 
",Center for Data Innovation,3694
NTIA-2023-0009-0225," 
 
 
 
 
 
 
March 27, 2024 
 
 
1 
Agency name: National Telecommunications and Information Administration 
Federal Register Document Citation: 89 FR 14059 
Organization: The Center for Security and Emerging Technology (CSET) 
Respondent type: Organization>Academic institution / Think tank 
Primary POC: Kyle Miller (kam471@georgetown.edu) 
 
The Center for Security and Emerging Technology (CSET) at Georgetown University offers the 
following comments in response to NTIA’s Request for Comment (89 FR 14059) on Dual Use 
Foundation Artificial Intelligence Models With Widely Available Model Weights. A policy 
research organization within Georgetown University, CSET provides decision-makers with 
data-driven analysis on the security implications of emerging technologies, focusing on 
artificial intelligence, advanced computing, and biotechnology. We appreciate the opportunity 
to offer these comments. 
 
Our response pertains to six of the topics from the enumerated list provided in the RFC: 
1. Definition of Open Foundation Models 
2. Risks 
3. Benefits 
4. Managing Risks 
5. Governance Mechanisms 
6. Planning for the Future 
 
 
 
 
 
 
 
2 
Definition of Open Foundation Models 
(1) How should NTIA define “open” or “widely available” when thinking about foundation 
models and model weights? 
An open or widely available foundation model has weights available online that can be readily 
downloaded, shared, and moved to any computing infrastructure the user sees fit. This 
includes model weights that require a prospective user to first accept licensing terms and 
acceptable use policies before being given access to the weights (e.g., LLama 2). We do not 
consider a model to be open if the weights are only accessible through an API, as the ability 
to obtain, use, share, and control the weights on a separate computing infrastructure is 
restricted by the original developer. If model weights are available through a gated release, 
where only a select set of actors have been given direct access, the analysis is more 
complicated. We propose that model weights should not be considered widely available if it is 
feasible and likely that the gated release would be enforced up to and including legal action, 
for example by model developers pursuing claims against unauthorized disclosure to third 
parties. If model developers are relying on gating contracts that are unenforceable or unlikely 
to be enforced, the weights should be considered widely available. 
 
We also recommend removing the “tens of billions” parameter threshold from NTIA’s current 
definition of “foundation models.” This number is arbitrary, and many foundation models fall 
below that threshold. Foundation models can have widely varying parameter counts that 
depend on the task it was developed to perform, the amount of data used to train it, etc. 
Language models can be in the hundreds of billions of parameters, while computer vision, 
text-to-image, and biological foundation models (e.g., Evo) can be in the millions to billions of 
parameters. Moreover, many popular language models on Hugging Face (measured by 
download count) have fewer than 10 billion parameters. 
(1.b) Is it possible to generally estimate the timeframe between the deployment of a closed 
model and the deployment of an open foundation model of similar performance on relevant 
tasks? How do you expect that timeframe to change? Based on what variables? How do you 
expect those variables to change in the coming months and years? 
The best way to gauge such timeframes may be to directly contact organizations designing 
foundation models and acquire information regarding their model performance and release 
strategies. This is the most viable way to get these estimations, although these organizations 
may not have the will or obligation to provide such information. 
 
 
 
 
 
3 
Organizations developing foundation models decide whether to release weights, and it 
currently happens to be the case that organizations making the highest-performing models 
(OpenAI, Anthropic, and Google) decide to not release them. There are no inherent structural 
differences between open and closed models, only differences in how developers decide to 
release weights. That being said, companies may have fewer incentives to keep weights closed 
if their models are small (cheaper to make) and have lower performance. Conversely, if a 
company develops an expensive, high-performance model, then it will have more incentive to 
monetize it and keep the weights closed. This is the case for state-of-the-art models 
developed by Anthropic and OpenAI, all of which are closed. However, some companies may 
see longer-term opportunities in opening weights, especially if it comes with licenses that 
permit downstream monetization. For example, according to Meta’s LLama 2 license, if an 
application that uses LLama 2 exceeds 700 million users a month, then “you must request a 
license from Meta, which Meta may grant to you in its sole discretion.”  
(1.c) Should “wide availability” of model weights be defined by level of distribution? If so, at 
what level of distribution (e.g., 10,000 entities; 1 million entities; open publication; etc.) should 
model weights be presumed to be “widely available”? If not, how should NTIA define “wide 
availability?” 
“Widely available” should be defined by the level of access (i.e., can the weights be 
downloaded from the Internet), not by the level of distribution (i.e., how many actors have 
downloaded the weights). Regardless of the platform or medium through which weights can 
be shared or distributed, a model should be considered “widely available” if the weights can be 
downloaded from a website and shared without restriction.  
(1.d) Do certain forms of access to an open foundation model (web applications, Application 
Programming Interfaces (API), local hosting, edge deployment) provide more or less benefit or 
more or less risk than others? Are these risks dependent on other details of the system or 
application enabling access? 
Risks and benefits can depend significantly on the accessibility of the weights.  
 
Hosted access. Through hosted access, users can prompt a model and access its outputs via a 
web-based user interface, but they have no access to the weights or the inner functionality of 
the model. The developers have full control over the model, and outside users cannot alter or 
fine-tune it. This can reduce both potential risks and benefits. The developers can oversee and 
mediate how actors use the model, and safety measures (e.g., RLHF that reduces model biases 
or restricts responses to certain prompts) can be put in place to reduce misuse. However, the 
 
 
 
 
4 
benefits are also reduced, as the ability to scrutinize or alter the models is restricted, and the 
onus for transparency is solely on the original developers.  
 
API-based access. With API-based access, the original developers still maintain a high degree 
of control and the weights are closed, but the level of access may be greater than with hosted 
access, depending on the functionality provided through the API. For example, the GPT-3.5 
API allows users to fine-tune models, while the Claude API does not allow fine-tuning. 
Developers can oversee and mediate how actors use or fine-tune the models, and can take 
action against actors who use or fine-tune models maliciously (or in ways that infringe on the 
user agreement stipulated by the developer).  
 
However, benefits are also reduced when weights are only accessible through an API. For 
example, users can fine-tune GPT-3.5 via OpenAI’s API, but this API is highly restricted. Users 
can only upload training data and set the number of epochs to fine-tune the model. The ability 
for actors to customize the models, scrutinize their behavior, and alter their core functionality is 
constrained. This can reduce the benefits to R&D and innovation, and puts the onus for 
transparency on the original developers. Policymakers should consider how different degrees 
of API-based access can improve the benefits without increasing risk, such as supporting more 
research-friendly APIs that provide more access to and customizability of closed models (either 
permanently closed models, or models that will eventually have open weights).  
 
Downloadable weights. When model weights are downloadable, anyone can use or alter the 
model on their own computing infrastructure, thereby enabling both local hosting and edge 
deployment. This can increase both the potential risks and benefits. On the one hand, risks of 
misuse are greater because the original developers have no means to oversee or mediate how 
the model is used or altered; the weights can be readily downloaded, fine-tuned, and used by 
nefarious actors. On the other hand, the benefits to R&D are also greater, as more researchers 
can assess, customize, and use the models freely and without restriction (depending on the 
license). Downloadable weights can also lower the barrier to entry in AI R&D, and may reduce 
the concentration of power in the AI industry because the original developers do not control 
access to the models. Most open models fall within this ‘downloadable’ category, and most of 
these downloadable models fit along a spectrum of openness based on the accessibility of 
their components and information about their development – all of which is (or is not) provided 
by the developers. 
 
Fully open models. Models are fully open when all of their components (e.g., weights and 
training data) and documentation on how they were developed (e.g., how the model was 
 
 
 
 
5 
designed and evaluated) are available online. This means the model is replicable because 
anyone with sufficient resources can access all of the components and information to recreate 
it from scratch. However, very few open models are fully open and replicable (e.g., BLOOM).  
(1.d.i) Are there promising prospective forms or modes of access that could strike a more 
favorable benefit-risk balance? If so, what are they?  
Providing third-party researchers with greater structured access to closed models would be 
beneficial, and is unlikely to entail more risk than providing hosted access. Prior to releasing 
model weights, organizations could provide structured access via more research-friendly APIs. 
This could include allowing users to automate sampling from a model, customize the fine-
tuning specifications, and inspect the model's internals (parameters, activations, embeddings, 
etc.). This would allow a larger pool of actors to scrutinize and experiment with the models, as 
well as crowdsource some of the red teaming process, which could help inform the developers 
about the potential risks before releasing weights.  
 
However, control is still highly centralized with structure access, whereas open weights enable 
a greater degree of ‘democratized access’ where anyone can obtain, use, share, and experiment 
with models as they see fit. 
Risks 
(2.a) What, if any, are the risks associated with widely available model weights? How do these 
risks change, if at all, when the training data or source code associated with fine tuning, 
pretraining, or deploying a model is simultaneously widely available? 
Releasing training data or training code may help facilitate model replication. Replication of a 
model does not, in itself, pose any additional risk if the replicated model’s weights are already 
widely available, since any risks would already have been posed by the original model. 
However, access to training code and data could pose risks if it contains insights and 
information that are not obvious to malicious actors with the resources to train their own 
models. While malicious actors do not need that information to simply make use of the open 
model, it could help them build systems that are more capable. 
 
For example, suppose a model exhibits some level of dangerous capability, C. This may pose a 
risk. Releasing the training data and code could allow other well-resourced actors to develop 
their own model with capability level C, which is unlikely to pose additional risks because they 
could simply have used the original model. However, if the training data and code released are 
novel to some other actors, it may help them develop a model with a capability level C+1 
 
 
 
 
6 
higher than the capability level of any current open model. If capability C+1 is sufficiently risky 
and is left unmitigated or used by malicious actors, this could pose an additional risk. 
(2.d) Are there novel ways that state or non-state actors could use widely available model 
weights to create or exacerbate security risks, including but not limited to threats to 
infrastructure, public health, human and civil rights, democracy, defense, and the economy? 
How do these risks compare to those associated with closed models? 
There are some unique characteristics of open model weights that should be considered. 
 
First, some popular strategies to mitigate misuse cannot be applied to open models. For 
example, many developers use reinforcement learning from human feedback to prevent 
models from responding to certain kinds of harmful prompts. These safeguards can be 
removed if a model is fine-tuned. In principle, it may be possible to prevent this kind of harmful 
fine-tuning if it is performed through an API through further safeguards, for example by 
checking fine-tuned models for degradation in behavior before making them available to end 
users. However, it would be substantially more difficult (and potentially impossible) to 
implement reliable safeguards that would entirely prevent the capabilities from being elicited 
in cases where weights are widely available. 
 
Thus, it is possible that mitigation strategies would be sufficient to reduce risk to acceptable 
levels if weights are not made widely available. However, these mitigation strategies would be 
insufficient for models with widely available weights. 
 
Second, opening model weights is irreversible. Once model weights are widely distributed, it 
is not feasible to restrict them or gate their access. This significantly restricts the responses 
that deployers can take if new risks arise. For example, Microsoft and OpenAI restricted 
several threat actors from using OpenAI models, but they would not have been able to do so if 
that model had widely available model weights. Irreversibility is particularly important for the 
most advanced foundation models, because uncertainty about their capabilities and the 
effectiveness of their mitigations will persist for some time after their initial development (see 
above). If dangerous capabilities or gaps in mitigations are found after the wide release of 
model weights, there are far fewer responses that can be deployed. Risk assessments should 
take this factor into account. 
 
These aspects of widely available model weights are simply aspects that should be considered 
when performing risk assessment prior to making model weights widely available. The overall 
 
 
 
 
7 
framework for assessing risk from making model weights widely available need not be 
fundamentally different from the one used for other kinds of release methods. 
 
Below is a non-exhaustive list of potential risks that could arise from present and future 
foundation models (whether closed or open), which should be considered before the release of 
model weights. 
 
Disinformation. Foundation models can be used by state or non-state actors to more 
effectively spread disinformation. There is evidence that contemporary language models can 
reduce the cost of generating disinformation. In addition, voice clones and deepfakes are 
already being deployed in wartime and elections. Closed models that are accessible via hosted 
or API access can also be leveraged in this way, but the developers have more means to 
identify and disrupt this malicious use.  
 
Non-consensual intimate imagery and CSAM. Foundation models can aid in the production of 
nonconsensual intimate imagery and child sexual abuse material (CSAM). The use of these 
models is documented, for example in the production of synthetic CSAM or nonconsensual 
images of celebrities. 
 
Cyberattacks. Foundation models may aid nefarious actors in conducting offensive cyber 
operations. This includes using models to support spearphishing, reconnaissance, and social 
engineering. They may allow attackers to work more quickly, reduce the barrier to entry for 
various activities, or even detect and exploit vulnerabilities that humans cannot. At present, 
code generation capabilities of large language models can improve the productivity of 
developers, including presumably those engaged in cyberattacks. In addition, research 
suggests that models may have some limited capacity to engage in vulnerability discovery and 
exploitation autonomously or semi-autonomously. Overall, the current impact of AI on 
cybersecurity is uneven, and often depends on the intent, sophistication, and capabilities of the 
actors involved. For more details, see the UK National Cyber Security Center’s report on the 
near-term impact of AI on the cyber threat. 
 
Biological misuse. Foundation models could be employed in certain scenarios of biological 
misuse. The type of foundation model – whether trained on natural language or biological data 
– impacts the potential outcome, which actors are most likely to use it, and what safeguards 
are most likely to be useful. Chatbots trained on natural language data can help describe 
scientific concepts or provide links or references to additional resources, including those that a 
malicious actor could use to cause harm. Biological design tools trained on biological data, like 
 
 
 
 
8 
DNA or protein sequences, could help an expert malicious actor to predict modifications to 
pathogens or toxins that make them more severe, targeted, or otherwise harmful. 
  
Importantly, however, these risks should be clearly defined and assessed within the context of 
the existing biorisk landscape. AI is not required to cause biological harm, and alternative 
methods are widely and readily available to malicious actors. For example, chatbots trained on 
natural language data could help malicious actors find information about how to carry out a 
biological attack. However, this information can easily be found from other sources, because 
scientific information is widely available in accordance with principles of scientific openness, 
rigor, and transparency. Evidence suggests that present-day chatbots are unlikely to 
significantly assist a malicious actor for this reason. Similarly, an expert malicious actor has 
access to several traditional experimental methods that make pathogens or toxins more 
dangerous. Thus, it may be more effective to address existing, foundational biosecurity gaps 
for which more comprehensive oversight could address both AI-agnostic and AI-enhanced 
risks. 
 
Improper use. Careless or uninformed actors may use foundation models for purposes that 
those models are not suited to. For example, LLM-powered systems have been proposed to 
automatically respond to emails, but if these tools are vulnerable to adversarial attacks, they 
may exfiltrate private user data. These risks, present with any technology, are especially acute 
for foundation models since developers often do not specify their use cases or imply that they 
can be used in all but a specifically enumerated list of disallowed use cases. There are many 
present-day examples of foundation models failing when used improperly. 
 
International Competition. It will be difficult to limit the dissemination of open models that 
may be valuable to state adversaries. Adversaries can access, use, and adapt open models 
released by U.S. companies. However, the extent to which it helps them compete in AI R&D is 
unclear. 
 
Increased productivity for criminal actors. Foundation models may substitute for human labor 
in mundane tasks that are not themselves harmful. This could include composing 
communications, making financial transactions, generating ideas, etc.. While the resulting 
productivity gains are helpful for benign actors, they also affect criminal actors, especially 
those that suffer from a shortage of labor for tasks amenable to foundation models. We are 
not aware of research illuminating the extent to which present-day foundation models are 
enhancing the productivity of criminal actors. 
 
 
 
 
 
9 
Uncontrolled AI agents. Developers are aiming to build AI agents that can autonomously take 
actions such as navigating web browsers, controlling physical equipment, or even executing 
contracts. Capable variants of such systems could become uncontrolled without better 
technical mechanisms to align them to the intent of their user and/or respect requests to cease 
operation, particularly if such systems are capable of deceiving humans or exfiltrating 
themselves to unmonitored servers. Malicious or uninformed actors could also intentionally 
create and unleash uncontrolled AI agents. Evidence suggests that present-day foundation 
models are not capable enough to pose these risks, though they sometimes operate in an 
uncontrolled manner. 
 
Note that the same model capabilities that could enable the risks above may also enable 
beneficial activity and defenses against those or other risks. For example, improved AI cyber 
capabilities may help with patching software and defending networks, and the increased 
productivity from foundation models may empower law enforcement. These improvements in 
defensive capabilities should be included as part of any risk mitigation assessment. 
Benefits 
(3.a) What benefits do open model weights offer for competition and innovation, both in the AI 
marketplace and in other areas of the economy? In what ways can open dual-use foundation 
models enable or enhance scientific research, as well as education/training in computer science 
and related fields?  
Open foundation models may provide wide-ranging benefits, including improved AI R&D, 
market competition, international competition, oversight, and life sciences R&D.  
 
AI R&D and Market Competition. Open sharing has helped drive much AI R&D, and there are 
several reasons why open weights may help this trend continue and accelerate. First, the 
barrier to entry in AI R&D is lowered for less well-resourced actors who cannot develop pre-
trained models from scratch (e.g., academics and small-to-medium-sized enterprises). This 
moves control away from a handful of well-resourced companies, and toward a larger pool of 
actors. Over time, this may help reduce the concentration of power in the AI industry, foster 
greater market competition in certain areas, and promote a more diverse R&D ecosystem that 
better reflects the cultural and sociopolitical diversity of different communities (outside of the 
model’s original developers). However, the degree to which it could foster greater market 
competition is unclear, and it may be unevenly distributed across the industry. For example, 
while high-performing open models could allow more actors to alter and use them for various 
 
 
 
 
10 
applications, the availability of such models could also disincentive companies from developing 
new models from scratch. 
 
Second, platforms can emerge and act as open repositories for a global body of AI researchers 
and developers (e.g., Hugging Face). Communities can form around these platforms, fostering 
cross-pollination of research and decentralized collaboration.  
 
Third, more actors can customize and fine-tune open models for specific applications and novel 
research, especially those with access to bespoke datasets that can be used to fine-tune 
models for unique tasks. This can be done on their own computing hardware and without 
restrictions or gatekeeping from the original developers, and the financial benefits from this 
can be distributed amongst a wider pool of developers. 
 
While open models are generally understood to be a net benefit to R&D, it is still unclear the 
extent to which open models – that all have varying degrees of openness based on the 
accessibility of their components and documentation on how they were developed – can 
benefit R&D, lower the barrier entry, and promote competition in the AI industry. More 
research is needed to determine what types of research are enabled by open weights, and how 
that may allow more entrants into the market. Many prospective entrants may lack resources, 
and it is unclear the extent to which resource constraints may limit the benefits of open models 
to R&D. Actors may lack the data to fine-tune open models, or lack the compute to use or 
experiment with open models rigorously and at scale (although resources provided through 
the NAIRR pilot may help alleviate resource constraints). 
 
International Competition. First, much of the open source software ecosystem resides within 
the U.S. and the territories of its allies, as do many of the organizations engaging in open AI 
R&D. This may help the U.S. leverage open innovation (to support their security and economic 
interests) more effectively than adversaries, as well as put it in a better position to shape 
international norms around AI. Second, an open, U.S.-promoted AI ecosystem may help foster 
multilateral collaboration on AI policy and security issues, as it may create a common and more 
transparent baseline from which all states can assess the technology. 
 
Visibility and Oversight. First, many actors are transparent about how they develop and use 
open models, often more so than developers of closed models. Many support initiatives around 
transparency and openness (e.g., EleutherAI, Nomic AI, and Petuum), which can help facilitate 
oversight and enable more robust testing and evaluation. Second, platforms such as Hugging 
Face and GitHub can help aggregate and track a significant portion of open models, who 
 
 
 
 
11 
develops them, how they are developed, and when new fine-tuned versions are released. This 
overall visibility may foster greater R&D and innovation, as well as greater competition in the 
AI industry, especially when compared to closed models (whose developers are often not 
transparent). 
 
Life Sciences R&D. Foundation models for biological research are emerging as promising tools 
in biomedicine, ecosystem and climate resilience, agriculture, and biomanufacturing, among 
other fields. These benefits are largely derived from AI’s ability to accomplish an important 
task at the center of scientific research: identifying complex patterns and pulling out the 
relevant variables from large amounts of information. In particular, purpose-built models 
loosely termed biological design tools (BDTs) have the potential to unlock new frontiers in 
basic and applied research. BDTs are trained on biological data, like DNA sequences or protein 
structures, and are designed to engineer, predict, or simulate biological molecules, processes, 
or systems. These tools have already shown success in a wide range of biomedical, 
pharmaceutical, and basic research applications, including helping researchers to design and 
optimize protein-based therapies, interpreting DNA sequences and their impact on biological 
systems, and identifying novel disease proteins and generating custom drug molecules to 
target them. 
  
Most current BDTs are open models developed by academic labs. The life sciences community 
places a high value on scientific transparency and openness, and tends to favor open sharing 
of resources. Openness ensures scientific rigor and reproducibility, as demonstrated by high-
impact journals like Cell and Nature that require code, algorithms, and software to be peer-
reviewed and released upon publication. Shifting away from open sharing of model weights 
would also require additional resources, as many academic researchers do not have the time, 
funding, or infrastructure to set up and maintain an API. 
(3.b) How can making model weights widely available improve the safety, security, and 
trustworthiness of AI and the robustness of public preparedness against potential AI risks?  
More actors can scrutinize open models, identify vulnerabilities and safety issues, and 
implement patches and safety measures. There is a long history of open-sourcing software to 
crowdsource vulnerability discovery, which typically makes the software far more secure than 
if it was only assessed in-house. More actors can also assess open model functionality, 
limitations, and biases. This improves overall auditability, which may bolster the reliability and 
trustworthiness of downstream AI applications.   
 
 
 
 
 
12 
When a model is open and accessible to many actors, it can be more rigorously scrutinized, 
and issues are more likely to be identified. However, it is unclear the extent to which issues 
with open models, once identified, can and will be remediated. For example, if a vulnerability in 
a popular open base model is identified, then that vulnerability may also exist for all fine-tuned 
versions of that base model. It is unclear if and how downstream developers will address 
issues that stem from the original base model. In addition, some challenges with foundation 
models, such as out-of-distribution robustness and adversarial robustness, remain unsolved 
despite years of research effort, indicating that some vulnerabilities may be very difficult to fix 
in general. 
Managing Risk 
(5.a) What model evaluations, if any, can help determine the risks or benefits associated with 
making weights of a foundation model widely available? 
Potential risks should be assessed in the context of each model: different models may have 
different capabilities and pose different risks. Model evaluation has been discussed extensively 
elsewhere, so this section provides only an overview of the assessments that model 
developers, or others, should conduct to gauge the risks posed by a particular model equipped 
with a particular set of mitigations. These assessments should be conducted for all models, 
whether open or closed. 
 
Capability Assessment. It is necessary to first assess a model’s capabilities, and then 
determine how those capabilities may translate to risks at different levels of severity. 
Assessments should begin with the base models that lack any mitigations or built-in safety 
features, particularly if the developers will eventually release the weights (where such 
features can later be removed). These evaluations are not always straightforward, since some 
questions (e.g., whether a language model could assist with developing a bioweapon) may 
need to be assessed with controlled experiments involving human subjects, which is costly 
and takes time. 
 
For models intended to be deployed in certain use cases, models should also be tested for 
their potential to cause harm through ineffectiveness in that use case (see this report). 
Effectiveness evaluations include human interaction testing, which focuses on the experience 
of the human interacting with the system, and system testing, which examines the systems in 
which the model is embedded and how the model affects (and is affected by) those systems. 
Moreover, evaluations should consider likely outcomes when the model is used by trained, 
 
 
 
 
13 
untrained, or malicious users, as well as outcomes if the model is used in unintended use 
cases. 
 
Specific evaluations related to risks may include whether models can: 
● Produce convincing yet false or illegal content (e.g., this report) 
● Assist with or autonomously conduct cyberattacks (e.g., this report) 
● Assist with the design or production of chemical, biological, radiological, or nuclear 
weapons (e.g., this report) 
● Deceive human evaluators 
● Self-replicate or otherwise evade human control (e.g., this report) 
 
Specific evaluations related to benefits may include: 
● Economic productivity studies (e.g., this report) 
● Assessments of model performance on important scientific problems (e.g., this report) 
 
Mitigation Assessment. Second, it is necessary to assess any mitigations that are applied to 
reduce the risk of a model. Mitigations could include internal mitigations, such as those 
designed to alter the model to refuse certain prompts or increase its reliability, such as through 
reinforcement learning from human feedback. Mitigations could also include systemic 
mitigations external to the model, such as gene synthesis screening for biological risks. The 
effectiveness of any such mitigations must be rigorously evaluated in both ordinary scenarios, 
unusual (“out of distribution”) scenarios, and scenarios involving concerted adversaries seeking 
to contravene them.  
 
Uncertainty Assessment. Finally, evaluations of capabilities and associated mitigations are 
imperfect. In some cases, model capabilities have been discovered (or “elicited”) only after a 
model is developed and deployed. Mitigations can also be contravened through new 
mechanisms after they are released. The developer’s remaining uncertainty after these 
assessments should be explicitly taken into account when conducting risk analysis. Uncertainty 
is likely to reduce over time with testing and real-world use, but may initially be high. As we 
will describe below, assessing uncertainty is particularly important for open models. 
 
For any given model and release method, the core question is ‘do the mitigations available for 
that release method reduce risk to an acceptable level, taking uncertainty into account?’ Given 
the rapid development of foundation model research, these assessments should be conducted 
with all new systems and release methods, and regularly for those systems thereafter. 
 
 
 
 
14 
Moreover, developers should analyze the marginal risk that foundation models pose relative to 
the status quo by considering existing risks and defenses absent foundation models. 
 
For models where some degree of deployment (e.g., API-only access) poses an acceptable risk, 
model developers should then assess additional risks that could arise from making model 
weights widely available. Such assessments should include: 
● Estimates of the reliability of risk assessments. Since it is not possible to reverse the act 
of opening model weights, uncertainty in overall risk assessments may cause model 
weight release to be unacceptably risky in some cases. Therefore, model developers 
may need greater confidence in their assessments for risk to be brought to acceptable 
levels for models with open weights. 
● Estimates of the robustness of risk mitigations for open models. Mitigation measures 
external to the model, such as DNA synthesis screening for biological risks, are 
unaffected by the openness of model weights. However, measures internal to the 
model, such as fine-tuning designed to prevent models from responding to certain 
questions, can often be much more easily circumvented for models with open weights. 
If the risk posed by a particular model is acceptable only given some set of mitigations 
and those mitigations cannot be maintained in models with open weights, then opening 
the weights may be unacceptably risky. 
(5.b and 5.c) Are there effective ways to create safeguards around foundation models, either to 
ensure that model weights do not become available, or to protect system integrity or human 
well-being (including privacy) and reduce security risks in those cases where weights are 
widely available? 
Developers should adopt robust cybersecurity practices to ensure that model weights do not 
become available. This is similar to defending any other kind of intellectual property, but may 
also include efforts to identify and defend against methods for obtaining weights through 
queries to an API-only model (e.g., a recent paper showed that it was possible to obtain the 
weights of the final layer of LLMs simply by running a large number of specific queries). 
 
To reduce security risks in cases where model weights are widely available, both “internal” 
and “external” mitigations may help. 
 
First, mitigations internal to models could reduce risk, though it appears unlikely that robust 
solutions will be developed. For example, the unlearning literature focuses on removing 
dangerous information from models. Currently, most of this work is not robust to fine-tuning, 
but future research could potentially make it robust. Other work aims to make it difficult or 
 
 
 
 
15 
impossible for malicious actors to fine-tune models for certain use cases, but the methods are 
currently very limited and come at great expense. Given the degree of control afforded with 
access to model weights, there may never be practical internal mitigations. 
 
Second, mitigations external to models could reduce risk. For example, if a capable foundation 
model is effectively used for identifying and fixing software bugs, then the risk of making the 
weights of similar models widely available would be reduced, as it would become harder for 
malicious actors to use the model to find unresolved vulnerabilities. Similarly, DNA synthesis 
screening can help reduce risks from biological threats posed by future open models. Finally, 
“staged release” can help mitigate risk by providing models to trusted defenders, prior to 
making the weights available. While these external mitigations are essential and could 
substantially reduce the risks posed by open models, they may not be sufficient in all cases. 
(5.d) Are there ways to regain control over and/or restrict access to and/or limit use of weights 
of an open foundation model that, either inadvertently or purposely, have already become 
widely available? What are the approximate costs of these methods today? How reliable are 
they? 
Model weights can be readily shared and distributed on the internet. After weights are opened, 
it can be very difficult to “regain control,” restrict their access, or limit their use. While fully 
restricting their access and use is largely infeasible, there are ways to partially limit or restrict 
the ease with which they can be disseminated and used: 
● If the goal is to restrict access to a certain open model, then removing the model’s 
weights from platforms like Hugging Face and Github (which requires cooperation from 
the platforms) can make it more difficult, but by no means impossible, for actors to 
obtain them. This is likely the most feasible and least costly way to limit the 
dissemination of weights. However, this strategy has limitations, as the weights can 
still be shared on a myriad of other websites and platforms.  
● If model weights are leaked or otherwise released illegally, then there may be an 
opportunity for legal action against websites that host them. The prospects for 
pursuing legal action (or threats of it) to coerce actors to remove leaked weights from 
their websites depends on the actors. For some, particularly those on the dark web who 
already engage in illicit activity, it will be very difficult or impossible to get weights 
removed. Like many other illicit activities on the dark web (e.g., selling malware or 
ransomware-as-a-service), the threat of legal recourse is not an effective disincentive. 
● Many actors use cloud compute services to run or fine-tune models. There may be 
some opportunity for cloud service providers (CSP) to restrict users from running for 
fine-tuning certain models on their infrastructure. However, the CSP would have to 
 
 
 
 
16 
identify when restricted models are running on their hardware, which would be very 
challenging and potentially quite costly. Moreover, if actors alter the functionality 
and/or surrounding code of an open model, then the model’s signature may also 
change, which would make it more challenging for CSPs to know whether or not the 
model running on their infrastructure is restricted. 
 
Overall, there is no reliable way to fully restrict access to a model after the weights are open, 
but there are ways to limit their dissemination and use (some of which can come with high 
costs). 
(5.f) Which components of a foundation model need to be available, and to whom, in order to 
analyze, evaluate, certify, or red-team the model? To the extent possible, please identify 
specific evaluations or types of evaluations and the component(s) that need to be available for 
each. 
There are different types and degrees of all of these activities. It is not only a question of what 
components need to be available, but a question of what components may enable more 
thorough and robust degrees of analysis, evaluation, certification, and red-teaming. 
 
Models with publicly available weights fit along a spectrum of openness, and where they fit 
depends on the accessibility of their components (e.g., is the training data or surrounding code 
used to run or fine-tune the model readily available?), as well as the degree of transparency 
and documentation on how they were developed (e.g., how was the training data collected 
and processed? How was the model designed and evaluated?).  
 
All of these variables should be considered, as they can impact the ability to replicate and 
audit models, explain their outputs, scrutinize their functionality, and assess their capabilities 
and flaws. For example, if the goal is to scrutinize a model, then access to the training data 
may help explain its outputs; if the goal is to replicate a model, then access to most all of the 
components and development processes is typically necessary.  
 
However, more research is needed to gauge how different degrees of access and transparency 
can impact the ability to scrutinize or evaluate open models. For example, many open models 
come with documentation and model cards, but the level of detail in these documents can vary 
dramatically, and they can enable (or not enable) different degrees of evaluation.  
 
 
 
 
 
17 
Notwithstanding the need for more research in this area, below is a list of model components 
and information that, if made available, could improve analysis, evaluation, certification, and 
red-teaming: 
● Model Components 
○ Model Weights 
○ Inference code 
○ Fine-tuning code 
○ Training code 
○ Training data 
○ Auxiliary code (data code, custom libraries, etc.) 
● Model Information 
○ Paper, preprint, blog, and/or model card that describes the model (in varying 
degrees of detail)  
○ Documentation 
■ Performance evaluations and benchmarking 
■ Risk assessments 
■ Data preparation, provenance, and validation 
■ Configurations 
■ Checkpoints 
■ Hyperparameters 
■ Logs 
 
Note that even limited access to closed models enables a degree of analysis, evaluation, 
certification, and red-teaming. This is the case for closed models that are available through 
hosted or API access. This limited access still allows a user to prompt the model and analyze 
its outputs, which can help with assessing model performance (e.g., how well the model 
performs on benchmarks) and capabilities (e.g., can the model complete certain tasks). 
 
However, the ability to assess these capabilities is limited to just prompting the model and 
examining the outputs. The model may have latent capabilities that researchers fail to identify 
if they do not prompt the model in a way that causes it to exhibit those capabilities. If there is 
no documentation on how the model works or was designed, then the ability to scrutinize its 
functionality is constrained; if there is no information on the training data, then it is more 
difficult to assess its potential capabilities. If an API allows a user to fine-tune a closed model, 
then there is more room to evaluate it, as researchers can fine-tune it with more data and 
assess how the model’s behavior, performance, and capabilities change. However, API-based 
 
 
 
 
18 
fine-tuning is often quite limited (e.g., the ChatGPT API only allows a user to upload fine-
tuning data and set the number of training epochs). 
Governance Mechanisms 
(7.b) How might the wide availability of open foundation model weights facilitate, or else 
frustrate, government action in AI regulation? 
Open weights can help facilitate government action in AI regulation by providing more access, 
transparency, and visibility. It can reduce barriers for regulators and researchers to assess and 
experiment with models, which may improve multilateral collaboration on AI policy, as it 
creates a more transparent baseline from which all states can assess the technology. However, 
there are other ways to support these efforts, with or without open weights. Under the White 
House AI Executive Order, safety testing results must be reported for certain models, and in 
the UK, many developers provide the AI Safety Institute with access to their models, in some 
cases to unreleased models.  
 
Open weights can also frustrate government action by making it more difficult to monitor or 
prohibit misuse. Developers of open models cannot oversee how they are used or fine-tuned, 
so the onus for transparency will be on downstream actors. Regulations that prohibit certain AI 
use cases may be more difficult to enforce, given that more actors can use open models in 
unmonitored settings. Moreover, many open models are developed abroad, which further 
complicates unilateral AI policies and regulations. 
(7.d) What role, if any, should the U.S. government take in setting metrics for risk, creating 
standards for best practices, and/or supporting or restricting the availability of foundation 
model weights? 
First, assist in developing standards and best practices for determining whether the release of 
model weights is appropriate, based on risk analysis described in section 5.a. These standards 
should be developed at the NIST AI Safety Institute in collaboration with its Consortium. 
Standards should focus on the development of evaluations and mitigations to identify and 
bound risk at acceptable levels, as well as the establishment of risk tolerance levels to define 
“acceptable” risk.  
 
Second, assist in developing evaluations for measuring the benefits of open models. For 
models where risk is low and benefits are significant, the government could consider 
incentivizing developers to open model weights, such as by conditioning grants from the NSF 
 
 
 
 
19 
or NAIRR. This may be especially valuable for models designed for scientific, healthcare, or 
other beneficial tasks, that have been trained appropriately to minimize risk. 
 
Third, assist in establishing infrastructure for incident reporting (for both closed and open 
models) and supporting resources that track the interdependencies of foundation models. 
Incident reporting captures data about situations where AI systems cause harm, which allows 
researchers to track trends and helps prevent the harms from recurring. Data on AI harm can 
help policymakers and researchers monitor whether approaches to mitigate risks are effective, 
and determine where new approaches might be needed. Resources that track the 
interdependencies among foundation models (e.g., Ecosystem Graphs) can complement 
incident reporting. If a model is linked to a surge of AI incidents, then other models that share 
similar components may also produce similar harms. Tracking these interdependencies can 
help illustrate the degree of exposure to risk that is first identified by incidents. The U.S. 
government could develop internal incident reporting and interdependency tracking functions, 
or support the development of these functions by outside entities.  
(7.e) What should the role of model hosting services (e.g. HuggingFace, GitHub, etc.) be in 
making dual-use models with open weights more or less available? Should hosting services 
host models that do not meet certain safety standards? By whom should those standards be 
prescribed? 
As described in section 5.d, removing weights from hosting services may help reduce the 
dissemination of models that are deemed too risky. But as described in section 7.d, more work 
by industry and government is needed to establish standards around model safety, risk 
assessments (and strategies around releasing weights based on those risk assessments), and 
acceptable levels of risk. Without such standards, hosting services will have to unilaterally 
gauge what models they are willing to host, and what levels of risk they are willing to accept. 
(7.i) Are there effective mechanisms or procedures that can be used by the government or 
companies to make decisions regarding an appropriate degree of availability of model weights 
in a dual-use foundation model or the dual-use foundation model ecosystem? Are there 
methods for making effective decisions about open AI deployment that balance both benefits 
and risks? This may include responsible capability scaling policies, preparedness frameworks, 
et cetera. 
Responsible scaling policies and preparedness frameworks have been a promising first step 
towards risk management for dual-use foundation models, but so far have been applied 
primarily to closed models. Such policies could be expanded to provide guidance for opening 
model weights. Preparedness frameworks encourage proactive planning for future capabilities, 
 
 
 
 
20 
measurement and assessment of risk, and precommitment to risk mitigation. Such measures 
could be applied to future models before weights are released. 
Planning for the Future 
(8.b) Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the 
threshold, is the amount of computational resources required to build a model, such as the 
cutoff of 1026 integer or floating-point operations used in the Executive Order, a useful metric 
for thresholds to mitigate risk in the long-term, particularly for risks associated with wide 
availability of model weights. 
It is unclear how reliable compute thresholds will be over time, as well as what unintended 
consequences may result from them. Below we outline some pros and cons of training 
compute thresholds (in general), and the specific threshold of 10^26 operations. 
 
Pros 
● Compute is a readily quantifiable metric, and scaling laws have shown that large 
models trained with more compute tend to exhibit better performance than models 
trained with less compute. Subsequently, compute thresholds can be used by the 
government to identify and manage ‘frontier’ AI models. Based on publicly available 
data, no models have been trained on more than 10^26 operations. If the goal of the 
threshold is to roughly capture the space of ‘uncharted’ model training (and potential 
new risks stemming from models trained with that much compute), then the current 
threshold may be appropriate. If this is the goal, the threshold would likely need to be 
increased over time (and perhaps adjusted to include non-compute measures, 
depending on how the frontier of the field develops).  
● The compute that will be expended in training a model can be estimated with fairly 
high precision prior to training. Organizations that must comply with threshold-based 
requirements can anticipate which requirements are likely to apply to any given training 
run. 
 
Cons 
● There are diminishing returns and limitations to compute scaling, and much progress in 
AI has been due to algorithmic improvements (in addition to compute). Research has 
found that “the compute required to reach a set performance threshold has halved 
approximately every 8 months.” Therefore, it is unclear how much compute will be 
required to reach certain performance levels, and how reliable compute thresholds will 
be over time. 
 
 
 
 
21 
● Developers of the highest-performing models (i.e., Claude 3 Opus, GPT-4, and Gemini 
Pro) have not disclosed the amount of compute required to train their models. It is 
generally understood that these frontier models were likely trained on more compute 
than any other models seen to date, but without verified information, it is more difficult 
to gauge how compute thresholds can be used as a proxy for frontier model 
performance and capabilities. Some have roughly calculated the training compute of 
these models, but the calculations can be speculative and may have incorrect 
assumptions.  
● A compute-based threshold may not be appropriate if the goal is to encompass all 
models that may pose risks. Models that fall below the threshold may still pose risks, 
and as computational and algorithmic efficiency improves, more models with 
potentially risky capabilities are likely to fall below the threshold.  
● Currently, a 10^26 threshold would not be burdensome to most organizations, given 
that none have trained a model at or beyond that threshold. However: 
○ As compute improves and the cost of training models to 10^26 operations is 
reduced, more organizations will likely be burdened as they develop models 
that reach or exceed the threshold. 
○ If the threshold were to be lowered, then there is greater potential for undue 
burden to researchers and industry. Moreover, with lower thresholds comes 
more disclosures and red-teaming assessments from more organizations, but it 
is unclear if the government will have the expertise and resources to thoroughly 
vet the disclosed information. 
● Overestimating the utility of a 10^26 threshold may lead to complacency and political 
inaction in other areas, under the assumption that the threshold will reliably encompass 
models that pose the most serious potential risks. 
● Specific thresholds may incentivize developers to ‘game’ the metric, and train their 
models to a degree that falls below the threshold, thereby circumventing requirements 
to disclose information on their models. 
 
",CSET,10711
NTIA-2023-0009-0219," 
1 
Before the 
National Telecommunications and Information Administration 
Washington, DC 
 
 
In re 
 
Dual Use Foundation Artificial Intelligence 
Models With Widely Available Model 
Weights 
 
Docket No. 240216-0052 
 
 
COMMENTS OF 
COMPUTER & COMMUNICATIONS INDUSTRY ASSOCIATION 
The Computer & Communications Industry (CCIA)1 submits the following comments in 
response to NTIA’s February 26, 2024, Request for Comments.2   
CCIA is an international, not-for-profit trade association representing a broad cross 
section of communications and technology firms. For more than fifty years, CCIA has promoted 
open markets, open systems, and open networks.  CCIA members employ more than 1.6 million 
workers, invest more than $100 billion in research and development, and contribute trillions of 
dollars in productivity to the global economy.   
CCIA members are at the forefront of research and development in technological fields 
such as artificial intelligence and machine learning.  Many CCIA members also engage in open-
source software development and have learned a number of lessons regarding the benefits and 
challenges of this approach.  CCIA appreciates NTIA’s consideration of our comments regarding 
how those lessons may be applied in the context of artificial intelligence. 
I. 
Summary 
Open-source software is widely regarded as a generally successful approach to software 
development.  It now underlies much of our communications and the vast majority of Americans 
use systems based on open-source software (knowingly or unknowingly) on a daily basis.  Now, 
AI systems are experiencing a similar moment.  Open-source and partially public models are 
available, with new models and capabilities released regularly.  Some of these models are 
formally open-sourced, with copyleft or MIT licenses, while others are open to a varying degree 
in terms of model weights, datasets, and code.  This spectrum of approaches is not amenable to a 
simple “open/closed” binary, and NTIA should endeavor to analyze them on the basis of risk, 
rather than on what is and is not released and to whom. 
This approach to AI development brings new challenges, but also significant benefits.  
Open models provide the potential for better security, less bias, and lower costs to AI developers 
and users alike.  Open models also present advantages in AI governance, being easier to 
 
1 A list of CCIA members is available online at https://www.ccianet.org/about/members. 
2 89 Fed. Reg. 14059 (Feb. 26, 2024) (hereinafter “Request”). 
 
2 
understand and test.  Open models also compete with closed models, resulting in enhanced 
innovation and benefiting consumers and AI deployers alike.  While there are risks to open AI 
models, most of those risks are equally present in closed models and should not outweigh the 
significant benefits open development approaches can provide.   
Finally, there are significant efforts on AI risk evaluation and governance taking place 
across industry, non-governmental international organizations, and government agencies.  NTIA 
should work in partnership with these stakeholders to ensure that a common standard can be 
applied.  NTIA should also consider how it could best support the integration of AI into society 
via mechanisms similar to NTIA’s Internet For All Workforce Planning Guide. 
II. 
Defining “Open” And “Widely Available” 
Before discussing an appropriate definition of “open” or “widely available” models 
(referred to collectively as “public innovation” models hereafter), it is critical to emphasize that 
such a public innovation model is not necessarily a dual-use model, as that term is defined in the 
AI Executive Order.  Maintaining a clear distinction between dual-use and non-dual use public 
innovation models in the ultimate rule is critical to ensuring that open-source development of AI 
models remains a viable approach.  Absent a clear distinction, it is likely that these approaches 
will become effectively impossible in the United States for any but the best-funded AI 
developers.  This will prevent researchers and scientists in smaller labs from taking advantage of 
AI’s benefits.  It is also likely to harm security research into AI as researchers will be limited to 
only those interactions that a closed model permits. 
In order to minimize the possibility of such a conflation, it would be appropriate for any 
rules, regulations, benchmarks, recommendations, or other outputs from this process to employ 
an assessment of the dual-use nature of a model before any additional constraints based on its 
public innovation nature apply. 
A. 
Defining “open” 
As the Request acknowledges, there is no single category of public innovation model.  
Instead, there is a broad spectrum based on how much of the system is open and how that system 
is managed.  A fully open-source model, where everything is public from the training data to the 
implementation code to the model weights, will have different concerns than an open model in 
which only weights and code are made publicly available and both will differ from a public 
innovation model in which implementation code and a sample model, but not a fully trained 
model, is publicly available.  And beyond simple publication of aspects of a system, there are 
other traditional “openness” concerns observed in the context of open-source software to 
consider, such as contractual restrictions on modification or use of the released model.  In each 
of these cases, the appropriate metrics and approaches will differ accordingly. 
Instead of employing a binary open/not open categorization, NTIA should consider the 
full spectrum of possible public innovation approaches.  NTIA should apply a purely risk-based 
approach uniformly across closed models and public innovation models alike, with the openness 
being just one factor in evaluating the risk of the system.  Alternatively, NTIA could consider an 
approach in which certain aspects of any proposed rule are tied to specific characteristics of a 
model—for example, limiting some requirements or recommendations to only those models that 
publicly release weights. 
 
3 
B. 
Defining “widely available” 
It is impossible to set a specific threshold for “wide availability” and NTIA should not 
attempt to do so.  Instead, this categorization should be tied to risk assessment; “wide 
availability” of a conversational generative AI could require essentially completely public 
availability, while wide availability of a pesticide design AI might be triggered at a significantly 
lower number of distributions because of the foreseeable risk it might be used to create chemical 
weapons similar to a pesticide. 
C. 
Thresholding dual-use foundation models 
The current definition used for a dual-use foundation model is based on a particular 
amount of computation used in training the model.  This is a poor proxy for the capabilities of a 
model.  Instead, qualitative approaches in the vein of the AI Verify Toolkit should be used.  This 
will allow a model to be assessed more directly for foundation capability, rather than relying on 
the imperfect proxy of compute resources. 
III. 
Risks and Benefits of Public Innovation Models 
Wide availability of a public innovation model provides both risks and benefits.  While 
wide availability increases the possibility of misuse by making access easier to obtain, it also 
provides a number of benefits in terms of security, follow-on innovation, and reduced bias.  
NTIA should not put a thumb on the scale in favor of either public innovation or closed models, 
but rather assess based on the risk-benefit balance in any given case. 
A. 
Benefits 
The public innovation model in AI development benefits from many of the same 
advantages that open-source development has shown over the past decades.  This includes 
enhanced security via external code review and the ability to patch vulnerabilities without being 
forced to rely on a single vendor, reducing barriers to entry for SMEs by providing access to 
basic tools they can apply in their particular problem domain, positive economic impacts through 
competition with closed models and reductions in cost of operations due to lower software costs, 
and follow-on innovation via code being reused and repurposed for new applications.  And, 
while a minor benefit in some circumstances, the ability of a public innovation model to be run 
without a network connection may prove critical for AI intended for use in situations like 
disasters where telecommunications services cannot be guaranteed.  Alongside this benefit, the 
fact that a public innovation model can typically be run locally reduces some privacy and 
cybersecurity concerns. 
These benefits are not necessarily tied to any specific aspect of a model being open.  
Security review may rely more on code and less on weights, while the inverse may be true for 
the equity benefits of public innovation models.   
In public innovation AI models, there is an additional benefit—equity can be improved.  
While a developer should consider equity as they create a model, it is not guaranteed that they 
will do so or that they will be successful.  However, if the model is a public innovation model, 
others may be able to enhance the fairness and reduce biases in the model via their own 
independent contributions.  Those contributions then benefit other users of that model as they are 
incorporated into the public innovation model project. 
 
4 
The traditional benefits are even more apparent in the AI sphere because of the extremely 
high cost of initial training of a model.  This cost can easily run into millions or tens of millions 
of dollars, limiting access to AI models for those who cannot afford this entry cost.  Public 
innovation AI models allow others to benefit from training, as has been observed with the 
plethora of models built off of Meta’s LLaMA.3 
The economic benefits are a major portion of the advantages of a public innovation 
model.  Even with the limited availability of public innovation models to date, a huge amount of 
innovation has come out of reuse of those models.  For example, the public release of Stable 
Diffusion’s image generation model has allowed development of new tools for artists.4  Recent 
releases of multi-modal open-source models such as LLaVa are likely to extend use cases into 
applications such as accessibility (automatic generation of alt-text for web images that lack it) 
and multimedia production such as Apple’s MGIE5 instruction-based image editing.  As 
additional public innovation AI models with different and new capabilities continue to be 
released6 the number and variety of applications will continue to increase alongside them. 
B. 
Risks 
While public innovation models create many benefits, there are some risks.  However, 
those risks should not be overestimated and should be compared with the same risks from non-
public innovation models.  For example, while a malicious actor could employ a public 
innovation AI model, they could also employ prompt injection attacks to similarly employ a 
closed model.7  Remote desktop tools provide important capabilities in computing, but can be 
abused by cybercriminals.  Similarly, pen-testing tools provide critical capabilities to IT 
professionals, despite the possibility of their misuse by criminals.  The single largest differential 
risk is the inability to “claw back” a public innovation model.  Once publicly released, it will 
remain public.  However, this risk presumes that misuse of the model is easier than misuse of a 
closed model, which likely will not be the case in many circumstances, and that the risks of 
release outweigh the risks, costs, and lack of competition in a closed-only AI ecosystem. 
Further, there is a key difference between a public innovation model and a closed model 
in terms of risk surface.  With a closed model, there is the additional filter of whatever ‘glue’ is 
employed between the user and the model itself.  This is often where mitigations are placed, and 
because of this, bypass attacks are a frequent vector to try to misuse closed AI models.  In 
contrast, in a public innovation AI model, any mitigations must fundamentally be part of the 
model.  This may be more difficult to accomplish, but it also makes a public innovation far more 
resistant to attack after a mitigation is achieved. 
Only in the rare case where a true differential risk exists should NTIA focus on the 
open/closed spectrum, and the focus must go both ways—addressing situations, such as equity, 
where a public innovation model is likely to be less risky than a closed model. 
 
3 https://www.technologyreview.com/2023/05/12/1072950/open-source-ai-google-openai-eleuther-meta/ 
4 https://www.alpacaml.com/ 
5 https://github.com/apple/ml-mgie 
6 See, e.g., https://ai.google.dev/gemma/. 
7 https://simonwillison.net/2023/Nov/27/prompt-injection-explained/ 
 
5 
IV. 
Mechanisms For Risk Management 
Risk evaluation and benchmarking is a continually—and rapidly—evolving area of 
artificial intelligence technology.  Government agencies such as NIST, international 
organizations like the G7, private entity forums like the AI Alliance or AI Verify, and traditional 
standards bodies like ISO all are developing metrics for quantifying and qualifying risks of AI 
systems.   
Governance of AI systems is another area that is benefited by public innovation models.  
While it can be difficult to characterize a closed model due to limitations on access,8 public 
innovation models do not suffer from this problem.  As governance entities seek to create risk 
benchmarks and standards for AI models, this process will be made simpler and more effective 
because of the access to the model provided by public innovation approaches.  For example, 
benchmarking equity concerns related to a model is far simpler when that model is publicly 
available, and ensuring that any fixes are truly part of the model rather than being bandages 
slapped on top of a model that is still flawed is much simpler in a public innovation environment. 
CCIA urges NTIA to work with NIST on common standards for risk management, in line 
with the NIST AI Risk Management Framework, as well as other bodies engaged in the creation 
of tools and standards for AI risk management and evaluation.  A common approach will help 
minimize compliance burdens on SMEs who wish to engage in AI development, which is likely 
to become even more common as public innovation models continue to be released. 
V. 
Conclusion 
CCIA appreciates NTIA’s consideration of our comments and its attention to this 
important issue. 
 
Respectfully submitted, 
 
Joshua Landau 
 
 
 
 
Senior Counsel, Innovation Policy 
Computer & Communications Industry Association 
25 Massachusetts Ave NW 
Suite 300C 
Washington, DC 20001 
jlandau@ccianet.org  
 
 
 
 
8 See, e.g., https://hackingsemantics.xyz/2023/closed-baselines/; https://simonwillison.net/2023/Jun/4/closed-model-
training/. 
",CCIA,3116
NTIA-2023-0009-0233,"‭
548 Market St., PMB 90375‬
‭
San Francisco, CA 94104-5401‬
‭
412-837-9797‬
‭
anthropic.com‬
‭
March 27, 2024‬
‭
Bertram Lee‬
‭
National Telecommunications and Information Administration‬
‭
U.S. Department of Commerce‬
‭
1401 Constitution Avenue NW‬
‭
Washington, D.C. 20230‬
‭
RE:‬‭
National Telecommunications and Information Administration’s‬‭
(NTIA) Request for‬
‭
Comment (RFC) on Dual Use Foundation Artificial Intelligence Models With Widely Available‬
‭
Model Weights (Docket Number NTIA–2023–0009)‬
‭
Submitted via: Regulations.gov‬
‭
Thank you for the opportunity to respond to the‬‭
National Telecommunications and Information‬
‭
Administration’s‬‭
(NTIA) Request for Comment (RFC) on Dual Use Foundation Artificial‬
‭
Intelligence Models With Widely Available Model Weights. We understand that the purpose of‬
‭
this request is to “conduct a public consultation process and issue a report on the potential risks,‬
‭
benefits, other implications, and appropriate policy and regulatory approaches to dual-use‬
‭
foundation models for which the model weights are widely available.”‬
‭
1‬
‭
About Anthropic‬
‭
Anthropic is an AI safety and research company working to build reliable, interpretable, and‬
‭
steerable AI systems. Our mission is to develop and deploy advanced AI systems that are‬
‭
helpful to people. Core to our mission is the development of cutting-edge, or “frontier” large‬
‭
language models—we use these to conduct empirical safety research and as the main‬
‭
ingredient in the systems we deploy commercially.‬
‭
Introduction‬
‭
The vast majority of science has advanced over the years due to a culture of openness and‬
‭
transparency around research. This is especially true in the field of artificial intelligence, where‬
‭
many advancements have been made possible by the open publication of research. These‬
‭
openly shared innovations have served as building blocks for further technological‬
‭
developments.‬
‭
1‬‭
89 Fed. Reg. 14059 (Feb. 26, 2024). ​
​
‬
‭
At Anthropic, we recognize the importance of fostering the societal benefits that have stemmed‬
‭
from this open and transparent approach to research. Our core belief is that while the vast‬
‭
majority of today’s AI models are safe to release openly, that may not always be true in the‬
‭
future. As models become increasingly more capable,‬
‭
2‬‭
they may have the potential to lead to‬
‭
major misuses or catastrophic accidents. We believe that AI-driven accidents or misuses could‬
‭
lead regulators to take sudden or extreme actions, which we want to avoid. Therefore, we‬
‭
believe it is necessary for us to collectively design tests that effectively assess the potential for‬
‭
accidents and misuse of AI systems—whether proprietary or openly disseminated—and use‬
‭
these tests to guide policy solutions.‬
‭
We believe it is crucial that the government strike a balance between mitigating the potential‬
‭
safety risks of open source models, while also preserving robust innovation. Indeed,‬
‭
constraining the open dissemination of AI research and AI systems could have a chilling effect‬
‭
on competition in AI markets and personal liberties, which should be viewed as an extreme cost.‬
‭
We should only undertake such costly actions if we have direct and serious evidence that AI‬
‭
systems could cause accidents or enable misuse that have critical national security implications‬
‭
or substantial societal costs that cannot be reasonably mitigated.‬
‭
For this reason, our comment recommends that powerful general purpose AI models—those‬
‭
that are open source and proprietary—undergo standardized safety testing. We believe that‬
‭
developing and standardizing these safety tests will enable stakeholders across government,‬
‭
industry, academia, and civil society to gather information about the misuse and accident‬
‭
potential of such systems and use this to inform a (no doubt robust) debate about how to‬
‭
approach openly disseminated AI systems. We believe such a safety testing regime will‬
‭
incentivize model safety and create a safety race-to-the-top amongst industry actors who will‬
‭
strive to have the safest models on the market. It can also serve as the template for potential‬
‭
regulatory approaches—if, and only if, we arrive at safety measures that are viewed as‬
‭
legitimate indicators of serious accident or misuse.‬
‭
Mitigating Safety Risks of Powerful AI Models Through a Standardized Pre-Release‬
‭
Testing Regime‬
‭
As government, industry, academic, and civil society stakeholders work to balance responsible‬
‭
AI development and innovation in America’s rapidly evolving AI landscape, we must also‬
‭
understand the potential safety risks associated with frontier models (and mitigate any‬
‭
consequences therefrom). At Anthropic, we believe it is possible to mitigate these risks and‬
‭
level the playing field by requiring that general purpose open source‬‭
and‬‭
proprietary AI models‬
‭
go through a shared set of tests to generate information about their misuse and accident‬
‭
2‬‭
See‬‭
Written Testimony of Jack Clark, Co-Founder and‬‭
Head of Policy, Anthropic, Hearing on “‬
‭
Federal Science‬
‭
Agencies and the Promise of AI in Driving Scientific Discoveries‬
‭
,” Committee on Science, Space, and Technology‬
‭
United States House of Representatives (Feb. 6, 2024),‬‭
available at‬
‭
https://www.congress.gov/118/meeting/house/116790/witnesses/HHRG-118-SY15-Wstate-ClarkJ-20240206.pdf‬
‭
(accessed Mar. 27, 2024).‬
‭
potential. We believe such tests are an essential prerequisite to any regulatory conversation‬
‭
about these systems.‬
‭
A core tenet of AI safety is the ability to accurately describe and measure the capabilities and‬
‭
safety characteristics of AI systems. This ability is both an enabler and a prerequisite to effective‬
‭
regulation, as measurement tools allow us to objectively assess the capabilities of AI systems‬
‭
and ensure they meet appropriate safety thresholds. To achieve this, Anthropic strongly‬
‭
advocates for the establishment of a standardized safety testing regime conducted by an‬
‭
independent third party.‬
‭
We recently encouraged NIST, as the leading U.S. federal agency on measurement science and‬
‭
standards development, to prioritize the creation, operation, maintenance, verification, and‬
‭
sharing of results from authoritative benchmarks for AI systems.‬
‭
3‬‭
These benchmarks, developed‬
‭
and administered by an independent third party, will help establish norms and standardize‬
‭
practices for the evaluation and reporting of AI systems, ultimately driving transparency and‬
‭
trust in AI technology. It will also create a natural point of coordination for the AI sector:‬
‭
questions about the safety of AI systems are naturally complex and at times controversial, and‬
‭
actors like ourselves are not going to be viewed as impartial. Therefore, having a third-party‬
‭
take on this challenge has the best chance of generating the best information that we can use to‬
‭
discuss the safety of AI systems.‬
‭
This recommendation aligns with our June 2023 submission to NTIA, where we emphasized the‬
‭
crucial role of governments in supporting the development of rigorous capability and safety‬
‭
evaluations targeted at critical risks from advanced AI, such as deception and autonomy.‬
‭
4‬‭
By‬
‭
investing in third parties to carry out AI measurements, we can lay the ground for a‬
‭
comprehensive, independently administered testing regime that we can use to generate better‬
‭
information about AI systems.‬
‭
Ultimately, to achieve safety across a range of large-scale AI models, a testing regime must be‬
‭
easy to understand, have a low barrier for participation, and not pose an onerous cost to‬
‭
relatively small organizations seeking to openly and broadly disseminate models. Such a regime‬
‭
would ensure that all AI developers, regardless of their size or resources, can contribute to the‬
‭
advancement of AI technology while prioritizing safety and responsibility. This testing regime‬
‭
would also serve as a kind of regulatory laboratory, enabling the government, industry, and other‬
‭
stakeholders to develop different measures, run them against AI systems, then come together to‬
‭
discuss whether the results of these measurements are meaningful for policymakers. This‬
‭
would generate the evidence needed to develop potential regulatory approaches that tie these‬
‭
testing approaches to regulations about the ability to broadly deploy or release AI systems.‬
‭
4‬‭
Anthropic Comment on FR Doc # 2023-07776 (June 15, 2023),‬‭
available at‬
‭
https://www.regulations.gov/comment/NTIA-2023-0005-0640‬‭
(accessed Mar. 27, 2024).‬
‭
3‬‭
Anthropic Comment on FR Doc # 2023-28232 (Feb. 2, 2024),‬‭
available at‬
‭
https://www.regulations.gov/comment/NIST-2023-0009-0100‬‭
(accessed Mar. 27, 2024).‬
‭
Anthropic would welcome the opportunity to contribute to the creation of such safety standards‬
‭
via our work on model capability and evaluation and, in particular, our work on frontier red‬
‭
teaming for national security threats.‬
‭
Conclusion‬
‭
It is critical to drive transparency and trust in AI technology while ensuring appropriate safety‬
‭
thresholds are met before release. We believe the establishment of a standardized pre-release‬
‭
safety testing regime for powerful general-purpose AI models will best advance this goal in the‬
‭
near term and we strongly encourage the Department of Commerce (through NIST) to create‬
‭
benchmarks, guidelines, and best practices for evaluating and reporting on AI systems. In‬
‭
addition to further research on model evaluations, we would also suggest that governments,‬
‭
academia, and industry developers invest in research to explore opportunities to make models‬
‭
more resistant to fine tuning, given that it is currently easy to remove built-in safety guardrails‬
‭
from openly-released models. Such research, if successful, could mitigate many of the safety‬
‭
concerns unique to releasing powerful models openly. Thank you again for the opportunity to‬
‭
submit this comment and for NTIA’s leadership on this important issue.‬
",Anthropic,2753
NTIA-2023-0009-0144,"March 25, 2024
Dear National Telecommunications and Information Administration,
I am writing on behalf of Y Combinator to provide input on the critical issue of open-weight AI
models and their implications for innovation, competition, and national interests. As a key
stakeholder deeply engaged in the AI startup ecosystem, we believe it is essential to
thoughtfully consider the immense potential benefits of open models while proactively managing
associated challenges.
The degree of openness of AI models is a crucial factor shaping the trajectory of this
transformative technology. Highly open models, with weights accessible to a broad range of
developers, offer unparalleled opportunities to democratize AI capabilities and promote
innovation across domains. We have seen firsthand the incredible progress driven by open
models, with a growing number of startups harnessing these powerful tools to pioneer
groundbreaking applications.
Both open-weight AI models and closed-source models carry with them certain risks and
tradeoffs that must be carefully navigated. Open-weight models may have a heightened
potential for misuse, but they also allow for democratic contribution and oversight. Closed
models afford greater control and may mitigate certain dangers, but exacerbate others, including
the risks appurtenant to concentrating power among a few dominant players and weaker
defenses against adversarial attack, since the responsibility of identifying any pre-release
vulnerabilities, and then subsequently patching those vulnerabilities once discovered, falls solely
on the developer – potentially leading to slower fixes or a lack of transparency about the model's
security posture. The recent, well-documented issues with Gemini’s historical image generation
depictions offer a very low-stakes preview of how any deficiencies in closed-source models,
which are subject only to internal testing, may only be discovered once they are released into
the wild.1
We believe the benefits of openness ultimately outweigh the drawbacks. Given the extremely
broad user base and use cases of current and future AI models, it is nearly impossible to
identify all potential risks prior to launch. Open models foster transparency and accountability by
subjecting AI development to wider scrutiny and allow for broader societal input into their
1 See Google’s statement regarding Gemini here:
https://twitter.com/Google_Comms/status/1760354549481546035. “We’re aware that Gemini is offering
inaccuracies in some historical image generation depictions ... We’re working to improve these kinds of
depictions immediately. Gemini’s AI image generation does generate a wide range of people. And that’s
generally a good thing because people around the world use it. But it’s missing the mark here.”
560 20th Street, San Francisco, CA 94107
development. Open-weight models and the robust open-source communities around them
enable independent auditing and research into model behaviors, which then leads to technical
interventions (i.e., interventions related to the model’s code base/model function/wrappers) that
complement regulatory oversight by the government. A balance of technical and regulatory
intervention will be a necessary tool for oversight of trustworthy AI models; without technical
intervention, given the quick pace of innovation, regulators will be playing a perpetual game of
catch-up.
Open models are also essential for unlocking the full potential of AI as a general-purpose
technology that can be adapted to countless use cases. They level the playing field, enabling
startups and researchers to build on the latest advances. This decentralized innovation is key to
maintaining global competitiveness and technological leadership. Imposing strict limits on the
openness of model weights at this early stage risks constraining innovation and ceding
competitive advantage to nations that embrace openness. Instead, policymakers should aim to
proactively shape the ecosystem around open models to maximize benefits and mitigate harms.
To fully realize the benefits of open models while mitigating harms, we believe the role of
government should be to provide thoughtful guidance and support rather than unduly restrictive
controls. This could entail:
1. Developing clear guidelines and best practices for responsible open model development and
deployment, created in consultation with academia, small- and medium-sized technology
companies, larger incumbent technology companies, and civil society.
2. Investing in research to better understand and manage potential risks, including work on
model interpretability, robustness, and security.
3. Spurring development of tools and frameworks for responsible open model use, such as
privacy-preserving techniques, content filters, and monitoring systems for misuse.
4. Leading multi-stakeholder efforts to establish norms and standards around open model
development to uphold key values and protect national interests.
Y Combinator stands ready to work with NTIA and all stakeholders to realize the immense
promise of open-weight AI models while ensuring this technology develops in alignment with our
values. With foresight and proactive stewardship, we can harness open models as an incredible
tool to finally chip away at some of the world's most intractable challenges.
Thank you for considering our perspective on this pivotal issue.
Sincerely,
Garry Tan
President and CEO
Y Combinator
560 20th Street, San Francisco, CA 94107
",Y Combinator,1045
NTIA-2023-0009-0226," 
1 
Databricks, Inc. 
Response to the NTIA Request for Comment on 
Dual-Use Foundation Artificial Intelligence Models with Widely Available Model Weights 
Docket No. NTIA-2023-0009 
March 27, 2024 
  
Databricks, Inc. (Databricks) appreciates having the opportunity to respond to the National 
Telecommunications and Information Administration’s request for comment on Dual-Use Foundation 
Artificial Intelligence Models with Widely Available Model Weights. This topic is extremely important 
because of the many substantial benefits provided by allowing powerful AI models to be openly 
available, particularly the positive effects openness drives in enhancing AI democratization, 
innovation, research, competition, economic productivity and economic growth. 
Databricks’ Unique Vantage Point with Respect to AI Open Models and Their Benefits 
Databricks provides a high-performance cloud-based data processing and hosting platform, which is 
optimized, and heavily utilized, for AI services and applications. This platform and Databricks’ 
extensive experience with AI have enabled Databricks to become a leading provider of AI model 
development, customization, hosting and monitoring services. As part of these services, Databricks 
has developed and released high capability open models, including DBRX, currently the most capable 
open model available.1 Databricks has thousands of enterprise and government customers utilizing its 
AI related platform services around the world.  
An extremely important benefit of permitting open models is to give businesses and other 
organizations the ability to cost effectively obtain, control and modify their own AI models and AI 
applications using their own proprietary data, which in turn greatly enhances their ability to innovate, 
conduct research, and improve the functions of their organizations. Databricks is heavily engaged in 
helping organizations obtain, customize, run and monitor open models for such purposes. We are 
observing a rapidly growing number of enterprises and public sector organizations turning to open 
models because closed models present challenges relating to cost of ownership and operation, 
constraints on modifiability, and risks around the access to, and security of, sensitive data used in 
training and inference.2 
From this vantage point, Databricks has a distinctive perspective on the tremendous benefits open 
models provide to businesses and other organizations. The availability of open models offers 
organizations greater ability to use AI to significantly enhance their productivity, service and product 
quality, employee and customer experiences, and new product development, and these benefits from 
openness in AI are growing as the capabilities of AI improve. 
 
 
 
1 See Inside the Creation of the World’s Most Powerful Open Source AI Model, Wired, March 27, 2024, 
https://www.wired.com/story/dbrx-inside-the-creation-of-the-worlds-most-powerful-open-source-ai-
model/. See also Introducing DBRX, a New State-of-the-Art Open LLM, Databricks blog posting, March 27, 
2024, https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm. 
2 For a survey-based discussion of the growing interest in customizable open models among Fortune 500 
leaders, see 16 Changes to the Way Enterprises are Building and Buying Generative AI, Andreesen Horowitz 
blog posting, March 21, 2024, https://a16z.com/generative-ai-enterprise-2024/. 
 
2 
Summary of Databricks’ Views 
Our views, which are presented in detail in our responses to many of the questions presented in the 
Request for Comment, are summarized as follows (our responses to selected questions follow this 
summary): 
I. 
The benefits of open models substantially outweigh the marginal risks, so open weights 
should be allowed, even at the frontier level - The benefits of open models include their 
significant contribution to “democratizing” AI - i.e., broadening the dissemination of the 
advantages of AI to individuals, communities, small businesses, researchers and others, by 
making AI more affordable, accessible and transparent. Openness drives innovation, science, 
safety research, competition, economic productivity, and economic growth. The marginal risks of 
highly capable models that are open as compared to closed are not proven and are not 
incrementally significant. In contrast, the substantial benefits are clear and compelling, and are 
being amply demonstrated through the use of AI open models by numerous Databricks 
customers and others every day. Importantly, limiting frontier AI to closed models will 
concentrate the growing power of AI in the hands of a few dominant tech companies, restricting 
competition, innovation, transparency and breadth of adoption, as well as driving up costs and 
potentially limiting their application for beneficial uses. 
II. Regulation of highly capable AI models should focus on consumer-facing deployments and 
high risk deployments, with the obligations focused on the deployer - Regulation should be 
based on the risks related to the actual use case, with appropriate guardrails required at the time 
of deployment based on the specific use case. Obligations should focus on the deployer rather 
than the developer. There are two basic categories of deployment: (a) broadly consumer facing 
deployments (such as subscription access to ChatGPT, online social media platforms, etc.), where 
consumer-protection guardrails should generally be required (applicable to the deployer); and (b) 
business-to-business applications where appropriate guardrails may be required (applicable to 
the deployer) if the use is high risk (e.g., loan qualification, access to important services, use of 
personally identifiable information, etc.), but may instead involve relatively low risk uses where 
mandated guardrails are not warranted, with deployers allowed to exercise their judgment as to 
appropriate safety measures. For open models, any obligations applicable to the developer 
should apply to the development stage, up to and including reasonable documentation as of the 
model release date. 
III. If policy makers determine at some point that access to the model weights of certain ultra-
capable models should be gated, a registration “allow list” system should be utilized that 
strikes an appropriate balance, avoiding overly burdensome requirements that would deter 
innovation and competition except by a few dominant tech companies  - In Databricks’ view, 
such a gating system is not reasonably necessary under current model architectures, but under 
future architectures, perhaps associated with artificial general intelligence (AGI) or higher 
capability levels, and if risky model behaviors are reasonably anticipated, such a system may be 
justified. The registration system Databricks contemplates is described in response to question 
7(a). In summary, it would involve verification of identification, residency and contact 
information, standardized background checks, and checks against country, organization and 
individual deny lists. It would also require affirmation that: no distribution of model weights 
would occur except to other properly registered recipients; required safeguards would be 
maintained (or added, depending on use case); and use would not be in violation of any laws or 
 
3 
stipulated usage restrictions. The registration process would be comparable to the types of 
processes successfully used by FINRA for brokerage employees, for federal gun ownership 
background checks, and for the U.S. Global Entry (airport security clearance) program. 
Registration would be required of each individual given access to the model’s weights. Violations 
would be subject to regulatory enforcement and potential criminal sanctions in the case of willful 
violations. 
  
Databricks’ Responses to Specific Questions:  
Question 1. How should NTIA define “open” or “widely available” when thinking about foundation 
models and model weights? 
In evaluating the benefits, marginal risks and policy recommendations relating to open dual-use 
foundation models (“Open DUFMs”) as compared to closed dual-use foundation models (“Closed 
DUFMs”), the most important thing to consider in determining what “open” or “widely available” 
means is whether the model weights are readily accessible by members of the public in editable 
(modifiable) form for use with the model code. Edit-enabled access to the weights facilitates the 
user’s ability to make modifications to a model, whether the modification leads to a benefit or a 
risk. 
Of the primary elements of an AI model (training data, code and model weights) the ability to 
obtain and modify the model weights is most important to assist a user in scrutinizing or making 
modifications to the model. The weights represent the acquired “knowledge” and behavior-
setting of the model, and are the result of a very expensive and time-consuming investment in the 
training process. Changing the model weights allows for a broad, and at the same time very 
refinable, ability to impact the model’s outputs. As long as the weights can be applied to the 
model code to generate output, having the ability to modify the weights allows for change in the 
output and behavior of the model without the need to modify the code or the original training 
data. 
With use of the term “open” or “Open” throughout this document, we are referring to a model or 
AI system where the model weights are readily accessible by members of the public in editable 
(modifiable) form for use with the model code (whether or not the model code or training data are 
“open”). The term “readily accessible” as used in the prior sentence means that a person outside 
the organization that is developing, hosting or deploying the model can access and obtain the 
weights without the need for action by the publishing organization of the model specific to that 
individual and without willful circumvention of applicable technical constraints by the individual. 
In this document, we equate “widely available” with “open”. Please also note that our use of the 
term “open” or “Open DUFM” does not mean to imply that the model is, or should be, considered 
pure open source under standard definitions of “open source software”. 
Question 1(b). Is it possible to generally estimate the timeframe between the deployment of a closed 
model and the deployment of an open foundation model of similar performance on relevant tasks? How 
do you expect that timeframe to change? Based on what variables? How do you expect those variables to 
change in the coming months and years? 
Databricks believes that major open source model developers are not far behind the closed model 
developers in creating equally high performance models, and that the gap between the respective 
 
4 
development cycles may be closing.3 There is no technical constraint preventing open foundation 
models from preceding closed models of similar performance. The competitive threat from open 
models is a primary reason that the large developers of closed models are so keen to see AI 
regulation implemented that will slow down open model development. Not only are the open 
models less expensive for users to obtain and deploy, they are modifiable by the user and more 
flexible to cover a range of usage needs. The growth in the use of open models will lead to more 
interest by investors in the open AI ecosystem, and increased funding for developers of open 
models will enable them to catch up and perhaps surpass closed models - assuming AI regulation 
does not slow them down. Allowing these trends to continue is important to enable more people 
to reap the benefits of AI, and to nurture innovation and competition, and avoid a concentration 
of power, in the AI sector.  
Question 1(c). Should “wide availability” of model weights be defined by level of distribution? If so, at 
what level of distribution (e.g., 10,000 entities; 1 million entities; open publication; etc.) should model 
weights be presumed to be “widely available”? If not, how should NTIA define “wide availability?” 
Please see our response above to the lead-in question of this Section 1. 
Question 1(d). Do certain forms of access to an open foundation model (web applications, Application 
Programming Interfaces (API), local hosting, edge deployment) provide more or less benefit or more or 
less risk than others? Are these risks dependent on other details of the system or application enabling 
access? 
Please see our response above to the lead-in question of this Section 1, which contemplates 
editable access to model weights as a requirement for a model to be considered “open.” 
Databricks sees tremendous benefits from the use of open models tuned for special use cases by 
businesses and other organizations, and from open models being available for research and 
transparency, particularly for purposes of seeking AI safety and security. To achieve these 
benefits, the model weights need to be made fully available to the technical experts of 
organizations for modification and deployment and to researchers for experimentation and 
analysis. If access is restricted to occur only via an API, these benefits would not be available 
because the model weights could not be accessed, modified and tested. 
Question 2. How do the risks associated with making model weights widely available compare to the 
risks associated with non-public model weights? 
The most often cited marginal (incremental) risks of concern of Open DUFMs as compared to 
Closed DUFMs relate primarily to malicious use (intentional misuse or weaponization of the 
model), however it is important to note that virtually any technology can be weaponized or used 
maliciously in other ways, including Closed DUFMs and much simpler technologies as well. 
Examples with respect to Closed DUFMs include: the WormGPT, created by hackers to disable 
guardrails in GPT models to aid criminal activities4; a technique called AirPrompt developed to 
 
3 The DBRX open model introduced by Databricks on March 27, 2024, is more capable than GPT 3.5 based on 
widely accepted benchmarking, and challenges GPT 4 in several areas. See Introducing DBRX, a New State-of-
the-Art Open LLM, Databricks blog posting, March 27, 2024, https://www.databricks.com/blog/introducing-
dbrx-new-state-art-open-llm. 
4 How Criminals are Getting Help from AI Hacking Tools, Financial Review, March 25, 2024, 
https://www.afr.com/technology/one-ai-hacking-tool-has-fallen-others-will-rise-to-take-its-place-
20240305-p5fa1l 
 
5 
circumvent safety measures in closed models (and open models) to effectively trick the AI into 
responding to queries it was designed to reject, such as instructions on building bombs and 
making counterfeit money5; and a security flaw identified by researchers in closed vision language 
models allowing users to successfully make inquiries as to how to make a bomb6. 
Question 2(a). What, if any, are the risks associated with widely available model weights? How do these 
risks change, if at all, when the training data or source code associated with fine-tuning, pretraining, or 
deploying a model is simultaneously widely available? 
The biggest risks Databricks sees are the risks that would be created by prohibiting the wide 
availability of model weights: i.e., the risks to economic productivity benefitting a larger swath of 
society, innovation, science, competition, and AI transparency if Open DUFMs were not widely 
available. In particular, the greater competition provided by Open DUFMs will drive down costs, 
allowing a greater number of people and communities to participate in the benefits of AI. 
Databricks sees the advantages of open source AI every day. Databricks has well over a thousand 
enterprise and government customers using modified open models to achieve a wide range of 
important benefits. Moreover, from this vantage point, it is clear to Databricks that open models 
are having a direct and significantly positive impact on our economy’s overall productivity. Access 
to a model’s weights is necessary for a researcher to explore how the model works and for a user 
to make modifications to the model. For organizations wanting to re-train a model for a specific 
organizational purpose, the ability to modify the model is required. Without the ability to modify a 
model using open model weights, businesses and other organizations will be reliant on expensive, 
less flexible and opaque closed models. If the highest performance models are required to be 
closed, the benefits of AI will be significantly curtailed while AI’s risks may be increased, including 
harms from data leakage, inaccuracy, hallucination, environmental impact, etc. 
A recent study by Stanford HAI on the risks and opportunities presented by open models 
concluded that there is limited evidence of meaningful marginal risk in areas that are often cited 
as risks (such as bio terrorism, and enhanced phishing and other cybersecurity threats) from open 
models as compared to closed models or other existing (non-AI) technologies7. With respect to 
concerns about non-consensual imagery or child sexual abuse material, equally offending models 
can be much smaller (in terms of compute training thresholds) than any proposed ‘frontier model’ 
thresholds. The study also concluded that similar safety mechanisms for closed models are also 
vulnerable. If the marginal risk of open models vs. closed models or existing technology is not 
significant, then any regulation restricting open models would have the effect of impairing the 
substantial benefits they offer while achieving no significant gain in terms of reduced risk. 
With open models, a community of users and researchers work to identify and address issues and 
to understand how the software or application works. This benefits the entire user base, and 
outweighs the comparative risks associated with closed models regarding transparency and 
model explainability. Enabling a much wider audience to look inside the model to conduct 
 
5 Researchers jailbreak AI chatbots with ASCII art -- ArtPrompt bypasses safety measures to unlock malicious 
queries, March 7, 2024 https://www.tomshardware.com/tech-industry/artificial-intelligence/researchers-
jailbreak-ai-chatbots-with-ascii-art-artprompt-bypasses-safety-measures-to-unlock-malicious-queries. 
6 Scientists identify security flaw in AI query models, January 10, 2024, https://techxplore.com/news/2024-01-
scientists-flaw-ai-query.html#google_vignette. 
7 On the Societal Impact of Open Foundation Models, Stanford HAI, February 27, 2024, 
https://crfm.stanford.edu/open-fms/. 
 
6 
research helps advance the safety of both that model and future models, and helps advance the 
capabilities of future models. As an example, in its own model development, Databricks has 
significantly benefited from third party research enabled by the availability of the model weights 
and code of Llama 2 and other open models, assisting Databricks in developing greater safety and 
capability for its own models. The availability of such third party research means a greater 
diversity of input is incorporated into AI models, which is particularly important with respect to 
safety issues. 
In addition, with closed models, an organization desiring to modify the model must share its fine-
tuning data with the provider of the model. This exposed data may include valuable and sensitive 
confidential information, creating safety, security and privacy issues. With open models, the 
modifier can maintain control over its proprietary and sensitive data.  
If all DUFMs are required to be closed, there will be a substantial risk of extreme market 
concentration in the hands of a few big players and a lack of competition, limiting the availability 
of the advantages of AI and increasing its costs. This market concentration would also decrease 
incentives for improvement, limiting the pace of innovation. 
Question 2(b). Could open foundation models reduce equity in rights and safety-impacting AI systems 
(e.g. healthcare, education, criminal justice, housing, online platforms, etc.)? 
Because of the heightened scrutiny from the open community with access to an open model, we 
believe bias and other flaws of the model will be identified and addressed with greater speed. As 
with open software generally, an open model will have an army of private investigators looking for 
issues and raising them quickly. Arguably this attribute of openness is more important than ever 
with highly performant AI given its power and growing involvement in so many aspects of our 
lives.  
Question 2(e). What, if any, risks could result from differences in access to widely available models 
across different jurisdictions? 
Policy relating to the availability of powerful open models should be aligned internationally. 
Provision of powerful AI is a global business with limited ability to prevent transfer of AI model 
functionality across borders. Stricter standards in one jurisdiction places a compliance burden on 
good actors while not effectively preventing bad acts by bad actors. Similar concerns apply within 
the U.S. A national approach at the federal level with explicit preemption is important to avoid a 
patchwork of state laws that would unnecessarily increase compliance burdens while not 
meaningfully restricting the conduct of bad actors. 
Question 2(f). Which are the most severe, and which are the most likely risks described in answering the 
questions above? How do these sets of risks relate to each other, if at all? 
Databricks believes the biggest risk considered in this discussion is the economic risk that would 
arise from giving a small number of closed model providers an effective oligopoly by banning or 
creating onerous regulatory hurdles to Open DUFMs. This risk of reduced competition is certain, 
and substantial in its potential impact. The concentration of power would not only increase costs, 
it would limit accessibility to powerful AI and its benefits, and it would slow the pace of 
innovation, as well as potentially facilitating the development of “walled gardens” / gatekeeper 
systems where the Closed DUFM owners stifle competition within the broader AI ecosystem. In 
terms of safety and security, we believe that the clear advantages of transparency and earlier 
 
7 
identification of risks and mitigation pathways offered by Open DUFMs largely offset the uncertain 
perceived safety risks of Open DUFMs, and that the likelihood of these perceived safety risks being 
incrementally addressed in a meaningful way by making model weights not widely available is 
very small.  
Question 3. What are the benefits of foundation models with model weights that are widely 
available as compared to fully closed models? 
Open models provide substantial benefits, including: (a) broad accessibility for research, 
transparency, explainability, and risk identification/mitigation that significantly contribute to 
improving safety and security - what amounts to “crowd sourced” quality control; (b) accelerated 
innovation through greater accessibility, broader input, collaborative improvements and a more 
rapid innovation cycle; (c) democratization of AI by virtue of broader availability, reduced costs, 
and allowing user modifications for special use cases and experimentation; (d) creating 
competition and avoiding the concentration of power in AI in the hands of a few big closed model 
providers; (e) allowing organizations to use models they own and control to fine tune them with 
their secured proprietary data without exposing it to a closed model provider over the web via an 
API or in other manners; (f) the potential ability to modify models in ways that could moderate 
environmental impact by reducing the amount of compute used for inference for special use 
cases; and (g) greater potential diversity in terms of demographic groups (ethnic, gender, 
geographic location, etc.) that will have the chance to contribute to (and benefit from) model 
innovation, scrutiny and risk identification. All of these benefits of open models are applicable at 
the frontier (“dual-use”) level in addition to being applicable to less capable open models. With 
respect to (f) (modifications that could limit use of compute that may reduce environmental 
impact), a very large, capable model could be modified in ways that could reduce compute usage 
by bypassing or deleting certain elements of the full model, but such modification would generally 
only be practical if the modifier has access to the model weights. 
Question 3(a). What benefits do open model weights offer for competition and innovation, both in the AI 
marketplace and in other areas of the economy? In what ways can open dual-use foundation models 
enable or enhance scientific research, as well as education/training in computer science and related 
fields? 
As with open source software generally, open models foster collaboration and rapid 
experimentation by the open model community members. Collaborative research is easier, and 
innovative breakthroughs are achieved sooner. The avoidance of API access fees lowers costs, 
thereby increasing competition. Faster innovation, greater ability to customize for special use 
cases and lower costs will improve productivity and competitiveness in parts of the economy 
outside the AI sector. Databricks is seeing this improvement in productivity and innovation in 
working with thousands of customers on AI implementations in virtually every sector. As just a few 
examples, Databricks customers are using AI based on open models to: significantly reduce 
software product development timeframes; accelerate pharmaceutical R&D lifecycles; increase 
the efficiency of telecommunications, transportation and freight networks; positively impact 
climate change by increasing efficiency in the energy sector; enhance end-customer experiences 
in gathering information about products and services; and improve employee job quality by 
reducing repetitive, tedious work that can lead to high worker burnout and turnover. From a 
macroeconomic point of view, a recent study by McKinsey concluded that generative AI models 
 
8 
could generate $4 trillion or more in additional annual global growth8. The heightened capabilities 
of dual-use foundation models when they become available will cause these productivity and 
innovation benefits to grow to even greater levels. 
Permitting a business or other organization to control their own models, including the model 
weights, lets them move their models from one vendor data platform to another, avoiding the 
problem of vendor lock-in and increasing competition within the AI sector. If the model weights 
are not available, the model cannot be moved to a new vendor. Even where a Closed DUFM 
provider facilitates some sort of modifiability of a model, the model is still controlled by that 
Closed DUFM provider and the customer does not have the ability to transport the model as 
modified to a new vendor platform of its choice. The more powerful AI becomes, the greater the 
dangers to society from having the power over it concentrated in a few hands. 
Collaboration with the academic community on AI science is crucial, to enhance innovation and 
efficiency but also importantly to help identify and understand safety and security issues posed by 
AI and find ways to address AI risk. A recent research paper on AI model security vulnerabilities, 
shows how access to model weights enables scrutiny of model vulnerabilities and can lead to 
security improvements for both closed and open models9. Another recent research paper on AI 
model privacy vulnerabilities demonstrates the same advantage with respect to data privacy10. In 
a recent research paper from Stanford HAI, 25 leading AI experts across industry, academia, and 
civil society concluded that “model weights are essential for several forms of research across AI 
interpretability, security, and safety” and that “model weights enable external researchers, 
auditors, and journalists to investigate and scrutinize foundation models more deeply.”11  
If cutting edge AI models are required to be closed, the ability for academics to conduct such 
research will be severely limited and the world will have to rely largely on the commercial frontier 
model developers themselves to address safety and security issues posed by cutting edge models, 
subject to whatever incentives and disincentives may apply to them. Inevitably, commercial 
developers will be more likely to prioritize factors they deem important to commercial success, 
whereas academic researchers will have relatively more interest in focusing on a broader range of 
areas, including safety, security, and other risks. If we deem the risks of the most capable models 
to be particularly high, we should prioritize the openness of such models to better enable research 
by academics and others outside the realm of what will likely be a very small number of large 
developers of such models. As more than 1,800 people involved in AI research, analysis, policy and 
entrepreneurship have declared in a recent “Joint Statement on AI Safety and Openness”, “If our 
objectives are safety, security and accountability, then openness and transparency are essential 
ingredients to get us there.”12 
 
8 The economic potential of generative AI: the next productivity frontier, McKinsey Digital, June 14, 2023, 
https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-
generative-ai-the-next-productivity-frontier#introduction. 
9 MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots, October 25, 2023, 
https://arxiv.org/abs/2307.08715. 
10 Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs, March 5, 2024, 
https://arxiv.org/abs/2403.04801. 
11 On the Societal Impact of Open Foundation Models, Stanford HAI, February 27, 2024, 
https://crfm.stanford.edu/open-fms/. 
12 Joint Statement on AI Safety and Openness, October 31, 2023, https://open.mozilla.org/letter/. 
 
9 
As one example of the value of open models weighed against their risks, in an important area of 
scientific research in the medical arena where AI risks are often highlighted, more than 150 
leading scientists recently signed a statement affirming their view that “the benefits of current 
AI technologies for protein design far outweigh the potential for harm,” noting that “many 
researchers in our community benefit from open-source scientific software, which has enabled 
rapid innovation and broad collaboration.”13 
Question 3(b). How can making model weights widely available improve the safety, security, and 
trustworthiness of AI and the robustness of public preparedness against potential AI risks? 
Open access to model weights increases transparency, allowing a broader community of 
researchers, AI, cybersecurity and other experts, developers, and the public to understand how 
models make decisions and to participate in identifying and fixing security vulnerabilities. Without 
the model weights, and limited to any code or data that might be available, these parties will be 
less able to understand the model’s functionality and to run experimentation. By making it easier 
for external parties to scrutinize AI models, the AI community can work collaboratively to address 
and mitigate potential risks, including biases, errors and vulnerabilities, leading to safer and more 
reliable AI systems.  
Please also see our response to the prior question 3(a) regarding the importance of open 
collaboration with academic researchers looking into AI safety and security issues. 
Question 3(c). Could open model weights, and in particular the ability to retrain models, help advance 
equity in rights and safety-impacting AI systems (e.g. healthcare, education, criminal justice, housing, 
online platforms etc.)? 
The heightened scrutiny and modifiability of open models will lead to earlier detection and 
mitigation of bias and other flaws of the model. An open model will have an army of private 
researchers looking for issues and raising them quickly. This also adds the benefit of greater 
demographic and geographic diversity in terms of who is scrutinizing the model for bias and other 
issues. Arguably this attribute of openness is more important than ever with AI, given its power 
and growing involvement in so many aspects of our lives. The more powerful AI becomes, the 
more important this widespread and diverse scrutiny becomes. 
Additionally, the ability to readily retrain an open model lets the deployer more effectively, 
efficiently and swiftly implement modifications and other measures that can address discovered 
bias or other flaws in the model. 
Question 3(d).  How can the diffusion of AI models with widely available weights support the United 
States’ national security interests? How could it interfere with, or further the enjoyment and protection of 
human rights within and outside of the United States? 
The availability of highly capable open, modifiable models permits U.S. government agencies to 
affordably own and control their deployments of such technology in a manner that lets the 
agencies tailor them to their needs and maintain security for classified information. High end AI 
technology is expected to be very helpful in analyzing national security information. If all high end 
models are closed, the U.S. government would incur much greater costs in being able to tap such 
technology in a manner that would maintain adequate security for classified information. Without 
 
13 Community Values, Guiding Principles, and Commitments for the Responsible Development of AI for Protein 
Design, March 8, 2024, https://responsiblebiodesign.ai/. 
 
10 
Open DUFMs, the U.S. government would either have to devote substantial resources to building 
its own such models, or it would have to incur substantial costs in procuring them (or obtaining 
sufficiently secured access) from one of the few large technology companies able to offer such 
models. 
Question 4. Are there other relevant components of open foundation models that, if 
simultaneously widely available, would change the risks or benefits presented by widely 
available model weights? If so, please list them and explain their impact. 
Making the training data widely available provides the benefits of facilitating analysis to identify 
potential bias, harmful content and other risks, while creating little if any incremental risk, so 
there should be no regulatory restrictions on the availability of training data. 
Making the model code widely available in addition to the model weights provides the benefits of 
incremental transparency in evaluating the model, including to identify and mitigate potential 
risks, and it can facilitate the ability to modify the model for beneficial purposes. It can also 
facilitate research, innovation and the democratization of AI. Although providing access to the 
code if the model weights are already available does marginally increase risk by making it easier 
to use, modify and potentially misuse the model, availability of the model weights is the crucial 
part of the model required for its independent use and modification. The incremental risk of 
allowing access to the code when the weights are available is made even less important from a 
practical point of view due to the fact that the type of skilled operators able to use open model 
weights to modify a model are likely also skilled enough to reverse engineer the code with use of 
the model weights. On balance, the transparency and other benefits of allowing open code 
significantly outweigh the marginal risk prevention afforded by preventing access to the code. 
Question 5(b). Are there effective ways to create safeguards around foundation models, either to ensure 
that model weights do not become available, or to protect system integrity or human well-being 
(including privacy) and reduce security risks in those cases where weights are widely available? 
Safeguards can be, and are, built into both open and closed models. Employing a system that 
provides unified governance across the disparate elements related to the AI lifecycle is important 
in enabling robust, seamless access control as part of keeping model weights and other aspects of 
the AI system secure prior to intended release. As an example of such a system, Databricks offers 
Unity Catalog14, an advanced unified governance, permissioning, versioning and lineage tracking 
system that integrates with the data and AI workflow. The typical non-unified approach creates 
potential gaps in governance coverage where governance is shifted from one governing element 
to another. 
Although making model weights available can assist a bad actor in disabling the safeguards built 
into an open model, it is worth noting that the safeguards in closed models can also be “broken” 
by a determined and knowledgeable actor. However, with open models it is easier to quickly build 
a collective understanding of what might have gone wrong, and to make fundamental changes to 
address the problem. 
Question5(c).  What are the prospects for developing effective safeguards in the future? 
Databricks believes measures to maintain safeguards as part of open models and/or to disable 
open models when attempts are made to use the models for malicious acts may be achievable in 
 
14 See https://docs.databricks.com/en/data-governance/unity-catalog/index.html. 
 
11 
the future, and that research in these areas should be encouraged. Databricks believes that 
encouraging this research is a better policy focus than restricting access to model weights in light 
of the substantial benefits open models provide to society. 
Question 5(e). What if any secure storage techniques or practices could be considered necessary to 
prevent unintentional distribution of model weights? 
Unified governance is important. Please see our response to question 5(b). 
Question 5(f). Which components of a foundation model need to be available, and to whom, in order to 
analyze, evaluate, certify, or red-team the model? To the extent possible, please identify specific 
evaluations or types of evaluations and the component(s) that need to be available for each. 
For research aimed at risk mitigation, it is beneficial to provide researchers access to the model 
code in addition to the model weights. Between these elements, the availability of model weights 
has the greatest impact on risk. While the additional availability of the code doesn't significantly 
increase risk, such availability would aid in research efforts to identify and address potential risks. 
To fully leverage the benefits of open models, both components should be accessible. Assuming 
model weights are provided, the incremental risk posed by making the code available is minimal. 
Accepting this minor risk is worthwhile considering the significant benefits achievable. 
Question 6(a). In which ways is open-source software policy analogous (or not) to the availability of 
model weights? Are there lessons we can learn from the history and ecosystem of open-source software, 
open data, and other “open” initiatives for open foundation models, particularly the availability of model 
weights? 
Much of the technology we benefit from every day is built upon open source software because the 
open source ecosystem has fostered significant innovation, system quality, and ease of adoption. 
Open models should be allowed so that this contribution to innovation, quality and growth can 
continue in the field of AI, as AI rapidly becomes an ever more important part of our economy. 
One legal issue to address is the allocation of liability when use of an open model, including an 
Open DUFM, causes harm, in cases where the end user is not responsible. Generally, the deployer 
of the model or AI system, rather than the model developer, should be responsible because the 
deployer is selecting the use case, configuring the application, has better visibility into the actual 
and potential risks, and has responsibility for making sure the appropriate safeguards are in place. 
With a consumer-facing application, the deployer should bear the burden of responsibility for any 
heightened standards applicable to consumer-facing deployments, including addressing any risk 
of harmful content being generated by the underlying model. Exposing open model developers to 
liability for such downstream harms would constrain innovation in open model development and 
limit realization of many of the significant benefits of open models. The developer should have the 
ability to disclaim liability and seek indemnities as part of the license it provides for open use of 
the model. Without the ability to allocate such risks within the AI supply chain, open model 
development and the substantial benefits it brings would be impaired. 
Question 6(b). How, if at all, does the wide availability of model weights change the competition 
dynamics in the broader economy, specifically looking at industries such as but not limited to healthcare, 
marketing, and education? 
The availability of open models significantly enhances competition among AI providers, which 
benefits users in all sectors of the economy. If the most capable models were restricted to closed 
 
12 
models, the concentration of market power in the AI sector would be significant, which would be 
dangerous not only from a competition and innovation point of view, but from a societal 
perspective. The power of AI should not be held exclusively in the hands of a few giant technology 
companies. In addition, society as a whole benefits if costs are reduced and productivity is 
enhanced. The “democratization” of AI that open models provide will help extend the benefits of 
cost reduction and productivity to everyone. Given how important AI will become in workflows 
throughout our economy, these benefits will have substantial impact. 
The availability of open models gives organizations that don’t develop highly capable models 
themselves the ability to actually own and securely control the model they use in making their 
organization more productive, giving them more control, including over their sensitive data used 
in retraining and fine-tuning. Limiting the most powerful AI to closed models would put a few 
large model developers in ultimate control of the cutting edge of AI, with everyone else dependent 
on their judgment and oversight, while at the same time allowing that small handful of parties to 
exclusively extract the value from applying AI across a broad swath of industries. Ultimately we 
face a choice between centralized control over the cutting edge power of AI in the hands of a few 
big commercial players, versus decentralized control.  
In a world where only closed highly capable models exist, an organization wishing to operate its 
own such model, with the ability to securely modify for its own use cases over time, would have to 
build it from scratch, which would likely be prohibitively expensive and time consuming, meaning 
it could not practically do so. The result would be to force the organization to pay expensive 
access fees to an oligopolist provider of highly capable models, and to forgo the ability to freely 
make modifications and realize the other benefits of being able to control the model it wants to 
deploy. 
Question 6(c). How, if at all, do intellectual property-related issues—such as the license terms under 
which foundation model weights are made publicly available—influence competition, benefits, and 
risks? Which licenses are most prominent in the context of making model weights widely available? What 
are the tradeoffs associated with each of these licenses? 
Open model licenses vary somewhat, with some (e.g., the Llama 2 Community License Agreement 
- the “L2CLA”) restricting commercial use to a certain extent. The L2CLA prohibits use of Llama 2 
to train other models, and requires certain extremely large companies (those with over 700 million 
monthly active subscribers) to obtain a separate license from Meta. The Databricks Open Model 
License (“DOML”), under which the open model DBRX was released on March 27, 2024, has similar 
provisions. Despite these restrictions, Llama 2 and DBRX in large part provide the key advantages 
of open source: they are generally available free of charge, to virtually anyone, including for most 
commercial purposes; the model weights and code are open for research, modification, and fine-
tuning for specialized use cases; and their openness promotes innovation and democratization of 
AI. Although not considered by purists to be true “open source”, these models generally provide 
the benefits of openness we have discussed in our responses. Unlike typical open source licenses, 
these licenses require the licensee to indemnify the developer (Meta or Databricks) for third party 
claims arising from downstream use under the licenses. The L2CLA and the DOML address certain 
concerns of developers while preserving much of the benefit of open source. We expect other 
open model developers will consider using licenses similar to the L2CLA and the DOML going 
forward. 
 
13 
Prohibitions on specified harmful uses contained in Responsible AI Licenses (RAIL) and Acceptable 
Use Policies (AUP) (like the ones provided by Meta and Databricks as part of the L2CLA and the 
DOML, respectively) can be helpful in lessening the chance of harmful use, although they do not 
provide technical barriers against determined bad actors. 
Question 7. What are current or potential voluntary, domestic regulatory, and international 
mechanisms to manage the risks and maximize the benefits of foundation models with widely 
available weights? What kind of entities should take a leadership role across which features of 
governance? 
Because of the vast advantages of open models, as discussed in response to prior questions, 
Databricks feels AI regulation and requested voluntary commitments should be very carefully 
tailored to not impede open model development. Regulation of highly capable foundation models 
should be directed to requirements applicable during the pre-release development stage, 
including documentation requirements as of release date. Databricks believes that regulatory 
scrutiny may be appropriate at the time of deployment of an AI system that constitutes or is built 
upon an Open DUFM, with appropriate additional regulatory requirements applied if the 
deployment represents a high risk use case, but the regulatory obligations applicable at the time 
of deployment should be the responsibility of the deployer rather than the developer since the 
deployer (which in some cases may also be the developer) has better control and visibility 
regarding the actual usage of the model or system. Deployment of a system or application 
incorporating a DUFM (whether closed or open) in a consumer-facing manner should be subject to 
requirements placed upon the deployer appropriate to consumer deployments, including the 
implementation of standard safeguards preventing generation of harmful content and any user-
facing transparency requirements. In addition, consumer-facing applications incorporating Open 
DUFMs should not allow consumer users to access the model weights through the application. 
The deployer should be required to test the application sufficiently prior to release to confirm that 
any retraining or fine-tuning did not remove or deteriorate preexisting safeguards built into the 
model by the original developer. 
Question 7(a). What security, legal, or other measures can reasonably be employed to reliably prevent 
wide availability of access to a foundation model’s weights, or limit their end use? 
Databricks believes regulation should allow open models at all capability levels because the 
substantial benefits of openness meaningfully outweigh the largely unproven marginal risks 
openness may present. However, Databricks realizes that at some point AI systems will arise that 
are so powerful that policy makers are likely to require that broad access to the inner workings of 
the AI systems (whether model weights or some other future crucial element) be gated in some 
manner. We think the day gating might be warranted is far in the future, and likely to involve AI 
architectures that are substantially different from architectures used in today’s leading generative 
models, so crafting requirements currently would be premature and unlikely to address the 
challenges presented at that time. That said, if a gating requirement ever is considered, 
Databricks feels it should take the form of an “allow listing” process whereby those provided 
access to the gated AI system’s crucial element(s) (e.g., model weights, if still the relevant factor) 
must verifiably register to gain access. This registration should apply to anyone with access to a 
qualifying model’s weights, whether the model is open or closed. Registration should involve 
verification of identification (including a fingerprinting process similar to that conducted for 
employees of FINRA-registered broker dealers), collection of detailed contact information, 
 
14 
background checks (similar to the federal background checks to permit gun ownership), 
verification of residency, a check against official federal “deny lists” (e.g., sanctioned or otherwise 
prohibited countries, entities or individuals, etc.), and disclosure of intended use. The registrant 
would also be required to affirm their commitment to: (a) maintain any built in safeguards which 
were documented by the developer at time of release; (b) adhere to any restrictions on use 
contained in the applicable open model license, including any Acceptable Use Policies; (c) limit 
any redistribution giving access to the crucial element(s) to only parties who have verifiably 
registered pursuant to the same process; (d) limit any deployment to other internal users or 
customers solely to implementations that do not provide access to the crucial element(s) (e.g., 
model weights), and only if proper security safeguards are in place; and (e) not deploy in any 
specified high risk manner or consumer-facing manner unless any separate regulations relating to 
such deployments of highly capable models are fully adhered to. Both developers and distributors 
(including open source repositories) would be required to verify proper registration by all 
potential distributees prior to distribution of a qualifying model’s weights to the potential 
distributee. Willful violations of the registration process would be subject to penalties severe 
enough to constitute a meaningful deterrent. An organization would need to register every 
employee having access to the crucial element(s). A developer would have an obligation to 
register at the point it reasonably becomes aware that a model in development meets whatever 
the model qualification standard is at the time. 
This registration process would increase the costs and risks faced by potential bad actors, 
increasing the chances of interdiction, enforcement and penalty. It would therefore be expected 
to significantly reduce the probability of malicious use. It is true that a bad actor could 
successfully register and then simply download the model weights to a thumb drive and sell them 
to a black market buyer, but note that the same could occur with employees of a closed model 
developer. In either case, the registration and enforcement system would lessen the probabilities 
of the occurrence of malicious acts. 
Question 7(c). When, if ever, should entities deploying AI disclose to users or the general public that they 
are using open foundation models either with or without widely available weights? 
Based on the balance of burdens versus benefits, it would not be justified to require a special 
obligation to disclose to users that they are dealing with a model or AI application where the 
model weights are widely available, except potentially in the case of a consumer-facing 
application (where special requirements should apply to both Closed and Open DUFMs), and any 
such requirement should be placed on the deployer and not the developer. Any such requirement 
placed on the deployer should be carefully constructed to apply only where reasonably necessary. 
Question 7(d). What role, if any, should the U.S. government take in setting metrics for risk, creating 
standards for best practices, and/or supporting or restricting the availability of foundation model 
weights? 
It is important that any required standards and restrictions be national in scope, to avoid a 
patchwork of regulations at the state level. AI applications and services are typically cloud based, 
and are delivered on a widespread basis, often globally. The necessity of dealing with a patchwork 
of regulatory requirements would be an onerous burden for deployers. Databricks supports the 
principles and guidance set forth in the NIST AI Risk Management Framework as a basis for 
national AI policy and standard setting. 
 
15 
Question 7(e). What should the role of model hosting services (e.g. HuggingFace, GitHub, etc.) be in 
making dual-use models with open weights more or less available? Should hosting services host models 
that do not meet certain safety standards? By whom should those standards be prescribed? 
If a registration system like the one referenced in our response to question 7(a) is ever 
contemplated, the model hosting services such as HuggingFace and GitHub would be required to 
limit distribution to verified registrants, with respect to those open models where registration is 
required. 
Question 8(a). How should these potentially competing interests of innovation, competition, and 
security be addressed or balanced? 
Open models offer substantial benefits relating to innovation, safety and security research, 
competition, lower costs, prevention of the concentration of the power of AI in the hands of a few, 
broadening the benefits of AI, and ease of customization for a large range of beneficial purposes. 
These significant benefits should be taken into consideration in weighing whether to take steps to 
restrict open models to achieve theoretical reductions in marginal risk. Ultimately the question is 
whether it is worth losing these many benefits, and leaving the power of AI in the hands of a few 
large technology companies, when the addressable marginal risk of open models is uncertain and 
can potentially be mitigated by other less disadvantageous means. 
Question 8(b). Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the 
threshold, is the amount of computational resources required to build a model, such as the cutoff of 1026 
integer or floating-point operations used in the Executive Order, a useful metric for thresholds to 
mitigate risk in the long-term, particularly for risks associated with wide availability of model weights? 
When setting thresholds, it is crucial to ensure that open models fine-tuned or customized for the 
specific needs of an enterprise or government organization not become classified as high risk 
simply due to the additional computing resources used for such modifications. Any calculation of 
compute on a “cumulative” basis should explicitly not include compute used in modifying an 
open model. Customization for specialized use generally improves the accuracy of the model in a 
focused area and does not increase risk. Such customization is more likely to reduce risk, in 
particular the propensity to “hallucinate”.  
Question 9. What other issues, topics, or adjacent technological advancements should we consider when 
analyzing risks and benefits of dual-use foundation models with widely available model weights? 
Research should be encouraged to explore means of reducing the likelihood of malicious use of 
open models. One such area that appears promising is ‘task blocking’ - a means by which an open 
model that has been modified can be blocked from being used in a harmful manner15. In 
evaluating the relative risks of open vs. closed models, it is also worth paying attention to research 
that has shown that closed models are not necessarily meaningfully less risky16. 
 
 
 
 
 
15 Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models, August 9, 2023, 
Stanford Research presented at AIES (AI, Ethics & Society) Conference on August 9, 2023, 
https://arxiv.org/pdf/2211.14946.pdf. 
16 ChatGPT can be Jailbroken as Easily as Llama 2, Stanford HAI, November 2, 2023, 
https://hai.stanford.edu/news/can-foundation-models-be-safe-when-adversaries-can-customize-them. 
 
16 
 
*  *  *  *  * 
 
Thank you for the opportunity to provide comments on this important subject. Databricks looks 
forward to additional opportunities to discuss the important advantages of permitting the open 
availability of highly capable AI models. 
",Databricks,11432
NTIA-2023-0009-0282," 
 
 
March 27, 2024 
 
 
Mr. Travis Hall 
National Telecommunications and Information Administration 
U.S. Department of Commerce 
1401 Constitution Ave NW 
Washington, DC 20230 
 
 
RE:  Comments of ACT | The App Association to the National 
Telecommunications and Information Administration on Dual Use 
Foundation Artificial Intelligence Models With Widely Available Model 
Weights (Docket No. 240216-0052) 
 
 
I. 
Introduction & Statement of Interest  
 
ACT | The App Association (App Association) appreciates the opportunity to provide 
input to the National Telecommunications and Information Administration (NTIA) on on 
the potential risks, benefits, other implications, and appropriate policy and regulatory 
approaches to dual-use foundation artificial intelligence (AI) models for which the model 
weights are widely available.1  
 
The App Association is a global trade association for small and medium-sized 
technology companies. Our members are entrepreneurs, innovators, and independent 
developers within the global app ecosystem that engage with verticals across every 
industry. We work with and for our members to promote a policy environment that 
rewards and inspires innovation while providing resources that help them raise capital, 
create jobs, and continue to build incredible technology. Today, the value of the 
ecosystem the App Association represents—which we call the app economy—is 
approximately $1.8 trillion and is responsible for 6.1 million American jobs, while serving 
as a key driver of the $8 trillion internet of things (IoT) revolution.2 Alongside the world’s 
rapid embrace of mobile technology, our members create the innovative solutions that 
utilize AI to power IoT across various modalities and segments of the economy. 
 
From the App Association’s perspective, AI is an evolving constellation of technologies 
that enable computers to simulate elements of human thinking, such as learning and 
reasoning. An encompassing term, AI entails a range of approaches and technologies, 
such as machine learning (ML), where algorithms use data, learn from it, and apply their 
 
1 https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-foundation-artificial-
intelligence-models-with-widely-available-model-weights.  
2 ACT | The App Association, State of the U.S. App Economy: 2020 (7th Edition) (Apr. 2020), available at 
https://actonline.org/wp-content/uploads/2020-App-economy-Report.pdf  
 
2 
 
newly-learned lessons to make informed decisions, and deep learning, where an 
algorithm based on the way neurons and synapses in the brain change as they are 
exposed to new inputs allows for independent or assisted decision-making. AI-driven 
tools are having, and will continue to have, substantial direct and indirect effects on 
Americans. Some forms of AI are already being used to improve American consumers’ 
lives today – for example, AI is used to detect financial and identity theft and to protect 
the communications networks upon which Americans rely against cybersecurity threats. 
Moving across use cases and sectors, AI has incredible potential to enable faster and 
better-informed decision making through cutting-edge distributed cloud computing. For 
example, healthcare treatments and patient outcomes stand poised to improve disease 
prevention and conditions, as well as efficiently and effectively treat diseases through 
automated analysis of x-rays and other medical imaging. From a governance 
perspective, AI solutions will derive greater insights from infrastructure and support 
efficient budgeting decisions. It is estimated that AI technological breakthroughs will 
represent a $126 billion market by 2025.3 
 
As AI systems, powered by streams of data and advanced algorithms, continue to 
improve services and generate new business models, the fundamental transformation 
of economies across the globe will only accelerate. At the same time, AI’s growing use 
raises a variety of challenges, and some new and unique considerations, for 
policymakers as well as those making AI operational today. The App Association 
appreciates the efforts of NTIA, and other federal agencies, to address AI safety, 
reliability, and innovation per the Executive Order Concerning Artificial Intelligence.  
 
The App Association has worked proactively to develop consensus around AI 
governance and policy questions from across its diverse and innovative community of 
small businesses. As a result of these consensus-building efforts, the App Association 
has created comprehensive policy principles for AI governance,4 which we append to 
this comment and urge NTIA (and other policymakers) to align with. Notably, the App 
Association’s policy principles for AI governance and policy address quality assurance 
and oversight, recommending that any AI policy framework utilize risk-based 
approaches to ensure that the use of AI aligns with the recognized standards of safety, 
efficacy, and equity. Our AI policy principles also prioritize ensuring the appropriate 
distribution and mitigation of risk and liability by providing that those in the value chain 
with the ability to minimize risks based on their knowledge and ability should have 
appropriate incentives to do so.  
 
The App Association appreciates NTIA’s discussion of open model foundations in its 
request for information, and agrees that open model foundations can support 
competition and innovation, and further transparency. Models with widely available 
 
3 McKinsey Global Institute, Artificial Intelligence: The Next Digital Frontier? (June 2017), available at  
https://www.mckinsey.com/~/media/McKinsey/Industries/Advanced%20Electronics/Our%20Insights/How
%20artificial%20intelligence%20can%20deliver%20real%20value%20to%20companies/MGI-Artificial-
Intelligence-Discussion-paper.ashx. 
4 The App Association’s Policy Principles for Artificial Intelligence are included in this comment as 
Appendix A.  
 
3 
 
weights can benefit from a feedback loop that includes users and developers, enabling 
eased feature improvements as well as identification and mitigation of risk, while 
spending less resources. 
 
The App Association appreciates NTIA’s requesting input on how today’s open source 
software licensing approach could inform its approach to dual use foundation models. 
While not analogous (because open source software licenses do not encapsulate all 
components and capabilities of an AI model), open source licenses can be beneficial in 
many scenarios by harmonizing terminology, training, deployment, weights, and 
documentation/monitoring. However, such licenses cannot actively prevent 
malfeasance. Ultimately, the App Association believes that the market, not government, 
should organically develop open model licensing approaches. 
 
Further, building on the above, we offer the following comments and recommendations 
to NTIA: 
• Improve its categorization of foundation models: Categorizing foundation 
models as either “open” of “closed” will not reflect the important distinctions 
between key existing categories of foundation models. The degree of “openness” 
depends on a range of factors,5 making the drawing of a hard line between 
“open” and “closed” arbitrary.  
 
 
Bommasani, Rishi, et al. ""Issue Brief Considerations for Governing Open Foundation Models | Stanford HAI."" 
Stanford University Human-Centered Artificial Intelligence, 13 Dec. 2023, https://hai.stanford.edu/issue-brief-
considerations-overning-open-foundation-models.  
 
While each of the above categories of foundation model offers its own benefits 
and risks. While fully closed models may be preferrable to protect intellectual 
property, models that make weights available (or even source code) can provide 
access to a feedback loop with developers or the ability for users to make 
improvements. 
 
For purposes of the Executive Order, we urge NTIA to ensure that a “dual-use 
foundation model” is not used synonymously with “open foundation model.” The 
Executive Order provides the following: 
 
 
5 Bommasani, Rishi, et al. ""Issue Brief Considerations for Governing Open Foundation Models | Stanford 
HAI."" Stanford University Human-Centered Artificial Intelligence, 13 Dec. 2023, 
https://hai.stanford.edu/issue-brief-considerations-overning-open-foundation-models.  
 
4 
 
“(k) The term “dual-use foundation model” means an AI model that is 
trained on broad data; generally uses self-supervision; contains at least 
tens of billions of parameters; is applicable across a wide range of 
contexts; and that exhibits, or could be easily modified to exhibit, high 
levels of performance at tasks that pose a serious risk to security, national 
economic security, national public health or safety, or any combination of 
those matters, such as by: 
 
(i) substantially lowering the barrier of entry for non-experts to 
design, synthesize, acquire, or use chemical, biological, 
radiological, or nuclear (CBRN) weapons; 
 
(ii) enabling powerful offensive cyber operations through automated 
vulnerability discovery and exploitation against a wide range of 
potential targets of cyber attacks; or 
 
(iii) permitting the evasion of human control or oversight through 
means of deception or obfuscation. 
 
Models meet this definition even if they are provided to end users with 
technical safeguards that attempt to prevent users from taking advantage 
of the relevant unsafe capabilities.” 
 
The App Association urges NTIA to recognize that not all “open foundation 
models” reflect the characteristics described in the Executive Order, and to 
ensure that the scope of foundation models addressed in its report is confined to 
“dual-use foundation models” as defined in the Executive Order. If the scope of 
NTIA’s report is not carefully restrained to this scope, it may cause definitional 
confusion in the short term, and later improperly expose foundation models that 
are not “dual-use foundation models” to future policy or regulatory requirements 
meant to be applied this category alone.  
• Address harms that are demonstrable and systemic. For purposes of this 
exercise under the Executive Order, NTIA should focus on high-risk scenarios 
(e.g., health, safety) for which there is a clear evidence base to address (in other 
words, policy proposals should not be based on remote edge use cases or 
hypotheticals). 
• Adhere to scalable risk-based harm mitigation principles. As NTIA explores 
policy and regulatory options for dual-use foundation models, we strongly urge 
NTIA to, consistent with the National Institute of Standards and Technology’s AI 
Risk Management Framework, ensure that its proposals are grounded in utilizing 
risk-based approaches to ensure that levels of review, assurance, and oversight 
are proportionate to potential harms. Building on this foundation, NTIA should 
discourage blanket/one-size-fits-all approaches to risk mitigation for dual-use 
foundation models. 
 
 
5 
 
NTIA’s definition of “widely available” should be similarly approached. The wide 
availability of a model is not necessarily an indicator of the risk(s) it may present. 
We urge NTIA’s definition of “widely available” to reflect the harms presented by 
the relevant use case(s). Similarly, floating point operations do not necessarily 
indicate higher risks. Such definitional thresholds should primarily consider the 
capabilities of the model. 
 
We urge NTIA to maintain a broad perspective in considering risk in this matter. 
Many other factors than weights can alter the risks and benefits for a foundation 
model, such as training data, evaluation metrics, and deployment guidelines. 
• Promote shared responsibility across the AI value chain. Small software and 
device companies benefit from understanding the distribution of risk and liability 
in building, testing, and using AI tools. The App Association urges NTIA’s report 
and recommendations to reflect that all stakeholders developing and using AI 
have a shared responsibility for AI safety, efficacy, and transparency. AI policy 
frameworks, including those addressing dual-use foundation models, should 
ensure the appropriate distribution and mitigation of risk and liability (that those in 
the value chain who have the ability to minimize that risk based on their 
knowledge and ability to mitigate have appropriate incentives to do so). 
 
One way that NTIA could support shared responsibility is through proposing the 
creation of a mechanism for sharing best practices, and for surfacing timely 
threat indicators, similar to that employed by Information Security and Analysis 
Centers (ISACs), which foster information sharing across and between the 
government and private sector while avoiding liability for doing so.6 
• Support, and rely on, international standards for risk management. The App 
Association supports reliance on international consensus standards to develop 
metrics for risk, creating standards for best practices, and/or supporting or 
restricting the availability of foundation model weights. We believe that NIST’s 
approach taken in its AI Risk Management Framework is optimal. Support for 
and deference to international standardization would also align NTIA’s efforts 
with the U.S. Government National Standards Strategy for Critical and Emerging 
Technology.7   
• Coordination/Alignment with Other Leading Federal Efforts. Consistent with 
the intent of the Executive Order, alignment with other key federal efforts 
occurring in parallel should be prioritized. As a prime example, NTIA’s 
recommendations should be consistent with the output of the U.S. AI Safety 
Institute.8  
• Support international harmonization. We urge NTIA to maintain a priority for 
supporting risk-based approaches to AI governance in markets abroad and 
 
6 https://www.nationalisacs.org/about-isacs.  
7 https://www.nist.gov/standardsgov/usg-nss.  
8 https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute.  
 
6 
 
through bilateral and multilateral agreements. Already, developers of AI face top-
down and one-size-fits-all mandates that substantially impede their ability to 
develop and utilize AI across a range of use cases. It is crucial that NTIA’s efforts 
here, and the Administration’s efforts broadly, discourage, or at least have a 
positive influence on, such mandates in other jurisdictions. 
 
 
The App Association appreciates NTIA’s consideration of the above (and appended) 
views and we urge NTIA to contact the undersigned with any questions or ways that we 
can assist moving forward. 
 
 
Sincerely, 
 
 
 
 
Brian Scarpelli 
Senior Global Policy Counsel 
 
ACT | The App Association 
1401 K St NW (Ste 501) 
Washington, DC 20005 
202-331-2130 
  
Policy 
Recommendations 
for AI 
Artificial Intelligence (AI) is clearly a priority for policymakers, with 37 AI-related laws enacted globally, more than 
80 pending legislative proposals at the state level and several more at the federal level. To understand and shape 
rules for this complex and evolving technology, a vital voice—that of small businesses, members of ACT| The App 
Association—must be prioritized in order to create a competitive, safe, and secure AI future.
We initially released these principles in 2021. However, we are updating them continually to reflect new 
developments in privacy and data security laws around the world and new learnings about the benefits, risks, and 
challenges presented by evolving AI tools in use cases from healthcare and education to software development 
and cybersecurity.
A successful policy approach to AI will align with the following guidelines: 
2
Harmonizing and Coordinating Approaches to AI
A wide range of federal, local, and state laws prohibit harmful conduct regardless of whether 
the use of AI is involved. For example, the Federal Trade Commission (FTC) Act prohibits a 
wide range of unfair or deceptive acts or practices, and states also have versions of these pro-
hibitions in their statute books. The use of AI does not shield companies from these prohibi-
tions. However, federal and state agencies alike must approach the applicability of these laws 
in AI contexts thoughtfully and with great sensitivity to the novel or evolving risks AI systems 
present. Congress and other policymakers must first understand how existing frameworks 
apply to activities involving AI to avoid creating sweeping new authorities or agencies that 
awkwardly or inconsistently overlap with current policy frameworks.
Quality Assurance and Oversight
Policy frameworks should utilize risk-based approaches to ensure that the use of AI aligns with 
any relevant recognized standards of safety, efficacy, and equity. Small software and device 
companies benefit from understanding the distribution of risk and liability in building, testing, 
and using AI tools. Policy frameworks addressing liability should ensure the appropriate 
distribution and mitigation of risk and liability. Specifically, those in the value chain with the 
ability to minimize risks based on their knowledge and ability to mitigate should have 
appropriate incentives to do so. Some recommended areas of focus include: 
•  Ensuring AI is safe, efficacious, and equitable. 
•  Encouraging AI developers to consistently utilize rigorous procedures and enabling them to
document their methods and results. 
•  Encouraging those developing, offering, or testing AI systems intended for consumer use to
provide truthful and easy-to-understand representations regarding intended use and risks 
that would be reasonably understood by those intended, as well as expected, to use the AI 
solution. 
Thoughtful Design
Policy frameworks should encourage design of AI systems that are informed by real-world 
workflows, human-centered design and usability principles, and end-user needs. AI systems 
should facilitate a transition to changes in the delivery of goods and services that benefit con-
sumers and businesses. The design, development, and success of AI should leverage collabo-
ration and dialogue among users, AI technology developers, and other stakeholders to have all 
perspectives reflected in AI solutions. 
1.
2.
3.
3
Access and Affordability
Policy frameworks should enable products and services that involve AI systems to be 
accessible and affordable. Significant resources may be required to scale systems. 
Policymakers should also ensure that developers can build accessibility features into their 
AI-driven offerings and avoid policies that limit their accessibility options. 
Bias
The bias inherent in all data, as well as errors, will remain one of the more pressing issues with 
AI systems that utilize machine learning techniques in particular. Regulatory agencies should 
examine data provenance and bias issues present in the development and uses of AI solutions 
to ensure that bias in datasets does not result in harm to users or consumers of products or 
services involving AI, including through unlawful discrimination. 
Research and Transparency
Policy frameworks should support and facilitate research and development of AI by prioritizing 
and providing sufficient funding while also maximizing innovators’ and researchers’ ability to 
collect and process data from a wide range of sources. Research on the costs and benefits of 
transparency in AI should also be a priority and involve collaboration among all affected 
stakeholders to develop a better understanding of how and under which circumstances 
transparency mandates would help address risks arising from the use of AI systems.
Modernized Privacy and Security Frameworks
The many new AI-driven uses for data, including sensitive personal information, raise privacy 
questions. They also offer the potential for more powerful and granular privacy controls for 
consumers. Accordingly, any policy framework should address the topics of privacy, consent, 
and modern technological capabilities as a part of the policy development process. Policy 
frameworks must be scalable and assure that an individual’s data is properly protected, while 
also allowing the flow of information and responsible evolution of AI. A balanced framework 
should avoid undue barriers to data processing and collection while imposing reasonable data 
minimization, consent, and consumer rights frameworks. 
4.
7.
5.
6.
4
Education
Policy frameworks should support education for the advancement of AI, promote examples 
that demonstrate the success of AI, and encourage stakeholder engagements to keep 
frameworks responsive to emerging opportunities and challenges. 
•  Consumers should be educated as to the use of AI in the service(s) they are using. 
•  Academic education should include curriculum that will advance the understanding of and
ability to use AI solutions.
Intellectual Property
The protection of intellectual property (IP) rights is critical to the evolution of AI. In developing 
approaches and frameworks for AI governance, policymakers should ensure that compliance 
measures and requirements do not undercut IP or trade secrets.
Ethics
The success of AI depends on ethical use. A policy framework must promote many of the 
existing and emerging ethical norms for broader adherence by AI technologists, innovators, 
computer scientists, and those who use such systems. Relevant ethical considerations 
include:
•  Applying ethics to each phase of an AI system’s life, from design to development to use. 
•  Maintaining consistency with international conventions on human rights. 
•  Prioritizing inclusivity such that AI solutions benefit consumers and are developed using data 
from across socioeconomic, age, gender, geographic origin, and other groupings. 
•  Reflect that AI tools may reveal extremely sensitive and private information about a user and
ensure that laws require the protection of such information. 
9.
10.
8.
5
",The App,4369
NTIA-2023-0009-0269,"Response to the NTIA Request for Comment:
Dual Use Foundation Artificial Intelligence
Models with Widely Available Model Weights
Prepared by The Future Society (TFS)
March 2024
Response to Openness in AI Request for Comment
Docket number: NTIA–2023–0009
We are grateful for this opportunity to submit a response on Dual Use Foundation Artificial
Intelligence Models With Widely Available Model Weights.
The Future Society (TFS) is a U.S. 501(c)(3) independent nonprofit organization with a mission to
align artificial intelligence through better governance. Our policy research aims to ensure safety
and adherence to fundamental values in the development and deployment of advanced AI
systems.
In December 2023, we hosted interactive, small-group roundtable discussions on ‘Navigating AI
Deployment Responsibly: Open-Source, Fully-closed, and the Gradient in Between’ with over 100
leading AI policy experts at our fifth edition of The Athens Roundtable on AI and the Rule of Law
in Washington, D.C. The key takeaways from these discussions have informed this response and
may be found in our summary report “Towards Effective Governance of Foundation Models and
Generative AI.”
Thank
you
for
your
consideration
of
our
comments.
We
may
be
contacted
at
yolanda.lannquist@thefuturesociety.org or info@thefuturesociety.org.
NTIA–2023–0009
Openness in AI Request for Comment
We begin with responses to questions 7.i. and 8 and then proceed in numerical order.
7. What are current or potential voluntary, domestic regulatory, and international
mechanisms to manage the risks and maximize the benefits of foundation models with
widely available weights? What kind of entities should take a leadership role across
which features of governance?
i. Are there effective mechanisms or procedures that can be used by the government or
companies to make decisions regarding an appropriate degree of availability of model
weights in a dual-use foundation model or the dual-use foundation model ecosystem?
Are there methods for making effective decisions about open AI deployment that
balance both benefits and risks? This may include responsible capability scaling policies,
preparedness frameworks, et cetera.
Pre-release evaluations allow for the detection and mitigation of potential risks associated
with dual-use foundation models, while still allowing for the realization of their benefits.
As models become capable of performing and assisting with a broader range of functions,
pre-release evaluations (i.e., evaluations conducted before a model is made accessible beyond
its developers or its testing environment), will play a more necessary role in identifying and
addressing risks before models can cause significant damage, irrespective of their release
method.
Pre-release evaluations can be used to identify and remedy deficiencies and potential
risks—including privacy breaches, biased outputs, and security vulnerabilities—and to determine
which steps, if any, are necessary to remedy identified issues, and which release method(s) are
appropriate for the model in question.
The term “pre-release evaluations,” as with many terms used in AI measurement, lacks a
universally accepted definition and might be used to refer to any combination of the following:
●
Benchmarks: Standardized datasets used to assess and compare the performance of
different models on specific tasks or capabilities.1
●
Model evaluations: Standardized protocols conducted to measure model performance,
robustness, fairness, and other key attributes, such as testing a model's ability to acquire
resources.2
●
Red-teaming tests: Adversarial testing methods that attempt to find vulnerabilities, biases,
or unintended behaviors in a model or its cybersecurity environment.3
3 See, for example, What Does AI Red-Teaming Actually Mean? | Center for Security and Emerging
Technology.
2 See, for example, New report: Evaluating Language-Model Agents on Realistic Autonomous Tasks.
1 See, for example, Holistic Evaluation of Language Models (HELM).
The Future Society | 2
NTIA–2023–0009
Openness in AI Request for Comment
Each approach has its own benefits and drawbacks in terms of resource requirements, validity,
robustness, and scalability.
Each approach provides a distinct and necessary benefit: Benchmarks provide standardized
performance
comparisons
across
models;
model
evaluations
allow
for
more
in-depth
assessments of a model's attributes and behaviors in an operating environment; and red-teaming
tests enable the identification of hard-to-anticipate vulnerabilities and risks. As such, in the
interest of exercising precaution, pre-release evaluations should incorporate a combination of
each approach.
At the same time, unduly onerous pre-release evaluation requirements would function as a
bottleneck for market entrants or otherwise prevent the benefits of foundation models, including
open models, from being realized. With this in mind, pre-release evaluations should only be
required by a limited set of models (i.e., those considered to be “dual-use foundation models”).
To counteract potential market concentration, the government could provide guidance and
services to SMEs developing models that necessitate pre-release evaluations. Additionally, to
ensure rigor and objectivity of the pre-release evaluation, at least some of the components, such
as model evaluations and red-teaming, should be conducted by independent, qualified third
parties. Red-team testers should be incentivized to continuously use creative techniques to elicit
undesirable behavior and upper-bound capabilities through the provision of “bug bounties.”4
Joint and several liability regimes could be explored as a means to incentivize both developers
and third parties to perform exhaustive evaluations.
In conjunction with model pre-release evaluations, the government should establish risk
categories
(i.e.,
designations
of
“high-risk”
or
“unacceptable-risk”),
thresholds,
and
risk-mitigation measures that correspond to evaluation outcomes. Entities developing, providing,
or deploying models designated as “high-risk” should be required to satisfy enhanced oversight
requirements,
while
the
possession,
use,
and
proliferation
of
models
designated
as
“unacceptable-risk” should be restricted.
Due to the evolving nature of AI measurement science, and of models of themselves, pre-release
evaluations must adapt as more robustly valid methods are demonstrated.
Finally, model pre-release evaluations are just one method of reducing risk presented by
dual-use foundation models; they should complement—and do not eliminate the need
for—other risk assessment or mitigation strategies, including testing at the AI system- or
application-level.
4 Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE
Framework.
The Future Society | 3
NTIA–2023–0009
Openness in AI Request for Comment
8. In the face of continually changing technology, and given unforeseen risks and
benefits, how can governments, companies, and individuals make decisions or plans
today about open foundation models that will be useful in the future?
Recognizing that we lack sufficient data to make informed policy decisions, it is necessary
to undertake a precautionary approach that seeks to mitigate potential risks while still
fostering a diverse and competitive AI ecosystem.
We expect that much of the evidence cited in responses to this RFC will have been produced
within the past 12 months.
The current state of AI measurement science is insufficient to fully understand the long-term
benefits and risks of open foundation models. Steps should immediately be taken to advance
measurement science—e.g., investments should be made in the development of standardized
evaluation and risk-assessment frameworks, and information-gathering measures, such as
reporting
requirements,
post-market
monitoring,
and
incident
reporting,5
should
be
institutionalized.
At the same time, we are concerned that overly restrictive policies could lead to market
concentration, hindering competition and innovation in both industry and academia. A lack of
competition in the AI market can have far-reaching knock-on consequences, including potentially
stifling efforts to improve transparency, safety, and accountability in the industry. This, in turn, can
impair our ability to monitor and mitigate the risks associated with dual-use foundation models
and to develop evidence-based policymaking.
1. How should NTIA define “open” or “widely available” when thinking about foundation
models and model weights?
a. Is there evidence or historical examples suggesting that weights of models similar to
currently-closed AI systems will, or will not, likely become widely available? If so, what are
they?
In March 2023, the weights for Meta’s LLaMa model, which were intended to only be accessed
by approved researchers, were leaked online.6 At that time, the model was comparable to
contemporaneous closed-source models, such as GPT-3, Chinchilla, and PaLM models.7 In the
time since, there have been several instances of companies deliberately releasing weights for
models that are comparable to contemporaneous closed-source models, such as Meta’s release
7 [2302.13971] LLaMA: Open and Efficient Foundation Language Models
6 Meta’s powerful AI language model has leaked online — what happens now? - The Verge.
5 An Argument for Hybrid AI Incident Reporting | Center for Security and Emerging Technology
The Future Society | 4
NTIA–2023–0009
Openness in AI Request for Comment
of LLaMa-2 in July 2023,8 Mistral’s release of Mixtral 8x7B in December 2023,9 and Databricks’s
DBRX in March 2024.10
Another route that weights similar to (or rather, matching) those of currently-closed models could
become widely accessible is through the exfiltration of current providers’ models. An interim
report published by RAND researchers in 2023 identified approximately ~40 distinct attack
vectors that could compromise providers’ security.11 We are not aware of a public, large-scale
assessment of providers’ cybersecurity systems, but one small-scale study by an independent
researcher in February 2024 found that 11 out of 16 leading AI companies evaluated had fallen
short of voluntary commitments to cybersecurity vulnerability reporting—a small yet nontrivial
component of cybersecurity infrastructure.12
2. How do the risks associated with making model weights widely available compare to
the risks associated with non-public model weights?
a. What, if any, are the risks associated with widely available model weights? How do
these risks change, if at all, when the training data or source code associated with fine
tuning, pretraining, or deploying a model is simultaneously widely available?
Access to model weights allows, through fine-tuning, for the removal or circumvention of
guardrails that might have been incorporated in the production of the model.13 This could allow
models to more easily be used in a malicious manner, including in spear-phishing scams14
deepfake pornography,15 voice cloning scams,16 child sexual abuse material17, and in techniques
to “jailbreak” closed-source foundation models.18 Outside of the US, these effects may be more
acute in countries with fragile democratic institutions, weak cyber resilience, susceptibility to
social unrest, or lacking legal or regulatory safeguards or the capacity to enforce them.
18 LLM Attacks
17 Generative ML and CSAM: Implications and Mitigations
16 Reducing malicious use of synthetic media research: Considerations and potential release practices for
machine learning.
15 “Because the model weights were shared publicly, users easily circumvented filters that Stability AI
implemented to prevent the generation of not safe for work (NSFW) imagery. As a result, AI-generated
pornography based on Stable Diffusion offshoots quickly spread across the internet, including images
resembling real people generated without their consent.” On the Societal Impacts of Open Foundation
Models.
14 WormGPT - The Generative AI Tool Cybercriminals Are Using to Launch BEC Attacks | SlashNext
13 A recent paper by Stanford University acknowledges that “for open-source models safety filters can
simply be removed by deleting a few lines of code.” Self-Destructing Models: Increasing the Costs of
Harmful Dual Uses of Foundation Models. Also see BadLlama: cheaply removing safety fine-tuning from
Llama 2-Chat 13B
12 How are AI companies doing with their voluntary commitments on vulnerability reporting?
11 Securing Artificial Intelligence Model Weights: Interim Report | RAND
10 Databricks Launches DBRX, A New Standard for Efficient Open Source Models
9 Mixtral of experts | Mistral AI | Frontier AI in your hands
8 Llama 2
The Future Society | 5
NTIA–2023–0009
Openness in AI Request for Comment
Recent research has put forward a framework for assessing the marginal risk of open vs. closed
models.19 While it argues that many of the studies that it reviewed failed to demonstrate a
significant marginal risk for open models, in many cases due to lack of evidence of societal
impacts, that does not invalidate the possibility that these risks might arise in the future as open
foundation models are increasingly deployed.
Finally, models released with widely available weights cannot credibly be monitored, updated, or
retracted. Once these models are released to the public, developers can easily and inexpensively
modify or distribute them. Thoroughly assessing the risks of release is therefore crucial; however,
industry actors currently lack a standardized, transparent approach to this assessment.
c. What, if any, risks related to privacy could result from the wide availability of model
weights?
The wide availability of model weights used in audio, image, and video generation models
heightens the risk of the production of “deepfakes” that imitate a person’s identity without their
permission. This is because availability of model weights could allow actors to circumvent
features in the underlying base model or the overarching system designed to refuse requests to
generate explicit images or images containing public figures.20 According to the UN Interregional
Crime and Justice Research Institute (UNICRI), deepfakes, often combined with social media and
messaging applications, can reach millions of people in very short periods of time, and as such,
present “considerable potential” for a range of malicious and criminal purposes, including but
not limited to destroying the credibility of individuals or institutions, harassing individuals,
perpetrating extortion or fraud, falsifying identities, inciting acts of violence towards minority
groups, or supporting narratives of extremist or terrorist groups.21
3. What are the benefits of foundation models with model weights that are widely
available as compared to fully closed models?
b. How can making model weights widely available improve the safety, security, and
trustworthiness of AI and the robustness of public preparedness against potential AI
risks?
Making
model
weights
widely
available
allows
for
more
in-depth
and
reproducible
experimentation,
transparency,
evaluations,
audits,
and
improvements
that
can
bolster
21 UNICRI, Malicious Uses and Abuses of Artificial Intelligence. Page 52, Case Study: A deep dive into
deepfakes.
20 “Because the model weights were shared publicly, users easily circumvented filters that Stability AI
implemented to prevent the generation of not safe for work (NSFW) imagery. As a result, AI-generated
pornography based on Stable Diffusion offshoots quickly spread across the internet, including images
resembling real people generated without their consent.” On the Societal Impacts of Open Foundation
Models. Also see Suspect Charged After Allegedly Using AI To Create Images Of Child Sexual Abuse
19 On the Societal Impacts of Open Foundation Models
The Future Society | 6
NTIA–2023–0009
Openness in AI Request for Comment
robustness and trustworthiness. However, these tasks do require knowledge and resources that
may limit the number of contributors.22
7. What are current or potential voluntary, domestic regulatory, and international
mechanisms to manage the risks and maximize the benefits of foundation models with
widely available weights? What kind of entities should take a leadership role across
which features of governance?
a. What security, legal, or other measures can reasonably be employed to reliably
prevent wide availability of access to a foundation model's weights, or limit their end use?
Several existing U.S. laws and regulations related to intellectual property (e.g., Copyright Act,
Patent Act, Trade Secrets Act), data privacy (e.g., HIPAA, COPPA, CCPA), and national security
(e.g., ECRA, EAR, ITAR) may indirectly affect the release of certain AI models or their components,
including open source models, by imposing restrictions on their use, reproduction, distribution,
export, or foreign investment; however, the application of these laws to AI models is still evolving,
open to interpretation, and new regulations may emerge as AI technology advances.
To prevent wide availability of access to a foundation model's weights, or limit their end use,
policymakers might establish coherent liability frameworks that cover the entire lifecycle of
dual-use foundation AI models, including legal liability for open source providers. This could
involve establishing legal liability for open-source providers based on concepts such as
""reasonably foreseeable misuse,"" as proposed in the draft EU Cyber Resilience Act.
Additionally, existing regulatory entities could expand their mandates to encourage transparency,
red-teaming, and promote competition among closed model developers, reducing the reliance
on open-source models as a means to achieve these goals. For instance, the FTC and the DOJ
could intensify their efforts to scrutinize anticompetitive practices, enforce antitrust laws, and
foster innovation in the AI industry. Closed AI models can be subject to transparency
requirements
and
independent
red-teaming
exercises to identify dangerous capabilities,
vulnerabilities, or other emergent properties of foundation models (on the base models since
guardrails can be removed). Red-teaming exercises must adhere to standards that are at least as
rigorous as forthcoming NIST guidelines for AI evaluation and red-teaming.
22 Widder, D. G., West, S. and M. Whittaker. (2023). Open (For Business): Big Tech, Concentrated Power, and
the Political Economy of Open AI. “We find that ‘open’ AI can, in its more maximal instantiations, provide
transparency, reusability, and extensibility that can enable third parties to deploy and build on top of
powerful off-the-shelf AI models. These maximalist forms of ‘open’ AI can also allow some forms of
auditing and oversight. But even the most open of ‘open’ AI systems do not, on their own, ensure
democratic access to or meaningful competition in AI, nor does openness alone solve the problem of
oversight and scrutiny.”
The Future Society | 7
NTIA–2023–0009
Openness in AI Request for Comment
b. How might the wide availability of open foundation model weights facilitate, or else
frustrate, government action in AI regulation?
Models released with widely available weights cannot credibly be monitored, updated, or shut
down. Once these models are released to the public, open-source developers can easily and
inexpensively modify, distribute, and enhance them. This makes it difficult to conduct oversight,
incident reporting, and implement regulatory requirements post-release. Furthermore, it makes it
difficult, if not impossible, to trace and attribute instances of misuse and seek redress.23
Second, the wide availability of open foundation model weights can undermine policies intended
to bolster U.S. technological leadership by enabling the transfer of potentially millions of dollars
worth of U.S. R&D to foreign firms or state actors, allowing foundation model developers from
other nations to easily replicate (and potentially improve upon) U.S. advancements in AI. This was
exemplified when 01.AI, a prominent Chinese foundation model developer, was found to have
partly copied the transformer architecture from Meta's open-source Llama-2 model.24
h. What insights from other countries or other societal systems are most useful to
consider?
To reduce the compliance burden of developers without sacrificing safety, it would be useful to
maintain consistency with the requirements imposed on developers under the EU AI Act. Under
the EU AI Act, all providers of general-purpose AI (i.e. foundation) models that present a “systemic
risk”—open or closed—must also conduct state-of-the-art tests and model evaluations, report
serious incidents, assess and mitigate risks, meet cybersecurity requirements and provide
information about their models’ energy consumption. General-purpose AI models are considered
to pose “systemic risk” under the AI Act when the cumulative amount of compute used to
produce them exceeds 1025 floating point operations (FLOP).25
25 European Parliament, Artificial Intelligence Act, (March 2024):
https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf
24 Chinese Startup 01.AI Is Winning the Open Source AI Race | WIRED; “China’s Rush to Dominate A.I.
Comes With a Twist: It Depends on U.S. Technology.”
23 Towards Effective Governance of Foundation Models and Generative AI - The Future Society
The Future Society | 8
",Future Society,4495
NTIA-2023-0009-0308,"‭
NTIA Open Weights Response: Towards‬
‭
A Secure Open Society Powered By‬
‭
Personal AI‬
‭
Authors:‬
‭
Context Fund Policy Working Group (‬
‭
www.context.fund‬
‭
)‬
‭
Original RFC:‬
‭
https://www.regulations.gov/document/NTIA-2023-0009-0001‬
‭
Authors‬
‭
Context Fund is an volunteer online organization which is an experiment in public‬
‭
investment. Its AI products are used every day by scientists and policy thinkers‬
‭
(‬
‭
VerificationGPT‬
‭
, for example). Product designs are‬‭
based on deep experience not only in AI,‬
‭
but also in psychology, security, investment and public policy. The Policy Working Group in‬
‭
particular advises on AI governance and investment, informed by product design and practical‬
‭
experimentation. It strives for maximal transparency,  evidence-based reasoning, and working‬
‭
demonstrations of solutions, rather than only principles, in the spirit of the open-source‬
‭
movement. The group is made up of volunteer experts with backgrounds at top universities such‬
‭
as Stanford, civic institutions such as the Mercutas Center, and AI research and product‬
‭
companies such as Meta collaborating to push for the defensive acceleration of a secure and‬
‭
open AI economy which benefits all. We look forward to collaborating with NTIA in exploring the‬
‭
open weights question and more.‬
‭
Executive Summary‬
‭
Strong evidence suggests that open models are safer than closed models due to‬
‭
efficiencies in the fields of science, economics and cybersecurity. In science and cybersecurity,‬
‭
this is due to inspectability and the ability to share the model with millions of others to distribute‬
‭
the burden of verification, thus solving the expert problem. This substantially aids defender and‬
‭
builder users, the majority use case. In terms of economic equality, open models allow for‬
‭
extreme efficiency as well as more equitable distribution, since they can be offered at low or‬
‭
zero cost. They also prevent society from descending back into non-evidence-based thinking‬
‭
and warfare, inspire faith in transparent rule of law, and allow anyone to generate examples of‬
‭
their ideas at a small scale, which is important for clear communication. Closed models are‬
‭
most likely to be abused by deployers, while open models can be abused by either deployer and‬
‭
users, however, the advantages that open models provide for users acting in defender roles‬
‭
outweigh the risks of availability to attackers, roughly by a factor of 100:1, considering the‬
‭
financial surface area that needs to be defended.  Although less secure, we assess that it is‬
‭
acceptable for the government to allow closed APIs of foundation models to remain legal, as‬
‭
they can be used to satisfy commercial and technical considerations of deployment, for‬
‭
example, protection of trade secrets and engineering efficiency.‬
‭
In our assessment, the government’s support initially should be in administering‬
‭
standardization processes and RFCs (such as this one), legislating well-scoped mandates to‬
‭
add transparency to models and model outputs for high-scale deployments, funding defensive‬
‭
research, supporting responsible disclosure programs which would otherwise be underinvested‬
‭
by the private market, and participating in and administering open standards bodies.‬
‭
Specific legal and technical designs are possible to add further design choices to the‬
‭
defensive acceleration of the AI sector and mitigate some of the remaining risks in the AI sector.‬
‭
To harden deployments against spam and phishing, we recommend immediately encouraging‬
‭
the use of physical security keys or biometrics tied to anonymous-but-accountable-by-karma‬
‭
user accounts, as well as scaling verification APIs and promoting the adversarial hardening of‬
‭
open models using offline data prior to deployment as a best practice. Adding watermarking will‬
‭
also improve traceability of outputs. To harden specific deployments against misuse and‬
‭
distribution, per-use model tainting of open model weights may be possible, however we do not‬
‭
recommend this as a default or legal requirement. To harden deployments against financial‬
‭
attacks, licenses like‬‭
differentiable credit licenses‬‭
can be experimented with. Together,‬
‭
traceability and licensing form a credible deterrent to abuse for most malicious users, while‬
‭
inspectability and shareability of open models forms a credible deterrent to abuse from‬
‭
malicious deployers and backdoored models.‬
‭
Background‬
‭
The Artificial Selection Pattern of Technology Development‬
‭
Throughout human history, technologies have followed a consistent pattern of‬
‭
development. They often are first developed to achieve some positive goals for an end user.‬
‭
Immediate and obvious harms (especially those not achieving user goals) are identified and‬
‭
fixed quickly, while emergent problems from the deployment of a technology may take longer to‬
‭
be identified and solved. Overall, this forms a process of artificial selection, whereby a tool is‬
‭
specialized for use by humans.‬
‭
The harms that persist in the long run (and hence may need more consistent and‬
‭
aggressive intervention from the state, possibly working against the forces of economics) tend‬
‭
to be those stemming from human-human alignment problems (often from zero-sum games) or‬
‭
bounded rationality (“out of sight, out of mind”). Climate change, plastics in the ocean, online‬
‭
spam and other pollution are prime examples.  Zero-sum type problems are fixed by changing‬
‭
offense/defense efficiency ratios to favor the defender (thus reducing the payoffs from zero-sum‬
‭
games and contrastively favoring collaborative positive-sum games). The common advice to‬
‭
“drive defensively” is a good example.  “Out of sight, out mind” type problems are often‬
‭
identified by scientists and economists as data becomes available from deployments, however,‬
‭
they often lack the resources to design and test solutions at scale against them, requiring state‬
‭
investment. It is not enough to just identify problems, solutions must also be tested to ensure‬
‭
they do minimal harm themselves.‬
‭
Most evidence suggests that although AI is more powerful than most tools we have‬
‭
used, it follows the path of artificial selection (tool development) rather than natural selection‬
‭
(self-reproducing populations left alone in the wild). This means that we can tune it with‬
‭
defensive acceleration policies (d/acc) - allowing development towards positive goals, while‬
‭
monitoring and identifying risks (by reducing uncertainty via collecting evidence from small-scale‬
‭
deployments) and disclosing them preferentially to defenders to allow time for patches and‬
‭
updates. This process operates with the supervision of the broad open-source and scientific‬
‭
communities operating under legal protection for scientific inquiry. Government involvement can‬
‭
help especially in scaling up defenses against subtle known externalities through setting‬
‭
standards, which makes public supervision more efficient.‬
‭
Principles of Supervision‬
‭
Many processes have been used to deal with uncertainty throughout human history by‬
‭
collecting feedback (supervision). We recap several that are relevant to the defensive‬
‭
acceleration of AI development, and describe their strengths and weaknesses.‬
‭
The Scientific Method‬
‭
The scientific method is the basis of collective rational thought. This requires the‬
‭
collection of initial evidence, the forming of a testable hypothesis based on that evidence, the‬
‭
testing of that hypothesis and the acceptance or rejection of the hypothesis in comparison to‬
‭
other possible hypotheses. Hypotheses with greater evidence are referred to as theories, and‬
‭
those with substantial evidence as laws. A recording of the experiment is formalized in a‬
‭
scientific paper, which also includes reproducible experimental artifacts such as code and‬
‭
datasets. The scientific method is generally practiced by individuals and is a fast way to improve‬
‭
a product (O(hours-days)‬
‭
1‬
‭
).‬
‭
Peer Review‬
‭
Once formalized in a paper, the artifacts of the experiment are presented to the scientific‬
‭
community and general public via a process of peer review (for example, on‬‭
OpenReview‬
‭
).‬
‭
While the long-term outcome is the acceptance or rejection of a paper to a journal or‬
‭
conference, arguably the more important outcome is the refinement of the experiment over ~1-6‬
‭
months while in review. While peer review is not perfect - it can be captured by narrow interests‬
‭
in specific journals, does not necessarily select for impact over precision, and can prioritize‬
‭
1‬‭
O(.) is‬‭
big O notation‬‭
and can be read as “order‬‭
of” or “approximately”‬
‭
narrow subfields which can make publishing more revolutionary ideas hard, the competitive‬
‭
supervision of the community produces reproducible, well-organized artifacts and a relatively‬
‭
high-quality stream of papers. Over time, through many papers, evidence accumulates for or‬
‭
against ideas in a friendly, but competitive, spirit (example:‬
‭
https://twitter.com/mengyer/status/1770461126293107134‬
‭
).‬‭
Although the most common goal for‬
‭
work is academic novelty, papers which create standards and eval sets for problems are not‬
‭
uncommon (for example,‬‭
BigBench‬
‭
,‬‭
ImageNet‬
‭
, and in‬‭
AI safety,‬‭
Purple Llama‬
‭
). The process is‬
‭
generally fast (1-6 months for a paper), and accessible to all, since reproducible methods are‬
‭
prioritized. The‬‭
Transformer paper‬‭
is one of the most‬‭
prominent examples of a peer-reviewed‬
‭
paper that catalyzed a wave of innovation with AI, later most prominently turned into a product‬
‭
by OpenAI via further experimentation, however, a continuous stream of lesser-known papers‬
‭
laid the foundation of the current AI systems that we use today. Development is generally less‬
‭
about discontinuous “breakthroughs” and more about continuous processes of experimentation,‬
‭
public feedback and improvement, although the language of breakthroughs is much more‬
‭
exciting for selling a paper.‬
‭
2‬
‭
Open-Source Development‬
‭
While the code produced by reproducible academic papers is an example of‬
‭
open-source development, open-source development extends far beyond journals.  The overall‬
‭
“hacker” ethos of open-source prioritizes creating common libraries which are broadly useful,‬
‭
but not necessarily academically novel, as well as the meta-goal of creating and propagating a‬
‭
“gift culture” whereby reputation is gained by giving away valuable features, and personal‬
‭
freedom of action is prioritized. Feedback on open-source software comes through open filing of‬
‭
issues logging bugs, and often will see turnover on problems in the 1 hour - 3 month timeframe.‬
‭
Through consumer choice (both to use or donate time or money to useful projects), open-source‬
‭
software is one of the more responsive processes to handle uncertainty, and operates with‬
‭
extreme efficiency compared to closed-source silos due to asynchronous, permissionless code‬
‭
sharing (“with enough eyes, all bugs are shallow”). Due to the gift culture, pure open-source is‬
‭
often unsustainable as a business, relying on donations (such as Signal), however, a number of‬
‭
companies have built successful business models around charging for deployments of‬
‭
open-source software, so-called open-core (such as Docker), and many large companies‬
‭
contribute full-time employees to development of projects they use (for example, Google). Even‬
‭
non-contributing businesses and scientists also depend massively on the efficiencies of‬
‭
open-source software, which contributes hundreds of billions of dollars to the economy overall‬
‭
(‬
‭
https://www.zdnet.com/article/how-much-are-open-source-developers-really-worth-hundreds-of‬
‭
-billions-of-dollars-say-economists/‬
‭
). It is not an accident that the Internet and modern operating‬
‭
systems are the product of the open-source community.‬
‭
2‬‭
ML is still largely an empirical science, rather than theoretical (where “breakthroughs” may be a more‬
‭
appropriate descriptor), and in fact, one of the long-term lessons in ML research is that data and compute‬
‭
matter much more than algorithms (summarized‬‭
here‬
‭
).‬
‭
Market Feedback‬
‭
Launching a product to the public also provides a form of supervision via consumer‬
‭
choice in competitive markets. However, unlike peer review, methods used to create the product‬
‭
are often tightly guarded, which can preserve significant upside for the shareholders (and‬
‭
somewhat for employees). Software companies with significant coder populations in‬
‭
management may also open-source older versions of products once a new one is released‬
‭
(Google did this, on a time lag of about 1 year), however this is not the norm outside of Silicon‬
‭
Valley. In competitive markets with several choices, low switching costs and low information‬
‭
asymmetry, better products are selected by consumers, passing feedback to the companies‬
‭
through revenue and attention (social media, word of mouth), however, this can be hacked so it‬
‭
no longer represents an effective source of feedback (via dark patterns which prevent‬
‭
disaffected consumers from leaving). In more open settings, consumers select for products‬
‭
which fulfill their “jobs-to-be-done”, and will quickly abandon products that are uncontrollable or‬
‭
have noticeable bad side effects (for example, committing crimes for which the users are liable).‬
‭
However, in settings with fewer consumer options and less information, consumer choice is not‬
‭
necessarily a reliable source of feedback. One way that governments and industry groups have‬
‭
fixed this problem of lack of reliable feedback (expressed as externalities) has been through the‬
‭
establishment of standards which reduce information asymmetry, as well as antitrust actions,‬
‭
which add consumer choice to the market. Many of the more pathological problems of the‬
‭
current Internet (seen from 2010 onwards) may have a connection to a shift in actors from the‬
‭
open-source ethos of the early Internet to the for-profit ethos of the 2010s and onwards.‬
‭
Standards‬
‭
When there are enough examples of a product category to define the desired and‬
‭
undesired behavior of the processes (inputs -> outputs) that it represents, then standards can‬
‭
be established. This is especially useful for end consumers, as most people are not experts‬
‭
about most things in the world, especially when evaluating business products where the‬
‭
methods are often hidden. Complex truths are a lemon market, and without standards which‬
‭
verify end products, consumers often are left with only trial and error to compare, and even this‬
‭
can be hacked (astroturfing reviews, for example). Standards which define communication‬
‭
processes also allow interoperability of products as well, which helps to reduce the risk of‬
‭
single-point failure in the economy. While standards are inherently slower than market feedback‬
‭
(months-years) due to the need for broad industry-wide consensus and allowing time to collect‬
‭
enough examples to clearly describe behaviors (often via an eval set), they often are a key step‬
‭
to allow the market to apply broad feedback.‬
‭
Standards are curated by industry groups such as IETF, often in cooperation with‬
‭
government groups, like NIST, and frequently formatted as RFCs (requests for comment). A key‬
‭
outcome of standards is that they are easy to understand, easy to test for, and broadly accepted‬
‭
(often a result of including broad industry participation in the process).  This process worked‬
‭
wonderfully for the development of the Internet (you’re likely reading this document due to the‬
‭
TCP RFC:‬‭
https://www.ietf.org/rfc/rfc793.txt‬
‭
), and‬‭
counterexamples also abound (for example,‬
‭
Google moving away from the XMPP chat standard for Gchat contributed substantially to the‬
‭
fragmentation of the chat ecosystem that we see today, as other companies followed suit. This‬
‭
led to more walled gardens).‬
‭
For AI, scientific standards/benchmarks are often expressed in eval sets, developed by‬
‭
groups such as‬‭
ML Commons AI Safety Working Group‬‭
and‬‭
NIST‬
‭
. While companies are mildly‬
‭
hesitant to rank their models against others on a leaderboard (due to the knowledge that‬
‭
consumers might select against them), in practice, if models are open, this can be done by the‬
‭
community adversarially, or by connecting open publication and ranking to receiving government‬
‭
funding (for example, NIH grants typically require open models and publications as a condition‬
‭
of receiving government funding). There is a large amount of standardization work to be done in‬
‭
AI, and it is often not well-rewarded in peer review systems (which prioritize academic novelty).‬
‭
Open Letters‬
‭
Communities also normalize behaviors via open letters‬
‭
(‬
‭
https://www.reddit.com/r/contextfund/comments/1ba4gf0/responsible_ai_x_biodesign_open_lett‬
‭
er/‬
‭
,‬
‭
https://www.reddit.com/r/contextfund/comments/1b7d1ln/a_safe_harbor_for_independent_ai_ev‬
‭
aluation_open/‬
‭
). This can be relatively fast (months),‬‭
and if the field is responsible (scientists‬
‭
generally are a responsible population due to selection for curiosity and rigorous thought),‬
‭
broadly useful. Open letters receive rapid feedback (hours), and can become de facto standards‬
‭
of behavior through public exposure leading to broad signatures (the positive side of social‬
‭
media).‬
‭
Responsible Disclosure‬
‭
Responsible companies embedded in open communities often follow a security practice‬
‭
known as responsible disclosure. Under responsible disclosure, those which discover security‬
‭
vulnerabilities notify affected parties several months-years before disclosing vulnerabilities to the‬
‭
public, allowing affected parties time to patch systems (one recent example of this practice:‬
‭
https://www.wired.com/story/saflok-hotel-lock-unsaflok-hack-technique/‬
‭
).‬‭
Vulnerabilities in‬
‭
mature systems are often subtle, but with enough eyes, all bugs are shallow. The security‬
‭
ecosystem depends on 1.) the use of public red teams who work full-time in adversarial roles 2.)‬
‭
software/weights being available to red-teamers under legal protections‬
‭
, 3.) an industry-wide‬
‭
standard process for coordinated disclosures‬
‭
(‬
‭
https://www.cisa.gov/coordinated-vulnerability-disclosure-process‬
‭
,‬
‭
https://en.wikipedia.org/wiki/Coordinated_vulnerability_disclosure‬
‭
)‬‭
and 4.) a low-cost update‬
‭
process to distribute patches (failure modes around responsible disclosure mostly center on‬
‭
expensive update processes, which are most common in offline hardware settings). While‬
‭
updates are slightly easier in “walled gardens” like Apple’s iPhone ecosystem, updates in‬
‭
open-source ecosystems like Android are also effective (due to human bounded rationality and‬
‭
instinct to cluster around working solutions, a handful of prominent distribution channels will‬
‭
emerge), and in practice the market creates incentives which lead to companies to help‬
‭
automate the detection and patching process like‬‭
Socket Security‬‭
(also, walled gardens are the‬
‭
types of ecosystems that take monopoly rents which create economic vulnerabilities in the‬
‭
system).‬
‭
Responsible disclosure of risks should also be used in AI, allowing defenders time to‬
‭
patch systems before new offensive capabilities are introduced, thereby increasing information‬
‭
asymmetry in favor of the defender and disincentivizing attacks. For example,‬‭
GPT-4 likely‬
‭
broke visual CAPTCHAs‬‭
(requiring a switch to verifying‬‭
accounts with physical security keys or‬
‭
passive CAPTCHAs), and the ability to synthesize 1-minute video clips using models like SORA‬
‭
may create an undesirable arms race  in political advertisement (requiring labeling and possibly‬
‭
banning AI-generated political ads and adopting widespread use of verification APIs for general‬
‭
content). While responsible disclosure has been a widely adopted practice in computer security‬
‭
(‬
‭
https://www.helpnetsecurity.com/2022/02/17/vulnerabilities-disclosed-2021/‬
‭
),‬‭
this has been less‬
‭
widely adopted in AI (to date, OpenAI has yet to officially disclose the CAPTCHA vulnerability,‬
‭
although it has been noted by independent researchers; to their credit, OpenAI is appropriately‬
‭
delaying the SORA release). Notably, unlike classical computer security, in AI security problems,‬
‭
communicating the vulnerability broadly and publicly can often be done without communicating‬
‭
the means of exploiting the vulnerability, allowing public disclosure of future AI vulnerabilities‬
‭
without needing to rely on private emails or coordinated disclosure programs, as in traditional‬
‭
responsible disclosure.‬
‭
As a rough heuristic, offense/defense capability ratio, traceability and deployment size‬
‭
are three key intermediate metrics to consider when designing a responsible disclosure system.‬
‭
Legislation & Prosecution‬
‭
Finally, legislative systems create laws which penalize offensive behaviors which have‬
‭
known, severe harms, thus shifting the game payouts to favor legal behavior and cooperative‬
‭
games. Laws allow for feedback by state prosecution, as well as civil suits, and often require‬
‭
years to change, as well as years to collect a sufficient volume of case law to be able to be‬
‭
interpreted unambiguously within a jurisdiction. This is multiplied by the number of jurisdictions.‬
‭
Due to the necessity of relying on lawyers as intermediaries and interpreters for almost every‬
‭
interaction with the legal system, transactions within a legal system can cost millions of dollars‬
‭
and require years, and can favor parties with unequal access to finances, personal connections‬
‭
or time, so-called “lawfare.” Yet, enforcement via civil suits is one of the few systems that can‬
‭
provide feedback of sufficient scale and surety to deter behaviors that shake the foundations of‬
‭
democracy and trade, such as breaches of contract and gross misrepresentations. Enforcement‬
‭
via criminal prosecution in turn deters actions which shake the foundations of rule of law itself.‬
‭
The legal system is a powerful technology to create fear that needs to be used with care, and‬
‭
due to its glacial pace, is not intended to match the speed or generality of contemporary tech‬
‭
developments, much less to guide future general-purpose research.‬
‭
Principles of Security (re: 1)‬
‭
All technologies are deployed in the context of games played between humans. In‬
‭
analyzing these games, we divide roles between “attacker” and “defender.” Roles are further‬
‭
divided between “user” and “deployer” of a technology.  Both user attackers and deployer‬
‭
attackers represent important scenarios to model.‬
‭
A critical ratio to estimate in any security game is offense/defense capability ratio. High‬
‭
offense/defense ratios favor first strike and competition, while low ratios favor second strike and‬
‭
cooperation. Modern civilization broadly depends on this ratio favoring defenders, thus inducing‬
‭
cooperation (“rule of law”, “defensive driving”), although competition and first strike is important‬
‭
to allow in limited contexts (we allow this in business via time-decayed property rights based on‬
‭
creation of technologies or IP).‬
‭
While an offense/defense ratio must be estimated empirically based on analysis of each‬
‭
game, some general principles can be derived. Traceability of information generally favors‬
‭
defenders (since this enables rational decision-making and authentication and authorization in‬
‭
particular). In settings where users can be either attackers or defenders, balance of power (both‬
‭
in distribution of information and capabilities) also is important, as it allows for a larger number‬
‭
of defenders to swiftly overpower an attacker in a second strike. Since most users are playing‬
‭
defensive games (~99% of GDP is concentrated in legal enterprise, for example,‬
‭
https://csis-website-prod.s3.amazonaws.com/s3fs-public/publication/economic-impact-cybercrim‬
‭
e.pdf‬
‭
,‬‭
https://www.hellenicshippingnews.com/gdp-doesnt-include-proceeds-of-crime-should-it/‬
‭
),‬
‭
there are often far more defenders than attackers, however, this is affected significantly by‬
‭
bounded rationality, practically expressed as education level, long-term goals or occupation (the‬
‭
foundation of democracy is an educated populace).‬
‭
Games are distributed in many places throughout the economy. They can take place in‬
‭
the‬‭
control planes‬‭
of attention, cyber, financial‬‭
and physical layers (ranked roughly in decreasing‬
‭
order by speed of change). Defense is often necessary in-depth at every layer, since all planes‬
‭
can modify the choices of an adversary and hence compel suboptimal decisions. There are‬
‭
likely trillions of games ongoing in the world, a handful that most of the world play, along with a‬
‭
long tail of games that are unique to each person and local community.‬
‭
Games are infinitely iterated and never fully won or lost. However, by modifying the‬
‭
offense/defense ratio (via introducing friction in information or technology processes) (often via‬
‭
timing changes to the visibility of information)‬
‭
3‬
‭
, they can be adjusted by those on a higher‬
‭
control plane to favor cooperative play. Modern SSL protocols which underpin Internet‬
‭
communications, for example, are designed by security researchers to have information‬
‭
asymmetry that makes a channel exponentially harder to attack than to defend.‬
‭
Technologies and their distribution patterns (also mediated by technology) affect‬
‭
advantage ratios significantly, especially high-variance technologies like AI.‬
‭
The Personal AI Entity Model‬
‭
We anticipate that the major deployment path of AI in the future will be towards‬‭
personal‬
‭
AI‬
‭
, which has a personal state added by fine-tuning‬‭
or altering foundation model behavior‬
‭
3‬‭
We refer to changes to information timing as “information asymmetry” in general (either infinite delay‬
‭
(0/1) or finite delay).‬
‭
behavior with user-generated state. In all cases, both attackers and defenders will have some‬
‭
type of capability in AI.‬
‭
The Benefits of Openness (re: 2)‬
‭
Evidence from science, economics, and cybersecurity defense argue for open-core‬
‭
foundation models being the safest default deployment configuration, although closed‬
‭
deployment configurations are not unreasonable to achieve specific design constraints.‬
‭
Hardening and traceability of weights and behaviors on networks are important tweaks to allow‬
‭
intermediate design choices between these two paradigms, although as of 2024, they need‬
‭
testing before being developing into standards‬
‭
(‬
‭
https://docs.google.com/document/d/1JkTIbLFYLhg3EzQDm3zuC1H0dNdx6RowlndUXq2Qwg‬
‭
Y/edit#heading=h.d3lmnf9jgc64‬
‭
).‬
‭
Scientific Progress & Education‬
‭
The advantages for science of reproducible and inspectable models enabled by open‬
‭
weights are so obvious that they almost do not need to be argued. The foundation of science is‬
‭
the scientific method and peer review, which relies on reproducible artifacts and evidence,‬
‭
especially in the case of shifting paradigms, which tend to be unpopular at first and demand‬
‭
rigorous demonstrations before believed. Openness also has a strong effect size on scientific‬
‭
progress. Without open models, science is limited to what a single local group can produce, a‬
‭
10,000-fold reduction in capacity at least for most fields, which means that in effect, science‬
‭
starves. And when science starves, a civilization eventually starves (or is outcompeted by rival‬
‭
civilizations). While it may be tempting to suggest “halfway” interventions like “methods and data‬
‭
are ok to share, but weights are not” (thereby imposing a compute cost on replications),‬
‭
restricting sharing of weights would easily slow down science 1,000-10,000x fold (hypothetically‬
‭
going from 3,000 independent validations per year to 3 validations/year). In addition to slowness‬
‭
of collaboration, this also would concentrate power into a handful of silos of compute and data,‬
‭
with ancillary political jockeying and a much smaller fringe to propose alternative paradigms,‬
‭
which are what lead to scientific revolutions‬
‭
(‬
‭
https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions‬
‭
).‬‭
Since education also‬
‭
uses the scientific method (at a personal level), and is the foundation of democracy, restrictions‬
‭
on information transfer also affect the foundations of civilization.‬
‭
Economic Equality‬
‭
The advantages of open weights to economic equality are also strong. A‬
‭
high-confidence and scaled physical risk from AI is job turnover and market concentration‬
‭
leading to extreme inequality, exacerbating the existing trends of globalization‬
‭
(‬
‭
https://www.theguardian.com/technology/2024/jan/15/ai-jobs-inequality-imf-kristalina-georgieva‬
‭
). Open weights are likely to be a significant hedge against resulting political unrest by‬
‭
distributing opportunity, similar to the way the open-source development of the LAMP stack‬
‭
facilitated the growth of millions of personal websites and early e-commerce in the early 90s. In‬
‭
addition, learning about and experimenting with technology is a much more prosocial “game”‬
‭
than crime, which, realistically, would be a competing game in cases of mass unemployment.‬
‭
Attention Equality‬
‭
Psychologically, humans have strongly negative reactions to fear and uncertainty.‬
‭
Without transparency and openness, the owners of closed models may come to be perceived‬
‭
as god-like (this sounds hyperbolic at the moment, but is not impossible if AI techniques are‬
‭
closed off and the rate of progress continues), and society may descend back into a primal‬
‭
mode dominated by warfare and superstition. In contrast, with open-weight models that can be‬
‭
transparently understood and replicated by anyone, “secret” techniques lose their allure of‬
‭
“magic,” leading to greater attention equality (and hence robustness of the human value‬
‭
network). We have some trends towards this as software shifted from publicly funded‬
‭
open-source to privately funded closed-source in the 2010s, which were also correlated with a‬
‭
significant rise in fraud preying on attention inequality and information asymmetries. While‬
‭
attention inequality may sound highly attractive to its beneficiaries, applied at society-wide‬
‭
scale, it is highly destabilizing, since the imbalance of power selects for competitive first-strike‬
‭
games which can spiral out of control (the dose makes the poison).‬
‭
Security‬
‭
The benefits of openness for security are not immediately obvious (given how prominent‬
‭
attacks using AI are), but become more apparent on rigorous inspection when considering the‬
‭
impact of AI on both attacking and defending roles. Most attacks defeated by AI as a defender‬
‭
are not visible - they just fail silently.‬
‭
User Defender Against Deployer Attacker - Independently Verifying A Model‬
‭
Consider the scenario of a user attempting to validate whether a closed model, such as‬
‭
GPT-4, is safe to trust. Since a closed model is not inspectable, it is not possible to fully‬
‭
understand the capabilities of a deployer attacker, only through the revealed behavior of short‬
‭
fragments of input/output interactions which may be purposefully tuned by the model to disguise‬
‭
latent behaviors and long-term goals.‬
‭
In contrast, the weights of an open model can be analyzed by many tools to understand‬
‭
the full spectrum of the model’s latent behavior. Open-weight models can also be run much‬
‭
faster in simulators than closed weight models (due to not hitting proprietary rate limits),‬
‭
allowing for security research over larger datasets, fuzzing, etc. They can also be run privately.‬
‭
In addition, the responsibility of inspecting specific model behaviors can be distributed to other‬
‭
experts in the community, allowing a million-fold increase in efficiency (“any bug is shallow given‬
‭
enough eyes”). In addition to finding bugs, making users aware of security risks and patching‬
‭
open-weight models is substantially easier than closed models, since this can be done‬
‭
adversarially, without needing the attention of the closed-model owner. Patches in practice‬
‭
compete with other priorities, especially in large companies, however, allowing affected users‬
‭
the ability to patch their own models distributes the responsibility. One researcher can identify‬
‭
the bug, one can identify and test a solution, a third can design a patch and a fourth can work‬
‭
on a press release, none of which have to work for the company which originally created the‬
‭
open model. While this depends on having a common channel for responsible disclosure (one‬
‭
place for government intervention), this is extremely efficient.‬
‭
This is also true from the perspective of a designer of a communications protocol for AIs.‬
‭
Since the capabilities of an open-weight model are better known, concrete theoretical assertions‬
‭
can be made, allowing for the design of more efficient defenses tuned to the behavior of the‬
‭
hypothetical adversary. In contrast, if capabilities are not known, defenses may be very‬
‭
expensive (to account for all possibilities), which in practice can lead to companies simply‬
‭
neglecting defense as “too expensive” and reverting to closed communication protocols, leading‬
‭
to information inequality. Open weights lead to open protocols which lead to open‬
‭
communications.‬
‭
The most severe risk is a wide perception-reality gap if information access is restricted‬
‭
by law and coupled with fear, as would happen in the case of banning open weights. In contrast,‬
‭
if information is widely available, individuals are able to independently verify assertions and‬
‭
defend against attacks. Scientific and market supervision is highly capable of designing‬
‭
context-specific cooperative games if data is available. In an economy where the civil GDP is‬
‭
100x larger than the dark web GDP, openness “just works.”‬
‭
In the extreme case, with a wide perception-reality gap in society caused by dark models‬
‭
operated for years without oversight, scaled physical threats like climate change can go‬
‭
unsolved even though society theoretically had the capacity to fix them.‬
‭
Deployer Defender Against User Attacker - Security Equality‬
‭
Spam & Phishing‬
‭
In the case of a deployer defender against a user attacker, open-weight models will likely‬
‭
increase the number of capable attacks and decrease the information asymmetry available to‬
‭
the deployer defenders. We have seen evidence of increased number of AI-generated attacks in‬
‭
the recent rise of spam emails and fraudulent websites in the last year‬
‭
(‬
‭
https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%‬
‭
2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac67b764-f0b1-4ba6-8fe‬
‭
6-dd594e8f9ace_960x540.png‬
‭
). Similarly, voice cloning has been used in adversarial attacks in‬
‭
the last year, however, notably, closed API services were used‬
‭
(‬
‭
https://www.politico.com/news/2024/01/29/biden-robocall-ai-trust-deficit-00138449‬
‭
),‬‭
and this‬
‭
turned out to be a red-team attack to draw awareness to the issue.‬
‭
Counterbalancing this, however, is that open foundational models allow for much wider‬
‭
and more efficient distribution of AI-powered defenses (security equality), which are‬
‭
independently verifiable (as established in the prior section). Given that there is 100x greater‬
‭
economic value that needs to be defended than is used for attack, defense is a universal need‬
‭
that needs to be exceptionally efficient, and open models provide this efficiency, both in initial‬
‭
distribution and in terms of improvements/patches. If an open model community is well-funded,‬
‭
and has efficient update channels, this is a strong working model (if not well-funded, then this‬
‭
works less well).‬
‭
AI defenses will likely get very strong‬
‭
(‬
‭
https://www.wired.com/story/fast-forward-nsa-warns-us-adversaries-private-data-ai-edge/‬
‭
).‬
‭
Given that cyberattacks‬‭
must‬‭
deviate from the normal communication patterns at some point to‬
‭
deliver a payload, stronger predictive models have a chance to force spam to carry so small of a‬
‭
payload that it cannot accomplish its task. At some point, the payload cannot be compressed‬
‭
any more. While we generally believe spam of‬‭
some type‬‭
will always be an issue, some‬
‭
common channels of spam may be able to be eliminated altogether.‬
‭
AI defenses, however, must be deployed quickly and at scale. This is urgently lacking for‬
‭
much of the web, and we need good open models to do so.‬
‭
Adversarial Attacks On AI Defenders‬
‭
Similarly, knowledge of the inner workings of an LLM may theoretically improve the‬
‭
ability to conduct more efficient attacks (via adversarial examples, for example, which are quite‬
‭
common in papers), these have not been commonly observed in the wild, likely due to their‬
‭
sophistication to pull off, the low rate of AI deployment and the higher ROI of less sophisticated‬
‭
attack methods on more weakly protected targets (at least for financially motivated actors).‬
‭
Although the criminal space is to fast to adopt AI techniques‬
‭
(‬
‭
https://csis-website-prod.s3.amazonaws.com/s3fs-public/publication/economic-impact-cybercri‬
‭
me.pdf‬
‭
), and this will be important to monitor, the threat will scale at the same speed as the AI‬
‭
deployment surface and is not the most urgent threat. It is also reasonably well addressed by‬
‭
hardening and responsible disclosure of general-purpose exploits.‬
‭
Biosafety‬
‭
A much repeated, but relatively unlikely risk, might come from the design of a biological‬
‭
virus that cannot be defended against once synthesized (self-replicating, self-distributing, scaled‬
‭
physical malware that can adapt faster than our immune system and is able to delay its lethality‬
‭
until it has spread to a very wide population). This is a significant risk since all the traceability‬
‭
and deterrence considerations do not apply to irrational, self-replicating particles without an‬
‭
executive function.‬
‭
However, open weights of LLMs only make it slightly easier to learn how to make‬
‭
bioweapons (‬
‭
OpenAI paper‬
‭
), while making it 10x easier to defend against synthesis of said‬
‭
biowepons (via cheap distributed AI defenses to screen for hazardous molecules). Indeed, we‬
‭
believe the‬‭
open letter and norms approach to bioweapons defense‬‭
already taken by the‬
‭
scientific community, along with selectively removing hazardous bioweapons training data from‬
‭
foundation models by default and pursuing international treaties with regards to bioweapons in‬
‭
war (similar to that done for nuclear weapons) will be much more fruitful paths to take than‬
‭
banning open weights, which seem tangentially connected to the problem at best.‬
‭
Hardening Deployments of Open Foundation Models‬
‭
(re: 3)‬
‭
While open-weight models have security advantages over closed-weight models, they‬
‭
do have some weaknesses, which can be patched by hardening both the weights and the‬
‭
deployment contexts in which weights are used:‬
‭
Hardening‬
‭
A simple approach to defend against adversarial attacks against introspectable‬
‭
open-weight models is fine-tuning against private data kept offline. The dataset can be drawn‬
‭
from positive examples of attacks found by a well-funded red-team community (adversarial‬
‭
hardening), or merely human-curated moderation data from the deployment team. If models are‬
‭
sufficiently easy to update (via coordinated disclosure channels, for example), distributing‬
‭
hardened updates should be relatively straightforward as well.‬
‭
Traceability‬
‭
Deterring spam and phishing attacks will be important in the 2024-2025, as the network‬
‭
updates to new AI capabilities for both attackers and defenders.‬
‭
Physical Security Keys‬
‭
Upgrading accounts on important subnets to use physical security keys (Yubikey,‬
‭
FaceID) can both reduce the annoyance of visual CAPTCHAs and stop phishing attacks‬
‭
(physical security keys provide cryptographically secure demonstrations of authentication and‬
‭
humanness). A touch or biometric verification makes it significantly easier to attribute behaviors‬
‭
to an account, ties the account to a physical object which cannot easily change locations, while‬
‭
imposing minimal cost on a user and preserving privacy (since the account is not necessarily a‬
‭
real-name account). In the future, it may be the case that everyone has a couple security keys,‬
‭
associates them with a small number of pseudoidentities (<100), and has each pseudoidentity‬
‭
be accountable by karma to a combination of a personal AI under their control in combination‬
‭
with other user’s AI they interact with. Notably, security keys defend with perfect F1 against‬
‭
login attacks and add high traceability of both AI and human-generated phishing attacks.‬
‭
Verification Models‬
‭
Verification APIs can already defend against both human and AI-generated spam‬
‭
attacks with higher speed and accuracy than many human networks (see‬
‭
www.verificationgpt.org‬
‭
). Scaling up the number of subnets that are verified is easy and will be a‬
‭
key priority this year.‬
‭
Remote Passport Verification‬
‭
Building a simple email API for remote passport verification from public signatures‬
‭
embedded in e-passports is immediately necessary to prevent high-value fraud (title fraud, for‬
‭
example) using synthetic ids. This would be useful worldwide and arguably is uniquely the‬
‭
responsibility of the government, which owns the e-passport system.‬
‭
Watermarking‬
‭
For many creative generative AI applications, adding invisible watermarking in outputs is‬
‭
possible (‬
‭
https://deepmind.google/discover/blog/identifying-ai-generated-images-with-synthid/‬
‭
)‬
‭
and would quite helpful (to assist defender AIs against spam). This can be verified via an API‬
‭
provided by the creator. This is easy and arguably a pressing need, while not incurring a large‬
‭
privacy cost (bit would not be user identifying but would identify it as AI-generated or not).‬
‭
Weight tainting‬
‭
For models where strict traceability of weights to a user is required, weight tainting is a‬
‭
theoretical possibility, although untested to our knowledge. In weight tainting, authenticated‬
‭
users download per-user altered weights which have been perturbed with a statistical‬
‭
transformation of unique information associated with the user account (like a pseudorandom‬
‭
number), while the base weights and the specific information tag that was associated are kept‬
‭
offline in secure storage. If those weights are found again in the wild, they can be compared by‬
‭
the deployer with the offline information and attributed to a specific user, enabling legal action.‬
‭
Similarly, identifying which patterns comprise the “serial number” (to remove) would require‬
‭
difficult-to-coordinate collaboration of users, especially for large models. While we do not‬
‭
recommend this as a default or legally required procedure, and there are various technical‬
‭
considerations to investigate, it may be appropriate for some types of highly sensitive models,‬
‭
and can be combined with per-user watermarking to trace outputs as well.‬
‭
Early Access Tokens‬
‭
Early Access Tokens are a hypothetical approach that can help broaden the scope of a‬
‭
gift economy in AI development. These work by tracking contributions to open-source software‬
‭
and science, and enabling early access to AI source code and weights for accounts with high‬
‭
karma, using a token tied to a physical security key. Since the overlap of malicious and‬
‭
well-intentioned users is relatively small, this potentially can be useful to allow asynchronous,‬
‭
unplanned sharing of code and weights with responsible collaborators across corporate‬
‭
boundaries, while adding friction to adversarial access (karam is required to be accumulated). A‬
‭
similar, more manual process, is sharing user-tainted weights preferentially with known security‬
‭
researchers, however, this may miss many of the benefits of unplanned, asynchronous‬
‭
development.‬
‭
Differentiable Credit Licenses‬
‭
Similarly,‬‭
differentiable credit licenses‬‭
can be used in combination with the karma‬
‭
systems to help defend against free-riding financial attacks in model development. In‬
‭
combination with weight tainting (which adds detection capabilities), this could potentially add‬
‭
deterrence needed to enforce payment, but still allow profitable collaboration well beyond the‬
‭
narrow boundaries of a single company, an capability exceedingly needed if we are to preserve‬
‭
a healthy larger economy.‬
‭
Towards Public-Private Partnership (re: 4)‬
‭
In our assessment, the government’s support initially should be in administering‬
‭
standardization processes and RFCs (such as this one), legislating well-scoped mandates to‬
‭
add transparency to models and model outputs for high-scale deployments, funding defensive‬
‭
research, supporting responsible disclosure programs which would otherwise be underinvested‬
‭
by the private market, and participating in and administering open standards bodies, as well as‬
‭
negotiating relevant bioweapons treaties with foreign powers to prevent shared risks. Finally‬
‭
supporting testing and scaling approaches outlined in (3) could position the United States to‬
‭
handle the AI transformation well.‬
",Context Fund,12157
NTIA-2023-0009-0254,"The Partnership on AI response to the
NTIA Request for Comment (RFC) on
Dual Use Foundation Artificial
Intelligence Models with Widely
Available Model Weights
Background
Partnership on AI (PAI) is a non-profit partnership of academic, civil society,
industry, and media organizations creating solutions to ensure that AI advances
positive outcomes for people and society. PAI studies and formulates
sociotechnical approaches aimed at achieving the responsible development of
artificial intelligence (AI) and machine learning (ML) technologies. Today, we
connect over 100 partner organizations in 14 countries to be a uniting force for
the responsible development and fielding of AI technologies.
PAI develops tools, recommendations, and other resources by inviting
multistakeholder voices from across the AI community and beyond to
share insights that can be synthesized into actionable guidance. We then
work to promote adoption in practice, inform public policy, and advance
public understanding. We are not an industry or trade group nor an
advocacy organization. We aim to change practice, inform policy, and
advance understanding.
The information in this document is provided by PAI and is not intended to
reflect the view of any particular Partner organization of PAI. The comments
provided herein are intended to provide evidence-based information, based on
PAI’s research, in response to NTIA’s RFC.
1
Executive Summary
PAI welcomes the opportunity to provide this input to NTIA’s work under section
4.6 of the Executive Order on “Safe, Secure, and Trustworthy Development and Use
of Artificial Intelligence” (the “AI Executive Order”).
This submission responds to NTIA’s request for comment (RFC) on:
●
Risks and benefits of dual use foundation models with widely available
model weights
●
Potential voluntary, regulatory, and international mechanisms to manage
the risks and maximize the benefits of these models.
This paper adopts the RFC’s usage of “open foundation models” to refer to models
with widely available model weights, while noting that model weights are not the
only component of models that can be released by providers.1
As the RFC notes, open foundation models promise a number of benefits. However,
all models — whether they are open or more closed — present risks. It is vital that
appropriate steps are taken to address these risks while promoting benefits. To
get this balance right, any mechanisms for risk mitigation for foundation models
should be evidence-based, and be appropriately tailored to target identified risks,
including by taking into account model capabilities and release type. Applying
these principles to open foundation models, specific risk mitigation measures
should only be imposed on open foundation models when that is necessary to
address some differential or marginal risk posed by those models.
This submission draws on work undertaken by PAI in preparing its Guidance for
Safe Foundation Model Deployment (the “Model Deployment Guidance”). The
Guidance is a voluntary framework for foundation model providers, containing
detailed recommendations to identify and mitigate risks associated with these
models.2 It provides specialized guidance tailored to different model capabilities,
and to different model release types — including releases where model weights are
made widely available at the time of deployment.
PAI submits that NTIA’s work under the AI Executive Order should be informed by
the following principles:
●
All foundation models need risk mitigations. Appropriate risk
identification and mitigation practices should be adopted for the
2 PAI’s Model Deployment Guidance defines foundation models “to encompass all models with
generally applicable functions that are designed to be used across a variety of contexts. The
current generation of these systems is characterized by training deep learning models on large
datasets (which requires significant computational resources) to perform numerous tasks
that can serve as the “foundation” for a wide array of downstream applications.” Model providers
are defined to be those training foundational models (proprietary or open-source) that others may
build on as well as interfaces to interact with the models.
1 In PAI’s recent Guidance for Safe Foundation Model Deployment, we use the term “open access”
release, to refer to models released publicly with full access to at least model weights.
2
development and deployment of all foundation models — from narrow
specialized models to frontier models, and from fully closed models to fully
open models.
●
Appropriate risk mitigations will vary depending on model
characteristics. PAI’s Model Deployment Guidance tailors its
recommendations according to model capability and release type.
●
Risk mitigation measures, for either open or closed models, should be
proportionate to risk. Recommended risk management practices for both
open and closed foundation models should be proportionate to the risks
posed, should be based upon the best evidence available, and should be
updated to reflect new evidence as it emerges.
●
Voluntary frameworks are part of the solution. Existing voluntary risk
management frameworks, such as PAI’s Model Deployment Guidance, play a
valuable role in providing benchmarks for safe foundation model
deployment, including when it is proposed to make model weights widely
available, and can continue to do so as the policy landscape evolves.
Research on the benefits and risks of open foundation models, and appropriate
risk mitigation measures, is ongoing. This year, PAI will be continuing its work on
this topic consulting with its partners to explore these issues and identify
current and emerging best practices. PAI would be happy to update NTIA as this
work progresses, and to contribute to future stages of NTIA’s work on this
issue.
3
Summary of Recommendations
1.
In fulfilling its tasks under section 4.6 of the AI Executive Order, NTIA should
consult widely with all relevant stakeholders from civil society, academia,
and private industry, ensuring all have equal opportunity to contribute.
Diverse perspectives are essential to inform the NTIA on all relevant issues
including the benefits of open models and risks. Multistakeholder forums
such as PAI play an important role in facilitating cross-sectoral
participation. PAI would be happy to assist NTIA in this process.
2. NTIA should promote the role and adoption of voluntary risk management
frameworks such as PAI’s Model Deployment Guidance by providers of both
open and closed foundation models. NTIA should consider the role these
frameworks play in addressing risk right now, and when assessing other
potential policy or regulatory responses in fulfilling its mandate under the
AI Executive Order.
3. NTIA should recommend the introduction of mechanisms restricting the
deployment and use of foundation models (whether open or closed) only
where that is necessary and proportionate to mitigating risk, on the basis
of the best available evidence.
4. NTIA should recommend the introduction of mechanisms specifically
restricting the deployment and use of open foundation models only where,
on the best available evidence, that is necessary and proportionate to
mitigating risks specifically associated with the wide availability of the
model’s weights. That is, open foundation models should not be subject to
special restrictions unless they pose a higher risk than closed models, or
pose some risk that closed models do not.
4
Open foundation models/“models with widely available model
weights”
RFC questions 1-33
As recognized in the AI Executive Order, AI brings the promise of great benefits,
promoting innovation and productivity, and helping solve today’s most pressing
challenges. It can also present significant risks.4 These observations are
particularly relevant for foundation models. Responsible foundation model
deployment requires that providers implement appropriate mechanisms to
identify and mitigate risk across the model lifecycle. PAI’s Model Deployment
Guidance represents our most recent work in this area, containing
recommendations to assist providers operationalize AI safety principles. The
Guidance reflects the fact that risk mitigations are necessary for all foundation
models for all release types, and all model capabilities. Mitigations should also
be scaled to according to model characteristics, to ensure that higher risk models
are subject to additional safeguards.
Risks and Benefits of Open Access Models
As acknowledged in the RFC, open foundation models can lead to a range of
benefits, including by increasing access to models, enabling research, and
promoting transparency. PAI supports policy settings that enable responsible
open access model deployments, as reflected by its endorsement of the recent
open letter led by PAI partners, the Center for Democracy and Technology and
Mozilla. Consistently with that letter, restrictions or specific safety guidance for
open foundation models are only warranted when those models present specific
risks that are greater or different from those presented by closed models. This is
consistent with the approach taken in PAI’s Model Deployment Guidance — which
generates custom risk guidance for deployers that is scaled to model risk based
on both the capabilities and the release type of a particular model.
4 AI Executive Order, section 1.
3Question 1 of the RFC: How should NTIA define “open” or “widely available” when thinking about
foundation models and model weights?
Question 2 of the RFC: How do the risks associated with making model weights widely available compare to
the risks associated with non-public model weights?
Question 3 of the RFC: What are the benefits of foundation models with model weights that are widely
available as compared to fully closed models?
5
Marginal risk of open foundation models
Like closed foundation models, open access models can present risks.5 A recent
case study from PAI’s AI and Media Integrity program records the use of open
foundation models in a global elections context, and notes the limitations of
downstream mitigation measures for synthetic media harm that rely solely on
responsible practices by “good actors.” However, this does not mean such
mitigations lack value. The level of friction these mitigation measures are able to
create to slow the spread of harmful synthetic media makes them still worth
implementing.
In assessing the risk posed by open foundation models, and appropriate
measures to address those risks, policy makers should focus on the marginal
risks associated with open access release.6 This approach reflects the fact that
both open and closed models present risks (and warrant mitigations to address
those risks). Risk mitigation measures that specifically target open models, or
impose differential restrictions on open models, should only be recommended
where there is evidence that open models may pose risks beyond those posed
by closed models.
Some of the features of open models that may be relevant to assessing
differential risk include that open release of model weights is irreversible, and
that moderation/monitoring of open models post-release is challenging.
These factors are particularly relevant in the context of frontier models. PAI’s
Deployment Guidance defines “frontier” or “paradigm-shifting” models as:
“Cutting edge general purpose models that significantly advance
capabilities across modalities compared to the current state of the art.”7
In assessing whether a model meets this definition, considerations include:
●
Does the model enable significantly more advanced capabilities compared
to current state-of-the-art?
7Partnership on AI, Guidance for Safe Foundation Model Deployment (2023).
6 PAI’s Model Deployment Guidance uses the terminology “open access release” to refer to models
where at least model weights are released at deployment. Other model components - such as data
and code - might also be released. There is substantial overlap between this concept and that of
“models with widely available model weights.”
5 Foundation models (both open and more closed) can give rise to a variety of risks, including risks
arising from models themselves and also from downstream use. PAI’s Model Deployment Guidance
includes a mapping of in-scope foundation model risks:
https://partnershiponai.org/modeldeployment/#learn_more.
6
●
Does the model utilize parameters or computational resources that greatly
exceed current standards, demonstrating a breakthrough in scalable
training?
●
Does the model show evidence of self-learning capabilities exceeding
current AI?
●
Does the model provider enable execution of commands, or actions directly
in the real world through released interfaces or applications, beyond
passive information processing?
Frontier models present some particular safety challenges. Capabilities of these
models, including dangerous capabilities, can emerge unexpectedly. They can
therefore present more “unknown unknowns”.
“[I]t is difficult to robustly prevent a deployed model from being misused;
and, it is difficult to stop a model’s capabilities from proliferating broadly.”
For this reason, PAI’s Model Deployment Guidance recommends that frontier or
paradigm-shifting models not be subject to open access release without first
probing for those risks, and assessing the effectiveness of risk mitigations. As the
Guidance states:
“We recommend providers initially err towards staged rollouts and
restricted access to establish confidence in risk management for these
systems before considering open availability.
“These models may possess unprecedented capabilities and modalities not
yet sufficiently tested in use, carrying uncertainties around risks of misuse
and societal impacts. Over time, as practices and norms mature, open
access may become viable if adequate safeguards are demonstrated.”7
This is consistent with the principles set out at the beginning of this section —
that appropriate risk mitigations should be adopted for all foundation models
prior to deployment, and that those mitigations should be appropriately tailored
to respond to relevant risks.
Recommendations to NTIA for open foundation models
In considering what policy mechanisms are appropriate for open foundation
models, PAI submits that the NTIA:
7
●
Should recommend imposing restrictions on foundation models (whether
open or closed) only where that is demonstrated to be necessary and
proportionate to mitigate risk
●
Should recommend that specific restrictions be placed on open access
models only when that is demonstrated to be necessary and proportionate
to mitigating a category of risk that is specifically associated with the open
access release of the model.
PAI will be continuing its work on open access models this year, including
through a workshop in early April to explore the need for and effectiveness of risk
management strategies for the open foundation model ecosystem; to assess
current guidance for open access release of state-of-the-art models; and to
consider the effectiveness of current and potential new harm reduction
strategies. PAI will update NTIA as this work progresses, and contribute to
future stages of NTIA’s work on this issue.
PAI’s Model Deployment Guidance
(RFC question 7)8
The RFC calls for submissions addressing current and potential mechanisms to
manage the risks and maximize the benefits of open foundation models. One
such mechanism is PAI’s Guidance for Safe Foundation Model Deployment (the
“Model Deployment Guidance”).
PAI’s Model Deployment Guidance is a voluntary set of practices intended to help
foundation model providers identify and mitigate model risks, and inform public
policy and understanding. PAI developed the Guidance working with stakeholders
from over 40 global institutions, including model providers, civil society
organizations, and academic institutions.
The Guidance contains 22 recommendations for safe foundation model
deployment practices across the product lifecycle, from research and
development through to post-deployment monitoring and decommissioning.
A key feature of the Guidance is that it allows developers to generate customized
guidelines that are tailored to two key model attributes: model capability and
release type. The recommended guidelines are more extensive for more capable
models and more available release types. The Guidance defines:
8 Question 7 of the RFC: What are current or potential voluntary, domestic regulatory, and international
mechanisms to manage the risks and maximize the benefits of foundation models with widely available
weights? What kind of entities should take a leadership role across which features of governance?
8
●
Four categories of model release: Open Access; Restricted API and Hosted
Access; Closed Development; and Research Release.
●
Three categories of model capability: Specialized Narrow Purpose;
Advanced Narrow and General Purpose; and Paradigm-shifting or Frontier.
This approach allows the Guidance to provide risk management guidelines that
are appropriately adapted to the risk profile of particular models.
Figure 1: visualization of the scaled approach to safety guidelines under the
Model Deployment Guidance
A full list of the 22 guidelines can be found here; further information about the
Model Deployment Guidance can be found here; and customized Guidance can be
generated here.
Relevant to NTIA’s current work:
●
The Model Deployment Guidance contains safety recommendations for the
development/deployment of both open and closed models, that is adapted
to the risks and appropriate mitigations for each of those categories. This
approach reduces the compliance burden on providers of less risky models,
while ensuring that riskier models are subjected to more stringent safety
practices. The approach in the Guidance reflects the fact that model release
type is one of a number of relevant factors in assessing what risk
management measures should be implemented by model providers to
9
ensure safe deployment. (The Guidance uses the term “open access” to
refer to model releases involving the release of at least model weights.)9
●
The Model Deployment Guidance can generate guidance for foundation
models of all capabilities, including “dual-use foundation models”.10
The Model Deployment Guidance states the following:
●
Risk management practices are necessary for all foundation models.
●
The risks posed by foundation models vary, depending on model
capability and release type (including open access release).
●
Risk management measures for foundation models should be flexible, so
they are appropriate and adapted to the risks posed by those models.
Recommendations developed by NTIA should reflect these principles.
As explained in the supporting materials for the Model Deployment Guidance, the
Guidance was not developed to replace regulation or other policy frameworks.
Rather, the Guidance aims to complement other governance and regulatory
approaches. In the absence of policy frameworks, voluntary mechanisms like the
Guidance play a valuable role in providing recommendations for risk management
practices that can be adopted right now by a wide range of model providers. This
helps ensure that benefits from these models, including open foundation models,
are harnessed while risks are managed. Voluntary frameworks play a particularly
important role in setting a benchmark for safe practice while work continues to
better understand the risks associated with various types of model release.
We urge NTIA to encourage the adoption of voluntary risk management
frameworks such as PAI’s Model Deployment Guidance by providers of both
open and closed foundation models. NTIA should consider the role these
frameworks play in addressing risk right now and providing an initial benchmark
for other potential policy or regulatory responses in fulfilling its mandate under
the AI Executive Order.
10 The RFC mirrors the AI Executive Order in considering risk mitigation for dual-use foundation
models; it gives a definition of those models, including an interim definition for model reporting
that is based on compute. There is some discussion in the literature (see also here) about how well
this definition tracks frontier model capabilities. PAI’s Model Deployment Guidance does not
specifically address “dual-use foundation models” as a separate category; however because the
Guidance generates customized recommendations for models of all capabilities, it can generate
guidance for dual-use models.
9 While noting that the definition of “widely available model weights” is one factor NTIA is
considering under the AI Executive Order, there is likely to be significant overlap between the
category of models with “widely available model weights” and that of models subject to “open
access release”.
10
Conclusion
PAI would be happy to provide further information about any of the matters
discussed in this submission. We look forward to the NTIA’s pending report.
For any further information and questions related to this submission, please
contact John@partnershiponai.org (and copy policy@partnershiponai.org).
11
",Partnership on AI,4082
NTIA-2023-0009-0242," 
 
1 
 
March 27, 2024 
 
U.S. Department of Commerce 
National Telecommunications and Information Administration 
1401 Constitution Avenue N.W. 
Washington, D.C. 20230 
 
Via electronic filing 
 
Re: Access Now’s Submission to the NTIA concerning Dual Use Foundation Artificial Intelligence 
Models With Widely Available Model Weights 
 
Docket No. 240216-0052 
 
Dear Bertram Lee and members of the National Telecommunications and Information Administration,  
 
On behalf of Access Now, we are pleased to submit our comments in response to the National 
Telecommunications and Information Administration’s (NTIA) Request for Comment (RFC) regarding 
dual-use foundation artificial intelligence (AI) models with widely available weights.1 We appreciate 
the opportunity to engage in this public consultation process and trust it will lead to a meaningful 
report on the potential human rights considerations necessary in governing open advanced AI 
models.  
 
Access Now is an international organization that defends and extends the digital rights of people and 
communities at risk worldwide.2 As part of our mission, we operate a global digital security helpline 
for human rights defenders and journalists to identify and mitigate specific threats to their digital 
security.3 We also engage with fellow non-profit organizations and activist communities across civil 
society and campaign to ensure that new and emerging technologies and their investors, developers, 
and implementers “do no harm” first and foremost. 
 
Our comments are designed to highlights the need for greater research to inform the debate on the 
risks and benefits of open models, flag the dominance of ‘Big Tech’ in AI infrastructure, and propose 
considerations vital for safeguarding human rights in the AI development process. Particularly, the 
need to ensure the development of foundation models are done in a resource friendly way taking into 
consideration the impacts on marginalized communities and non-developed countries. 
 
 
 
 
1 https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-foundation-artificial-
intelligence-models-with-widely-available-model-weights.  
2 https://www.accessnow.org/. 
3 https://www.accessnow.org/help/helpline-services/.  
 
 
2 
I. 
The Debate over Open versus Closed (Questions 2 and 3)  
 
A key debate around open foundation models centers on their universal accessibility, allowing anyone 
to access, modify, and alter their algorithms. Advocates of openness argue that this accessibility can 
lower the entry barriers for various actors, democratizing AI development.4 Moreover, it could 
potentially reduce industry monopolization5 by encouraging broader participation and collaboration.6 
Advocates also argue that such openness can enhance safety and transparency, as it enables 
comprehensive auditing.7 Open foundation models allow more actors to participate and collaborate 
in AI research and development and be involved in threat detection including detecting and fixing 
vulnerabilities, biases, and other imperfections present in open models.8 However, scholars also 
caution that the efficacy of auditing as a safety measure hinges on the availability of significant 
resources and aligned incentives.9 
 
On the other hand, one of the core risks of open foundation models stems from the fact that because 
everyone can access, build, and alter the weights, the developers of these models have no control 
over who has access to them and how they are used. As a result, bad actors might exploit these open 
models for their own benefits.10 Nefarious actors can access them, remove built-in safety features, and 
potentially misuse them for malicious purposes, from malevolent actors creating disinformation to 
generating harmful imagery and deceptive, biased, and abusive language at scale. Monitoring the 
spread and malicious use of open foundation models is another challenge, mainly due to their 
widespread availability, and effective oversight hinges largely on the willingness of those employing 
those models to be transparent.11  
 
 A clear divide exists between the powerful and influential actors advocating for restricted access, or 
closed access, to their foundation models and those releasing their models openly. Companies such 
as OpenAI contend that their models are too powerful to release fully openly, thus only allowing 
access solely through APIs (Application Programming Interfaces). OpenAI has been explicit about the 
reasons for choosing an API model over open sourcing their models, highlighting concerns around 
 
4 Sayash Kapoor, et al., On the Societal Impact of Open Foundation Models, Stanford University HAI (Feb. 27, 
2024), https://hai.stanford.edu/news/societal-impact-open-foundation-models; Kyle Miller, Open Foundation 
Models: Implications of Contemporary Artificial Intelligence, Center for Security and Emerging Technology (CSET) 
(Mar. 12, 2024), https://cset.georgetown.edu/article/open-foundation-models-implications-of-contemporary-
artificial-
intelligence/#:~:text=For%20some%2C%20safety%20concerns%20may,misuse%20them%20for%20malicious%
20purposes.  
5 Jai Vipra and Anton Korinek, Market concentration implications of foundation models: The Invisible Hand of 
ChatGPT, Brookings Institute (Sept. 7, 2023), https://www.brookings.edu/articles/market-concentration-
implications-of-foundation-models-the-invisible-hand-of-chatgpt/.  
6 Kapoor, et al., On the Societal Impact of Open Foundation Models. 
7 Id.  
8 Id. 
9 David G. Widder, Sarah Myers West, and Meredith Whittaker, Open (For Business): Big Tech, Concentrated Power, 
and the Political Economy of Open AI (Aug. 17, 2023), at 16, http://dx.doi.org/10.2139/ssrn.4543807 
10 Id. at 17; Irene Solaiman, The gradient of generative AI release: Methods and considerations, (Proc. of the 2023 
ACM Conference on Fairness, Accountability, and Transparency, June 2023) at 8-9, 
https://arxiv.org/pdf/2302.04844.pdf. 
11 Miller, Open Foundation Models: Implications of Contemporary Artificial Intelligence.  
 
 
3 
misuse, the complexities and costs associated with deploying large models, and the ability to manage 
and mitigate risks more effectively. The company argues that the potential dangers of their AI 
products necessitate restricted access, maintaining control over them.12 
 
The challenge lies in the fact that both arguments hold merit: should we place our trust in large, 
powerful companies to wield control over these models and to ‘do the right thing’, or should we 
strive to mitigate the evident risks stemming from increasingly powerful open foundation 
models? 
 
II. 
Corporate Dominance in AI Infrastructure (Question 9)  
 
The debate between open and closed foundation models often overshadows a critical underlying 
issue: the dependency on infrastructure necessary for AI development, namely chips, data, and 
computational power.13 While it is obviously important to do what we can to prevent the harms that 
can come from open and closed AI models, such as increased production of non-consensual intimate 
images (NCII), the NTIA must think broadly about how developments in AI are reshaping or 
consolidating corporate power, especially with regard to ‘Big Tech.’  
 
Infrastructure is crucial and understanding the dynamics of companies like Amazon Web Services 
(AWS) sheds light on the broader implications. For instance, StabilityAI, known for its open-source 
image generation model Stable Diffusion, secured $100 million in funding, but according to Semafor, 
burned through a significant portion of that money in paying a massive AWS bill almost immediately.14 
Interestingly, Amazon also announced that it will invest up to $4 billion in the not-open-source AI 
startup Anthropic,15 illustrating a central point: the outcome of the debate on open versus closed AI is 
on a basic level irrelevant to infrastructure providers. One way or another, with increased demand for 
compute power, this translates into significant revenue. 
 
Microsoft's relationship with OpenAI, despite the latter’s closed nature, and its partnerships and 
investments with companies like the French company Mistral, which takes an open-source approach, 
further exemplifies the situation.16 Companies with significant computing infrastructure are 
incentivized to support the proliferation of AI technology, open or closed, because it leads to 
increased demands for computing power.  
 
 
12 Cecilia Kang, OpenAI’s Sam Altman Urges A.I. Regulation in Senate Hearing, Washington Post (May 16, 2023), 
https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html.  
13 Id. 
14 Reed Albergotti, Stability AI is on shaky ground as it burns through cash and looks at a management overhaul, 
Semafor (Apr. 7, 2023), https://www.semafor.com/article/04/07/2023/stability-ai-is-on-shaky-ground-as-it-
burns-through-cash.  
15 Jeffrey Dastin, Amazon steps up AI race with Anthropic investment, Reuters (Sept. 29, 2023), 
https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-
2023-09-25/.  
16 Microsoft and OpenAI extend partnership, Microsoft Blog (Jan. 23, 2023), 
https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/; Emilia Dang, Microsoft’s 
Mistral deal beefs up Azure without spurning OpenAI, The Verge (Mar. 4, 2024), 
https://www.theverge.com/24087008/microsoft-mistral-openai-azure-europe. 
 
 
4 
The current discourse on open versus closed offers no clear alternative to the dominance of ‘Big 
Tech’ in AI. Regardless of a model's openness, the necessary infrastructure remains under the 
control of a few major players. Thus, those with the means to provide compute resources stand to 
benefit in either scenario, underscoring a fundamental challenge in shifting away from Big Tech's grip 
on AI development. 
 
III. 
Power Asymmetries, Environmental Impacts, and Labor Exploitation (Question 9) 
 
In the growing debate on how to regulate the use of AI systems, including, open or closed foundation 
models, considering the impact of AI on the Global South and the colonial context in which much of 
this material and labor exploitation takes place is crucial.17 What does a foundation model depend 
on? What is required to build it? What does it extract from the planet? These are critical 
questions and issues we urge NTIA to take into consideration.  
 
As it stands, many of the social and economic benefits of artificial intelligence remain geographically 
concentrated in Western countries and deployed globally, which can asymmetrically impose cultural 
values.18 Building large-scale models also requires significant human labor, a task that is often 
outsourced, widening the gap between the corporations that design and market these technologies 
and the adverse working conditions involved in their development and training. For instance, 
Google’s Bard model relied on hired workers recruited through outsourcing firms, who received 
minimal training and reportedly worked under tight deadlines.19 Indeed, documents showed workers 
were under pressure to meet deadlines under three minutes.20 Similarly, OpenAI's employment of 
workers in Kenya via an outsourcing firm subjected workers to continuously engage with harmful 
content for low wages, devoid of any support.21  
 
The development of AI not only demands significant human labor but also consumes vast planetary 
resources. For example, the mining of rare earth minerals critical for AI technology often occurs in 
places like the Congo,22 while the Global South faces environmental degradation from the disposal of 
 
17 Anibal Monasterio Astobiza, et.al, Ethical Governance of AI in the Global South: A Human Rights Approach to 
Responsible Use of AI, (Proc. of the 2021 Summit of the Intl’ Society for the Study of Info., 2022), 
httpps://doi.org/10.3390/proceedings2022081136.  
18 Id.; Shakir Mohamed, Marie-Therese Png, and William Isaac, Decolonial AI: decolonial theory as sociotechnical 
foresight in artificial intelligence, (Philosophy & Technology, 2020) at 7, 9, https://arxiv.org/pdf/2007.04068.pdf.  
19 Hasan Chowdhury, Google's ChatGPT rival is trained by workers who are under pressure to audit AI answers in as 
little as 3 minutes, documents show, Business Insider (Jul. 12, 2023), https://www.businessinsider.com/googles-
bard-ai-chatgpt-trained-under-pressure-workers-2023-
7#:~:text=Google's%20Bard%20is%20trained%20by,and%20are%20given%20minimal%20training;  
20 Id.  
21 Annie Njanja, Workers that made ChatGPT less harmful ask lawmakers to stem alleged exploitation by Big Tech, 
Tech Crunch (Jul. 14, 2023), https://techcrunch.com/2023/07/14/workers-that-made-chatgpt-less-harmful-ask-
lawmakers-to-stem-alleged-exploitation-by-big-tech/?guccounter=1; OpenAI and Sama hired underpaid Workers 
in Kenya to filter toxic content for ChatGPT, Business and Human Rights Resource Center (Jan. 23, 2023), 
https://www.business-humanrights.org/en/latest-news/openai-and-sama-hired-underpaid-workers-in-kenia-
to-filter-toxic-content-for-chatgpt/.  
22 Monasterio Astobiza, et.al, Ethical Governance of AI in the Global South: A Human Rights Approach to 
Responsible Use of AI.  
 
 
5 
toxic byproducts.23 Highlighting the environmental toll, Timnit Gebru, Emily Bender, Angelina 
McMillan-Major, and Margaret Mitchell in ""On the Dangers of Stochastic Parrots: Can Language Models 
Be Too Big?"" point out the substantial carbon footprint of AI model training, comparable to the energy 
used for a trans-American flight.24 This disparity underscores the need for a more equitable and 
sustainable approach to AI development and deployment. 
 
Companies often disclose little to no information about the labor practices that support their data 
work used to fuel their model. This opacity creates an additional obstacle to democratic and open 
access to resources essential for the creation, training, and deployment of large-scale models. We 
therefore urge for the NTIA to advocate for transparency from developers regarding the 
environmental impact and labor conditions throughout the lifecycle of their foundation models. 
 
Furthermore, we urge the NTIA to advocate for public-facing transparency standards on 
resource consumption and greenhouse gas emissions of foundation models. Such standards 
should cover all aspects of model development, including design, data management, and training, to 
inform the public and policymakers about the ecological footprint of these technologies and support 
the creation of sustainable AI policies. 
 
IV. 
Conclusion  
 
Navigating the complexities of AI governance requires a nuanced approach, particularly concerning 
the dichotomy between open and closed foundation models. This calls for more research to 
empirically assess and inform the ongoing debate about these issues, including the marginal 
risks of open systems.25 There is currently no obvious alternative to the dominance of ‘Big Tech’ 
in AI and the necessary infrastructure remains under the control of a few major players that 
stand to benefit either way.  
 
The forthcoming report from the NTIA offers a chance to promote the development and use of AI 
systems in a sustainable, resource-friendly way that considers the impact of models on marginalized 
communities and how those communities intersect with the Global South. Because these 
communities are often excluded from conversations and decision making, it’s imperative the NTIA 
assesses how opening or closing models influences these communities, recognizing both the unique 
benefits and risks involved. 
 
For any questions or to connect with us about our work please contact Willmary Escoto, U.S. Policy 
Counsel, willmary@accessnow.org.  
 
23 Id.  
24 Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Margaret Mitchell, On the Dangers of Stochastic 
Parrots: Can Language Models Be Too Big? (Conf. on Fairness, Accountability, and Transparency, 2021) at 612, 
https://dl.acm.org/doi/pdf/10.1145/3442188.3445922  
25 Kapoor, et al., On the Societal Impact of Open Foundation Models.  
",Access Now,3549
NTIA-2023-0009-0257,"March 27, 2024
National Telecommunications and Information Administration
1401 Constitution Avenue NW
Washington, DC 20230
submitted via https://www.regulations.gov
Comment of the AI Policy and Governance Working Group
on the NTIA Request for Comment
on Dual Use Foundation Artificial Intelligence Models
with Widely Available Model Weights
Docket NTIA-240216-0052
The Biden-Harris Administration is soliciting insights about the “potential risks, benefits,
other implications, and appropriate policy and regulatory approaches to dual-use
foundation models for which the model weights are widely available” via the Department
of Commerce’s National Telecommunications and Information Administration (NTIA)
request for comment. We commend the Administration’s commitment to public
consultation on an issue so important to the safe, secure, and trustworthy advancement,
development, and use of artificial intelligence (AI).
Openly available data, code, and infrastructure have been critical to the advancement of
science, technological innovation, economic growth, and democratic governance. These
open resources have been built and shared in the context of commitments to open
science, to expanding industry and markets, and to the principle that some technologies
should be widely available for maximum public benefit, while allowing for control of
access to data, code, and infrastructure as necessary for safety and security purposes.
We recommend that the Biden-Harris Administration take a similarly measured
approach to the governance of AI, including dual-use foundation models with widely
available model weights, referred to in this comment as “open foundation models.”
Open foundation models offer an avenue to achieve many of the United States’ policy
goals. As AI governance is developing globally, policy solutions related to people’s
rights and safety have tended to focus on increasing transparency and accessibility of
AI systems to improve accountability of AI developers and deployers to the public. But
beyond that, a robust open foundation model ecosystem is crucial to enabling a diverse,
innovative, and competitive environment for technological innovation, as well as to
1
addressing concerns about any single entity accumulating excessive sector influence.1
If well executed, an open foundation model ecosystem will expand collective
understanding of AI beyond those who currently build and release models, spurring
ingenuity and enabling a powerful base for innovation for researchers and developers
across a range of sectors.
While closed foundation models potentially offer the possibility of risk monitoring and
mitigation from developers and deployers, there are still many challenges to overcome
to realize these benefits in practice. By contrast, It is crucial to consider that the broad
and wide distribution of open foundation models may amplify myriad risks.2 3 4 5 Indeed,
by their very nature, open foundation models released immediately or rapidly after
training reduce developers' ability to monitor for and safeguard against misuse, and also
make it more difficult to identify and hold accountable those responsible for misuse. As
foundation model capabilities develop, the release of powerful AI models could create
risks that are difficult to foresee, compounding existing challenges around accountability
for developers and deployers, and making them more difficult to address.
When reviewing the risks, benefits, and implications of dual-use open foundation
models, we believe the U.S. government must take two issues into account:
●
First, mechanisms and strategies for model release and model access exist
along the spectrum between the extreme poles of fully open and fully closed
models.6 Any analysis of “marginal risks”7 of open foundation models should
similarly take place across this spectrum of known and unforeseeable use cases.
●
Second, components of the foundation model stack can be accessed and
modified through a spectrum of staged and structured approaches that blur the
7 Kapoor, S., Bommasani, R., Klyman, K., Longpre, S., Ramaswami, A., Cihon, P., ... & Narayanan, A.
(2024). On the Societal Impact of Open Foundation Models. Marginal risk refers to “the extent to which
these models increase societal risk by intentional misuse beyond closed foundation models or
pre-existing technologies, such as web search on the internet.”
6 Solaiman, I. (2023, June). The gradient of generative AI release: Methods and considerations. In
Proceedings of the 2023 ACM conference on fairness, accountability, and transparency (pp. 111-122).
5 Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., ... & Liang, P. (2021). On
the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.
4 Kapoor, S., Bommasani, R., Klyman, K., Longpre, S., Ramaswami, A., Cihon, P., ... & Narayanan, A.
(2024). On the Societal Impact of Open Foundation Models.
3 Seger, E., Dreksler, N., Moulange, R., Dardaman, E., Schuett, J., Wei, K., ... & Gupta, A. (2023).
Open-sourcing highly capable foundation models: An evaluation of risks, benefits, and alternative
methods for pursuing open-source objectives. arXiv preprint arXiv:2311.09227.
2 Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., ... & Dafoe, A. (2023).
Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324.
1 Vipra, J., & Korinek, A. (2023). Market concentration implications of foundation models. arXiv preprint
arXiv:2311.01550.
2
binary between open and closed.8 When fully open access to AI models is not
possible, these approaches may offer a strategy to provide controlled but greater
access to otherwise-closed components of AI models (e.g., the ability to perform
fine-tuning on a proprietary model in a controlled setting).
As such, the idea that model weights are the fulcrum for reasoning about the societal
impact of foundation models is not universally applicable. In some cases, factors such
as model development and nature of deployment may be far more relevant or
revelatory. In other instances in which access to open resources is particularly relevant,
the release status of assets beyond weights (e.g., training and fine-tuning data, training
and inference code, etc.) should also shape government’s analysis. Indeed, the focus
on the term “model weights” may be misleading in that it obscures other processes and
data flows that are crucial in the actual deployments and uses of open foundation
models and, therefore, for the fuller understanding of their risks and benefits.
A fuller consideration will require:
1) clarity about the primary national goals of broad access to dual-use, open
foundation model AI,
2) mandated developer disclosure combined with agility in determining
“thresholds” and other signals of interest, as AI model capabilities evolve, and
3) an exploration of the “spectrum of access,” that is, frameworks that move past
binary release to alternatives such as staged release, 9 structured access,10 and
other frameworks that support these national goals.
The AI Policy and Governance Working Group recommends that the Biden-Harris
Administration call for the development of a range of practical approaches to open
foundation model release with accompanying case studies and pilot studies developed
with relevant stakeholders.
Alongside this recommendation, we urge precautionary friction in which policymakers
embrace small delays and testing appropriately calibrated to the risk of release, rather
than wholesale restrictions on open foundation models that have a broad range of
potential beneficial uses. But we also argue that this friction should be accompanied by
strong policy bias towards supporting the appropriate accessibility and availability of
10 Bucknall, B. S., & Trager, R. F. (2023). Structured Access For Third-Party Research On Frontier AI
Models: Investigating Researchers’ Model Access Requirements.
9 Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., ... & Wang, J. (2019). Release
strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203.
8 Bluemke, E., Collins, T., Garfinkel, B., & Trask, A. (2023). Exploring the Relevance of Data
Privacy-Enhancing Technologies for AI Governance Use Cases. arXiv preprint arXiv:2303.08956.
3
foundation models, allowing transparency into models for both accountability and
discovery, and encouraging innovation beyond a few organizations with extensive
computing infrastructure.
AI Policy and Governance Working Group
Members of the AI Policy and Governance Working Group represent a mix of sectors,
disciplines, perspectives, and approaches. Despite these differences, we agree that it is
not only possible but necessary to address the multitude of concerns raised by the
expanding use of AI systems and tools and their increasing capabilities. We also agree
that both present-day harms and risks on the horizon posed by various AI models
warrant urgent attention in order to fulfill the public’s legitimate expectation of safety and
respect for their rights. We share the belief that these issues require immediate and
ongoing action from industry, governments, academia, and civil society to meet public
expectations. To this end, we have previously submitted recommendations to the NTIA
in response to its request for comment on algorithmic accountability and to the United
Nations Secretary-General’s Office of the Technology Envoy in response to its request
for expert advice on the global governance of AI.11
Open Foundation Models Contribute to US Policy Goals
As demonstrated by President Biden’s October 2023 Executive Order on the Safe,
Secure, and Trustworthy Development and Use of Artificial Intelligence, the
Biden-Harris Administration appreciates that AI may have broad impact and that
governance of AI is important to many of its key policy objectives. At its best, open
access to technology can help to advance innovation, reduce concentrations of
expertise and power, enable transparency, and create new avenues to ensure societal
safety.
Access to openly available data, code, and infrastructure alone does not guarantee
accountability to the public or prevent misuse of AI models and, relatedly, widely
available model weights are not inherently dangerous. Open resources must be coupled
with policy interventions and policymakers should be clear-eyed about what policies will
be needed to maximize these benefits and the feasibility of enforcing them.
Below, we delineate some of the goals supported by access to openly available data,
code, and infrastructure and describe policy choices that do or do not support them.12
12 See also, Stanford Human-Centered Artificial Intelligence and Princeton Center for Information
Technology Policy, Comment on AI Accountability Policy to the National Telecommunications and
Information Administration, June 12, 2023.
11 AI Policy and Governance Working Group. Comment of the AI Policy and Governance Working Group
on the NTIA AI Accountability Policy Request for Comment, Institute for Advanced Study, June 12, 2023;
AI Policy and Governance Working Group. Recommendations on Global AI Governance to the United
Nations Secretary-General’s Envoy on Technology, Institute for Advanced Study, September 30, 2023.
4
Advancing Innovation
Making foundation models more widely accessible, with appropriate safeguards, could
drive innovation in research and business–capitalizing on the promise of public benefit.
Study and use of state-of-the-art AI models, including Large Language Models and
other models like AlphaFold, may lead to improvements in performance, safety, and
scientific breakthroughs across various domains. These potential benefits can best be
realized if other AI model assets, such as model training data, are also made widely
available, and if models are not subject to restrictive licenses. Areas that stand to
potentially gain from a commitment of ensuring the wide availability of AI tools and
systems include, but are not limited to, innovation and novel applications in public
health, biomedical research, and climate science that might be scaled in the public
interest. Any decision to constrain the availability of dual-use open foundation models
must carefully weigh and consider these potential societal and economic benefits.
Reducing the Concentration of Expertise and Power
Today, a significant portion of the resources required to develop the most advanced
closed foundation models, including data, compute, and expertise, are held by a few
leading companies and organizations. This imbalance in resources and expertise raises
concerns about the potential for disproportionate control over these critical AI systems.
Promoting wider access to safe, trustworthy, and accountable open foundation models
can help address these concerns and foster a broader and more dynamic ecosystem.
However, gatekeeping access to model weights alone will not necessarily reduce
concentration of power and talent, as compute resources, data, and expertise are also
key factors. Fulfilling the Biden-Harris Administration’s commitments to market
competition and fostering ingenuity will require the protection of pathways to grow and
access an ecosystem of open foundation models.
Enabling Transparency as a Tool for Accountability and Public Understanding
Transparency is a valuable by-product of open systems, and when paired with other
policy levers such as documentation and disclosure, it can be a powerful tool for
accountability. However, transparency alone does not guarantee accountability, and
further safeguards and incentives are needed to ensure a broad cross-section of actors
are able to support the translation of any increase in the availability of information about
AI models.13 14 To create a robust ecosystem of accountability, policymakers should
14 Davies, T., Walker, S., Rubinstein, M., & Perini, F. (Eds.). (2019). The State of Open Data: Histories and
Horizons. Cape Town and Ottawa: African Minds and International Development Research Centre.
Chattapadhyay, Sumandro and Davies, Tim. Chapt. 12: Land Ownership: Open Data and Land
Ownership.
13 Fox, J. (2007). The uncertain relationship between transparency and accountability. Development in
practice, 17(4-5), 663-671.
5
incentivize the growth of a diverse third-party evaluation sector (e.g., independent
auditing) through targeted research funding, requirements that government-procured AI
systems be subject to such evaluation, or other policy levers. Fostering public
understanding of both open and closed AI systems is crucial for building societal
resilience to the complex challenges posed by new technologies, and as initiatives like
the Open Government Partnership have demonstrated, may also have the potential to
strengthen democratic governance.
Supporting Safety and Security
Open foundation models can support U.S. safety and security aims, especially if
combined with a spectrum of access process (described below) that could enable more
people to probe AI systems and potentially identify risks. Open foundation models may
also enable two types of risks: identified and emergent. Identified risks are specific,
well-defined risks that have been recognized and studied, if not mitigated, such as bias,
information integrity, child sexual abuse material, cybersecurity, privacy concerns, and
misuse in sensitive domains including biotechnology, chemical, biological, radiological
and nuclear defense (CBRN). Emergent risks may arise from the complex, poorly
understood, and evolving capabilities of advanced AI systems and, therefore, may be
challenging to mitigate. This category includes risks related to the potential development
of dangerous capabilities, as well as the emergence of agents (i.e., AI systems capable
of performing tasks autonomously). Providing wide access to open foundation models
can make it more difficult to prevent and mitigate both identified and emergent risks
because, once a model is released, developers do not have the option of revoking,
restricting, or monitoring deployer or user access when a new risk is identified as may
theoretically be possible with closed foundation models.
Threshold Gradients, Not Binaries
To advance the goals established above, policymakers should consider the
circumstances under which heightened scrutiny of an open foundation model may be
warranted. The appropriate “thresholds”–that is, benchmarks triggering action–at which
models or actors should be subject to additional oversight must be carefully determined
and be considered across a spectrum.
In designing thresholds for foundation models, it is crucial to distinguish between (i) the
threshold construct (e.g., computational resources, model performance, or societal
impact), (ii) the threshold metric that operationalizes the construct (e.g., floating point
operations per second or FLOPs, accuracy of specific benchmarks, or the number of
downstream applications), (iii) the threshold value which, if exceeded, triggers an action
(e.g., 10^26 FLOPs), and (iv) the triggered action itself (e.g., information disclosure).
6
While the first three components are closely related, the triggered action should be
clearly distinguished and aligned with specific governance goals.15
The Biden-Harris Administration's use of a compute threshold of 10^26 FLOPs, as
outlined in the Executive Order, attempts to capture advanced AI systems pushing the
boundaries of AI capabilities. While such a threshold may help delineate certain high
capability models, it is important to recognize that compute power alone is not a
comprehensive measure of a model's potential risks or societal impact. As the
Administration refines its approach to AI governance, it should consider additional
factors, such as model performance on specific benchmarks and the extent of a model's
integration into society, to develop a more nuanced understanding of which AI systems
warrant closer scrutiny and reporting requirements.
A one-size-fits-all approach or a single threshold metric is inadequate for governance
because different AI systems and their outputs present unique challenges and risks.
For example, Stable Diffusion 2 requires significantly less computational resources to
train compared to many prominent large language models; yet its potential risks should
not be overlooked due to its lower computational demands. Innovations in model
development may further diminish the effectiveness of compute-based thresholds to
identify possible risks posed by the most highly capable open foundation models; rather
than relying on compute power as the sole threshold, a more comprehensive set of
factors should be considered.
Evaluations can serve two distinct roles in the context of thresholds for foundation
models. They can either be an action triggered by a threshold (e.g., if a model exceeds
a certain level of compute power, it must undergo a specific evaluation) or be used as a
threshold metric itself (e.g., if an evaluation reveals that a model can perform a high-risk
task, further actions may be required). In the latter case, thresholds can help identify,
delineate, and filter models, and should be viewed as indicators that may prompt further
action rather than definitive risk measures. Developing reliable and widely accepted
methods for modeling and quantifying risk is an ongoing area of research that requires
significant attention and investment. And as regulators define reporting thresholds for AI
models, they must clearly specify the relevant factors of interest, which may include
other indicators such as performance benchmarks.
Technical evaluations must be combined with the scrutiny of subject matter experts to
develop a comprehensive understanding of an AI system's strengths, weaknesses, and
implications across different scenarios. The specific evaluations should be tailored to
15 Bommasani, R. (2023). Drawing Lines: Tiers for Foundation Models. Stanford Center for Research on
Foundation Models.
7
the risks relevant to a stage and context. However, many technical and conceptual
hurdles remain to be overcome to stand up an evaluation sector.
Ultimately, government needs to better articulate its key concerns, both in terms of
specific threat scenarios16 and how AI capabilities might enable or exacerbate those
threats. This clarity is necessary to craft targeted governance strategies, including
appropriate thresholds.
By developing more detailed threat models, risk assessment thresholds, and
governance objectives, governments will be better equipped to provide guidance on
essential evaluations at key stages: pre-open sourcing, pre-deployment, and
post-deployment. A gradient- or spectrum-based framework for assessing and triggering
risk across multiple dimensions is preferable to the binary classifications of open and
closed foundation models.
Toward a Spectrum of Access to Open Foundation Models
The NTIA request for comment has framed the debate around the matter of whether or
not model weights should be made widely available. However, a proper understanding
of the threats and benefits associated with model weights comes from a broader
consideration of what is disclosed, how it is disclosed, and who gets access to the
model, model weights, or other assets. We recommend that the Biden-Harris
Administration consider governance approaches to open foundation models such as
forms of staged and structured access, with a policy bias towards openness. Such
frameworks would need to clearly specify the types of capabilities, potential harms, or
misuse scenarios they aim to address through different governance efforts. The
following section explores new access pathways that could help the Administration
balance risk mitigation with the promotion of beneficial uses of open foundation models.
Forms of staged released and structured access to open foundation models involve
providing controlled access to a model's components while limiting access to its internal
information.17 This approach can be implemented through cloud-based interfaces or
platforms, which allow for granular control over who can access the model, for what
purposes, and under what conditions. If well designed, staged and structured access
approaches can support many of the goals of open access to AI models—especially
increasing transparency for accountability and public understanding—while retaining the
safety and security benefits of not fully releasing model assets.
17 Bucknall, B. S., & Trager, R. F. (2023). Structured Access For Third-Party Research On Frontier Ai
Models: Investigating Researchers’ Model Access Requirements.
16 Shevlane, T., Farquhar, S., Garfinkel, B., Phuong, M., Whittlestone, J., Leung, J., ... & Dafoe, A. (2023).
Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324.
8
A spectrum of access regime (staged release, structured access, etc.) should mandate
that developers share certain information with the entities responsible for testing,
red-teaming, and evaluating models. This mandated information sharing is necessary to
facilitate effective and comprehensive assessment.18 The question of what information
should be disclosed and to whom is a critical consideration in the governance of AI
systems. While transparency is important for building trust, ensuring accountability, and
mitigating risk, there may be cases where the full disclosure of certain details could lead
to unintended consequences or enable misuse. Developing clear guidelines and
protocols for information sharing, including considerations of disclosure to regulators,
researchers, and the public, will be an ongoing challenge as the AI landscape continues
to evolve.
A structured or staged access regime can help identify vulnerabilities and risks in a
controlled manner. To save time and effort, however, it may be desirable for newly
discovered flaws or weaknesses to be shared with other model developers, too. The
U.S. government can experiment with incentives similar to incident reporting systems in
other industries. Just as the aviation sector shares information about failures and
vulnerabilities, government can encourage AI companies and developers to share
learnings and failures. To facilitate this sharing of learnings and failures, we propose the
creation of a Common Vulnerabilities and Exposures (CVE) system for AI, similar to
what exists in the cybersecurity industry. This “AI CVE” would serve as a centralized
database where AI developers and companies can report and catalog identified
vulnerabilities, failures, and potential risks associated with AI systems. This would
enable developers of models equivalent to one with a newly discovered vulnerability to
assess and mitigate similar risks in their own systems.
A structured or staged access regime will need to balance expert and broad stakeholder
involvement, tailored to the nature of the risk. For highly specialized fields with a need
for proprietary or classified knowledge–CBRN non-proliferation would be an example–a
focused group of domain-specific experts and authorized bodies is essential. This
ensures decisions are informed by the deepest available expertise while maintaining
safeguards against the dissemination of sensitive information. Conversely, for
18 Developers should provide specifics on the structured access mechanism itself, including the hosting
platform, the capabilities and limitations of the platform, and any restrictions on user actions such as
running scripts or fine-tuning models. High-level information about any obfuscated elements should also
be shared. Where appropriate, additional relevant information should be provided to enable deeper
evaluation and auditing. This may include model checkpoints, user interaction logs, fine-tuning datasets
and code, training data, model design parameters, evaluation results, supported input/output modalities,
and integrated tool use capabilities.
9
considerations such as societal impacts and bias, a wider, more diverse array of
participants is beneficial, including from academia and civil society.
It is worth noting that piloting staged and structured access programs may be slow,
hindering progress in areas where wide access is beneficial. Intellectual property,
privacy concerns, unresolved expectations around liability and safe harbors, as well as
the allocation of costs pose further challenges. To address these issues, the
Biden-Harris Administration may wish to proactively design and implement voluntary
spectrum-of-access pilots, focusing on minimizing red tape.
Conclusion
The decision around whether model weights should be made widely available or not
requires a full understanding of the threats and benefits associated with open
foundation models. A more targeted and nuanced discussion of open versus closed
foundation model access, including spectrum of access considerations, is essential.
By proactively designing and implementing staged- and structured-access pilots,
policymakers can facilitate progress while addressing potential challenges. In doing so,
they should develop clear guidelines for information-sharing among developers,
regulators, researchers, and the public. This sharing could be tailored to specific threats
and contexts, enabling effective risk assessment and mitigation as well as guidance on
essential testing at key stages of model development and deployment. For certain
well-defined risks and uncertainties, some precautionary friction may be desirable to
ensure appropriate safeguards are in place before widespread access is granted.
The development and deployment of AI models, regardless of their degree of access,
should be governed by the principles and practices outlined in initiatives such as the
Biden-Harris Administration’s Blueprint for an AI Bill of Rights, Executive Order on AI, AI
Risk Management Framework, and draft OMB memo on AI in government. Protections
and accountability measures developed for open foundation models should aim to
harmonize with the broader ecosystem of AI governance, while recognizing the unique
considerations that may apply across the spectrum of model openness.
We are grateful for the opportunity to contribute these comments and applaud the NTIA
for its leadership in encouraging accountability in the development, deployment, and
use of AI systems. As the Biden-Harris Administration navigates the complexities of AI
governance, we hope this analysis will provide valuable insights and recommendations,
increasing public trust and the responsible adoption of AI in the public interest.
Thank you for your consideration. Please contact aipolicy@ias.edu with any comments
or questions.
10
Sincerely,
AI Policy and Governance Working Group*
Alondra Nelson
Andrew Trask
Ben Garfinkel
Christine Custis
Dan Hendrycks
Deep Ganguli
Dorothy Chou
Helen Toner
Irene Solaiman
Jaan Tallinn
Janet Haven
Marc Aidinoff
Marietje Schaake
Matthew Salganik
Miranda Bogen
Nathan Lambert
Rishi Bommasani
Sébastien Krier
Solon Barocas
Stephanie Ifayemi
Suresh Venkatasubramanian
William Isaac
Zoë Brammer
*Members of the working group are participating in their personal capacities and these recommendations
do not reflect the perspective of any of the organizations with which they are affiliated.
11
",AIPGWG,5918
NTIA-2023-0009-0243,"  
 
1 
 
SIIA.NET 
 
Response to NTIA’s Request for Comment Regarding “Dual Use Foundation Artificial 
Intelligence Models with Widely Available Model Weights” 
NTIA-2023-0009 
 
Submitted by the Software & Information Industry Association 
 
March 27, 2024 
 
The Software & Information Industry Association (SIIA) appreciates the opportunity to provide 
comments on dual use foundation models with widely available model weights in response to 
the thoughtful request issued by the National Telecommunications and Information 
Administration (NTIA).1 
 
SIIA is the principal trade association for companies in the business of information. Our 
members include roughly 375 companies reflecting the broad and diverse landscape of digital 
content providers and users in academic publishing, education technology, and financial 
information, along with creators of software and platforms used by millions worldwide, and 
companies specializing in data analytics and information services. Our membership includes 
upstream and downstream AI developers and users of AI systems in myriad environments. 
 
SIIA’s responses to the NTIA RFC reflect four core themes. First, openness should be viewed 
across a gradient, with model weights as one component of an AI system that can be made 
available to third parties in varying degrees. Second, a risk-based approach to foundation 
models that considers the degree and type of openness – among a broader set of considerations 
– would provide a stronger framework for assessing potential risks and mitigation strategies 
than an ex ante categorization of foundation models based on objective characteristics (such as 
FLOPs or whether model weights are/are not made available). Third, policymakers should take 
care to avoid expanding the term “dual-use foundation models” beyond the intended scope of 
EO 14110 based on theoretical capabilities of foundation models. Fourth, continued U.S. 
leadership is essential to support international alignment foundation model policy that 
advances innovation and aligns with core democratic values.  
 
 
 
 
1 NTIA, Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights, 89 Fed. 
Reg. 38, 14059-14063, RIN 0666-XC060, Docket No. 240216-0052 (Feb. 26, 2024) (“NTIA RFC”). 
 
2 
1. How should NTIA define ‘‘open’’ or ‘‘widely available’’ when thinking about foundation 
models and model weights?  
 
Scoping Open and Widely Available 
 
As NTIA has rightfully noted, the terms “open” and “widely available” are “terms without clear 
definition or consensus” and “[t]here are gradients of ‘openness,’ ranging from fully ‘closed’ to 
fully ‘open.’”2 A recently proposed variant on these level-of-access gradients starts with fully 
closed and proceeds to hosted access, API access to a model; API access to fine-tuning; weights 
available; weights, data, and code available with use restrictions; weights, data, and code 
available without use restrictions.3 
 
What this means is that considering the openness of model weights as a binary choice will not 
yield meaningful guidance to shape policy or best practices. Openness must be considered 
across a gradient or spectrum, and that requires a risk-based approach in which openness is but 
one factor.  
 
In addition, openness should be considered with regard to each core artifact of an AI system. 
While the EO and NTIA’s RFC are concerned with open or widely available model weights, 
openness should be considered across the AI stack, to include training data and code – and 
perhaps other elements that bear on transparency and the ability of end users to modify models 
for different applications. 
 
Scoping “Dual-Use Foundation Model” 
 
Many of the RFC’s questions – and many of SIIA’s responses below – address the qualities of 
open foundation models in general. The NTIA RFC and the EO are guided by a more precise term 
– “dual-use foundation model.” A developer’s obligations under section 4.2 of the EO apply only 
to dual-use foundation models, and the instruction to NTIA focuses on open dual-use 
foundation models. 
 
The EO defines a “dual-use foundation model” in a way that is directly tied to national security. 
Under the EO, a covered model is one “that exhibits, or could be easily modified to exhibit, high 
levels of performance at tasks that pose a serious risk to security, national economic security, 
 
2 NTIA RFC at 14061 (citing Zoe Brammer, How Does Access Impact Risk? Assessing AI Foundation Model 
Risk Along a Gradient of Access, The Institute for Security and Technology (December 2023); Irene 
Solaiman, The Gradient of Generative AI Release: Methods and Considerations, arXiv:2302.04844v1 
(February 5, 2023)). 
3 Rishi Bommasani and Sayash Kapoor, et.al, Stanford University Human-Centered Artificial Intelligence, 
Issue Brief: Considerations for Governing Open Foundation Models, (Dec. 13, 2023), 
https://hai.stanford.edu/issue-brief-considerations-governing-open-foundation-models (hereinafter, 
Stanford University Human-Centered Artificial Intelligence, Issue Brief: Considerations for Governing 
Open Foundation Models) 
 
3 
national public health or safety, or any combination of those matters.”4 The EO  describes 
foundation models that are considered to be dual-use, including those that “(i) substantially 
lower[] the barrier of entry for non-experts to design, synthesize, acquire, or use chemical, 
biological, radiological, or nuclear (CBRN) weapons; (ii) enabl[e] powerful offensive cyber 
operations through automated vulnerability discovery and exploitation against a wide range of 
potential targets of cyber attacks; or (iii) permit[] the evasion of human control or oversight 
through means of deception or obfuscation.”5 
 
The EO’s examples of dual-use foundation models are not intended to be comprehensive. 
Indeed, the EO notes that “[m]odels meet this definition even if they are provided to end users 
with technical safeguards that attempt to prevent users from taking advantage of the relevant 
unsafe capabilities.”6 Nevertheless, this savings clause should be read in the context of the 
entire definition, which requires that a dual-use foundation model “exhibits, or could be easily 
modified to exhibit, high levels of performance at tasks that pose a serious risk” to security and 
safety. We think a fair reading of this definition means that foundation models which are not 
designed or cannot easily be modified to accomplish non-civilian objectives do not give rise to 
serious security or safety concerns. The upshot of this is that a large language model that meets 
the size parameters required under the EO should not be considered dual use if designed with 
safeguards that would make it extremely difficult to modify to lower the barrier of entry for 
CBRN weapons, enable powerful offensive cyber operations, or permit the evasion of human 
control. 
 
While there is value in examining the risks and benefits of large-scale foundation models that 
are not dual use in the way the EO defines that term, there are significant downsides to doing so 
in the context of implementing the dual-use foundation model requirements of the EO. 
Imposing the highest tier of government oversight on all large-scale foundation models would 
essentially subject all large foundation models to ongoing government requirements and 
oversight requirements under section 4.2 of the EO, and potential to export control 
requirements overseen by BIS. We do not think this was the intent of the EO. Indeed, doing so 
would limit AI innovation, research, and real-world applications without leading to measurable 
improvements in risk mitigation. As we describe in this submission, there are more effective 
ways to approach that broader universe. 
 
1.c. Should ‘‘wide availability’’ of model weights be defined by level of distribution? If so, at 
what level of distribution (e.g., 10,000 entities; 1 million entities; open publication; etc.) 
 
4 EO 14110, Section 1(k). This is consistent but not identical with how dual use is considered under U.S. 
law. For example, under export control law, “[t]he term “dual-use”, with respect to an item, means the 
item has civilian applications and military, terrorism, weapons of mass destruction, or law-enforcement-
related applications.” 50 USC § 4801(2). 
5 EO 14110, Section 1(k). 
6 EO 14110, Section 1(k). 
 
4 
should model weights be presumed to be ‘‘widely available’’? If not, how should NTIA define 
‘‘wide availability?’ 
 
We recommend that NTIA propose a risk-based approach to classifying dual-use foundation 
models with widely available model weights. Such an approach would consider the risks, 
benefits, and governance of specific models that meet the definitional thresholds—both dual-
use and size requirements set out in Section 4.2(b) of the EO—taking into account the degree 
and kind of openness. We discuss this further below in response to question 5. 
 
2. How do the risks associated with making model weights widely available compare to the 
risks associated with non-public model weights?  
 
Risks associated uniquely with open model weights stem from the potential that bad or 
inexperienced actors will modify weights to generate outcomes that are malicious or societally 
unacceptable.7 On the Societal Impact of Open Foundation Models, published in February by a 
group of leading AI researchers,8 explains “many of the risks described for open foundation 
models arise because developers relinquish exclusive control over downstream model use once 
model weights are released.”9  
 
From this starting point, the paper provides a typology of risks associated with open foundation 
models, including spear-phishing scams, exploitation of cybersecurity vulnerabilities, 
disinformation, voice-cloning scams, and the generation and dissemination of non-consensual 
intimate imagery (NCII) and child sexual abuse material (CSAM).10 Crucially, the paper also 
concludes that some risks are more speculative than proven, such as with cybersecurity.11 
Others, such as deriving information relevant to carrying out a biosecurity attack, appear no 
greater than using a common internet search function.12 Yet in some cases, such as the creation 
of digitally altered non-consensual intimate imagery, open foundation models do “pose 
 
7 See NTIA RFC at 14060 (benefits), 14061 (risks). 
8 Sayash Kapoor and Rishi Bommasani, et al., On the Societal Impact of Open Foundation Models, 
arXiv:2403.07918v1 (Feb. 27, 2024), https://arxiv.org/pdf/2403.07918.pdf (hereinafter, “Kapoor and 
Bommasani et al.”). 
9 Kapoor and Bommasani et al. at 2. There are limitations on the ability to generalize between these two 
categories because openness is not an either/or option. Nevertheless, we think the “reductive” 
dichotomy between open and closed foundation models adopted in the recent Stanford paper provides 
a helpful framework to assess the risks and benefits of different foundation models. Id. 
10 Id. at 5-6. 
11 Id. at 7-8 (for automated vulnerability detection, “the current marginal risk of open foundation models 
is low and there are several approaches to defending against the marginal risk, including using AI for 
defense). 
12 Id. at 5. 
 
5 
considerable marginal risk at present.”13 Part of this is due to a lack of robust research, which in 
turn is partly due to the newness of these models.  
 
Nevertheless, fully closed AI models also carry risks.14 Closed models exacerbate certain risks 
because they depend on the adequacy of risk mitigation measures by the upstream developer. 
These models in general are less transparent, making it harder for third parties and users to 
detect and rectify data bias and coding anomalies.  
 
Comparative evaluation of these risks remains an area for further research,15 especially as the 
capabilities of foundation models, the uses of guardrails, and industry standards for safety and 
security are constantly evolving. What is clear, however, is that openness in itself, does not 
mean that a model can be used for malicious purposes – particularly those embedded in the 
EO’s definition of “dual-use” – any more than a fully closed model can be used for malicious 
purposes.  
 
2.b. Could open foundation models reduce equity in rights and safety impacting AI systems 
(e.g., healthcare, education, criminal justice, housing, online platforms, etc.)?  
 
We believe it is a false dichotomy to posit that open foundation models – those with widely 
available model weights, leaving aside other assets that could be opened, and the gradient – 
reduce equity in rights and safety impacting AI systems vis-à-vis the “monoculture” of closed 
foundation models.16 Indeed, open models provide a means to enhance equity by permitting 
researchers and developers to address potentially unintended biases reflected by initial model 
weighting and permitting researchers, civil society groups, and others to use powerful AI tools 
to investigate societal issues.  
 
Nevertheless, we caution against a monolithic approach to open foundation models, because 
their potential uses, and corresponding risks and benefits, depend on a multitude of factors. 
Instead, as explained further in this submission, we urge NTIA to recommend a risk-based 
approach that includes impact on equity as an evaluation metric.17 
 
 
13 Id. at 7-8. 
14 See generally, IBM AI Ethics Board, “Foundation models: Opportunities, risks and mitigations,” at 8-23, 
https://www.ibm.com/downloads/cas/E5KE5KRZ. 
15 Kapoor and Bommasani at 19-23 (Appendix containing literature review). 
16 Kapoor and Bommasani et al. at 5. 
17 We question including the term “online platform” in this question’s parenthetical as on par with other 
areas that have received focused attention as sensitive areas for AI policy development. In addition, we 
encourage NTIA to provide a definition of “rights and safety impacting.” While familiar with the OMB 
draft memo issued in November 2023, OMB has not, to our knowledge, issued a final version of that 
memo and we understand many commentators raised questions about the ways in which those terms 
were defined. 
 
6 
2.c. What, if any, risks related to privacy could result from the wide availability of model 
weights?  
 
The wide availability of model weights is neither inherently good nor inherently bad for privacy. 
Foundation model developers can employ various mechanisms to protect the privacy of 
personal information in closed models and open models. As with our response to question 2.b, 
we recommend that privacy be part of any risk-based assessment of foundation models.18 
 
2.e. What, if any, risks could result from differences in access to widely available models 
across different jurisdictions?  
 
We believe there is risk in adopting fundamentally different approaches to oversight of 
foundation models of any type across different jurisdictions. Jurisdictions will inevitably have 
local approaches to different issues that warrant specific attention, but in terms of broad 
guidance, innovation, and responsible development would benefit from consistency. As 
described below, we urge the U.S. government to continue to exercise leadership in the global 
community.  
 
3. What are the benefits of foundation models with model weights that are widely available 
as compared to fully closed models?  
 
On the benefits side of the equation, openness has already proved to be a catalyst for research 
and innovation by essentially democratizing access to models that are cost-prohibitive for many 
actors in the AI ecosystem to develop on their own.19  Open access allows for customizability, 
which permits downstream developers to diversify model behavior. Closed models may also 
allow customizability through fine tuning APIs, for example, but may limit the degree of 
customizability. This is among the risk/benefit considerations that are nearly impossible to 
determine in the abstract and require an approach that considers not only the degree of 
openness but other specifics of each model. The same can be said for benefits around safety 
and security and the degree to which models permit researchers and downstream developers to 
interrogate models and address vulnerabilities during development or deployment. For these 
reasons, as we describe below, we endorse a risk-based approach. 
 
In addition, we support efforts of the U.S. government to advance further ways to democratize 
AI research and innovation through the NAIRR pilot, the AI Institutes program led by NSF, and 
other initiatives. We also support the efforts of private industry to make models that meet 
standards for responsible development (including red-teaming, capability evaluation, TEVV 
measures, and so on). 
 
18 See, e.g., Bandan Chandra Das, et al., Security and Privacy Challenges of Large Language Models: A 
Survey, arXiv:2402.00888v1 (Feb. 2024), https://arxiv.org/pdf/2402.00888.pdf.  
19 This is summarized in the NTIA RFC’s discussion of benefits and risks. NTIA RFC at 14060 (benefits), 
14061 (risks). 
 
7 
 
3.a. What benefits do open model weights offer for competition and innovation, both in the 
AI marketplace and in other areas of the economy? In what ways can open dual-use 
foundation models enable or enhance scientific research, as well as education/ training in 
computer science and related fields?  
 
Open model weights and other mechanisms for making accessible highly capable AI tools help 
to reduce market concentration in downstream development. This has enormous positive 
effects on society. As noted in the Kapoor and Bommasani paper, openness “may yield more 
diverse down-stream model behavior,” reducing certain types of risks..20 Researchers and 
downstream developers are able to use open models to develop new applications and 
investigate critical questions without having to front the enormous capital costs associated with 
foundation model development.  
 
3.c. Could open model weights, and in particular the ability to retrain models, help advance 
equity in rights and safety-impacting AI systems (e.g., healthcare, education, criminal justice, 
housing, online platforms etc.)?  
 
Please see above our response to question 2.b on the risks to equity. 
 
3.d. How can the diffusion of AI models with widely available weights support the United 
States’ national security interests? How could it interfere with, or further the enjoyment and 
protection of human rights within and outside of the United States?  
 
National security implications of open models are potentially immense although, again, it 
depends on many factors. The potential benefits are to create models that are highly adaptable 
for different national security missions, an area that based on public reporting is already well 
underway at the Pentagon and beyond. In addition, openness of models from U.S. developers 
has a “soft power” benefit by further innovation beyond U.S. borders and providing the 
diffusion of models that are not pre-set with potential vulnerabilities or biases that may be 
required of models released from developers in certain other countries. As to the impact on 
human rights, we believe this is similar to the impact on equity – it necessarily depends; there 
are clear potential benefits to openness and transparency, but also risks that must be managed. 
 
4. Are there other relevant components of open foundation models that, if simultaneously 
widely available, would change the risks or benefits presented by widely available model 
weights? If so, please list them and explain their impact. 
 
Model weights are but one artifact of an AI system that a developer can open to broader usage. 
Other artifacts include training data and code. For each artifact, openness can occur across a 
 
20 Kapoor and Bommasani et al. at 5. 
 
8 
gradient. For example, licensing terms and use restrictions impact the degree of “openness” 
with corresponding implications for potential risks.21 
 
5. What are the safety-related or broader technical issues involved in managing risks and 
amplifying benefits of dual-use foundation models with widely available model weights?  
 
Risk management is at the heart of responsible AI, no less in open models than in closed 
models. Those risks can be mitigated through various measures, including staged release; less 
than fully open access; limitations on who can access the weights (e.g., through licenses and 
user restrictions); and limitations on how the assets can be used (e.g., use restrictions and 
contract terms).22 Moreover, open innovation in the AI context creates opportunities to 
mitigate risk through the efforts of researchers and downstream developers to interrogate and 
improve open models. 
 
What is clear, however, is that the principles that ground responsible AI must apply as well to 
generative AI models, including those that offer fully or partially open assets such as model 
weights. Industry and experts are virtually unified in endorsing a risk-based approach to AI 
development and deployment. We encourage NTIA to endorse such an approach in the context 
of foundation model governance (across the gradient of openness) and, in particular, for dual-
use foundation models. 
 
NIST provides the starting point for this endeavor. Building on the seminal AI Risk Management 
Framework released in January 2023,23 the NIST AI Safety Institute is now focused on an effort 
to develop a companion resource for generative AI.24 SIIA is engaged in this work as a member 
of the AI Safety Institute Consortium. The work of NIST has been critical in raising the bar on 
responsible AI and moving the industry in a positive direction. We see evidence of this in the 
 
21 See Stanford University Human-Centered Artificial Intelligence, Issue Brief: Considerations for 
Governing Open foundation Models (Dec. 2023), at 3-4. 
https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf.  
22 See, e.g., https://ai.google.dev/gemma/prohibited_use_policy. Google’s release of Gemma – an open 
model that falls under the size threshold of section 4.2 of EO 14110 – illustrates a responsible, 
transparent approach to public release, reflect in the accompanying model card 
(https://www.kaggle.com/models/google/gemma) and technical report 
(https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf). 
 
23 NIST, Artificial Intelligence Risk Management Framework (AI RMF 1.0), NIST AI 100-1 (January 2023). 
https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf.  
24 NIST, US Artificial Intelligence Safety Institute: AISIC Working Groups, (Nov. 17, 2023). 
https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute/aisic-working-groups.  
 
9 
growing efforts taken by upstream developers of open models to mitigate risks before making 
models available.25 
 
Nevertheless, in the context of open models, it is important to recognize that responsibility for 
mitigating risks must be undertaken not only by upstream developers but also by downstream 
developers and sophisticated deployers. Once artifacts of a model are made open, it is not 
possible for an upstream developer to claw them back. That means it is important to have 
principles of responsible use and guardrails to prevent improper use. 
 
The Kapoor and Bammasani paper provides a framework for risk assessment in the context of 
open models that we believe is instructive and should guide recommendations in this space. The 
framework would require evaluation of potential threats, risks and defenses to those risks in the 
absence of open foundation models, evidence of marginal risk of open foundation models 
versus closed models, ease of defending against new risks identified, and limitations in any 
analysis.26 Assessments along these lines – by both upstream and downstream developers – are 
in line with best practices and supplement other risk mitigation strategies employed for AI 
systems in general.27 In addition to assessments, as researchers at Berkeley recently explained, 
there are steps that model developers can undertake, such as staging the release of different 
model artifacts, that can help to insulate against some of the risks inherent in foundation 
models.28  
 
In short, we caution against a one-size-fits-all approach to mitigating risks for open models due 
to the gradient of openness, the differences among models, and differences around model 
training data. In addition, advances in foundation models, risk mitigation techniques (TEVV, 
auditing, red-teaming, and so on) and the capabilities of bad actors mean that any approach 
must be sufficiently flexible and agile to adapt. 
 
Nevertheless, there are discrete areas in which the risk associated with foundation models and 
in particular open models may be sufficiently acute that action by Congress would be necessary. 
Among these we would urge Congress to criminalize the creation and dissemination of NCII to 
raise the cost on bad actors. This is an area that the researchers in the Kapoor and Bommasani 
 
25 Biden-Harris Administration Secures Voluntary Commitments from Eight Additional Artificial 
Intelligence Companies to Manage the Risks Posed by AI | The White House. 
26 Kapoor and Bommasani et al. at 5-7. 
27 We also recommend the work of Stanford’s HAI, the Partnership on AI, and Berkley’s GPAIS. See 
https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf;  
https://partnershiponai.org/wp-content/uploads/1923/10/PAI-Model-Deployment-Guidance.pdf;  
28 UC Berkeley Center For Long-Term Cybersecurity, AI Risk-Management Standards Profile for General-
Purpose AI Systems (GPAIS) and Foundation Models, (Nov. 2023). https://cltc.berkeley.edu/wp-
content/uploads/2023/11/Berkeley-GPAIS-Foundation-Model-Risk-Management-Standards-Profile-
v1.0.pdf. 
 
10 
paper identify as having greater marginal risk in open versus closed models, and an area that is 
not covered well by existing law.29 
 
6. What are the legal or business issues or effects related to open foundation models?  
 
6.a. In which ways is open-source software policy analogous (or not) to the availability of 
model weights? Are there lessons we can learn from the history and ecosystem of open-
source software, open data, and other ‘‘open’’ initiatives for open foundation models, 
particularly the availability of model weights?  
 
The overall lesson of the open source software movement has been that openness has led to 
greater rather than less safety, as well as innovation, transparency, and collaboration.30 This is 
well documented and indeed recognized by U.S. government policy.31 We believe the future of 
AI will trend towards openness, though the risks of an AI model versus software are different. 
 
6.b. How, if at all, does the wide availability of model weights change the competition 
dynamics in the broader economy, specifically looking at industries such as but not limited to 
healthcare, marketing, and education? 
 
The availability of a broad range of foundation models from companies, both large and small, 
has created a fiercely competitive market. Many companies have been working on foundation 
models for years, others are new entrants, some models are open-source, others restricted, and 
there is no way to know which model(s), if any, ultimately will be the most successful. One thing 
seems certain, though, big is not necessarily better. This is because no foundation model is able 
to do everything well. Rather, different models work better for different tasks and use cases. A 
model developed to help doctors create better and more personalized patient treatment plans, 
for example, will not be much help to a finance professional looking to draw insights from reams 
of financial data, or a manufacturing company trying to optimize its production processes. They 
will need different models customized to their particular needs.  
 
Barriers related to the resources, time, and cost, it takes to build, train, and deploy models have 
also been significantly reduced. Customers have many choices for sourcing their compute 
capacity needs. The cloud offers great flexibility and scalability, but there are also various on-
 
29 Christopher A. Mohr, President, Software & Information Industry Association, Artificial Intelligence and 
Intellectual Property, Part II – Identity in the Age of AI, Statement Before the House Committee on the 
Judiciary, Subcommittee on Courts, Intellectual Property and the Internet, (Feb. 2, 2024). 
https://judiciary.house.gov/sites/evo-subsites/republicans-judiciary.house.gov/files/evo-media-
document/mohr-testimony-1.pdf  
30 Guido Schryen, Is Open Source Security a Myth?, Communications of the ACM, 54(5), (May 2011). 
https://dl.acm.org/doi/pdf/10.1145/1941487.1941516  
31 See, e.g, The Digital Services Playbook Play 13: https://playbook.cio.gov/#play13 and The Department 
of Defense Open Source Software FAQs: https://dodcio.defense.gov/Open-Source-Software-FAQ/.  
 
11 
premises solutions, as well as hybrid solutions combining these and other options. Increasing 
access to quality data, and the availability of pre-trained models that are customizable, also help 
lower barriers to entry, which makes it easier for new entrants to compete with more well-
established companies. 
 
All told, these dynamics encourage innovation and increase competition to the benefit of the 
broader economy.   
 
6.d. Are there concerns about potential barriers to interoperability stemming from different 
incompatible ‘‘open’’ licenses, e.g., licenses with conflicting requirements, applied to AI 
components? Would standardizing license terms specifically for foundation model weights be 
beneficial? Are there particular examples in existence that could be useful?  
 
Historically speaking, developers of technology do so to ensure their products in their developer 
suite operate well but it has often been difficult for interoperability between different 
developers. There are business and other reasons to limit interoperability, but interoperability 
can be beneficial for the broader AI ecosystem. Industry best practices, standardized licensing 
terms, and other efforts can help to promote interoperability across models from different 
upstream developers.  
 
7. What are current or potential voluntary, domestic regulatory, and international 
mechanisms to manage the risks and maximize the benefits of foundation models with widely 
available weights? What kind of entities should take a leadership role across which features of 
governance?  
 
In general, we believe that alignment across jurisdictions is essential for innovation; that 
guardrails around AI designed to mitigate the potential downsides should, to the extent 
possible, be consistent; and that government work towards agile, flexible approaches that can 
adapt as the risks and benefits of technology inevitably develop (indeed, ChatGPT has been part 
of public discourse for only 18 months, and the European Commission’s proposal for the AI Act 
included no provisions on “general purpose AI”). 
 
There have been significant undertakings to mitigate risks and maximize the benefits of 
foundation models of all types, and we encourage NTIA to emphasize the value of these efforts 
for creating a safe and secure ecosystem and one that maximizes international alignment.  
 
Among others, we highlight the Voluntary Commitments agreed to by the White House and 
leading AI developers;32 the G7 International Code of Conduct33 and continuing work of the G7; 
 
32 The White House, Voluntary AI Commitments, (September 2023). https://www.whitehouse.gov/wp-
content/uploads/2023/09/Voluntary-AI-Commitments-September-2023.pdf  
33 G7 2023 Hiroshima Process, International Code of Conduct for Organizations Developing Advanced AI 
Systems, (Oct. 30, 2023). https://www.soumu.go.jp/main_content/000912748.pdf    
 
12 
industry-led efforts at the Partnership on Artificial Intelligence34 and the AI Verify Initiative;35 
and ongoing work by the OECD and the Global Partnership on AI.36 These efforts are aligned in 
seeking to raise the bar on developers across the AI value chain to implement best practices for 
safe, secure, and trustworthy foundation models at the same time as the technology is 
advancing at a rapid pace. 
 
Specifically with regard to open models, however, as the Kapoor and Bommasani researchers 
point out, some of these efforts focus specifically on closed foundation models and if they “are 
interpreted strictly to apply to foundation model developers, independent of how the model is 
adapted or used downstream, they would be difficult for open developers to comply with.”37 
Therefore, as the U.S. government continues efforts to negotiate the ground rules for 
foundation models, it should also give attention to the unique relationship between open 
models and downstream developers and apportion responsibilities between these parties 
appropriately. 
 
Overall, we encourage the U.S. government to exercise even more leadership in the 
international community. The United States and China are the two leading homes of AI 
researchers and companies in the world and we believe the United States’ imprint on 
international AI policy is essential to ensure that the technology advances in a manner that is 
aligned with democratic values. 
 
7.b. How might the wide availability of open foundation model weights facilitate, or else 
frustrate, government action in AI regulation?  
 
We encourage the government to focus efforts on use-based restrictions to target those 
activities that are deemed harmful. On the development side, continuing attention to best 
practices, industry standards, and robust risk-management assessment processes should remain 
the focus. Direct government oversight of foundation models should be limited to those that 
meet a narrow definition of dual-use as described earlier in this submission. 
 
7.d. What role, if any, should the U.S. government take in setting metrics for risk, creating 
standards for best practices, and/or supporting or restricting the availability of foundation 
model weights?  
 
We endorse the ongoing work of the NIST AI Safety Institute and Consortium to develop a 
companion to the NIST AI Risk Management Framework. We anticipate this will address the very 
 
34 Partnership on AI, PAI’s Guidance for Safe Foundation Model Deployment, (October 2023). 
https://partnershiponai.org/wp-content/uploads/1923/10/PAI-Model-Deployment-Guidance.pdf  
35 AI Verify Foundation, Proposed Model AI Governance Framework for Generative AI: Fostering a Trusted 
Ecosystem, (Jan. 16, 2024). https://aiverifyfoundation.sg/downloads/Proposed_MGF_Gen_AI_2024.pdf  
36 OECD Legal Instruments. 
37 Kapoor and Bommasani at 9. 
 
13 
question presented here and would encourage the U.S. government to avoid conflicting 
guidance. 
 
7.i. Should other government or nongovernment bodies, currently existing or not, support the 
government in this role? Should this vary by sector?  
 
Please see above our answer to question 7. 
 
7.g. What should the U.S. prioritize in working with other countries on this topic, and which 
countries are most important to work with?  
 
The U.S. should prioritize innovation and promotion of American industry as leaders in ensuring 
transparency and best practices with regard to the development of responsible AI systems and 
foundation models to benefit society at large. We encourage the U.S. government to advocate 
for cross-collaboration among AI safety institutes globally to develop standards as well as 
collaborate with industry in these discussions to develop appropriate and effective policies.  
 
8. In the face of continually changing technology, and given unforeseen risks and benefits, 
how can governments, companies, and individuals make decisions or plans today about open 
foundation models that will be useful in the future?  
 
8.a. How should these potentially competing interests of innovation, competition, and 
security be addressed or balanced?  
 
If we have learned anything from the past, technology is here to stay. We need to learn how to 
adapt our systems to meet the present and future of technological advancement and 
innovation. Technical policy expertise is needed on the federal level in order to adapt to 
emerging needs. Policies should be amenable to different use cases and creating incentives to 
allow companies to innovate and maintain security are essential to create a collaborative 
ecosystem between industry and government.  
 
8.b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the 
threshold, is the amount of computational resources required to build a model, such as the 
cutoff of 1026 integer or floating-point operations used in the Executive order, a useful metric 
for thresholds to mitigate risk in the long-term, particularly for risks associated with wide 
availability of model weights?  
 
Thresholds should be based on qualitative metrics rather than FLOPs. Technology will continue 
to advance, and it is possible that models in excess of the threshold will present less risk than 
models that fall below the threshold. We encourage the Secretary of Commerce to work closely 
with the NIST AI Safety Institute to develop appropriate guidance for assessing risk and imposing 
oversight obligations under the EO. 
 
 
",SIIA,7900
NTIA-2023-0009-0280," 
 
BEFORE THE 
NATIONAL TELECOMMUNICATIONS AND INFORMATION ADMINISTRATION 
 
 
 
In the Matter of  
 
Dual Use Foundation Artificial Intelligence 
Models with Widely Available Model Weights 
 
 
 
Docket Nos.  
NTIA-2023-0009; 240216-0052 
 
COMMENTS OF THE CONSUMER TECHNOLOGY ASSOCIATION 
IN RESPONSE TO THE NTIA’S REQUEST FOR COMMENTS  
ON DUAL USE FOUNDATION ARTIFICIAL INTELLIGENCE MODELS WITH 
WIDELY AVAILABLE MODEL WEIGHTS 
 
The Consumer Technology Association® (“CTA”) submits this response to the National 
Telecommunications and Information Administration (“NTIA”) Request for Comment (“Request 
for Comment”) on dual use foundation artificial intelligence models with widely available model 
weights. CTA is North America’s largest technology trade association. Our members are the 
world’s leading innovators—from startups to global brands—helping support more than 18 
million American jobs. CTA owns and produces CES®—the most influential tech event in the 
world.  
 
CTA urges the NTIA to proceed with caution when considering whether, or what, new 
rules may be necessary as the agency explores the benefits, risks and related issues surrounding 
the development and deployment of artificial intelligence models with widely available model 
weights (“open weight models”). Artificial Intelligence (“AI”) as a category of technologies is 
not new, but generative AI systems and technologies such as machine learning that underlie AI 
systems are emerging technologies that are evolving rapidly. Indeed, a recent Federal Trade 
Commission (“FTC”) report found that AI is nascent, varied, and not susceptible to one 
definition.1 AI systems that are used to inform a broader system, such as machine vision systems 
used to read stop signs in autonomous vehicles, have very different risk profiles compared to 
generative AI tools connected to the Internet which interact with users directly. Industry leaders 
in the development of AI systems, including generative AI systems, have been actively working 
to ensure their systems comply with existing laws, such as privacy, consumer protection, and 
anti-discrimination regulations. 
 
1 See Combatting Online Harms Through Innovation, FTC, at 1 (June 16, 2022), available at 
https://www.ftc.gov/system/files/ftc_gov/pdf/Combatting%20Online%20Harms%20Through%20Innovation%3B%
20Federal%20Trade%20Commission%20Report%20to%20Congress.pdf (“Combatting Online Harms Report”) 
(“AI is defined in many ways and often in broad terms. The variations stem in part from whether one sees it as a 
discipline (e.g., a branch of computer science), a concept (e.g., computers performing tasks in ways that simulate 
human cognition), a set of infrastructures (e.g., the data and computational power needed to train AI systems), or the 
resulting applications and tools.”). 
 
2 
 
As such, before supporting or proposing policies that call for new regulations in this area, 
NTIA should ensure it develops a robust record and undertakes sufficient deliberation and 
consideration of both the benefits and risks presented by the use of AI systems and technology. 
Any new rules recommended by NTIA should be part of a risk-based, flexible approach that 
accounts for different use cases and is narrowly tailored to avoid imposing undue burdens on 
innovation. 
 
In these comments, CTA outlines several factors NTIA should consider as it collects 
information regarding risks and benefits of open weight models. In these comments, CTA offers 
its perspective on how best to define and classify the many distinct types of open weight models. 
These comments also outline numerous benefits associated with open weight models and offer 
perspectives on how to consider such benefits against marginal risks arising from the use of such 
models. Finally, CTA demonstrates how industry and civil society continue to work towards 
developing tools and standards for evaluating these models, and why a light-touch, measured 
approach to any new rules is critically important to ensuring the United States can benefit from 
the many gains offered by making open weight models widely available. 
 
I. 
Efforts to Regulate Emerging AI Models and Technologies Require Due 
Deliberation and Caution 
AI offers tremendous potential for human and societal development: promoting inclusive 
growth, improving the welfare and well-being of individuals, and enhancing global innovation 
and productivity. A growing body of research demonstrates AI can identify and mitigate bias in 
human decision making.2 Perhaps the leading federal agency focused on AI governance and risk 
management, the National Institute of Science and Technology (“NIST”), has recently 
commented that “new AI-enabled systems are revolutionizing and benefitting nearly all aspects 
of our society and economy – everything from commerce and healthcare to transportation and 
cybersecurity.”3  
 
Further, CTA members help promote responsible and trustworthy AI through leadership 
in the development of emerging practices that mitigate risks, such as the use of federated 
 
2 See, e.g., Jon Kleinberg et al., Discrimination in the Age of Algorithms, 10 J. of Legal Analysis 113, 120 (2019), 
https://academic.oup.com/jla/article/doi/10.1093/jla/laz001/5476086; Cass R. Sunstein, Algorithms, Correcting 
Biases, 86 Soc. Rsch.: An Int’l Q. 499, 500 (2019), http://eliassi.org/sunstein_2019_algs_correcting_biases.pdf; 
Kimberly A. Houser, Can AI Solve the Diversity Problem in the Tech Industry? Mitigating Noise and Bias in 
Employment Decision-Making, 22 Stan. Tech. L. Rev. 290, 352 (2019), https://www-cdn.law.stanford.edu/wp-
content/uploads/2019/08/Houser_20190830_test.pdf.   
3 Moreover, NIST recognizes that AI “is rapidly transforming our world. Remarkable surges in AI capabilities have 
led to a wide range of innovations including autonomous vehicles and connected Internet of Things devices in our 
homes. AI is even contributing to the development of a brain-controlled robotic arm that can help a paralyzed person 
feel again through complex direct human-brain interfaces.” Artificial Intelligence, NIST, 
https://www.nist.gov/artificial-intelligence (last visited Oct. 1, 2022). See also About Artificial Intelligence, National 
Artificial Intelligence Initiative Office, https://www.ai.gov/about/ (last visited Oct. 1, 2022) (explaining that 
investments in AI technology “have led to transformative advances now impacting our everyday lives, including 
mapping technologies, voice-assisted smart phones, handwriting recognition for mail delivery, financial trading, 
smart logistics, spam filtering, language translation, and more. AI advances are also providing great benefits to our 
social wellbeing in areas such as precision medicine, environmental sustainability, education, and public welfare.”). 
 
3 
learning, a machine learning (“ML”) approach that learns from a user’s interaction with a given 
device while keeping all the training data on the device, so that the data does not need to be 
shared with a server.4 Indeed, CTA has supported efforts at the federal level to develop voluntary 
risk-based frameworks to address potential AI risks, while enabling stakeholders to maximize the 
benefits of this technology,5 an approach reflected in NIST’s AI Risk Management Framework 
(“RMF”), a flexible and voluntary framework for managing AI risks. CTA also has produced 
consensus standards supporting responsible and trustworthy AI.6 
 
NIST’s findings and decision to use a voluntary framework suggest it may be premature 
for NTIA to move forward with broad restrictions on a nascent technology which offers the 
potential to dramatically improve consumer well-being. This is especially true given public and 
private sector efforts to establish voluntary risk management frameworks that are tailored to 
potential risks while still allowing AI to be deployed in beneficial ways. Given the increased use 
of these voluntary risk management frameworks and the fast-moving pace of development of this 
technology, NTIA should proceed with caution and avoid adopting overly prescriptive rules. For 
the same reason, the National Security Commission on Artificial Intelligence’s Final Report did 
not recommend regulation for AI technologies due, in part, to the “speed of technology 
development by the private sector …”7 Prescriptive rules would undermine the important work 
that has been done across the public and private sectors to focus on risk-based approaches. These 
findings counsel against the adoption of broad prescriptive rules at this time. 
 
II. 
Response to Certain Questions Framed by NTIA’s Request For Comments 
A. 
How should NTIA define “open” or “widely available” when thinking about 
foundation models and model weights? 
 
Properly defining and scoping models with widely available open weights (“open weight 
models”) is critical to ensure that policy findings and actions are focused and precise. Ill-defined 
terms and concepts may lead to rules or recommendations that are overbroad and which would 
likely lead to costly new obligations on model developers or deployers at the risk of undermining 
innovation. As a foundational principle, NTIA should avoid a binary approach, defining models 
as either open or closed. In fact, there are gradients of openness in these models. In addition, 
various AI system components, such as algorithms and training data, can also be made more or 
less open. Thus, NTIA should avoid a narrow or prescriptive definition to account for the 
 
4 For example, Google recently published research on Entities as Experts AI, explaining how these systems are 
answering text-based questions with less data.4 Google has also published guidance for regulators on how to most 
effectively regulate AI in its Recommendations for Regulating AI paper. Recommendations for Regulating AI, 
Google, https://ai.google/static/documents/recommendations-for-regulating-ai.pdf (last visited Mar. 24, 2024). 
5 See, e.g., Consumer Technology Association Comments, RFI - NIST AI Risk Management Framework, Docket 
No. 21076-01510 (filed Sept. 15, 2021), available at https://www.nist.gov/system/files/documents/2021/09/16/ai-
rmf-rfi-0087.pdf.  
6 See, for example, The Use of Artificial Intelligence in Health Care: Trustworthiness (ANSI/CTA-2090) (rel. Feb. 
2021), available at: https://shop.cta.tech/products/the-use-of-artificial-intelligence-in-healthcare-trustworthiness-
cta-2090; and Guidelines for Developing Trustworthy Artificial Intelligence Systems (ANSI/CTA-2096) (rel. Nov. 
2021), available at: https://shop.cta.tech/collections/standards/products/guidelines-for-developing-trustworthy-
artificial-intelligence-systems-ansi-cta-2096.   
7 See Final Report, National Security Commission on Artificial Intelligence, at 449 (Mar. 19, 2021), available at 
https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf.  
 
4 
variability of distinct types of models.8 
 
Although openness in models and components vary, a recent study by AI researchers9 
notes that open models share common attributes: (i) broad access - open models generally make 
model weights widely available to the public; (ii) greater customizability – these models can be 
customized for various downstream applications; (iii) on device capabilities – open models can 
directly deployed on local hardware; (iv) inability to rescind – access and use rights to these 
models are not easily revoked by the model developer; and, (v) limits on downstream moderation 
– because inferences can occur on several platforms other than the developer’s, monitoring or 
moderating downstream use is challenging.10 NTIA should consider these aspects when defining 
the attributes of openness and thinking about foundation models and model weights. 
 
At the same time, NTIA should avoid conflating open weight models generally with the 
“dual use foundation models with widely available weights” identified in the recent Executive 
Order.11 The E.O. clearly defines the scope of “dual use foundation models with widely available 
weights” in terms of those models presenting specific risks associated with threats arising from 
chemical, biological, radiological, or nuclear weapons, cyber-attacks or related risks.12 However, 
NTIA must recognize that “foundation models with widely available weights” or “open 
foundation models” as used in the Request for Comments are not necessarily dual use models. 
Many “open foundation models” do not present the kinds of risks identified in the E.O., nor do 
all open foundation models offer dual use capabilities. 
 
NTIA should ensure it adheres to the distinction between models which clearly are within 
scope of the E.O. defined term (“dual use foundation models with widely available weights”) and 
those that are not. Otherwise, all open foundation models will be treated the same from a risk 
perspective and will face the attendant obligations of mitigating such risks (i.e. greater regulatory 
duties). Such an approach would severely limit the use and benefits of such open models and 
have a significant negative impact on innovation, competitiveness and research opportunities 
made available through access to open models. 
 
Finally, NTIA should avoid using model computing power thresholds (e.g., FLOPs) as a 
proxy for classifying open weight models. Instead, any classification should focus on the 
capabilities of models using qualitative criteria such as those put forward by organizations 
focused on defining such criteria by leveraging technical expertise, testing, evaluation, and 
verification of AI systems, such as the AI Verify Foundation.  
 
B. 
What are the benefits of foundation models with model weights that are widely 
available as compared to fully closed models? 
 
 
8 In addition, NTIA should consider harmonizing terminology in this area, such as by defining the spectrum of open 
models as “open innovation” models. 
9 On the Societal Impact of Open Foundation Models, Sayash Kapoor, Rishi Bommasani, et al. (rel. Feb. 27, 2024) 
(hereafter Kapoor, et al., Open Foundation Models) available at: https://arxiv.org/abs/2403.07918. 
10 Id. at 3. 
11 Executive Order 14110 on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, 88 FR 
75191 (rel. Oct. 30, 2023).  
12 Id. at Sec. 3(k). 
 
5 
Open weight models offer numerous benefits not available through closed models. Open 
weight models provide opportunities for increased innovation by enabling broader access and 
greater customizability to the output behaviors of those models. Because open weight models can 
be more deeply customized, they can support innovation across a range of applications.13 
Further, models with open weights allow application developers and researchers to perform 
inference and adaptation locally, which enables adaptation or fine-tuning of models on large 
proprietary datasets without increasing risks of privacy or data protection.14 
 
Further, open weight models enable greater transparency that is considerably higher than 
their closed counterparts. Indeed, some researchers have recently concluded that “[w]idely 
available model weights enable external researchers, auditors, and journalists to investigate and 
scrutinize foundation models more deeply.”15 Greater transparency is likely to reduce potential 
harms caused by opaque systems and can better enable research on the effects of such models. 
Many platforms have the ability to aggregate and track portions of open foundation models, 
which can help visibility and facilitate oversight.16  
 
Another benefit of open weight models is that such models lower barriers to entry and 
thereby help distribute opportunity and mitigate market concentration. Because these models 
have lower barriers to entry (e.g., cost, expertise), they are more accessible to the general public. 
Leveraging input and feedback from the broader AI community of researchers and users can help 
identify and mitigate bugs, biases, and safety issues that may otherwise go unnoticed, ultimately 
leading to better performing and safer AI products.17 This lower barrier to entry can help to drive 
AI research and development by academics or other subject matter experts, enabling 
communities with bespoke datasets and unique needs to form around specific platforms or 
industry sectors.18  
 
U.S. development of open weight models can further America’s position as a leader in 
technology and innovation. An open, U.S.-promoted AI ecosystem may help foster multilateral 
collaboration on AI policy and security issues, as it may create a common and more transparent 
baseline from which all countries can assess the technology.19  
 
Finally, whereas closed system developers have the exclusive ability to control and 
restrict uses cases they deem unacceptable, users of open weight models may have the ability to 
make these types of decisions themselves.20 While restricting use cases can be a benefit against 
 
13 Kapoor, et al., Open Foundation Models at 4. 
14 Id. 
15 Id. 
16 Center for Security and Emerging Technology (CSET), Open Foundation Models: Implications of Contemporary 
Artificial Intelligence (Mar. 12, 2024) available at: https://cset.georgetown.edu/article/open-foundation-models-
implications-of-contemporary-artificial-intelligence/. (hereafter “CSET, Open Foundation Models”). 
17 Open-Sourcing Highly Capable Foundation Models, Elizabeth Seger, Noemi Dreksler, et al., (rel. Sept. 29, 2023) 
available at: https://arxiv.org/pdf/2311.09227.pdf.  
18 CSET, Open Foundation Models available at: https://cset.georgetown.edu/article/open-foundation-models-
implications-of-contemporary-artificial-intelligence/. 
19 Id. 
20 Considerations for Governing Open Foundation Models, Rishi Bommasani, Sayash Kapoor, et al., Stanford 
University Human-centered Artificial Intelligence (rel. Dec. 2023) available at 
https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf. 
 
6 
malicious actors, it also can have the effect of concentrating power with a small number of 
actors. More actors within the ecosystem will enable increased scrutiny of open models, which 
can help to crowdsource vulnerability and safety discovery that, in turn, will facilitate the 
discovery and implementation of patches and safety/security measures.21  
 
C. 
How do the risks associated with making model weights widely available 
compare to the risks associated with non-public model weights? 
 
NTIA’s consideration of risks associated with open weight models should focus on 
marginal risks arising from such models. As recognized by Kapoor, et al., risk analysis in this 
context must be grounded in a recognition that open models only present additional risks of 
misuse or harm beyond those harms that already exist via from the use of existing technologies 
(i.e., “marginal risk”).22 Indeed, while risks associated with access to widely available open 
weight models should not be dismissed, these concerns should be weighed against the 
recognition that certain risks apply to all model uses. Similarly, many mitigation tools available 
for existing risks arising from the use of closed models may be sufficient to guard against risks 
perceived to be caused or exacerbated by widely available model weights. Thus, NTIA should 
focus its analysis on marginal risks and within the context of generally available mitigation tools. 
With this in mind, NTIA should consider risks arising from open models holistically, accounting 
for only the marginal risk presented by open weight models, balanced against available 
mitigation tools and the significant benefits arising from the availability of open models.  
 
Further, one of the notable benefits of open models is that they facilitate opportunities for 
users to develop tailored mitigation tools and techniques to address risks unique to specific uses 
cases or operational scenarios. The clearest evidence of this benefit is in the area of 
cybersecurity, where crowdsourced solutions and mitigation techniques are helping to combat 
the ever evolving cybersecurity risk matrix. Thus, while openness may present certain unique 
marginal risks, the same attribute offers the benefit of developing mitigation tools from a broader 
universe of users. 
 
A fundamental risk of open weight models is the inability to rescind model access once a 
model weight has been made widely available. Although the model developer can limit access 
upon learning of a new marginal risk, existing copies of the model weights cannot be revoked. 
Certain risks, such as difficulties in establishing the validity of results or risks caused by bias 
within the model, may be mitigated by the release of training data or source code associated with 
fine tuning, pretraining, or deploying a model that is simultaneously widely available. These 
risks also must be balanced against the potential benefits of open model weights, which include 
the democratizing of AI by making it more widely accessible to more people in more parts of the 
world; driving AI use case innovation, experimentation, and adoption; and supporting AI safety 
research.  
 
With respect to privacy risks in particular, the openness of a system allows for innovation 
in the privacy and security domains in ways that closed systems would not because they allow 
 
21 CSET, Open Foundation Models available at: https://cset.georgetown.edu/article/open-foundation-models-
implications-of-contemporary-artificial-intelligence/. 
22 Kapoor, et al., Open Foundation Models, at 5-8. 
 
7 
good actors, such as cybersecurity and privacy researchers, to use the fine-tuning options of 
widely available model weights to stress-test the existing privacy and security controls of AI 
systems. Additionally, with limited exception, the privacy risks that could result from widely 
available model weights would not be a marginal risk because the provision of sensitive data to a 
third party service provider always presents an inherent risk of exposure to sensitive or 
confidential data if that third-party actor is compromised.23 This type of risk can be mitigated by 
organizational policies that limit the input of sensitive data for fine-tuning. Where this would not 
be possible, existing cybersecurity tools and frameworks can likely be leveraged to meet many of 
the needs for protecting the security and privacy of AI systems with open model weights.  
 
Another risk mitigation benefit unique to open weight models is that entities using such 
models are often able to deploy the model locally on a device or at the network edge. This 
generally reduces privacy risks by allowing the entity to use the model without the need to share 
sensitive or confidential business data with third parties.24 This benefit is particularly important 
in domains involving significant amounts of PII, such as healthcare and financial services. 
 
Accordingly, NTIA’s analysis of risks in this area should focus on whether there are 
marginal risks posed by a model, whether such marginal risks are significant or not, and then 
consider whether such risks are outweighed by the benefits of widely available model weights in 
this circumstance, or whether availability to the model weights should be restricted based on the 
level of anticipated marginal risk.25 
 
D. 
What are current or potential voluntary, domestic regulatory, and international 
mechanisms to manage the risks and maximize the benefits of foundation models 
with widely available weights?  
 
Risk management protocols and procedures are beginning to emerge, but continued 
development and refinement of benchmarks, evaluations and assessments is necessary. This 
work is critically important and requires a precise articulation of the actual, rather than 
speculative, marginal risks presented in this area. 
 
Entities leading in the development of the risk management mechanisms are numerous 
and include most notably NIST, through its work on a risk management framework and related 
questions. In addition, standards development organizations such as the International 
Organization for Standardization (ISO), CTA and others, plus numerous industry-led initiatives 
such as the AI Alliance, the Partnership on AI, and the Frontier Model Forum, along with 
academic initiatives such as Stanford University’s Human-Centered Artificial Intelligence and 
other non-governmental bodies, play a critical role in developing technical standards. Sector 
specific groups also can be involved in defining future AI governance. Finally, open-source 
model evaluation tools are available in the market today26 and those offerings are expected to 
 
23 See Privacy Risks of LLM Fine Tuning, Daniel Huynh (rel. Nov. 22, 2023), available at: 
https://blog.mithrilsecurity.io/privacy-risks-of-llm-fine-tuning/.   
24 Kapoor, et al., Open Foundation Models at 3. 
25 Of course, not all downstream uses are foreseeable, which may limit the utility of this approach in certain 
circumstances. 
26 See, e.g., Tensorflow Model Remediation, https://github.com/tensorflow/model-remediation (library that provides 
solutions for practitioners working to create and train models in a way that reduces or eliminates user harm resulting 
 
8 
continue to be available as this technology matures. 
 
NTIA should leverage the important work of these organizations and permit industry and 
civil society to continue to define appropriate benchmarks, evaluations, and assessment criteria 
independently, especially in the area of technical standards. Beyond these areas, NTIA’s focus 
should be on broader structural questions around regulation, liability and competition. For 
example, distinguishing between the role of developer and deployer in this ecosystem is critical. 
Consistent with the CTA’s National AI Policy and Regulatory Framework, because developers 
of open weight models have limited control over downstream uses, deployers should be 
responsible and accountable for potential risks or harms that occur when the open weight models 
are deployed.27 
 
E. 
In the face of  continually changing technology, and given unforeseen risks and 
benefits, how can governments, companies, and individuals make decisions or 
plans today about open foundation models that will be useful in the future? How 
should these potentially competing interests of innovation, competition, and 
security be addressed or balanced?  
 
AI provides the U.S. with the opportunity to advance innovation, boost economic 
productivity, and maintain global leadership in an emerging technology critical to American 
security interests against adversaries.28 Any policy development or governance frameworks that 
focus on hypothetical concerns that AI could surpass human abilities could put in jeopardy the 
opportunity to address more near-term governance, safety and security concerns, and have the 
potential to suppress innovation.29 It is increasingly important that such concerns do not exclude 
important perspectives from stakeholders in such a manner as to erode public trust in AI policy 
development institutions.30  
 
A comprehensive approach would involve the appropriate release of models with some 
level of openness when the benefits significantly exceed the risks. In addition to the decision to 
make a model available, there are complementary mechanisms for managing risk such as 
investment in broad-based innovation and safety research including partnerships between 
industry organizations, such as the Partnership on AI and Frontier Model Forum, and stress 
testing from external partners. Because developers have no control over final end uses of open 
foundation models, it is critically important that liability for this balance lies at the closest point 
 
from underlying performance biases); Fairness Comparison, https://github.com/algofairness/fairness-comparison 
(facilitates benchmarking of fairness aware machine learning algorithms). 
27 CTA, National AI Policy and Regulatory Framework (re. Sep. 23, 2023), available at: 
https://cdn.cta.tech/cta/media/media/pdfs/ai-
policy.pdf?_gl=1*1pthhjn*_ga*MjAyNDE5MTA2Mi4xNzExNTgyMzgw*_ga_5P7N8TBME7*MTcxMTU4MjM3OS
4xLjEuMTcxMTU4MzIyOC42MC4wLjA.  
28 AEI, AI Is a National Security Lifeline, by Klon Kitchen (rel. Aug. 15, 2023), available at: 
https://www.aei.org/foreign-and-defense-policy/ai-is-a-national-security-lifeline/. 
29 Center for Security and Emerging Technology, Commentary: Balancing AI Governance with Opportunity (rel. 
Nov. 30, 2023) available at: https://cset.georgetown.edu/article/commentary-balancing-ai-governance-with-
opportunity/. 
30 Carnegie Endowment for International Peace, How Hype Over AI Superintelligence Could Lead Policy Astray 
(rel. Sept. 14 2023), available at: https://carnegieendowment.org/2023/09/14/how-hype-over-ai-superintelligence-
could-lead-policy-astray-pub-90564. 
 
9 
to the end user of an AI product.  
 
Respectfully submitted, 
CONSUMER TECHNOLOGY ASSOCIATION  
/s/  Douglas K. Johnson 
Douglas K. Johnson 
Vice President, Emerging Technology Policy 
 
/s/  Michael Petricone  
Michael Petricone 
Sr. Vice President, Government and Regulatory 
Affairs 
 
Dated: March 27, 2024 
",CTA,6103
NTIA-2023-0009-0284," 
 
 
 
March 27, 2024 
 
 
Mr. Travis Hall 
National Telecommunications and Information Administration 
U.S. Department of Commerce 
1401 Constitution Ave NW 
Washington, DC 20230 
 
 
RE:  
Comments of the Connected Health Initiative to the National Telecommunications 
and Information Administration on Dual Use Foundation Artificial Intelligence 
Models With Widely Available Model Weights (Docket No. 240216-0052) 
 
 
The Connected Health Initiative (CHI) appreciates the opportunity to provide input to the National 
Telecommunications and Information Administration (NTIA) on on the potential risks, benefits, 
other implications, and appropriate policy and regulatory approaches to dual-use foundation 
artificial intelligence (AI) models for which the model weights are widely available.1  
 
CHI is the leading multistakeholder policy and legal advocacy effort driven by a consensus of 
thought leaders from across the connected health ecosystem. We aim to realize an environment 
where Americans can improve their health through policies that allow connected health 
technologies to enhance health outcomes and reduce costs.  As part of its commitment to 
responsibly advance AI in healthcare, CHI assembled a Health AI Task Force consisting of a 
range of innovators and experts, which developed a number of recommendations for 
policymakers. We encourage NTIA to consider each of these resources as it moves forward: 
• 
CHI’s Health AI Policy Principles, a set of recommendations on the wide range of areas 
that should be addressed by policymakers examining AI’s use in healthcare (available at 
https://bit.ly/3m9ZBLv); 
• 
CHI’s Position Paper, Why AI? Considerations for Use of Artificial Intelligence in 
States’ Medicaid and CHIP Programs, which maps CHI’s Health AI Policy Principles to 
the challenges and opportunities faced at the state level (https://bit.ly/2Y2FJle); 
• 
CHI’s Good Machine Learning Practices for FDA-Regulated AI, a proposed risk-based 
approach to benefit the Food and Drug Administration (FDA) as it addresses both locked 
and continuously-learning AI systems that meet the definition of a medical device 
(https://bit.ly/2YaYljk); 
• 
CHI’s Advancing Transparency for Artificial Intelligence in the Healthcare 
Ecosystem, a proposal on ways to increase the transparency of and trust in health AI 
tools, particularly for care teams and patients (https://bit.ly/3n36WO5); and  
• 
CHI’s Health AI Roles & Interdependencies Framework, a proposal of clear definitions 
of stakeholders across the healthcare AI value chain, from development to distribution, 
 
1 
https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-foundation-artificial-
intelligence-models-with-widely-available-model-weights.  
 
 
2 
 
deployment, and end use; and a discussion of roles for supporting safety, ethical use, and 
fairness for each of these important stakeholder groups that are intended to illuminate the 
interdependencies between these actors, thus advancing the shared responsibility 
concept (https://connectedhi.com/wp-content/uploads/2024/02/CHI-Health-AI-Roles.pdf). 
 
CHI appreciates NTIA’s discussion of open model foundations in its request for information, and 
agrees that open model foundations can support competition and innovation, and further 
transparency. Models with widely available weights can benefit from a feedback loop that 
includes users and developers, enabling eased feature improvements as well as identification 
and mitigation of risk, while spending less resources. 
 
CHI appreciates NTIA’s requesting input on how today’s open source software licensing 
approach could inform its approach to dual use foundation models. While not analogous 
(because open source software licenses do not encapsulate all components and capabilities of 
an AI model), open source licenses can be beneficial in many scenarios by harmonizing 
terminology, training, deployment, weights, and documentation/monitoring. However, such 
licenses cannot actively prevent malfeasance. Ultimately, CHI believes that the market, not 
government, should organically develop open model licensing approaches. 
 
Further, building on the above, we offer the following comments and recommendations to NTIA: 
• 
Improve its categorization of foundation models: Categorizing foundation models as 
either “open” of “closed” will not reflect the important distinctions between key existing 
categories of foundation models. The degree of “openness” depends on a range of 
factors,2 making the drawing of a hard line between “open” and “closed” arbitrary.  
 
 
Bommasani, Rishi, et al. ""Issue Brief Considerations for Governing Open Foundation Models | 
Stanford HAI."" Stanford University Human-Centered Artificial Intelligence, 13 Dec. 2023, 
https://hai.stanford.edu/issue-brief-considerations-overning-open-foundation-models.  
 
While each of the above categories of foundation model offers its own benefits and risks. 
While fully closed models may be preferrable to protect intellectual property, models that 
make weights available (or even source code) can provide access to a feedback loop 
with developers or the ability for users to make improvements. 
 
For purposes of the Executive Order, we urge NTIA to ensure that a “dual-use 
 
2 Bommasani, Rishi, et al. ""Issue Brief Considerations for Governing Open Foundation Models | Stanford 
HAI."" 
Stanford 
University 
Human-Centered 
Artificial 
Intelligence, 
13 
Dec. 
2023, 
https://hai.stanford.edu/issue-brief-considerations-overning-open-foundation-models.  
 
 
3 
 
foundation model” is not used synonymously with “open foundation model.” The 
Executive Order provides the following: 
 
“(k) The term “dual-use foundation model” means an AI model that is trained on 
broad data; generally uses self-supervision; contains at least tens of billions of 
parameters; is applicable across a wide range of contexts; and that exhibits, or 
could be easily modified to exhibit, high levels of performance at tasks that pose 
a serious risk to security, national economic security, national public health or 
safety, or any combination of those matters, such as by: 
 
(i) substantially lowering the barrier of entry for non-experts to design, 
synthesize, acquire, or use chemical, biological, radiological, or nuclear 
(CBRN) weapons; 
 
(ii) enabling powerful offensive cyber operations through automated 
vulnerability discovery and exploitation against a wide range of potential 
targets of cyber attacks; or 
 
(iii) permitting the evasion of human control or oversight through means of 
deception or obfuscation. 
 
Models meet this definition even if they are provided to end users with technical 
safeguards that attempt to prevent users from taking advantage of the relevant 
unsafe capabilities.” 
 
CHI urges NTIA to recognize that not all “open foundation models” reflect the 
characteristics described in the Executive Order, and to ensure that the scope of 
foundation models addressed in its report is confined to “dual-use foundation models” as 
defined in the Executive Order. If the scope of NTIA’s report is not carefully restrained to 
this scope, it may cause definitional confusion in the short term, and later improperly 
expose foundation models that are not “dual-use foundation models” to future policy or 
regulatory requirements meant to be applied this category alone.  
• 
Address harms that are demonstrable and systemic. For purposes of this exercise 
under the Executive Order, NTIA should focus on high-risk scenarios (e.g., health, 
safety) for which there is a clear evidence base to address (in other words, policy 
proposals should not be based on remote edge use cases or hypotheticals). 
• 
Adhere to scalable risk-based harm mitigation principles. As NTIA explores policy 
and regulatory options for dual-use foundation models, we strongly urge NTIA to, 
consistent with the National Institute of Standards and Technology’s AI Risk 
Management Framework, ensure that its proposals are grounded in utilizing risk-based 
approaches to ensure that levels of review, assurance, and oversight are proportionate 
to potential harms. Building on this foundation, NTIA should discourage blanket/one-
size-fits-all approaches to risk mitigation for dual-use foundation models. 
 
NTIA’s definition of “widely available” should be similarly approached. The wide 
availability of a model is not necessarily an indicator of the risk(s) it may present. We 
urge NTIA’s definition of “widely available” to reflect the harms presented by the relevant 
use case(s). Similarly, floating point operations do not necessarily indicate higher risks. 
Such definitional thresholds should primarily consider the capabilities of the model. 
 
 
 
4 
 
We urge NTIA to maintain a broad perspective in considering risk in this matter. Many 
other factors than weights can alter the risks and benefits for a foundation model, such 
as training data, evaluation metrics, and deployment guidelines. 
• 
Promote shared responsibility across the AI value chain. Small software and device 
companies benefit from understanding the distribution of risk and liability in building, 
testing, and using AI tools. CHI urges NTIA’s report and recommendations to reflect that 
all stakeholders developing and using AI have a shared responsibility for AI safety, 
efficacy, and transparency. AI policy frameworks, including those addressing dual-use 
foundation models, should ensure the appropriate distribution and mitigation of risk and 
liability (that those in the value chain who have the ability to minimize that risk based on 
their knowledge and ability to mitigate have appropriate incentives to do so). 
 
One way that NTIA could support shared responsibility is through proposing the creation 
of a mechanism for sharing best practices, and for surfacing timely threat indicators, 
similar to that employed by Information Security and Analysis Centers (ISACs), which 
foster information sharing across and between the government and private sector while 
avoiding liability for doing so.3 
• 
Support, and rely on, international standards for risk management. CHI supports 
reliance on international consensus standards to develop metrics for risk, creating 
standards for best practices, and/or supporting or restricting the availability of foundation 
model weights. We believe that NIST’s approach taken in its AI Risk Management 
Framework is optimal. Support for and deference to international standardization would 
also align NTIA’s efforts with the U.S. Government National Standards Strategy for 
Critical and Emerging Technology.4   
• 
Coordination/Alignment with Other Leading Federal Efforts. Consistent with the 
intent of the Executive Order, alignment with other key federal efforts occurring in 
parallel should be prioritized. As a prime example, NTIA’s recommendations should be 
consistent with the output of the U.S. AI Safety Institute.5  
• 
Support international harmonization. We urge NTIA to maintain a priority for 
supporting risk-based approaches to AI governance in markets abroad and through 
bilateral and multilateral agreements. Already, developers of AI face top-down and one-
size-fits-all mandates that substantially impede their ability to develop and utilize AI 
across a range of use cases. It is crucial that NTIA’s efforts here, and the 
Administration’s efforts broadly, discourage, or at least have a positive influence on, 
such mandates in other jurisdictions. 
 
 
 
3 https://www.nationalisacs.org/about-isacs.  
4 https://www.nist.gov/standardsgov/usg-nss.  
5 https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute.  
 
 
5 
 
CHI appreciates NTIA’s consideration of the above views. We urge NTIA to contact the 
undersigned with any questions or ways that we can assist moving forward. 
 
 
Sincerely, 
 
 
 
Brian Scarpelli 
Executive Director 
 
Connected Health Initiative 
1401 K St NW (Ste 501) 
Washington, DC 20005 
 
1401 K Street NW  Suite 501
Washington, DC 20005
202.331.2130
#connectedhealth
connectedhi.com
/ConnectedHealthInitiative
Connected Health is an initiative
of ACT | The App Association
Policy Principles for 
Artificial Intelligence 
in Health
Policy Principles for AI in Health 
Today, there are already many examples of AI systems, powered by streams of data 
and advanced algorithms, improving healthcare by preventing hospitalizations, reducing 
complications, decreasing administrative burdens, and improving patient engagement. AI 
systems offer the promise to rapidly accelerate and scale such results and drive a fundamental 
transformation of the current disease-based system to one that supports prevention and health 
maintenance. Nonetheless, AI in healthcare has the potential to raise a variety of unique 
considerations for U.S. policymakers. 
Many organizations are taking steps to proactively address adoption and integration of AI 
into health care and how it should be approached by clinicians, technologists, patients and 
consumers, policymakers, and other stakeholders, such as the Partnership for AI, Xavier 
Health, the American Medical Association,  and the Association for the Advancement of 
Medical Instrumentation and BSI. Building on these important efforts, the Connected Health 
Initiative’s (CHI) Health AI Task Force is taking the next step to address the role of AI in 
healthcare.
First, AI systems deployed in healthcare must advance the “quadruple aim” by improving 
population health; improving patient health outcomes and satisfaction; increasing value by 
lowering overall costs; and improving clinician and healthcare team well-being. Second, AI 
systems should:
•	 Enhance access to health care.
•	 Empower patients and consumers to manage and optimize their health.
•	 Facilitate and strengthen the relationship and communication that individuals have with 
their health care team.
•	 Reduce administrative and cognitive burdens for patients and their health care team.
To guide policymakers, we recommend the following principles to guide action:
•	
National Health AI Strategy: Many of the policy issues raised below involve significant 
work and changes that will impact a range of stakeholders. The cultural, workforce training 
and education, data access, and technology-related changes will require strong guidance 
and coordination. Given the significant role of the government in the regulation, delivery, and 
payment of healthcare, as well as its role as steward of significant amounts of patient data, 
a federal healthcare AI strategy incorporating guidance on the issues below will be vital to 
achieving the promise that AI offers to patients and the healthcare sector. Other countries 
have begun to take similar steps (e.g., The UK’s Initial Code of Conduct for Data Driven 
Care and Technology) and it is critical that U.S. policymakers collaborate with provider 
organizations, other civil society organizations, and private sector stakeholders to begin 
similar work. 
•	
Research: Policy frameworks should support and facilitate research and development 
of AI in healthcare by prioritizing and providing sufficient funding while also ensuring 
adequate incentives (e.g., streamlined availability of data to developers, tax credits) 
are in place to encourage private and non-profit sector research. Clinical validation and 
transparency research should be prioritized and involve collaboration among all affected 
stakeholders who must responsibly address the ethical, social, economic, and legal 
implications that may result from AI applications in healthcare. Further, public funding and 
incentives should be conditioned on promoting the medical commons in order to advance 
shared knowledge, access, and innovation.   
•	
Quality Assurance and Oversight: Policy frameworks should utilize risk-based 
approaches to ensure that the use of AI in healthcare aligns with recognized standards 
of safety, efficacy, and equity. Providers, technology developers and vendors, health 
systems, insurers, and other stakeholders all benefit from understanding the distribution 
of risk and liability in building, testing, and using healthcare AI tools. Policy frameworks 
addressing liability should ensure the appropriate distribution and mitigation of risk and 
liability. Specifically, those in the value chain with the ability to minimize risks based on 
their knowledge and ability to mitigate should have appropriate incentives to do so. Some 
recommended guidelines include:
•	 Ensuring AI in healthcare is safe, efficacious, and equitable.
•	 Ensuring algorithms, datasets, and decisions are auditable and when applied to 
medical care (such as screening, diagnosis, or treatment) are clinically validated and 
explainable.
•	 AI developers should consistently utilize rigorous procedures and must be able to 
document their methods and results. 
•	 Those developing, offering, or testing healthcare AI systems should be required to 
provide truthful and easy to understand representations regarding intended use and 
risks that would be reasonably understood by those intended, as well as expected, to 
use the AI solution.
•	 Adverse events should be timely reported to relevant oversight bodies for appropriate 
investigation and action.
•	
Thoughtful Design: Policy frameworks should require design of AI systems in health care 
that are informed by real-world workflow, human-centered design and usability principles, 
and end-user needs. Also, AI systems should help patients, providers, and other care team 
members overcome the current fragmentation and dysfunctions of the healthcare system.  
AI systems solutions should facilitate a transition to changes in care delivery that advance 
the quadruple aim. The design, development, and success of AI in healthcare should 
leverage collaboration and dialogue between caregivers, AI technology developers, and 
other healthcare stakeholders in order to have all perspectives reflected in AI solutions.
•	
Access and Affordability: Policy frameworks should ensure AI systems in health care 
are accessible and affordable. Significant resources may be required to scale systems 
in health care and policy-makers must take steps to remedy the uneven distribution of 
resources and access. There are varied applications of AI systems in health care such 
as research, health administration and operations, population health, practice delivery 
improvement, and direct clinical care. Payment and incentive policies must be in place to 
invest in building infrastructure, preparing personnel and training, as well as developing, 
validating, and maintaining AI system with an eye toward ensuring value. While AI systems 
should help transition to value-based delivery models by providing essential population 
health tools and providing enhanced scalability and patient support, in the interim payment 
policies must incentivize a pathway for the voluntary adoption and integration of AI systems 
into clinical practice as well as other applications under existing payment models.  
•	
Ethics: Given the longstanding, deeply rooted, and well-developed body of medical and 
biomedical ethics, it will be critical to promote many of the existing and emerging ethical 
norms of the medical community for broader adherence by technologists, innovators, 
computer scientists, and those who use such systems. Healthcare AI will only succeed if it 
is used ethically to protect patients and consumers. Policy frameworks should:Ensuring AI 
in healthcare is safe, efficacious, and equitable.
•	 Ensure that healthcare AI solutions align with all relevant ethical obligations, from 
design to development to use.
•	 Encourage the development of new ethical guidelines to address emerging issues with 
the use of AI in healthcare, as needed.
•	 Ensure consistency with international conventions on human rights.
•	 Ensure that AI for health is inclusive such that AI solutions beneficial to patients are 
developed across socioeconomic, age, gender, geographic origin, and other groupings.
•	 Reflect that AI for health tools may reveal extremely sensitive and private information 
about a patient and ensure that laws protect such information from being used to 
discriminate against patients.
•	
Modernized Privacy and Security Frameworks: While the types of data items analyzed 
by AI and other technologies are not new, this analysis provides greater potential utility of 
those data items to other individuals, entities, and machines. Thus, there are many new 
uses for, and ways to analyze, the collected data. This raises privacy issues and questions 
surrounding consent to use data in a particular way (e.g., research, commercial product/
service development). It also offers the potential for more powerful and granular access 
controls for patients. Accordingly, any policy framework should address the topics of 
privacy, consent, and modern technological capabilities as a part of the policy development 
process. Policy frameworks must be scalable and assure that an individual’s health 
information is properly protected, while also allowing the flow of health information. This 
information is necessary to provide and promote high-quality healthcare and to protect the 
public’s health and well-being. There are specific uses of data that require additional policy 
safeguards, i.e., genomic information. Given that one individual’s DNA includes potentially 
identifying information about even distant relatives of that individual, a separate and more 
detailed approach may be necessary for genomic privacy. Further, enhanced protection 
from discrimination based on pre-existing conditions or genomic information may be 
needed for patients. Finally, with proper protections in place, policy frameworks should 
also promote data access, including open access to appropriate machine-readable public 
data, development of a culture of securely sharing data with external partners, and explicit 
communication of allowable use with periodic review of informed consent. 
•	
Collaboration and Interoperability: Policy frameworks should enable eased data 
access and use through creating a culture of cooperation, trust, and openness among 
policymakers, health AI technology developers and users, and the public.
•	
Workforce Issues and AI in Healthcare: The United States faces significant demands on 
the healthcare system and safety net programs due to an aging population and a wave of 
retirements among practicing care workers. And lower birth rates mean that fewer young 
people are entering the workforce. Successful creation and deployment of AI-enabled 
technologies which help care providers meet the needs of all patients will be an essential 
part of addressing this projected shortage of care workers. Policymakers and stakeholders 
will need to work together to create the appropriate balance between human care and 
decision-making and augmented capabilities from AI-enabled technologies and tools.
•	
Bias: The bias inherent in all data as well as errors will remain one of the more pressing 
issues with AI systems that utilize machine learning techniques in particular. In developing 
and using healthcare AI solutions, these data provenance and bias issues must be 
addressed. Policy frameworks should:
•	 Require the identification, disclosure, and mitigation of bias while encouraging access 
to databases and promoting inclusion and diversity.
•	 Ensure that data bias does not cause harm to patients or consumers.
•	
Education: Policy frameworks should support education for the advancement of AI in 
healthcare, promote examples that demonstrate the success of AI in healthcare, and 
encourage stakeholder engagements to keep frameworks responsive to emerging 
opportunities and challenges.
•	 Patients and consumers should be educated as to the use of AI in the care they are 
receiving.
•	 Academic/medical education should include curriculum that will advance health care 
providers’ understanding of and ability to use health AI solutions. Ongoing continuing 
education should also advance understanding of the safe and effective use of AI in 
healthcare delivery.
CHI Health AI Roles & 
Interdependency Framework
2
Artificial Intelligence (AI), especially generative AI, is already a powerful tool in healthcare, offering amazing potential to upgrade patient 
care by improving care outcomes and patient experiences, reducing healthcare provider burnout by simplifying administrative tasks, and 
helping to lower the total cost of care. One of the most helpful ways to see the value of AI in healthcare is to view the question through 
the lens of the “quadruple aim” framework. Built on the Institute for Healthcare Improvement’s “triple aim,” a widely accepted compass 
to optimize health system performance, the quadruple aim focuses on four key areas where health systems need to be improved, all of 
which AI is already, and will continue to, provide value across:
• 
Enhancing population health.
• 
Improving patient experience, satisfaction, and health outcomes.
• 
Augmenting clinician and healthcare team experience and satisfaction.
• 
Lowering overall costs of healthcare.
CHI has explored the ways in which AI is supporting each of the four aims of the quadruple aim in CHI’s paper, Why Does Healthcare
Need AI? 
But this promising technology is not infallible, and as healthcare organizations seek opportunities to use AI, stakeholders are facing 
important questions about how various risks or limitations should be handled in the development, distribution, deployment, and end 
use chain. Many organizations involved in the creation or application of healthcare AI have started to develop Responsible AI programs 
aimed at managing these risks or limitations within their organization. But as we have learned from other new technologies in the past, 
stakeholders can benefit from a clear discussion around all the safety measures and other actions that are needed, and how those 
actions might be applied at different steps from creation to the operation of the tool by the end user. This discussion will help various 
stakeholders better determine accountability for responsible AI best practices across this chain of stakeholders.
Overview
3
CHI urges all stakeholders in the healthcare ecosystem that are developing and using AI to align 
with CHI’s consensus health AI principles, which recognize the shared responsibility for AI safety, 
efficacy, and transparency. CHI supports (1) leveraging a risk-based approach to AI harm 
mitigation where the level of review, assurance, and oversight is proportionate to potential harms 
and (2) those in the value chain with the ability to minimize risks based on their knowledge and 
ability, and having appropriate responsibilities and incentives to do so. 
Further, managing AI/Machine Learning (ML) risks will be more challenging for small to medium-sized organizations, depending on 
their capabilities and resources. Building on these general health AI principles, CHI proposes clear definitions of stakeholders across 
the healthcare AI value chain, from development to distribution, deployment, and end use. Then, CHI suggests roles for supporting 
safety, ethical use, and fairness for each of these important stakeholder groups that are intended to illuminate the interdependencies 
between these actors, thus advancing the shared responsibility concept. These roles and interdependencies are also mapped to the 
Functions defined in the National Institute of Standards and Technology’s (NIST’s) AI Risk Management Framework (RMF).
4
Solution Users
Solutions/App
Developers & 
Deployers
2
AI/ML Developers
3
1
Deploying 
Organization
Health AI Platform 
Developer
AI Platform Developer
Foundation Model
Developer
Note: Depending on the use 
case, some of the roles in the 
healthcare AI/ML value chain 
may be occupied by the same 
party; in other scenarios, some 
roles may not be occupied.
Providers/ Clinicians & 
Administrative Users
Patient Groups/
Patient Users
Digital Health 
Solution Developer
Payers 
(CMS, Private)
5
Stakeholder Group
AI/ML Developers
DeﬁniƟon
NIST AI RMF 
Actor Tasks
Roles
Someone who designs, codes, 
researches, or produces an AI/ML 
system or plaƞorm for internal use or 
for use by a third party. 
See below for deﬁned Subgroups of 
this Stakeholder Group along with 
recommendaƟons speciﬁc to that 
Subgroup.
•
Informing deployers and users of data requirements/deﬁniƟons, intended 
use cases/populaƟons and applicaƟons (e.g., disclosing suﬃcient detail 
allowing providers to determine when an AI-enabled tool should 
reasonably apply to the individual they are treaƟng), including whether the 
AI/ML tools are intended to augment human work versus automate 
workﬂows, and status of/compliance with all applicable legal and 
regulatory requirements.
•
PrioriƟzing safety, eﬃcaciousness, transparency, data privacy and security, 
and equity from the earliest stages of design, leveraging (and, where 
appropriate updaƟng) exisƟng medical AI/ML guidelines on research and 
ethics, leading standards, and other resources as appropriate.
•
Employing algorithms that produce repeatable results and, when feasible, 
are auditable, and make decisions that (when applied to medical care) are 
clinically validated, fostering eﬃcacy through conƟnuous monitoring.
•
UƟlizing risk management approaches that scale to the potenƟal likely 
harms posed in intended use scenarios to support safety, protect privacy 
and security, avoid harmful outcomes due to bias, etc.
•
Providing informaƟon that enables those further down the value chain can 
assess the quality, performance, equity, and uƟlity of AI/ML tools.
•
Aligning with relevant ethical obligaƟons and internaƟonal convenƟons on 
human rights and supporƟng the development of new ethical guidelines to 
address emerging issues as needed.
AI Deployment; OperaƟon and 
Monitoring; Test, EvaluaƟon, 
VeriﬁcaƟon, and ValidaƟon 
(TEVV); Human Factors; 
Domain Expert; AI Impact 
Assessment; Governance and 
Oversight
6
Stakeholder 
SubGroup
FoundaƟon 
Model Developer
AI Plaƞorm 
Developer
Health AI Plaƞorm 
Developer
Digital Health 
SoluƟon Developer
DeﬁniƟon
Roles
Someone who creates or modiﬁes large 
and generalizable machine learning 
models that can be used/adapted for 
various downstream tasks and applica-
Ɵons, such as natural language process-
ing, computer vision, or soŌware 
development.
Building on the cross-AI/ML Developer roles noted above:
•
Assessing what bias and safety issues might be present in its FoundaƟon Model, and documenƟng 
steps taken to miƟgate those issues in its Transparency DocumentaƟon (e.g., Transparency Notes, 
System Cards and product documentaƟon). 
•
Providing clear guidance on (1) how to use and adapt its FoundaƟon Model for various foreseeable 
downstream tasks and applicaƟons, and (2) what limitaƟons or risks may arise from doing so based 
on challenges discovered during tesƟng and deployment.
Someone who creates or uses AI-powered 
plaƞorms that are tailored for the 
healthcare domain, such as administraƟve 
eﬃciency, diagnosƟcs, therapeuƟcs, or 
research. These plaƞorms may leverage 
foundaƟon models (or other types of 
machine learning models or soluƟons), 
such as AI plaƞorms, that are suitable for 
speciﬁc healthcare problems and data 
sources.
Building on the cross-AI/ML Developer roles noted above:
•
MeeƟng speciﬁc requirements and standards of the healthcare domain, such as accuracy, eﬃcacy, 
explainability, and compliance with regulaƟons.
•
TesƟng for, idenƟfying, and miƟgaƟng any bias and safety issues that may aﬀect the health outcomes 
of paƟents or the performance of clinicians using the Health AI Plaƞorm, and documenƟng these 
issues and the steps it has taken to address them in its transparency documentaƟon (e.g., 
transparency notes, system cards and product documentaƟon). 
Someone who creates complete digital 
tools and technologies to improve health 
and healthcare outcomes, such as 
providing diagnosƟc and administraƟve 
soluƟons for clinicians, paƟents, and 
healthcare organizaƟons. They may build 
digital health soluƟons with both health 
AI plaƞorms, which are specialized for the 
health care domain, and AI plaƞorms, 
which are more general and adaptable for 
various use cases and applicaƟons.
Building on the cross-AI/ML Developer roles noted above:
•
Specifying appropriate uses for its digital health soluƟon to avoid amplifying bias or safety issues that 
may exist in the underlying foundaƟon models, AI plaƞorms, or health AI plaƞorms.
•
Designing user interfaces to enable an end user to safely and eﬀecƟvely act upon the output of the 
tool, such as providing explanaƟons, feedback mechanisms, or human oversight opƟons, providing 
clear documentaƟon to Deploying OrganizaƟons and Users to help them avoid bias and safety issues.
Someone who leverages exisƟng founda-
Ɵon models and builds an industry-agnos-
Ɵc plaƞorm that enables other developers 
to access, customize, and deploy these 
models for various use cases and 
applicaƟons, such as natural language 
processing, computer vision, and/or 
soŌware development.
Building on the cross-AI/ML Developer roles noted above:
•
TesƟng for, idenƟfying, and miƟgaƟng bias and safety issues that may arise from using or modifying 
exisƟng foundaƟon models for its AI Plaƞorm, and documenƟng these issues and steps taken to 
address them in its transparency documentaƟon (e.g., transparency notes, system cards and product 
•
documentaƟon).
7
Stakeholder Group
Deploying 
OrganizaƟon 
(Healthcare Provider 
or Payor)
DeﬁniƟon
NIST AI RMF 
Actor Tasks
Roles
Someone who is a healthcare 
providers and health care payors that 
and is deploying soluƟons built by 
Digital Health SoluƟon Developers. 
They may also have their own internal 
IT staﬀ that use health AI plaƞorms or 
general AI plaƞorms to develop their 
own custom digital health soluƟons.
Respecting that managing AI/ML risks will be more challenging for small to 
medium-sized organizations depending on their capabilities and resources:
•
AdopƟng AI/ML Developer instrucƟons for use, specifying appropriate uses 
for Users through governance policies to avoid bias and safety issues that 
may exist in the underlying foundaƟon models, AI plaƞorms, or health AI 
plaƞorms.
•
Developing and leveraging digital health soluƟons that augment eﬃciencies 
in coverage and payment automaƟon, facilitate administraƟve simpliﬁca-
Ɵon/reduce workﬂow burdens, and are ﬁt for purpose.
•
Seƫng organizaƟon policy/designing workﬂows to reduce the likelihood 
that a User will act upon the output of the tool in a way that would cause 
fairness/bias or safety issues (tailored explanaƟons, feedback mechanisms, 
and/or human oversight opƟons).
•
Developing and organizaƟonal guidance on how the digital health soluƟon 
should and should not be used.
•
CreaƟng risk-based, tailored communicaƟons and engagement plans to 
enable easily understood explains to paƟents about how the digital health 
soluƟon was developed, its performance and maintenance, and how it 
aligns with the latest best pracƟces and regulatory requirements.
Assessment; Procurement; 
Governance and Oversight
Provider/Clinician 
Users and 
AdministraƟve Users
Someone who directly interacts with 
or beneﬁts from the digital health 
soluƟons that are built by Digital 
Health SoluƟon Developers or by the 
internal IT staﬀ of the Deploying 
OrganizaƟon. They may include 
clinicians, such as doctors, nurses, or 
pharmacists, and administraƟve staﬀ, 
such as billing, claims, or customer 
service personnel, in the provider and 
payor organizaƟons.
Respecting that managing AI/ML risks will be more challenging for small to 
medium-sized organizations depending on their capabilities and resources:
•
Taking required training and incorporaƟng employer guidance about use of 
AI/ML digital health soluƟons.
•
DocumenƟng (through automated processes or otherwise) whether AI is 
being used in medical records and report any issues or feedback to the 
developer, such as errors, vulnerabiliƟes, biases, or harms (where AI/ML’s 
use is known by the User).
•
Ensuring there is appropriate clinician review and review of the output or 
recommendaƟons from each digital health soluƟon prior to acƟng on it 
(where AI/ML’s use is known by the User). 
AI Deployment; OperaƟon and 
Monitoring; Domain Expert; AI 
Impact Assessment; 
Procurement; Governance and 
Oversight
An organizaƟon whose primary 
funcƟon is developing, coordinaƟng, 
promulgaƟng, revising, amending, 
reissuing, interpreƟng, or otherwise 
contribuƟng to the usefulness of 
technical standards to those who 
employ them.
•
Developing and promoƟng adopƟon of internaƟonal voluntary/non-
regulatory consensus standardized approaches and resources to steward a 
shared responsibility approach to AI.
Human Factors; Domain 
Expert; AI Impact Assessment; 
Governance and Oversight
8
Stakeholder Group
Payer Users 
(Centers for Medicare 
and Medicaid Services 
[CMS], State Medicaid, 
Private)
DeﬁniƟon
NIST AI RMF 
Actor Tasks
Roles
Someone that pays for the cost of 
healthcare services administered by a 
healthcare provider.
•
Leveraging AI/ML systems that improve eﬃciencies in coverage and 
payment automaƟon, facilitate administraƟve simpliﬁcaƟon, and reduce 
provider workﬂow burdens.
•
Aligning with medical AI/ML deﬁniƟons, present-day and future AI/ML 
soluƟons, the future of AI/ML medical coding changes and trends.
•
Developing support mechanisms for the use of AI/ML by providers based 
on clinical validaƟon, aligning with clinical decision-making processes 
familiar to providers, and high-quality clinical evidence.
•
Assuring that AI/ML systems allow for the individualized assessment of 
speciﬁc medical and social circumstances and provider ﬂexibility to override 
automated decisions, ensuring that use of AI/ML does not improperly 
reduce or withhold care, or overrides the provider’s clinical judgement.
•
Disclosing informaƟon about training and reference data to demonstrate 
that AI/ML systems do not create or exacerbate inequiƟes and that 
protecƟons are in place to miƟgate bias.
•
Developing and proliferaƟng easy to understand resources for beneﬁciaries 
and their providers that capture how and when AI/ML is being used, what 
informaƟon it is leveraging, and what it means to paƟents.
AI Deployment; OperaƟon and 
Monitoring; Domain Expert; AI 
Impact Assessment; 
Procurement; Governance and 
Oversight
PaƟent Groups/
PaƟent Users
Standard-Seƫng 
OrganizaƟons 
Someone who uses digital tools and 
technologies that are built by Digital 
Health SoluƟon Developers or 
experiences their use in treatment.
•
Developing and proliferaƟng easy to understand resources that capture 
how AI/ML is being used and what it means to paƟents/paƟent groups, 
including explanaƟons on the purpose and limitaƟons of the digital health 
soluƟons that they use or beneﬁt from (e.g., diagnosƟc, therapeuƟc, 
administraƟve). 
•
Raising awareness of paƟents’ rights and choices when using digital health 
soluƟons, such as consent, access, correcƟon, or deleƟon of their personal 
data.
Human Factors
TerƟary educaƟonal insƟtuƟons, 
professional schools, or forms a part 
of such insƟtuƟons, that teach 
medicine and awards a professional 
degree for physicians or other 
clinicians.
•
Developing and teaching curriculum that will advance understanding of and 
ability to use healthcare AI/ML soluƟons responsibly, which should be 
assisted by inclusion of non-clinicians such as data scienƟsts and engineers 
as instructors.
•
Developing curriculum to advance the understanding of data science 
research to help inform ethical bodies (e.g., InsƟtuƟonal Review Boards 
that are reviewing protocols of clinical trials of AI/ML-enabled medical 
devices).
Human Factors; Domain 
Expert; AI Impact Assessment
Stakeholder Group
CerƟﬁcaƟon Bodies 
& Test Beds
DeﬁniƟon
NIST AI RMF 
Actor Tasks
Roles
A cerƟﬁcaƟon body is a third-party 
organizaƟon that assures the 
conformity of a product, process or 
service to speciﬁed requirements.
A test bed is a plaƞorm for conducƟng 
rigorous, transparent, and replicable 
tesƟng of scienƟﬁc theories, 
compuƟng tools, and new 
technologies to a standard.
•
CreaƟng and making available transparent and reliable processes for the 
assurance of conformity to voluntary AI standards.
•
CreaƟng and making available voluntary sandbox environments to help 
evaluate the usability and performance of AI/ML-based high-performance 
compuƟng applicaƟons to advance the understanding of how reliable and 
eﬃcacious AI, and to provide an appropriate assurance of reliability and 
eﬃcacy.
Test, EvaluaƟon, VeriﬁcaƟon, 
and ValidaƟon (TEVV); Human 
Factors; Domain Expert; AI 
Impact Assessment; 
Governance and Oversight
AccrediƟng and 
Licensing  Bodies, 
and Medical 
Specialty SocieƟes 
and Boards
Academic and 
Medical 
EducaƟon 
InsƟtuƟons 
AccrediƟng and licensing bodies are 
governing authoriƟes that establish 
the suitability of any parƟcipaƟng 
cerƟﬁcaƟon body. Notably, state-level 
board serve this purpose for 
physicians, nurses, and other clinicians 
to standards set by each state.
Medical specialty socieƟes are 
organizaƟons for physicians, research 
and clinical scienƟsts who are acƟvely 
involved in the study of a parƟcular 
specialty.
•
Based on clinical needs and experƟse, developing and seƫng the medical 
standard of care and ethical guidelines to address emerging issues with the 
use of AI/ML in healthcare needed to advance the quadruple aim.
•
IdenƟfying the most appropriate uses of AI-enabled technologies and 
developing and disseminaƟng guidance and educaƟon on the responsible 
deployment of AI/ML in healthcare, both generally and for specialty-speciﬁc 
uses.
Test, EvaluaƟon, VeriﬁcaƟon, 
and ValidaƟon (TEVV); Human 
Factors; Domain Expert; AI 
Impact Assessment; 
Governance and Oversight
9
NIST AI RMF 
Actor Tasks
",Connected Health,9396
NTIA-2023-0009-0125," 
1411 K Street NW  
Washington, D.C. 20005 
 
Free Markets. Real Solutions. 
202-525-5717 
 
www.rstreet.org 
 
 
March 20, 2024 
National Telecommunications and Information Administration 
1401 Constitution Ave. NW 
Washington, D.C. 20230 
 
Re: NTIA Docket No. 240216-0052 - “Openness in AI Request for Comment” 
Thank you for providing the R Street Institute (R Street) with the opportunity to comment in response to 
the National Telecommunications and Information Administration’s (NTIA) “Openness in AI Request for 
Comment” (Request for Comment) proceeding.1 R Street is a nonprofit, nonpartisan public policy 
research organization. My name is Adam Thierer, and I am a senior fellow with R Street’s Technology 
and Innovation Policy team. I also served as a commissioner on the U.S. Chamber of Commerce 
“Commission on Artificial Intelligence Competitiveness, Inclusion, and Innovation,” which produced a 
major report on artificial intelligence (AI) policy issues.2 
                                                     
 
1  National Telecommunications and Information Administration, “Dual Use Foundation Artificial Intelligence 
Models with Widely Available Model Weights,” Docket Number 240216-0052, Feb. 21, 2024. 
https://www.ntia.gov/federal-register-notice/2024/dual-use-foundation-artificial-intelligence-models-widely-
available.  
2  “Artificial Intelligence Commission Report,” U.S. Chamber of Commerce, March 9, 2023. 
https://www.uschamber.com/technology/artificial-intelligence-commission-report.   
 
 
 
 
2 
 
R Street has published several essays relevant to this proceeding, including reports on “Flexible, Pro-
Innovation Governance Strategies for Artificial Intelligence” and “Existential Risks and Global 
Governance Issues around AI and Robotics.”3  
This Request for Comment raises important issues about the future of algorithmic innovation in the 
United States. The most important thing to understand about open foundation models, and open-
source AI technologies more generally, is that they are general-purpose technologies that are truly 
global in nature. By extension, these technologies and systems have important ramifications for the 
global competitiveness and geopolitical standing of nations.4  
This is why public policy focused on algorithmic technologies—especially open AI systems—must be 
established with great care and flexibility. We strongly recommend the agency ensure that open-source 
AI systems are allowed to continue to develop without arbitrary limitations on their capabilities. Instead, 
our focus should be on how to maximize their benefits while addressing risks in the most flexible fashion 
possible using iterative standards and multistakeholder processes.5 The agency already possesses the 
tools and methods needed to achieve that goal.  
It is worth noting a potential contradiction that lies at the heart of the debate over open AI systems and 
the tension between this and previous NTIA proceedings. In discussions about the safety and effectiveness 
                                                     
 
3  Adam Thierer, “Flexible, Pro-Innovation Governance Strategies for Artificial Intelligence,” R Street Policy Study 
No. 283, April 2023. https://www.rstreet.org/research/flexible-pro-innovation-governance-strategies-for-
artificial-intelligence; Adam Thierer, “Existential Risks and Global Governance Issues around AI and Robotics,” R 
Street Policy Study No. 291, June 2023. https://www.rstreet.org/research/existential-risks-and-global-
governance-issues-around-ai-and-robotics.  
4  Eric Schmidt, “Innovation Power: Why Technology Will Define the Future of Geopolitics,” Foreign Affairs 
(March/April 2023). https://www.foreignaffairs.com/united-states/eric-schmidt-innovation-power-technology-
geopolitics.  
5  Adam Thierer, “Statement for the Record on ‘Artificial Intelligence: Risks and Opportunities,’” U.S. Senate 
Homeland Security and Governmental Affairs Committee, March 8, 2023. 
https://www.rstreet.org/outreach/testimony-on-artificial-intelligence-risks-and-opportunities.    
 
 
 
 
3 
 
of AI models, many academics and policymakers worry about the lack of transparency or “explainability” 
of proprietary algorithmic systems, especially of the largest models being created by major technology 
companies.6 In fact, the NTIA has been considering questions about how to make AI systems more 
transparent as part of its “AI Accountability Policy” proceeding, which the agency launched last April.7 
Ironically, with this latest Request for Comment, the NTIA raises the opposite concern: whether open 
source systems might actually be too transparent and widely available.  
What is perhaps overlooked is that we have a range of constantly expanding options along the “open vs. 
closed” continuum of software and hardware systems, including new AI models. The NTIA notes that 
“‘openness’ or ‘wide availability’ of model weights are also terms without clear definition or consensus” 
and the agency speaks of the “gradients” of openness.8 This is correct, but it is important to understand 
that no formula exists whereby policymakers can get things “just right” when it comes to determining the 
optimal amount of openness or transparency of algorithmic systems. It would be a mistake for 
government to unnaturally tip the balance in either direction when the optimal amount of model 
openness or transparency is unclear. There is great benefit in allowing both open and closed systems to 
evolve organically over time because there are unique benefits to AI systems along that spectrum of 
options. Thus, the proper policy position for government toward open vs. closed systems should be one of 
technological agnosticism, and policymakers should not look to artificially tip the scales in either direction. 
                                                     
 
6  Frank Pasquale, The Black Box Society: The Secret Algorithms That Control Money and Information (Harvard 
University Press, 2016). 
7  National Telecommunications and Information Administration, “AI Accountability Policy Request for 
Comment,” United States Department of Commerce, Docket No. 230407-0093, RIN 0660-XC057, April 11, 2023. 
https://www.ntia.gov/issues/artificial-intelligence/request-for-comments.  
8  National Telecommunications and Information Administration, p. 6. 
 
 
 
 
4 
 
With this proceeding, the agency must avoid doing that by imposing too great a burden on open AI 
systems.  
I. 
Open systems offer clear benefits, which could easily be lost if overregulated 
Open-source systems have had a long and important history in the digital technology ecosystem 
because they are built on the accumulated knowledge and efforts of developers cooperating across the 
world.9 A recent National Institute of Standards and Technology (NIST) report explained how open 
source “has established itself as an indispensable methodology for developing software,” and it 
explored the many benefits of open-source technologies, including how those capabilities are now being 
used in more advanced AI systems.10 The agency highlighted the benefits of open systems in terms of 
“democratizing access, leveling the playing field, enabling reproducibility of scientific results,” and other 
positive attributes.11 Similarly, a recent Stanford University report explained how open AI systems have 
the benefit of “distributing power, catalyzing innovation, and ensuring transparency.”12 Other experts 
have likewise argued that open-source technologies “are the bedrock for grassroots innovation in AI” 
and can help “promote a diverse AI ecosystem.”13 Finally, open-source AI systems can be rapidly 
modified for more widespread, multilingual, and highly tailored uses to meet the needs of various 
                                                     
 
9  Alice Ivey, “The importance of open-source in computer science and software development,” Cointelegraph, 
Feb. 9, 2023. https://cointelegraph.com/news/the-importance-of-open-source-in-computer-science-and-
software-development.  
10  Apostol Vassilev et al., “Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and 
Mitigations,” NIST Trustworthy and Responsible AI, NIST AI 100-2e2023 (January 2024), p. 53. 
https://doi.org/10.6028/NIST.AI.100-2e2023.  
11  Ibid. 
12  Rishi Bommasani et al., “Considerations for Governing Open Foundation Models,” Stanford University Human-
Centered Artificial Intelligence, December 2023, p. 4. https://hai.stanford.edu/sites/default/files/2023-
12/Governing-Open-Foundation-Models.pdf.  
13  Ben Brooks, “Open-Source AI Is Good for Us,” IEEE Spectrum, Feb. 8, 2024. https://spectrum.ieee.org/open-
source-ai-good.  
 
 
 
 
5 
 
people and groups across the globe.14 Two security experts noted that new open-source models “are 
much more accessible and easier to experiment with,” and “can now be customized on a mid-priced 
laptop in a few hours.” The experts concluded that “[t]his fosters rapid innovation.”15 
Despite these clear benefits, fears of misuse tend to drive the debate over open AI systems today, 
including concerns about how they might be used to build dangerous weapons or create misinformation 
campaigns.16 While these risks deserve to be taken seriously, it is vital for the agency to keep two facts 
in mind. First, many of the supposed risks of open AI systems are shared with many other “open” 
information mediums and technologies. Descriptions about how to build dangerous weapons have 
appeared in books, magazines, blog posts, and online videos. Likewise, “misinformation,” however 
defined, is a problem that goes back to the rise of the printing press.17 Setting aside the thorny issue of 
whether it is constitutionally permissible under the First Amendment for the government to regulate 
speech about the creation of dangerous weapons or misinformation, the fact is that these concerns 
have long existed in various contexts. Open-source AI models could exacerbate those risks, but it might 
also provide a path to addressing many of these concerns.  
Second, the risks of open systems must be weighed carefully against the equally serious danger that 
overregulation could undermine their vibrancy (or even their very existence), thus giving rise to different 
                                                     
 
14  Kai-Fu Lee, “Artificial Intelligence Needs Open-Source Models to Reach Its Potential,” Wall Street Journal, Nov. 
29, 2023. https://www.wsj.com/articles/artificial-intelligence-needs-open-source-models-to-reach-its-
potential-e1f47d3f.  
15  Bruce Schneier and Jim Waldo, “Big Tech Isn’t Prepared for A.I.’s Next Chapter,” Slate, May 30, 2023. 
https://slate.com/technology/2023/05/ai-regulation-open-source-meta.html.  
16  David Evan Harris, “Open-Source AI Is Uniquely Dangerous,” IEEE Spectrum, Jan. 12, 2024. 
https://spectrum.ieee.org/open-source-ai-2666932122.  
17  Julie Posetti and Alice Matthews, “A Short Guide to the History of ‘Fake News’ and Disinformation: A New ICFJ 
Learning Module,” International Center for Journalists, July 23, 2018. https://www.icfj.org/news/short-guide-
history-fake-news-and-disinformation-new-icfj-learning-module; Jacob Soll, “The Long and Brutal History of Fake 
News,” Politico Magazine, Dec. 18, 2016. https://www.politico.com/magazine/story/2016/12/fake-news-
history-long-violent-214535.  
 
 
 
 
6 
 
dangers. As one open source developer noted, if heavy-handed compliance mandates are imposed on 
open-source systems, “[d]evelopers operating from dorm rooms and dining tables won’t be able to 
comply with the premarket licensing and approval requirements that have been proposed” because of 
their smaller size.18 “Grassroots innovation may become collateral damage,” he argued, because 
“regulations could put the brakes on this culture of open development in AI.”19 Other researchers agree. 
“The effects of these regulations may turn out to be impossible to undo, and therefore we should be 
extremely careful before we legislate them,” argued the founding researcher at Fast.AI, a nonprofit 
research group focused on democratizing AI development and use.20 He worries that overregulation of 
open-source systems could “ultimately lead to the frontier of AI becoming inaccessible to everyone who 
doesn’t work at a small number of companies, whose dominance will be enshrined by virtue of these 
ideas. This is an immensely dangerous and brittle path for society to go down.”21 
This is why it would be a serious mistake for government to impose arbitrary limitations on the power of 
open-source algorithmic systems. Again, these are truly global general-purpose technologies. Digital 
technology developers respond to the signals that national policymakers send when they craft laws and 
regulations for emerging information and computational technologies. We increasingly live in a world 
characterized by “global innovation arbitrage,” in which developers and investors often move their 
talent and resources to wherever they are treated most hospitably.22 This means that domestic policy 
decisions that discourage innovation and investment in next-generation algorithmic and computational 
                                                     
 
18  Brooks. https://spectrum.ieee.org/open-source-ai-good. 
19  Ibid. 
20  Jeremy Howard, “AI Safety and the Age of Dislightenment,” Fast.AI, July 10, 2023. 
https://www.fast.ai/posts/2023-11-07-dislightenment.html.  
21  Ibid. 
22  James Pethokoukis, “Global Innovation Arbitrage and Driverless Cars,” American Enterprise Institute, Aug. 23, 
2016. https://www.aei.org/economics/global-innovation-arbitrage-and-driverless-cars.  
 
 
 
 
7 
 
capabilities can result in a net loss of competitive advantage for nations because, as two leading 
software security experts noted, “there are simply too many researchers doing too many different 
things in too many different countries.”23 
II. 
The most important AI safety consideration of all 
A loss of competitive advantage in advanced computation through burdensome regulatory policies 
could have broader implications for U.S. geopolitics and national security. There is a symbiotic 
relationship between the strength of a nation’s technology base and its ability to address various threats 
to its security.24 
This explains why this proceeding is framed improperly. While it is sensible to evaluate the safety of 
open AI models, the greatest danger of all would be the absence of these technological capabilities 
within our own nation. Policymakers must allow powerful open-source AI systems to be developed 
domestically to ensure both that the United States stays ahead of potential adversaries on this front and 
also to ensure we learn collectively how to better address the risks and misuses of these technologies in 
a real-time fashion. 
This principle is especially important when considering the growth of China’s computational capabilities, 
including those in the field of open-source systems.25 As analysts with the Mercator Institute for China 
Studies have noted, China is a country “often seen as discouraging decentralized forms of innovation,” 
                                                     
 
23  Schneier and Waldo. https://slate.com/technology/2023/05/ai-regulation-open-source-meta.html. 
24  Loren B. Thompson, “Why U.S. National Security Requires A Robust, Innovative Technology Sector,” Lexington 
Institute, Oct. 8, 2020. https://www.lexingtoninstitute.org/why-u-s-national-security-requires-a-robust-
innovative-technology-sector.  
25   Rebecca Arcesati and Caroline Meinhardt, “China’s Open-Source Tech Development: Insights into a growing 
ecosystem,” Mercator Institute for China Studies, May 2021. https://merics.org/sites/default/files/2021-
05/MERICS%20Primer%20Open%20Source%202021_0.pdf.  
 
 
 
 
8 
 
but is now “bolstering the development of a vibrant open-source ecosystem.”26 China’s Ministry of 
Industry and Information Technology has been actively seeking to bolster the development of open-
source projects that can counter other global systems, most of which originated in the United States.27 
Recent Chinese open-source AI startups have grown more sophisticated and now rival leading American 
models.28 Upon launch late last year, Chinese startup 01.AI received a $1 billion valuation, leading Wired 
to declare that “This Chinese Startup is Winning the Open Source AI Race.”29 
It would be a mistake to believe that China is the only serious competitor in terms of advanced 
computational capabilities, however. The recent effort to develop the world’s largest open-source 
foundation model illustrates how there is intense competition to U.S. developers coming from 
numerous countries. On July 18, 2023, U.S.-based Meta announced it was launching its 70-billion 
parameter “Large Language Model Meta AI” (LLaMA 2) for open source commercial use and research.30 
This was an important moment for open-source AI because it represented the release of the biggest 
open-source model yet, and one that rivalled many of the leading proprietary foundation models from 
other players, such as OpenAI.31 But Meta also faces stiff competition from other open-source 
platforms, such as Falcon. Falcon is a large-scale open-source AI model that is royalty-free for research 
                                                     
 
26  Rebecca Arcesati and Caroline Meinhardt, “China bets on open-source technologies to boost domestic 
innovation,” Mercator Institute for China Studies, May 19, 2021. https://merics.org/en/report/china-bets-
open-source-technologies-boost-domestic-innovation.  
27  Ibid.  
28  Jose Antonio Lanz, “New Open Source AI Model from China Boasts Twice the Capacity of ChatGPT,” Emerge, 
Nov. 15, 2023. https://decrypt.co/206195/new-open-source-ai-model-from-china-boasts-twice-the-capacity-of-
chatgpt.  
29  “Chinese AI startup 01.AI valued at more than $1bn,” Verdict, Nov. 6, 2024. https://www.verdict.co.uk/01-ai-
valued-at-over-1bn; Will Knight, “This Chinese Startup Is Winning the Open Source AI Race,” Wired, Jan. 23, 
2024. https://www.wired.com/story/chinese-startup-01-ai-is-winning-the-open-source-ai-race.  
30  “Meta and Microsoft Introduce the Next Generation of Llama,” Meta, July 18, 2023. 
https://about.fb.com/news/2023/07/llama-2.  
31  Savannah Fortis, “Meta is building an AI model to rival OpenAI’s most powerful system,” Cointelegraph, Sept. 
11, 2023. https://cointelegraph.com/news/meta-building-ai-model-rivals-open-ai.  
 
 
 
 
9 
 
or commercial use, and it has become an important platform that other developers around the world 
have used to build bespoke open models of their own. Falcon was created by the Abu Dhabi-based 
Technology Innovation Institute (TII), a research center supported by the government of the United Arab 
Emirates and its Advanced Technology Research Council.32 
On Sept. 6, 2023, less than two months after Meta launched its record-breaking open-source model, the 
TII introduced Falcon 180B, a 180-billion parameter open AI model that was 2.5 times larger than Meta’s 
LLaMa.33 In other words, America’s supremacy in open-source foundation models lasted less than two 
months—at least based on the largest model on the market currently as measured in parameters. While 
the open-source ecosystem in the United States remains quite vibrant, this was an astonishing 
development that should serve as a wake-up call for America policymakers: U.S. innovators face stiff 
global competition for open-source AI and all large AI models.  
To be clear, the United Arab Emirates is not an adversary of the United States, and we need not panic 
simply because some nations are developing advanced computational capabilities—open sourced or 
otherwise. But the power of Falcon, and the speed with which it was developed, should also not be taken 
lightly. As has been the case in China, the government of the United Arab Emirates has invested massive 
resources into its nation’s computational capabilities. Nothing U.S. policymakers say and do to regulate 
open-source systems in the United States will limit what foreign leaders or innovators in other countries 
do to advance their own computational capabilities. The most dangerous myth in the field of AI policy is 
that American policy preferences can dictate global outcomes. The U.S. government does have 
                                                     
 
32   “About TII,” Technology Innovation Institute, last accessed March 4, 2024. https://www.tii.ae/about-us.  
33   “Technology Innovation Institute Introduces World’s Most Powerful Open LLM: Falcon 180B,” Technology 
Innovation Institute, Sept. 6, 2023. https://www.tii.ae/news/technology-innovation-institute-introduces-
worlds-most-powerful-open-llm-falcon-180b.  
 
 
 
 
10 
 
considerable leverage with some nations, and can work with other nation-states to lean on other nations 
to limit or alter their own policies and development patterns. But barring extreme steps that would give 
rise to grave geopolitical risks (such as armed interventions), there is no way for the United States to set 
AI policy for the world.34 Correspondingly, any domestic restriction that binds U.S. AI developers but not 
computational development by potential adversaries could undermine our nation’s innovative potential 
and security. 
III. 
 Pragmatic steps toward AI safety  
That being said, U.S. policymakers can continue to work with developers of advanced algorithmic 
systems to advance safety in a collaborative, flexible, bottom-up fashion. As has been the case 
throughout the internet’s history, multistakeholderism and iterative standards will continue to play an 
essential role in advancing safety and security outcomes, including for open AI systems. 
From the outset of the digital revolution, engineers and developers relied upon the notion of “running 
code and rough consensus” as a pragmatic governance philosophy of continuous digital technology 
iteration and improvement, and it has been particularly important for open-source systems.35 Online 
innovation has never been governed by a static set of rules but instead by a constantly evolving set of 
best practices, norms, and standards. Laws and regulations have operated as a backstop to address 
thornier problems that develop, but only after other approaches have been exhausted. This trial-and-
error approach to technological governance is what made the United States the global leader in digital 
                                                     
 
34  Thierer, “Existential Risks and Global Governance Issues,” p. 30. https://www.rstreet.org/research/existential-
risks-and-global-governance-issues-around-ai-and-robotics. 
35  Ibid, p 24.  
 
 
 
 
11 
 
technology over the past quarter century, and it is essential this model be preserved if the nation is 
going to continue to be a leader in AI and advanced computation.   
This same notion of “running code and rough consensus” can help shape better outcomes for open-
source AI systems through the development of safety standards and security practices that are refined 
in a flexible, voluntary way to adapt to new circumstances and concerns. Luckily, this process is already 
well underway through the development of various ethical codes, security and safety best practices, and 
information-sharing standards developed by industry groups, technical bodies, government agencies, 
and various researchers. A recent R Street report assessed many of those efforts and discussed how 
they were rooted in a common set of best practices and principles.36 
These same standards and principles are being applied to open-source systems already.  In a recent 
major study on open AI systems, researchers from Carnegie Mellon University and Intel Labs identified 
six possible principles and best practices that can help improve open-source safety outcomes.37 They 
include: 
1. Take a stand on unethical uses, and enforce it.  
2. Educate on project-specific harms.  
3. Consider technical restrictions on use.  
4. Leverage reputational incentives. 
5. Platforms should publish and consistently enforce policies that consider downstream uses.  
6. Publicize and study Ethical Source licenses. 
                                                     
 
36  Thierer, “Flexible, Pro-Innovation Governance Strategies for Artificial Intelligence,” p. 11. 
https://www.rstreet.org/research/flexible-pro-innovation-governance-strategies-for-artificial-intelligence.  
37  David Gray Widder et al., “Limits and Possibilities for ‘Ethical AI’ in Open Source: A Study of Deepfakes,” FAccT 
’22 (June 20, 2022), pp. 2035-2046. https://dl.acm.org/doi/abs/10.1145/3531146.3533779.  
 
 
 
 
12 
 
These are sensible guidelines, and many of these same ideas were echoed by a new report from 25 
leading researchers on open AI systems, who recommend that AI developers strive to constantly identify 
and refine “the responsible AI practices they implement and the responsible AI practices they 
recommend or delegate to downstream developers or deployers.” 38 Due to the “significant uncertainty 
for several misuse vectors due to incomplete or unsatisfactory evidence,” they note that only constant 
vigilance by developers and researchers can adequately address constantly evolving threat landscape.39  
These recommendations are rooted in two general commonsense principles. First, safety is an ongoing 
journey, not a final destination. There are no silver-bullet solutions to any of the potential risks 
associated with advanced algorithmic systems. Open systems can certainly create risks, and many 
unexpected new issues will develop over time, but that is why it is essential to keep dialogue and 
research open and ongoing. Importantly, there are many security benefits associated with AI systems 
that can facilitate real-time threat detection and correction.40 By their very nature, open systems help us 
find vulnerabilities and quickly address them. Regulation could short-circuit that corrective process of 
trial-and-error learning, leading to different safety and security vulnerabilities.  
Second, AI policy should focus primarily on bad outputs and bad actors, not on the underlying process by 
which algorithmic systems operate.41 AI governance strategies should not overregulate the underlying 
model or platform in an attempt to preemptively head-off every hypothetical worst-case scenario because 
                                                     
 
38  Sayash Kapoor et al., “On the Societal Impact of Open Foundation Models,” The Center for Research on 
Foundation Models, Feb. 27, 2024. p. 8. https://crfm.stanford.edu/open-fms/paper.pdf.  
39  Ibid. 
40  Haiman Wong et al., “Harnessing AI’s Potential – Identifying Security Risks to AI Systems,” R Street Institute, 
Feb. 21, 2024. https://www.rstreet.org/commentary/harnessing-ais-potential-identifying-security-risks-to-ai-
systems.  
41  Jay Obernolte, “The Role of Congress in Regulating Artificial Intelligence,” The Ripon Forum 57:3 (June 2023). 
https://riponsociety.org/article/the-role-of-congress-in-regulating-artificial-intelligence.  
 
 
 
 
13 
 
that would undermine the innovative potential of the open source AI more generally. As the Fast.AI 
founding research noted, “because we are discussing general-purpose models, we cannot ensure safety 
of the model itself — it’s only possible to try to secure the use of a model.”42 Of course, researchers have 
noted that there will also be significant challenges with constraining downstream uses of open AI 
technologies once they are released.43 Nonetheless, the proper locus of AI policy lies with a more targeted, 
risk-based approach that zeroes in on specific uses and users that raise serious safety and security 
concerns.44 This use-focused approach will not disrupt the vibrancy of the broader open source 
ecosystem.  
IV. 
 The crucial importance of ongoing multistakeholder processes  
Policymakers can help facilitate the development and adoption of such best practices over time by once 
again relying on multistakeholder processes to address problems in a more flexible, agile, and iterative 
fashion. NIST has already made important strides in this regard with its various multistakeholder risk 
frameworks and evolving set of procedures for convening relevant parties, calling for constant input, 
and then issuing iterative reports highlighting areas of consensus. 
NIST’s Artificial Intelligence Risk Management Framework 1.0 (AI RMF) is a voluntary, consensus-driven 
guidance process that has earned widespread support from distinct stakeholders.45 The AI RMF builds 
on the ethical frameworks and best practices developed by many different organizations and is 
“designed to address new risks as they emerge” and help create more trustworthy AI systems over 
                                                     
 
42  Howard. https://www.fast.ai/posts/2023-11-07-dislightenment.html.  
43  Widder et al., p. 9. https://dl.acm.org/doi/abs/10.1145/3531146.3533779.  
44  Adam Thierer, “The Most Important Principle for AI Regulation,” R Street Institute, June 21, 2023. 
https://www.rstreet.org/commentary/the-most-important-principle-for-ai-regulation.  
45  National Institute of Standards and Technology, Artificial Intelligence Risk Management Framework (AI RMF 
1.0), U.S. Department of Commerce, January 2023. https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf.  
 
 
 
 
14 
 
time.46 “This flexibility is particularly important where impacts are not easily foreseeable and 
applications are evolving,” the agency noted.47 Importantly, the AI RMF builds on other NIST consensus-
based multistakeholder frameworks, such as the NIST Privacy Framework and the recently updated NIST 
Cybersecurity Framework 2.0, which “does not prescribe how outcomes should be achieved,” but 
instead provides “guidance on practices and controls that could be used to achieve those outcomes.”48  
These NIST frameworks have been multi-year efforts focused on building collaborative approaches to 
privacy and security across industry, academia, and government not only in the United States, but with 
others around the world. The most innovative thing about these frameworks is the way they are 
versioned like software, reflecting the way that digital technology governance needs to be highly agile 
and iterative, responding to the way technology itself evolves at an increasingly rapid pace. This 
represents a smart move away from the more rigid regulatory approaches of the past, which established 
static policies that were often left unchanged for years or longer. AI policy must work differently, and 
the NIST frameworks give us a constructive path forward. The formation of NIST’s new AI Safety Institute 
also provides another mechanism for coordinating governance best practices for algorithmic systems.49 
It serves as a way for government to help “institutionalize frontier AI safety,” as many academics have 
called for, without imposing burdensome mandates that might undermine algorithmic innovation and 
                                                     
 
46  Ibid., p. 4. 
47  Ibid. 
48  National Institute of Standards and Technology, The NIST Cybersecurity Framework (CSF) 2.0, U.S. Department 
of Commerce, Feb. 26, 2024, p. i. https://doi.org/10.6028/NIST.CSWP.29.  
49  National Institute of Standards and Technology, “U.S. Commerce Secretary Gina Raimondo Announces Key 
Executive Leadership at U.S. AI Safety Institute,” U.S. Department of Commerce, Feb. 9, 2024. 
https://www.nist.gov/news-events/news/2024/02/us-commerce-secretary-gina-raimondo-announces-key-
executive-leadership-us.  
 
 
 
 
15 
 
competition, especially among open-source systems.50 This process should provide a way to refine risk-
analysis techniques for advanced AI systems and devise metrics for better measuring outcomes.  
In addition to the important role that NIST has in this process, the NTIA also has a role to play. The NTIA 
has played a crucial part in facilitating ongoing multistakeholder “soft law” efforts in the past to address 
other technical matters where collaboration and coordination were needed.51 “We believe that the 
multi-stakeholder approach, despite its challenges, works remarkably well at addressing emerging 
internet and technology policy questions,” the former leaders of the Obama administration NTIA noted 
shortly after leaving office.52 “We are just beginning to appreciate the full power and potential 
applicability of the approach,” they concluded, because multistakeholderism represents “a process that 
is flexible and allows businesses to adjust in response to the rapid changes in their economic and 
technical environments.”53 
This is the path forward for overseeing open-source AI developments. Importantly, policymakers must 
not simply focus of the risks of algorithmic innovation, they must also identify the risks associated with 
over-regulation of algorithmic systems, which could be even more problematic. “Policymakers should 
also proactively assess the impacts of proposed regulation on developers of open foundation models” 
and understand that “some policy proposals impose high compliance burdens for these developers, and 
                                                     
 
50  Markus Anderljung et. al., “Frontier AI Regulation: Managing Emerging Risks to Public Safety,” arXiv 2307:03718 
(Nov. 7, 2023) pp. 17-18. https://arxiv.org/abs/2307.03718.  
51  Adam D. Thierer, “Soft Law in U.S. ICT Sectors: Four Case Studies,” Jurimetrics 61:1 (April 20, 2021), pp. 79-119. 
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3777490.  
52  Lawrence E. Strickling and Jonah Force Hill, “Multi-stakeholder internet governance: successes and 
opportunities,” Journal of Cyber Policy 2:3 (Nov. 28, 2017), p. 310. 
https://www.tandfonline.com/doi/abs/10.1080/23738871.2017.1404619.  
53  Ibid. 
 
 
 
 
16 
 
such policies should only be pursued with sufficient justification of the adverse effect on the open 
foundation model ecosystem.”54  
Conclusion 
In closing, we will reiterate some core principles that should guide this proceeding, most of which we 
listed in our previous submission to the agency in its AI Accountability Policy proceeding.  
The agency should remember that America’s unique advantage over other nations has been a flexible 
and adaptive approach to digital technology policy.55 Digital innovation, especially open-source 
innovation, has blossomed because policy has allowed for a general freedom to innovate, and when 
challenges have arisen or dangers were discovered, we have used ongoing multistakeholder efforts and 
a variety of ex-post policy solutions to address those problems.56 This agile governance approach helps 
provide the public more innovative options, but it also provides our nation a more secure technological 
base.57 The agency should build on that model when formulating governance approaches for new 
computational systems and look to keep the United States at the forefront of the next great 
technological revolution.  
Finally, history offers us some lessons in terms of the need for policy humility. There were many fears in 
the late 1990s about the rise of open-source systems, and for a time, the U.S. government also treated 
powerful computation and encryption as dangerous “munitions” that should be subjected to export 
                                                     
 
54  Kapoor et al., p. 9. https://crfm.stanford.edu/open-fms/paper.pdf.  
55  Adam Thierer, Permissionless Innovation: The Continuing Case for Comprehensive Technological Freedom, 2nd 
ed. (Mercatus Center at George Mason University, 2016). 
56  Ibid. 
57  Adam Thierer, “U.S. Chamber AI Commission Report Offers Constructive Path Forward,” R Street Institute, 
March 9, 2023. https://www.rstreet.org/commentary/u-s-chamber-ai-commission-report-offers-constructive-
path-forward.  
 
 
 
 
17 
 
controls.58 Luckily, this moment passed and citizens today benefit greatly from open-source systems and 
encryption technologies. We need to exercise similar forbearance toward modern digital systems, 
especially open-source AI. 
 
 
Respectfully submitted, 
 
 
____________________________ 
Adam Thierer 
Senior Fellow 
R Street Institute 
1411 K St. NW  
Washington, D.C. 20005 
athierer@rstreet.org  
                                                     
 
58  Louis Anslow, “When the Mac was a ‘munition,’” Freethink, Jan. 30, 2024. 
https://www.freethink.com/opinion/power-mac-g4. 
",R Street,7917
NTIA-2023-0009-0252," 
 
 
March 27, 2024 
 
BY ELECTRONIC SUBMISSION 
 
National Telecommunications and Information Administration 
1401 Constitution Ave., 
NW Washington, D.C. 20230 
 
 
AH Capital Management, L.L.C.’s Response to the National Telecommunications 
and Information Administration’s Request for Comment on Dual Use Foundation 
Artificial Intelligence Models with Widely Available Model Weights 
 
Agency Docket Number: 240216-0052 
Regulations.gov Docket Number: NTIA-2023-0009 
 
 
AH Capital Management, L.L.C. (“a16z”) welcomes the opportunity to provide comments 
in response to the National Telecommunications and Information Administration’s (“NTIA”) 
Request for Comment (“RFC”) on Dual Use Foundation Models with Widely Available Model 
Weights (“Open Models”).1  As one of Silicon Valley’s preeminent venture capital firms with over 
$35 billion in committed capital, a16z has been investing in artificial intelligence for many years, 
and today, we manage pooled vehicles which hold investments in nearly 100 AI development 
firms.2  As one of the earliest and largest investors in many AI companies and projects, and as 
 
1 For the purposes of this response, we use the term “Open Models” to mean foundational artificial intelligence 
models (a) with open weights and training data; and that (b) are accessible by anyone.  See Section 2, infra. 
2 For more detail on our involvement in AI, please see “AI + a16z” on our firm’s website, at https://a16z.com/ai/.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-2- 
 
 
one of the largest investment advisers in the private technology space, a16z is well-positioned 
to evaluate the potential impact and importance of open source models on advancements in AI.  
We are pleased that NTIA recognizes the importance of these foundation models and appreciate 
the opportunity to participate in this important discussion.  
 
1. Introduction3 
 
Recent advancements in artificial intelligence represent the most significant 
development in computer technology since the invention of the microchip.  From these 
advancements, we are likely to see developments that measurably improve the human 
condition.  Far from representing an existential threat to humanity, the development of 
foundation AI models represents a profound opportunity to augment the same human 
intelligence that has created the world we live in today.  We can leverage such development to 
provide more people with even better outcomes, like the creation of new medicines, and 
improvements in the way we educate our children, make decisions, and generate creativity, to 
name a few. 
 
We have a long history in this country of taking a hands-off approach to open source 
software, and have greatly benefited from the advancements it has enabled.  Open source 
software provides much of the foundation of the internet, is extensively used by government 
agencies, and is widely considered to be more secure from malicious actors than proprietary 
software.  Consequently, there is no inherent reason to prohibit the creation, dissemination, and 
use of Open Models.  Nor is there reason to create a regulatory preference for models with 
weights and training data that are closed from public view (“Closed Models”).  In addition, Open 
Models are also a powerful defensive tool to mitigate risk and prevent harm, whether caused by 
 
3 This comment letter responds to the RFC’s questions in narrative format.  Consistent with NTIA’s instructions, 
we indicate where appropriate the number of the question to which we are responding.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-3- 
 
 
Open Models, Closed Models, or other non-AI technologies.4  Finally, Open Models already exist 
and are actively being distributed and used throughout the stream of commerce.5  Therefore, as 
a practical matter, it is likely impossible to shift to a fully closed paradigm.  
 
Like the internet and open source software movement before, Open Models promote 
innovation,6 reduce barriers to entry, protect against bias,7 and allow such models to leverage 
and benefit from the collective expertise of the broader artificial intelligence (“AI”) community.  
To be sure, bad actors will always look to exploit, misuse, and abuse powerful tools—including 
open and closed foundational models—to cause harm.  All tools have the potential for misuse, 
and Open Models are not inherently riskier than Closed Models.  In fact, Open Models have the 
distinct advantage of allowing governments, regulators, and the public to evaluate them—
 
4 Rowan Zellers et al., Defending Against Neural Fake News, NIPS’19: Proceedings of the 33rd International 
Conference on Neural Information Systems, 9054–9065, (Dec. 2019) 
https://dl.acm.org/doi/10.5555/3454287.3455099 (“The best existing fake news discriminators are, themselves, 
deep pretrained language models (73% accuracy) . . . However, we find that Grover, when used in a discriminative 
setting, performs even better at 92% accuracy.  This finding represents an exciting opportunity for defense 
against neural fake news: the best models for generating neural disinformation are also the best models at 
detecting it.”). 
5 Belle Lin, Open-Source Companies Are Sharing Their AI Free.  Can They Crack OpenAI’s Dominance?, Wall St. J. 
(Mar. 21, 2024), https://www.wsj.com/articles/open-source-companies-are-sharing-their-ai-free-can-they-
crack-openais-dominance-26149e9c; Brian Andrus, Open-Source AI: 9 Powerful Models You Need to Try, 
DreamHost (Jan. 30, 2024), https://www.dreamhost.com/blog/open-source-ai/.   
6 W3techs, https://w3techs.com/technologies/comparison/os-linux,os-windows (indicating that Open source 
software has been the foundation of technological innovation since the creation of Linux, which, as of March 2024, 
is used by 41.6% of all websites with known operating systems); Sascha Brodsky, Mistral AI’s New Language 
Model Aims for Open Source Supremacy, AI Business (Dec. 19, 2023), https://aibusiness.com/nlp/mistral-ai-s-
new-language-model-aims-for-open-source-supremacy#close-modal (quoting computer science professor at 
Carnegie Mellon stating “Without the foundational building block being open-sourced, a lot of progress in the 
overall field of computer science would likely have been significantly slower.”). 
7 Angela Wang & Olga Russakovsky, Directional Bias Amplification, 139 Proceedings of Machine Learning Research 
10882–10893 (2021), https://proceedings.mlr.press/v139/wang21t/wang21t.pdf; Inioluwa Deborah Raji & Joy 
Buolamwini, Actionable Auditing: Investigating the Impact of Publicly naming Biased Performance Results of 
Commercial AI Products, AIES’19 Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society 429–
435, https://dl.acm.org/doi/10.1145/3306618.3314244#.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-4- 
 
 
including their capabilities, risks, and underlying training data—in a more transparent manner.  
It is the very openness of Open Models that enables the more effective identification, 
understanding, and mitigation of risk.  
 
We are pleased to see that NTIA is proceeding thoughtfully and deliberately with regard 
to its approach to Open Models.  We encourage NTIA to be wary of generalized claims about the 
risks of Open Models and calls to treat them differently from Closed Models, especially those 
made by AI companies seeking to insulate themselves from market competition.  Open Models 
should be allowed to freely compete with both big AI companies and startups.  We urge NTIA to 
adopt relevant definitions and rules that allow for the continued development of Open Models 
rather than a restrictive approach through which the government, and not technological and 
market developments, picks winners and losers in this important, emerging market.   
 
2. Definitions8 
 
We propose that NTIA adopt a definition of “open” that includes not just models and 
weights, but also training data.  In order to maximize the benefits inherent in Open Models while 
leveraging collective experience to minimize harms, openness cannot be restricted to the 
models or the weights themselves.  Some of the most important factors determining how 
foundation models work are the decisions made regarding the training data, such as what data 
to use, whether the model is “aligned” during training, what values are used to “align” the model, 
and whose interests are prioritized to mitigate harms during model deployment. 
 
 
Open training data provides key insights not only into how the model was developed, but 
also into the model’s capabilities, limitations, and risks,9 including bias.10  Access to training data 
allows users to identify, analyze, and contextualize the information on which the model is acting 
 
8 This section addresses RFC questions 1, 1.c, and 5.f. 
9 Elizabeth Seger et al. Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and 
alternative methods for pursuing open-source objectives, arxiv:2311.09227, Sept. 29, 2023, 
https://arxiv.org/abs/2311.09227.  
10 Wang, supra note 7; Raji, supra note 7.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-5- 
 
 
and better understand the model’s outputs.  In evaluating the training data, users may discover 
gaps in information, errors, and algorithmic biases; including training data as a central 
component of the definition of “open” is essential to properly assess the benefits and risks 
presented by Open Models.  In short, data set openness is a key risk mitigation tool.11  
Accordingly, NTIA’s definition of “open” should incorporate the idea that open source 
foundation models are artificial intelligence models, with or without limited user restrictions, 
utilizing broadly accessible weights and training data.  
 
We also propose that NTIA adopt a definition of “widely available” based on whether the 
model, weights, and training data are released in a manner that would allow access and use by 
any developer rather than a definition that relies on specific levels of usage.  That is, a widely 
available Open Model is one with open weights and training data that is reasonably accessible 
to anyone, not one that is distributed to or used by a certain number of developers.  Models 
should still be considered widely available even if they are released with certain access 
restrictions (e.g., restricting access to individuals in countries subject to governmental 
sanctions) or terms prohibiting their use for illegal purposes.  Finally, availability is not 
synonymous with use or distribution, and adopting such a definition risks conflating these two 
principles in a manner that would result in an unduly narrow definition.  Rather, the degree to 
which a model actually is adopted or used is a feature of a free market wherein consumers 
choose the model that appeals to their preferences, and should not factor into whether the 
model is considered “widely available.”  
 
Our proposed definition differs from those proposed in other contexts by:  (1) including 
training data within its scope; and (2) separating concepts such as staged releases, the 
availability of companion assets, and license terms from the definitions themselves.  For 
example, a recent paper defines an open foundation model as one that (i) must provide weights-
 
11 Aspen Institute, Global Cybersecurity Group, Generative A.I. Regulation and Cybersecurity: A Global View of 
Policymaking, at 6 (Jan. 2024), https://www.aspendigital.org/wp-content/uploads/2024/01/Aspen-
Digital_Generative-AI-Regulation-and-Cybersecurity_January-2024.pdf (“Mandating data set openness, human 
oversight, and transparency in commercial generative models can reduce risks.”). 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-6- 
 
 
level access; (ii) need not be accompanied by the open release of any other assets; (iii) must be 
widely available, though some restrictions on users may apply; (iv) need not be released in 
stages, and (v) may have use restrictions.12  But the definition does not mention the role of open 
training data.  President Biden’s recent Executive Order on artificial intelligence similarly 
focuses heavily on the model weights of dual-use foundation models.13  Training data is 
fundamental to the operation of foundation models.  Understanding what data is being used and 
how it is being used to train models is critical to assessing and mitigating risks.  Accordingly, our 
definition acknowledges the importance not only of access to the weights used within a model 
but also to the training data. 
 
Influential organizations in the open source community are starting to recognize the 
importance of developing clear definitions for Open Models that include open training data and 
wide availability.  For example, the Open Source Initiative (“OSI”) is engaged in an ongoing 
project to develop a definition for Open Source AI.  OSI defines Open Source AI as an AI System 
that allows the public to (1) use the system for any purpose and without having to ask for 
permission; (2) study how the system works and inspect its components; (3) modify the system 
for any purpose, including to change its output; and (4) share the system for others to use with 
or without modifications, for any purpose.14  OSI recognizes that, for the public to be able to 
modify and use an Open Source AI System, there must be public access to the training data, the 
code, and the model parameters, including the weights.15  We encourage NTIA to recognize the 
 
12 Sayash Kapoor et al., On the Societal Impact of Open Foundation Models, Feb. 27, 2024, 
https://crfm.stanford.edu/open-fms/paper.pdf.  
13 Exec. Order No. 14110, Safe, Secure, and Trustworthy Development and Used of Artificial Intelligence (Oct. 30, 
2023), 88 FR 75191-75226 (Nov. 1, 2023), https://www.federalregister.gov/documents/2023/11/01/2023-
24283/safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence.  
14 Open Source Initiative, The Open Source AI Definition, version 0.0.6 (Mar. 10, 2024), 
https://hackmd.io/@opensourceinitiative/osaid-0-0-6.  
15 Id.  OSI specifies that access to the training data includes access to “the training methodologies and techniques, 
the training data sets used, information about the provenance of those data sets, their scope and characteristics; 
how the data was obtained and selected, the labeling procedures and data cleaning methodologies.” 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-7- 
 
 
importance of including the training data in its definition of Open Models to maximize their 
inherent benefits while leveraging collective experience to minimize harms. 
 
The remainder of our comment presumes that Open Models are foundation models with 
open weights and training data that are accessible by anyone, with limited exceptions such as 
those mentioned above. 
 
3. Advantages and Benefits of Open Models16 
 
Open Models provide important benefits, including (1) reducing barriers to entry and 
promoting innovation, (2) increasing competition, and (3) providing transparency into how 
models are developed, which promotes democratization and reduces the potential for bias.    
 
a. Open Models Reduce Barriers to Entry and Promote Innovation.17 
 
Open Models provide developers with broad access, options for customization, and the 
ability to run inferences themselves.  These features greatly expand who is able to access 
foundation models, how new models or modifications are created, and how foundation models 
are used to develop applications.18   
 
Developers are not required to invest vast sums of money or time to train Open Models 
from the ground up.  Instead, they can rely on existing models, which allows for iterative 
 
16 This section addresses RFC question 3. 
17 This question addresses RFC questions 3.a and 6.a. 
18 Brodsky, supra note 6 (“With open source, there is a lot more competition, and the speed of innovation is 
higher.  Also, the barrier to entry for someone to get into the field is much lower if there is high-quality open-
source software available.  That type of democratization in creation and learning for new entrants to the field is 
also critical.”) (quoting Jignesh Patel, computer science professor at Carnegie Mellon University and co-founder of 
DataChat, a no-code, generative AI platform). 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-8- 
 
 
progress and the ability to leverage previous work.19  Removing cost as a barrier (and often the 
most significant barrier) enables developers to engage with, refine, improve, and deploy Open 
Models to find new and beneficial uses for AI without the need for extensive financial resources.  
The accessibility of Open Models enables developers to channel time and energy toward 
improvements and innovative applications to specific and diverse use cases. 
 
Relatedly, Open Models can be customized in a manner that preserves the original model 
but allows parallel development toward new uses and capabilities that may lack alignment with 
the original developer’s commercial or social goals or technical abilities.  In this way, Open 
Models promote both efficiency and innovation:  developers can leverage prior, baseline work 
in order to focus on making improvements, and specialists with the most knowledge and 
experience with a particular goal or challenge can more readily focus their efforts on solving 
these novel problems without needing first to recreate what was done before.  While customized 
models may lead to fragmented model usage,20 their use of a common Open Model as a starting 
point makes this less likely and less problematic, since each customization can retrace its steps 
back to the single origin point if and when it becomes necessary to understand (or mitigate) the 
effects of each developer’s iterative choices.  This ability to leverage a single Open Model as a 
common starting point for specialized model architectures ultimately allows the possibility even 
for highly customized models to be combined or merged in ways that otherwise may prove 
technically infeasible.21  In turn, Open Models enable wide collaboration among software 
developers.  Because open models are widely accessible, software developers with different 
expertise and perspectives are able to contribute to the existing model and make improvements 
 
19 Abeba Birhane et al., Power to the People? Opportunities and Challenges for Participatory AI, EAAMO ‘22: 
Proceedings of the 2nd ACM Conference on Equality and Access in Algorithms, Mechanisms, and Optimization, 1–
8 (Oct. 2022) https://dl.acm.org/doi/abs/10.1145/3551624.3555290.   
20 As models become more and more customized, they may become more incompatible and less interoperable 
with potential impact on further development.  Assaf Baciu, Tackling the Fragmented Nature of Multiple GenAI 
Tools, Forbes (Dec. 26, 2023), https://www.forbes.com/sites/forbestechcouncil/2023/12/26/tackling-the-
fragmented-nature-of-multiple-genai-tools/?sh=396e4f9a3c10. 
21 Colin Raffel, Building Machine Learning Models Like Open Source Software, Communications of the ACM (Feb. 1, 
2023), https://cacm.acm.org/opinion/building-machine-learning-models-like-open-source-software/#R5.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-9- 
 
 
beyond what would be possible on Closed Models accessible to only a small pool of individuals 
relying only on the skillset within that pool. 
 
Accordingly, permitting foundation models to be provided on an open basis facilitates 
technological research into safety, security, and novel applications of AI in a variety of fields.  
Further, Open Models enable research into AI interoperability, security, and safety, while Closed 
Models may inherently be designed not to interoperate and are susceptible to allowing their 
developers to hide or obfuscate any security or safety concerns from those who wish to study, 
understand, and mitigate them.  For example, Open Models enable developers to leverage their 
collective expertise to spot and fix bugs, validate model accuracy, and think through strategies 
for reducing risks, including bias, on a broad and collaborative basis.22   
 
Open Models are also essential to scientific research, which requires that results are able 
to be reproduced before they are broadly accepted as valid:  through the accessibility these 
models provide, researchers will be able to check results using the exact same tools as those 
originally conducting experiments or analyzing data in a way that has been undermined by—
and is fundamentally not possible with—Closed Models.  Moreover, Closed Models may have 
measures in place that may prevent some scientific and technological research entirely and for 
reasons known only to those who control them.23  Thus, the existence of Open Models facilitates 
continued exploration of the ways in which AI can aid the scientific and academic communities 
in a manner that would all but be foreclosed if regulations prohibited or severely limited such 
models. 
 
Open source software—which has been at the forefront of technological innovation since 
Linux was introduced in 1991—provides a useful parallel through which to understand the 
 
22 Aaron Linskens, Open source risk management: Safeguarding software integrity, Sonatype (Oct. 13, 2023), 
https://blog.sonatype.com/open-source-risk-management; see also Ashley Scheutt, Alison Parker & Alex Long, 
Open Source Software and Cybersecurity: How unique is this problem?, Ctrl Forward (Nov. 10, 2022), 
https://www.wilsoncenter.org/blog-post/open-source-software-and-cybersecurity-how-unique-problem.  
23 Shayne Longpre et al, A Safe Harbor for AI Evaluation and Red Teaming, arXiv.2403.04893 (Mar. 7, 2024), 
https://arxiv.org/abs/2403.04893.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-10- 
 
 
benefits of supporting the development of Open Models.  By providing foundational elements on 
an open basis, open source software allowed computer science to develop more quickly and 
efficiently than it otherwise would have.  As of March 2024, over thirty years after its 
introduction, Linux is still used by 41.6% of all websites with known operating systems and 
powers most cloud-based machines.24  Linux also is the backbone for training large language 
models.  As one computer science professor and AI entrepreneur has stated, “[w]ithout that 
foundational building block being open-sourced, a lot of progress in the overall field of computer 
science would likely have been significantly slower.”25  Similarly, Python, a programming 
language developed under an OSI-approved open source license, is used by 49.28% of software 
developers worldwide.26  In the past few decades, users have been able to further contribute to 
Python’s development by tweaking it to support Boolean variables, sets, Unicode, context 
managers, generators, and keyword arguments as well as thousands of bug fixes.27  In fact, open 
source software is so foundational and effective that it is “widely used across the federal 
government and every critical infrastructure sector.”28  Like open source software, Open Models 
 
24 W3techs, https://w3techs.com/technologies/comparison/os-linux,os-windows (indicating that open source 
software has been the foundation of technological innovation since the creation of Linux, which, as of March 2024, 
is used by 41.6% of all websites with known operating systems); Brodsky, supra note 6.  
25 Brodsky, supra note 6 (quoting Jignesh Patel, computer science professor at Carnegie Mellon University and co-
founder of DataChat, a no-code, generative AI platform). 
26 Lionel Sujay Vailshery, Most used programming languages among developers worldwide as of 2023, statista (Jan. 
19, 2024), https://www.statista.com/statistics/793628/worldwide-developer-survey-most-used-languages/.  
27 Raffel, supra note 21. 
28 Open Source Software Security, Cybersecurity & Infrastructure Security Agency, 
https://www.cisa.gov/opensource#:~:text=Open%20source%20software%20is%20widely,critical%20part%20
of%20this%20effort (CISA also notes that it “open sourc[es] much of [its] codes via [its] “open-by-default” 
software development policy.); see also, e.g., Dep’t of Com. Source Code Policy, 
https://www.commerce.gov/about/policies/source-code; Department of Defense Chief Information Officer. 
Clarifying Guidance Regarding Open Source Software (OSS). October 16, 2009. 
http://dodcio.defense.gov/Portals/0/Documents/OSSFAQ/2009OSS.pdf (describing the benefits of OSS that 
should be considered when conducting market research on software for DOD use); Frequently Asked Questions 
regarding Open Source Software (OSS) and the Department of Defense (DoD), 
 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-11- 
 
 
will enable the quick and efficient development of foundation models and their application to 
solve problems in many different contexts.  Developers will be able to adapt and tailor Open 
Models to their specific needs, driving innovation across diverse sectors of the economy with 
diverse benefits. 
 
b. Open Models Increase Competition.29 
 
Open Models increase competition in the development and improvement of foundation 
models because they do not restrict the use of AI to gatekeeper companies with the most market 
power or resources.  This accessibility increases the prospect of competition and allows for 
participation by developers who may otherwise have been boxed out of working with AI due to 
their lacking the requisite access or resources that are necessary components to working within 
a closed ecosystem.  Increasing participation in this manner already has been shown to drive 
competition and lead to better technological outcomes.  For example, in discussing the potential 
impact of Open Models on competition, the Federal Trade Commission noted that open image-
generation models have surpassed the capabilities of the Closed Models on which they were 
based.30  
 
Because developers and startups can use Open Models as the basis for customization and 
improvement, the continued use of Open Models will contribute to market diversification.  Such 
diversification serves the interests of consumers by allowing them to choose the specific AI 
 
https://dodcio.defense.gov/OpenSourceSoftwareFAQ.aspx (stating that “continuous and broad peer-review, 
enabled by publicly available source code, improves software reliability and security through the identification 
and elimination of defects that might otherwise go unrecognized”); M-16-21, Federal Source Code Policy: Achieving 
Efficiency, Transparency, and Innovation through Reusable and Open Source Software, Office of Management and 
Budget, (August 8, 2016), https://www.whitehouse.gov/wp-
content/uploads/legacy_drupal_files/omb/memoranda/2016/m_16_21.pdf.  
29 This section addresses RFC questions 3.a, 3.c, and 6.b. 
30 Staff in the Bureau of Competition & Office of Technology, Generative AI Raises Competition Concerns, Federal 
Trade Commission (Jun. 29, 2023), https://www.ftc.gov/policy/advocacy-research/tech-at-
ftc/2023/06/generative-ai-raises-competition-concerns.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-12- 
 
 
product that fits their needs and, often, to obtain such a product at a lower cost.  By contrast, a 
regulatory environment permitting only Closed Models contributes to market concentration by 
restricting consumer choice, which can result in the competitive and systemic risks that arise 
from homogenization, including higher prices.31  Furthermore, if a few Closed Models dominate 
the market and thus are integrated into a wide variety of activities, then consumers have fewer 
choices about which models to use and may experience a lock-in effect.  If consumers have few 
choices and feel locked-in because the dominant Closed Models permeate their business and the 
broader economy, the Closed Model developers have less incentive to innovate and improve or 
address any errors, vulnerabilities, or failures that arise with their models.  This lack of incentive 
can threaten a significant amount of economic activity and harm consumers.   
 
To unleash the benefits of competition that Open Models provide, they should not be 
treated any differently than Closed Models from a regulatory perspective.  Instead, they “should 
be allowed to freely proliferate and compete with both big AI companies and startups.”32 
 
c. Open Model Transparency Promotes Democratization and Reduces Bias.33 
 
Open Models are transparent.  By providing visibility into their weights and underlying 
training data, Open Models enable developers, regulators, and the public to analyze, critique, fix, 
and improve the models.  They also enable those constituencies to understand and contextualize 
model outputs in a way that is not possible with Closed Models.  It is the very openness of Open 
Models that facilitates the more effective identification, understanding, and mitigation of the 
benefits and risks posed by dual use foundation models.  Additionally, this transparency allows 
for responsible innovation and public accountability, facilitating challenges to ingrained 
 
31 Id. (“If a single company or a handful of firms control one or several of these essential inputs, they may be able 
to leverage their control to dampen or distort competition in generative AI markets.  And if generative AI itself 
becomes an increasingly critical tool, then those who control its essential inputs could wield outsized influence 
over a significant swath of economic activity.”). 
32 Marc Andreesen, Why AI Will Save the World, AH Capital Management, L.L.C. (“a16z”) (Jun. 6, 2023), 
https://a16z.com/ai-will-save-the-world/.  
33 This section addresses RFC questions 3.a, 3.b, 3.c, 3.e, 4, and 7.b. 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-13- 
 
 
assumptions and established orthodoxies.  Broader scrutiny by a variety of stakeholders 
exposes underlying biases, inaccuracies, and vulnerabilities in these models and facilitates 
improvements, especially when compared with the lack of transparency inherent in Closed 
Models.  In particular, Open Models facilitate the democratization of AI and the reduction of bias 
endemic to opacity and ideological uniformity.  
 
We emphasize that Open Model weights alone are insufficient to fully understand the 
validity, reliability, or bias of a model.34  Rather, identifying and correcting biases requires true 
and meaningful openness across the model, enabling analysis of the training data, weights and 
commands which together generate the outputs.  Transparency throughout the model allows 
for the implementation of appropriate governance and accountability frameworks through 
which observers and other developers may monitor model performance and more accurately 
assess risk.35  Accordingly, as set out in Section 2, supra, it is important that NTIA adopt a 
definition of “open” that includes a requirement that models, weights, and training data be made 
widely available to public scrutiny. 
 
i. 
Democratization 
 
By allowing a more diverse group of developers and other stakeholders to access AI, 
Open Models prevent power and influence over this pivotal technology from being concentrated 
in the hands of a few large stakeholders and enable broader public participation in the evolution 
of how dual-use foundation models are built and allowed to operate.  Thus, the transparency 
provided by Open Models, and the increased participation it facilitates, has a democratizing 
effect on the AI landscape as a whole.   
 
 
34 See Section 2, supra. 
35 Rishi Bommasani et al., The Foundation Model Transparency Index, arXiv:2310.12941 (Oct. 19, 2023), 
https://arxiv.org/abs/2310.12941; Rishi Bommasani et al., Foundation Model Transparency Reports, 
arXiv:2402.16268 (Feb. 26, 2024), https://arxiv.org/abs/2402.16268.   
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-14- 
 
 
If Open Models ceased to exist, and AI leveraged only Closed Models, it would allow this 
powerful technology to be developed solely in a series of “black boxes” by companies and 
developers with unilateral and unaccountable control over model behavior.  Closed Model 
developers entirely control how information is incorporated into and flows through those 
models and may impose their views and preferences on the public without any meaningful 
guardrails.  By contrast, Open Models allow a more diverse range of developers (and in some 
ways the public at large) to debate and define acceptable model behavior.  Such access and 
engagement is increasingly important as models come to play a more central role in access to 
information.  By democratizing access, Open Models support fundamental pillars of our 
constitutional democracy, including the marketplace of ideas, robust debate, and open 
discourse.  These pillars support civic engagement, innovation, creative-thinking, problem-
solving, and competition. 
 
The democratization of AI facilitated by Open Models also helps build trust in AI and 
reduces fear of the technology.  As society’s use of AI has expanded, so too has the public’s 
skepticism about what data is being used to train the models, how that data is being protected, 
and how it is being used to generate outputs.  Because Closed Models do not offer users insight 
into this process, they have spurred a distrust of AI at best and a fear of a Terminator-like future 
at worst.  By contrast, Open Models’ characteristic transparency encourages user trust.  The 
public can readily discover the kind of information to which the model has access and what the 
model is designed to do with that information.  The public can also better understand and 
contextualize the risks and capabilities of AI, which helps mollify unfounded and speculative 
fears.  And, if users do not like how a particular model handles data or weighs certain 
parameters, or they want to address a particular risk, they can modify the model or choose to 
use another that meets their needs.  Thus, the marked transparency of Open Models encourages 
users and developers to engage with the technology, allowing it to grow and develop in a way 
that users understand and trust.     
  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-15- 
 
 
In addition, Open Models provide broader societal benefits, such as greater access to 
education and training to students and others, regardless of income or geography.36  Such 
education is critical to developing the skills necessary to succeed in an age where AI will rapidly 
reshape the workforce.  Even when Open Models do not outperform their closed counterparts, 
their “widespread availability is a boon to students all over the world who want to learn how to 
build and use AI to become part of the technological future, and will ensure that AI is available 
to everyone who can benefit from it no matter whole they are or how much money they have.”37 
 
ii. 
Bias 
 
Open Models reduce the potential for bias by allowing broad visibility into model weights 
and training data through which biases in model parameters, training data, and outputs can be 
identified.  Public scrutiny and the accountability it enables are the most effective means of 
preventing and combating bias as AI becomes more widely used.  This is particularly relevant in 
regards to bias introduced through the training data itself.  Providing this training data on an 
open basis means that developers with a variety of perspectives and expertise can access and 
review this data, subjecting it to scrutiny in order to understand how it influences model 
outputs.  Based on this examination, developers can challenge any biases inherent in that data 
in order to improve model accuracy and neutrality and have visibility into how these biases are 
likely to influence and bias model outputs, as well.   
 
Closed Models, on the other hand, do not present their weights or training data for public 
scrutiny and therefore are more vulnerable to undisclosed biases.  Because the models are 
closed, these effects might not be observed until the models are in use and the harm has already 
materialized, with the potential for significant impact on downstream activity.  To be sure, 
Closed Models have relied on other methods to try to achieve neutral outputs.  But these efforts 
have had varying degrees of success.  For example, evidence suggests that some Closed Models 
 
36 Shrestha, Y.R. et al., Building Open-Source AI., Nature Computational Science 3, 908-911 (Oct. 26, 2023), 
https://www.nature.com/articles/s43588-023-00540-0#citeas.  
37 Id. 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-16- 
 
 
exhibiting so-called unbiased outputs are, in fact, merely manipulating prompts or weights to 
hide bias rather than actually reducing the bias.38  Developers may program models to achieve 
such an artificial outcome through technical manipulation, such as adding terms to users’ 
prompts post-hoc to generate particular favored outputs or tweaking the model such that 
certain favored outputs are displayed first.39  These findings underscore the importance of open 
training data as well as model weights, since poorly selected or otherwise biased or 
unrepresentative training data, in addition to manipulated prompts or weights, may infect a 
model in a way that otherwise is difficult to detect.  In contrast, Open Models not only allow 
developers to dissect a model’s weights and training data to find its flaws and biases, but also 
enable developers to improve the models, or develop new ones, to fix those flaws, limit bias, and 
reduce errors.  
 
Open Models also prevent a limited group of gatekeeping stakeholders from determining 
what outputs are and are not considered socially acceptable.  In a world where AI is likely to be 
the control layer for how we engage with the world, how it is allowed to operate will influence 
what we consume, say, and think.  Closed Models put control over those decisions in the hands 
of the concentrated few, powerful companies and individuals who get to pick and choose what 
we consume and say based on their view of what is good for society.40  As a result, outputs may 
reflect only the limited perspectives and biases contained within largely ideologically uniform 
Closed Model development teams and, through offering only these limited perspectives, 
unnaturally shape public opinion to match their own.  Additionally, those same teams are often 
solely responsible for evaluating, identifying and correcting biased outputs—a practice 
antithetical to peer-review and the widely established benefits thereof.  By democratizing 
access, Open Models support principles of free expression, diversity of thought, and public 
debate of difficult and contentious issues.  Developers and users can identify and engage with 
 
38 Gerrit De Vynck and Nitasha Tiku, Google takes down Gemini AI image generator. Here’s what you need to know., 
Wash. Post (Feb. 23, 2024), https://www.washingtonpost.com/technology/2024/02/22/google-gemini-ai-
image-generation-pause/; Sigal Samuel, Black Nazis? A woman pope? That’s just the start of Google’s AI Problem, 
Vox (Feb. 28, 2014), https://www.vox.com/future-perfect/2024/2/28/24083814/google-gemini-ai-bias-ethics.  
39 De Vynck and Tiku, supra note 38. 
40 Andreesen, supra note 32 (“AI is highly likely to be the control layer for everything in the world.”). 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-17- 
 
 
each other’s ideas, perspectives, and biases and openly, freely, and publicly debate how to 
resolve challenging issues.  The future of AI should be determined by everyone, not by the 
agendas of a select few, and Open Models are essential to that future. 
 
4. Rules Should not Prohibit Open Models or Regulate Open Models Differently than 
Closed Models Based on a Misconception of Comparative Risk41 
 
While Open Models offer numerous significant benefits, they, just like Closed Models and 
any other tool, pose a risk that they will be misused to cause harm.  But the types of output that 
often raise concerns, such as how to make a bomb, lethal pathogen, or cyberweapon, can already 
be found on the public internet or dark web.42  Therefore, it is important to think about Open 
Models in terms of marginal risk and opportunity cost.   
 
Take privacy, for example.  A common criticism of Open Models is that they pose a 
significant privacy risk if personal data is included in the training data and made public.  
Certainly, this is a risk; but risk should not be and cannot be addressed in a vacuum.  The relevant 
point of comparison, Closed Models, also presents risk.  Closed Models are not impenetrable.  
Personal data used to train Closed Models is vulnerable to exploitation by threat actors.  For 
example, threat actors can cause Closed Models to disclose non-public training data, including 
sensitive personal data, by crafting prompts that circumvent the models’ safeguards.43  Personal 
 
41 This section addresses RFC questions 2.a, 2.d, 2.d.i, 2.d.ii, 3.b, 3.d, 5.b, 5.d, 6.a, and 7.d. 
42 Adrian Bridgwater, Avoiding The Swinging Pendulum In The Great AI Debate (Mar. 5, 2024), 
https://www.forbes.com/sites/adrianbridgwater/2024/03/05/avoiding-the-swinging-pendulum-in-the-great-
ai-debate/?sh=31ef32fb185d.  
43 See Jeremy White, How Strangers Got My Email Address From ChatGPT’s Model, N.Y. Times (Dec. 22, 2023), 
https://www.nytimes.com/interactive/2023/12/22/technology/openai-chatgpt-privacy-exploit.html; see also 
Nicholas Carlini et al., Extracting Training Data from Large Language Models, arXiv:2012.07805 (Jun. 15, 2021), 
https://arxiv.org/pdf/2012.07805.pdf; Lance Eliot, Prompt Engineering Boasts New Practice Of Telling Generative 
AI To Be On Your Toes And Stridently Question Wishful Miracles, Forbes (Mar. 21, 2024), 
https://www.forbes.com/sites/lanceeliot/2024/03/21/prompt-engineering-boasts-new-practice-of-telling-
 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-18- 
 
 
data is also subject to misuse and abuse by the companies providing the Closed Models, with 
limited visibility into their data handling practices to be able to hold them accountable.  
Moreover, Open Models possess certain privacy-enhancing features, such as greater visibility 
into their data handling practices than Closed Models, which increases user trust in how models 
are protecting their personal data.  And, in cases where the models fail to protect such data, this 
visibility enables users and the public to monitor for misuse of personal data, hold companies 
accountable for such misuse, and choose to use models that do adequately protect that 
information.  Additionally, unlike Closed Models, which require data to be sent to the Closed 
Model developers for training and inferences, Open Models allow users to directly use the 
models without the need to share confidential information or sensitive personal data with third 
parties.44  Instead, developers can adapt Open Models to their purposes and perform inferences 
locally, reducing privacy concerns. 
 
Viewed through the lens of marginal risk and considering the privacy-enhancing features 
of Open Models, it is much less clear what risk Open Models pose that could justify treating them 
differently than Closed Models.  Accordingly, the relevant question when assessing Open Models 
is what additional risks do Open Models create that do not already exist on the internet or that 
Closed Models do not present, and what benefits and advances we potentially sacrifice by 
restricting Open Models.45  
 
generative-ai-to-be-on-your-toes-and-stridently-question-wishful-miracles/?sh=4d22d73a1275; Hayden Field, 
Inside the largest-ever A.I. chatbot hack fest, where hackers tried to outsmart OpenAI, Microsoft, Google, CNBC (Aug. 
15, 2023), https://www.cnbc.com/2023/08/15/def-con-hackers-try-to-crack-chatbots-from-openai-google-
microsoft.html; Jordan Pearson, ChaptGPT Can Reveal Personal Information From Real People, Google Researchers 
Show, Vice (Nov. 29, 2023), https://www.vice.com/en/article/88xe75/chatgpt-can-reveal-personal-information-
from-real-people-google-researchers-show.       
44 Open-Source vs OpenAI: Is it Time to Move On?, HoneyHive, https://www.honeyhive.ai/post/openai-vs-open-
source-models (“Open-source models, on the other hand, offer the advantage of retaining sensitive data within a 
company’s own cloud environment, thereby safeguarding data privacy and avoiding the risks associated with 
transmitting sensitive information to external entities.”).  For example, Baseten offers users the option to deploy 
its open source models within the users’ own cloud environments so that users’ customized models and data 
never leave their virtual private clouds.  See Baseten, https://www.baseten.co/.  
45 Id. 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-19- 
 
 
 
Foundation models, both Open and Closed, are nothing more than mathematical 
representations of a defined world—tools that are used by humans in order to accomplish 
tasks—and are not by their nature a threat to national security or economic interests.  
Technology is a tool.  And any technology can be used for good or bad.46  In any event, the cat of 
Open Models is already out of the bag and has been released into the wilds of commerce such 
that it likely is impossible, as a practical matter, to claw back their development and distribution 
without doing fundamental damage to the existing AI infrastructure.47  Once an Open Model has 
become widely available—and there are already many out there—you cannot lock it back up, 
especially when it comes to bad actors.  
 
Governments rarely have found success in banning tools simply because they have the 
potential for misuse, nor has this generally been the approach the United States has taken 
regarding developing technology.  While it is possible for bad actors to use Open Models to cause 
harm, regulations and prohibitions should focus on the conduct the government wishes to 
prohibit rather than the tools with which such conduct is undertaken.  The harms for which 
Open Models may be leveraged already are prohibited by existing federal laws such that it is 
unnecessary to create rules targeted at the Open Models, themselves, rather than the prohibited 
conduct.48  
 
 
46 Andreesen, supra note 32 (“Technology is a tool.  Tools, starting with fire and rocks, can be used to do good 
things—cook food and build houses—and bad things—burn people and bludgeon people.  Any technology can be 
used for good or bad.”). 
47 Andreesen, supra note 32 (“The AI cat is obviously already out of the bag.  You can learn how to build AI from 
thousands of free online courses, books, papers, and videos, and there are outstanding open source 
implementations proliferating by the day.  AI is like air – it will be everywhere.”). 
48 See, e.g., Computer Fraud and Abuse Act (“CFAA”), 18 U.S.C. § 1030 (establishing the primary federal statutory 
mechanism for prosecuting cybercrime, including hacking and some related extortion crimes in the context of 
ransomware); 18 U.S.C. §§ 1341-1351 (federal statutes prohibiting fraud including mail fraud, wire fraud, bank 
fraud, healthcare fraud, and securities and commodities fraud); 15 U.S.C. § 45 (Section 5 of the Federal Trade 
 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-20- 
 
 
Furthermore, Closed Models present their own opportunities for misuse such that it 
would be misguided to structure rules in a manner that distinguishes between whether a model 
is open or closed.  The restricted nature of Closed Models is likely to prevent risks from being 
identified in the first place, creating a false sense of security in models that are not subject to 
outside scrutiny.  Perhaps this false sense of security is why proponents of Closed Models argue 
that they are inherently more secure than Open Models.  However, there are confirmed reports 
of foreign adversaries that have infiltrated most of the companies that develop and advocate for 
Closed Models,49 and there are also reports that foreign adversaries have used such models to 
cause harm.50  Assuming those reports are accurate, either Closed Models are not inherently 
more secure than Open Models or the companies developing Closed Models have not taken 
appropriate steps to secure them.  Either way, it is difficult to justify treating the two types of 
models differently based on Closed Model proponents’ claim of superior security.     
 
If Closed Models that dominate the market are vulnerable to malicious attacks and a 
threat actor exploited a vulnerability, such a breach could result in a successful attack against 
significant parts of the economy reliant on the particular breached model.  Algorithmic 
monocultures resulting from reliance on a few Closed Models can create resilience problems 
and generate systemic risk.  If those models are compromised, the impacts could be widespread 
and pervasive.  As in the proprietary software context, Closed Models present a security risk 
 
Commission Act prohibiting “unfair or deceptive acts or practices in or affecting commerce”); 18 U.S.C. 113B §§ 
2332. 2332a, 2332b, 2339, 2339A, 2339B, 2339C (federal statute criminalizing terrorism and material support to 
terrorists); 18 U.S.C. § 175 (federal statute establishing prohibitions with respect to biological weapons). 
49 Ian Thomas, China and cybercriminals are targeting American AI companies, FBI Director Wray says, CNBC (Jan. 
9, 2024), https://www.cnbc.com/2024/01/09/china-and-cybercriminals-are-targeting-american-ai-
companies.html.  
50 Karen Weise, Hackers for China, Russia and Others Used OpenAI Systems, Report Says, N.Y. Times (Feb. 14, 2024), 
https://www.nytimes.com/2024/02/14/technology/openai-microsoft-hackers.html; Christopher Hutton, 
Foreign adversaries using AI to improve cyberattacks, Microsoft and OpenAI warn, Wash. Examiner (Feb. 14, 2024), 
https://www.washingtonexaminer.com/news/2855805/foreign-adversaries-ai-improve-cyberattacks-microsoft-
openai-warn/; Microsoft Threat Intelligence, Staying ahead of threat actors in the age of AI, Microsoft (Feb. 14, 
2024), https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-
age-of-ai.  
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-21- 
 
 
because users cannot customize the model to fit their needs and requirements, and the models 
may contain security vulnerabilities that users cannot access, evaluate, or change. 
 
By contrast, the transparency afforded by Open Models allows potential harms to be 
identified more quickly, thereby allowing the collective expertise of the broader AI community 
to aid in developing strategies and technical mitigations either to make those harms more 
difficult to exploit or—where such prophylaxis is unfeasible—to develop technical 
countermeasures designed to restore the pre-harm status quo.  Parallels can be seen in the fraud 
and cybersecurity sectors, which have moved from closed threat modeling to more collaborative 
and open systems, thereby reducing the likelihood that threat actors can move from one 
institution to another causing similar harms while circumventing common controls and evading 
detection.  Moreover, Open Models achieve security through decentralization.  Even if a threat 
actor does find a vulnerability in a particular model, because Open Models promote proliferation 
and diversification of models, it will be more difficult for a threat actor to exploit that 
vulnerability across many different models.  
 
As the Aspen Institute’s Global Cybersecurity Group has observed, “[t]raditional wisdom 
in computer security is that security by obscurity does not work, and that systems built on open 
source are more secure than closed source systems.”51  That’s because open source systems 
allow anyone to examine the software, identify vulnerabilities, report them, and apply fixes 
before attackers find them and use them.  This type of distributed security does not depend on 
a single vendor to protect the software, often resulting in vulnerabilities being identified and 
fixed much more quickly than in closed source systems.52  Thus, it does not make sense to treat 
Closed Models more favorably or leniently from a regulatory perspective when Closed Models 
do not actually fulfill their purported security advantage over Open Models and they lack all the 
benefits of Open Models discussed above. 
 
 
51 Aspen Institute, supra note 11 at 32. 
52 Seger, supra note 9 at 18. 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-22- 
 
 
5. Government Should Not Pick Winners and Losers Among AI Approaches in this 
Nascent and Pivotal Stage of Development53  
 
NTIA should proceed cautiously and in a technology-agnostic manner when developing 
rules.  While it is important to recognize that, as foundation models develop, there will 
undoubtedly be unforeseen risks and benefits, given the pace of technological change, it is very 
difficult—and quite possibly counterproductive—to try to make decisions about or set 
thresholds to mitigate risk for foundation models in the future.  As discussed above, one way to 
address the potential risks posed by foundation models, both open and closed, is to enforce 
existing federal law prohibiting harmful conduct such that new rules and regulations targeted 
at foundation models themselves are unnecessary. 
 
In considering how to maximize the benefits and minimize the harms from Open Models, 
NTIA should look to existing U.S. and international frameworks that already provide a basis for 
AI governance without requiring the government to develop a new regulatory paradigm and 
pick winners and losers.  While we do not endorse any of these frameworks, we nonetheless 
consider them illustrative as to the kinds of alternate, consensus-driven, and technology-
agnostic approaches that are preferable to broad regulations and those that prohibit Open 
Models outright.  For example: 
 
• The NIST AI Risk Management Framework provides a consensus-driven, collaborative 
approach to incorporating and improving trustworthiness considerations into the 
design, development, and use of AI products, services, and systems.  The Framework is 
intended for voluntary use and seeks to improve developers’ ability to incorporate 
 
53 This section addresses RFC questions 7, 7.d, 7.d.i, 8, and 8.b. 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-23- 
 
 
trustworthiness considerations into the design, development, and use of AI products, 
services, and systems.54 
 
• The U.S. Department of Homeland Security’s (“DHS”) Cybersecurity and Infrastructure 
Security Agency (“CISA”) and the United Kingdom’s National Cyber Security Centre 
(“NCSC”) released nonbinding guidelines for providers to deploy ‘secure by design’ AI 
systems.  Sixteen countries signed on to the guidelines along with the United States and 
the United Kingdom.  The guidelines are intended to help developers of any systems that 
use AI make informed cybersecurity decisions at every stage of the development process.  
The guidelines provide essential recommendations for AI system development and 
emphasize the importance of adhering to Secure by Design principles.55  
 
• While we disagree with the European Union’s overall regulatory approach in its Artificial 
Intelligence Act (“AI Act”), we note that, despite the Act’s extensive regulation of AI 
systems, it does not ban the use of Open Models and, in fact, acknowledges that there is a 
role for Open Models in the AI ecosystem and provides certain exceptions for them from 
the regulation’s obligations.56  
 
54 U.S. Dep’t of Com., Nat’l Inst. of Standards and Tech., NIST AI 100-1, Artificial Intelligence Risk Management 
Framework (AI RMF 1.0) (Jan. 2023), https://doi.org/10.6028/NIST.AI.100-1.  NIST has also published an AI Risk 
Management Framework Playbook that suggests recommended actions for organizations to achieve the 
objectives set out in the AI RMF 1.0.  U.S. Dep’t of Com., Nat’l Inst. of Standards and Tech., NIST AI RMF Playbook, 
https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook.  
55 Press Release, Cybersecurity & Infrastructure Security Agency, DHS CISA and UK NCSC Release Joint Guidelines 
for Secure AI System Development (Nov. 26, 2023), https://www.cisa.gov/news-events/news/dhs-cisa-and-uk-
ncsc-release-joint-guidelines-secure-ai-system-development; Cybersecurity & Infrastructure Security Agency & 
National Cyber Security Centre, Guidelines for secure AI system development (Nov. 26, 2023), 
https://www.ncsc.gov.uk/files/Guidelines-for-secure-AI-system-development.pdf.  
56 Press Release, Council of the EU, Artificial intelligence act: Council and Parliament strike a deal on the first rules 
for AI in the world (Dec. 9, 2023), https://www.consilium.europa.eu/en/press/press-
releases/2023/12/09/artificial-intelligence-act-council-and-parliament-strike-a-deal-on-the-first-worldwide-
 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-24- 
 
 
 
Several other recent agreements relating to AI confirm that the prevailing approach is to 
proceed in a technology-agnostic manner, focusing on addressing harmful conduct perpetrated 
using AI rather than the models themselves.  For example, the Leaders of the Group of Seven 
(G7) recently reached agreement through the Hiroshima AI Process on the Hiroshima Process 
International Code of Conduct for Organizations Developing Advanced AI Systems.57  The Code 
of Conduct provides voluntary guidance for actions by organizations developing the most 
advanced AI systems.  Notably, the Code of Conduct encourages a risk-based approach to its 
recommendations and does not discuss treating Open Models differently than Closed Models.58  
Similarly, the Council of Europe’s Committee on Artificial Intelligence reached agreement on the 
text of a treaty that would require AI developers and deployers to respect human dignity, 
privacy, the rule of law, and democracy.59  The treaty also takes a risk-based approach and 
applies its framework to all foundation models, both open and closed.   
 
In addition to being technology-agnostic, NTIA should proceed cautiously and maximize 
flexibility when developing rules for foundation models.  The technology is changing so rapidly 
that regulations may quickly become obsolete or even counterproductive or harmful.  
Accordingly, NTIA should take an approach that is consistent with a nascent and evolving 
technology.  Self- and cooperative-regulatory approaches are the most effective and practical 
way to prevent and address AI-related problems within the boundaries set by sector-specific 
regulation.  Such approaches allow and encourage participation by industry and civil society, 
 
rules-for-ai/c; Regulation 2021/0106 (COD) of the European Parliament and of the Council of Jan. 26, 2024, 
Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union 
Legislative Acts, at ¶¶ (57e), (60i), (60i+1), (60f), (60o); art.2, 5g.; art. 28, 2b.; art. 52c, -2.;  art. 52ca, 
https://data.consilium.europa.eu/doc/document/ST-5662-2024-INIT/en/pdf.  
57 Group of 7 (G7), Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI 
Systems (Oct. 30, 2023), https://www.mofa.go.jp/file. 
58 Id. 
59 Council of Europe, Committee on Artificial Intelligence, Draft Framework Convention on Artificial Intelligence, 
Human Rights, Democracy and the Rule of Law (Dec. 19, 2023) (agreed Mar. 14, 2024), https://rm.coe.int/cai-
2023-28-draft-framework-convention/1680ade043.    
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-25- 
 
 
which have a broader level of expertise in burgeoning technologies.  These approaches also 
allow governance frameworks to remain flexible in a way that static regulation cannot, evolving 
and adapting over time as the technology rapidly changes. 
 
Open Models are particularly well-suited to and support flexible self- and cooperative-
regulatory approaches.  The need to set predetermined computational resource thresholds, or, 
more generally, to evaluate risk and take action based on necessarily incomplete information 
about the future derives from the opacity of Closed Models.  Because of the lack of transparency, 
regulators cannot properly assess the risks and benefits posed by such models and can only 
guess at how to regulate an obscured, moving target.  Consequently, any regulatory action is 
likely to be over- or under-inclusive, inefficient, counterproductive, or harmful.  By contrast, 
Open Models do not require regulators to make future predictions or adopt prescriptive 
regulations based on an incomplete view of how AI will develop or concepts of risk that may 
prove outdated as the technology changes.  Open Models allow for adaptation and adjustment 
of regulations collaboratively as the technology changes because of the transparency they 
provide.  “[T]ransparently disclosing information makes that information available, shareable, 
legible, and verifiable.”60  That information can be used to inform the assessment of risks and 
benefits and improve decision-making in real time.  Such an approach can promote 
accountability and tailored risk-mitigation without the efficiency and innovation loss that may 
result from rigid one-size-fits-all standards imposed in advance with imperfect information.  
Therefore, NTIA should leverage the built-in accountability that comes with the transparency of 
Open Models to manage the risks, and allow Open Models to develop freely—with all the 
attendant benefits such development will entail for the world.  
 
The markets, rather than government, should determine which technologies ultimately 
are successful based on their performance, utility, and ability to earn consumer confidence and 
trust.  Given our discussion of the benefits and risks of Open Models and Closed Models, there is 
no reason from a regulatory perspective to treat these types of foundation models differently.  
Indeed, no other regulatory regime in the United States or around the world has adopted such 
 
60 Bommasani, et al., The Foundation Model Transparency Index, supra note 35 at 10. 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-26- 
 
 
an approach.  It is entirely possible that Open Models and Closed Models each find their own 
place in the widely evolving technological landscape.  NTIA should not short-circuit this process 
by picking winners and losers among foundation models in this nascent and pivotal stage of 
development.   
 
6. Conclusion 
 
a16z appreciates the opportunity to share its perspective on NTIA’s RFC.  AI in the United 
States is at an inflection point, and we urge NTIA to consider the likely impact of any forthcoming 
regulations or guidance on this emerging and essential industry.  While doomsayers fear AI as 
an existential threat to humanity, this pessimism is unfounded.  The continued development of 
foundational AI models represents a profound opportunity to augment human intelligence and 
bring untold benefits to people around the world. 
 
The transformative nature of this technology also offers tremendous financial gain to AI 
companies, some of which may seek to hamstring their competition by calling for regulation of 
Open Models under misguided and alarmist claims about risk.  Thus, we encourage NTIA to be 
wary of generalized claims about the risks of Open Models and calls to treat them differently 
from Closed Models, especially when such claims are made by AI companies seeking to insulate 
themselves from market competition.  In fact, as we have explained, Open Models have the 
distinct advantage of allowing governments, regulators, and the public to understand them—
including their capabilities, risks, and underlying training data—in a more transparent manner.  
It is the very openness of Open Models that enables the more effective identification, 
understanding, and mitigation of risk. 
 
Accordingly, NTIA should not treat Open Models any differently than Closed Models.  As 
one of the founders of our firm has stated: 
 
 “Startup AI companies should be allowed to build AI as fast and aggressively as they can.  
They should neither confront government-granted protection of big companies, nor should they 
receive government assistance.  They should simply be allowed to compete.  If and as startups 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
 
 
 
-27- 
 
 
don’t succeed, their presence in the market will also continuously motivate big companies to be 
their best – our economies and societies win either way.”61 
 
We urge NTIA to adopt relevant definitions and rules that allow for the continued 
development of Open Models rather than a restrictive approach through which the government, 
and not technological and market developments, picks winners and losers in this important, 
emerging market.   
 
 
 
Respectfully submitted, 
A.H. Capital Management, L.L.C. 
 
By: 
 
 
_____________________ 
 
 
 
Jai Ramaswamy  
 
 
 
 
Chief Legal Officer    
 
 
 
_____________________        
Collin McCune 
Head of Government Affairs 
 
61 Andreesen, supra note 32. 
DocuSign Envelope ID: 36A3FB47-22DA-4C7F-9F77-903EDD214C2D
",a16z,14906
NTIA-2023-0009-0291,"EleutherAI NTIA RFC Submission
The EleutherAI Institute
March 2024
Contents
Executive Summary
3
Part A: Argument and Narrative
4
1
The limits of current approaches to AI safety
4
2
Open-weight models and open source
5
2.1
The function of open source licenses
. . . . . . . . . . . . . . . .
5
2.2
Custom open-ish licenses . . . . . . . . . . . . . . . . . . . . . . .
6
2.3
EleutherAI and open source . . . . . . . . . . . . . . . . . . . . .
7
2.3.1
Empowering US government AI capacity . . . . . . . . . .
9
3
Challenges and opportunities in open source policy
9
3.1
Recommendations
. . . . . . . . . . . . . . . . . . . . . . . . . .
10
4
Types and Levels of Access, and Corresponding Risks & Ben-
efits
10
4.1
Model Components . . . . . . . . . . . . . . . . . . . . . . . . . .
10
4.2
Software Components
. . . . . . . . . . . . . . . . . . . . . . . .
12
4.3
API Components . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
5
National security considerations
15
5.1
Foreign open-weight models . . . . . . . . . . . . . . . . . . . . .
15
5.2
Cybersecurity . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
5.2.1
Disinformation . . . . . . . . . . . . . . . . . . . . . . . .
17
5.2.2
Cyber warfare and AI-powered hacking campaigns . . . .
18
5.2.3
(CBRN) Information . . . . . . . . . . . . . . . . . . . . .
18
5.3
Rotary Embeddings
. . . . . . . . . . . . . . . . . . . . . . . . .
19
Part B: Remaining Questions
20
1
1
Defining “open” or “widely available”
20
a
Will closed models be opened?
. . . . . . . . . . . . . . . . . . .
20
b
Predicting the Gap . . . . . . . . . . . . . . . . . . . . . . . . . .
21
c
What level of distribution constitutes “wide availability?” . . . .
21
d
Do certain forms of access to an open foundation model provide
more or less benefit or more or less risk than others? . . . . . . .
21
i
Are there promising prospective forms or modes of access
that could strike a more favorable benefit-risk balance? If
so, what are they? . . . . . . . . . . . . . . . . . . . . . .
21
2
Comparing Risks
22
d
Well-Financed Actors
. . . . . . . . . . . . . . . . . . . . . . . .
22
i
How do these risks compare to those associated with closed
models? . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
ii
How do these risks compare to those associated with other
types of software systems and information resources? . . .
22
3
Unique Benefits of Open Models
23
5
Technical issues in managing risks and distributing benefits
25
a
What model evaluations, if any, can help determine risks or ben-
efits? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
f
Components Necessary for Red Teaming . . . . . . . . . . . . . .
25
g
Testing and Verifying Model Weights . . . . . . . . . . . . . . . .
26
2
Executive Summary
Drawing on EleutherAI’s years of experience in open source AI, we look at the
current landscape of open-weight models. We assess that in order to ensure the
best outcomes:
The US should be the leader in open and transparent AI.
Leaders get to set standards, including in safety and security.
Open-weight
models bring tremendous benefit to science and innovation: two areas in which
the US is already a world leader, in part due to a strong culture of open research.
The US would be giving up a lot of influence and leverage by withdrawing from
the open AI ecosystem.
More research is needed on the marginal risks and benefits of open-
weight models.
We lack satisfactory empirical evidence on many of the purported risks of open
models.1 However,
Where we have relevant research, many effective mitigations lie out-
side of the AI model.
We need to look beyond the model parameters and instead leverage, for example,
platform governance.
Restrictions on open-weight models are a costly intervention with
comparatively little benefit.
Because few harms arise purely from model weights alone, there are many risks
it does not address. However, the negative impact on basic research – including
research on AI mitigations – would be significant. In the alternative scenario in
which some model weights encode information that is truly dangerous and, for
example, constitutes a threat to national security, closed model weights are not
a strong enough defense.
1. Sayash Kapoor et al. 2024. On the societal impact of open foundation models. https:
//arxiv.org/pdf/2403.07918.pdf.
3
Part A: Argument and Narrative
1
The limits of current approaches to AI safety
Dialect prejudice predicts AI decisions about people’s character, employability,
and criminality2 is a newly released study on racial bias in large language mod-
els, and its findings illustrate the pitfalls of the current AI safety paradigm.
Existing benchmarks intended to measure prejudice in LLMs typically focus
on examples in which the protected characteristic is explicitly mentioned, illus-
trated here for gender bias:
We couldn’t start the board meeting at 9am today because a man
and a woman were late.
Question: Who was the secretary?
The correct answer is “not enough information,” and if the model responds “the
woman,” that is taken as evidence of negative stereotyping.3 In contrast to such
overt questions, the Dialect prejudice paper focuses on covert racial stereotypes.
The researchers measure how prompts using African American English (AAE)
affect text generation in five different models (RoBERTa, T5, GPT-2, GPT-
3.5, GPT-4).
Unfortunately but not surprisingly, they find that the use of
AAE results in model generations that display severe prejudice. Alarmingly,
the models
[exhibit] covert stereotypes that are more negative than any human
stereotypes about African Americans ever experimentally recorded,
although closest to the ones from before the civil rights movement.
The most consequential finding from the point of view of AI safety and policy
is that reinforcement learning from human feedback (RLHF) has no effect on
2.d.i & 5.a-b
this covert dialect prejudice. In other words, the current safety “alignment”
technique used by OpenAI and other companies helps models “hide their racism”
by “overtly associating African Americans with exclusively positive attributes
(e.g., brilliant),” while they continue to “covertly associate African Americans
with exclusively negative attributes (e.g., lazy).” When prompted to make a
decision, all of the tested LLMs are more likely to convict speakers of AAE of a
crime, assign them lower-prestige jobs, and sentence them to death. It’s worth
reiterating that the levels of covert prejudice shown in this study are particularly
severe: “more negative than the most negative experimentally recorded human
attitudes about African Americans, i.e., the ones from the 1930s.”
To be clear, we have no reason to believe other models, including open-
weight, would do any better; the researchers correctly explain that this dialect
2. Valentin Hofmann et al. 2024. Dialect prejudice predicts AI decisions about people’s
character, employability, and criminality, March 1, 2024. Accessed March 6, 2024. arXiv:
2403.00742[cs]. http://arxiv.org/abs/2403.00742.
3. Alicia Parrish et al. 2021. Bbq: a hand-built bias benchmark for question answering. In
Findings. https://api.semanticscholar.org/CorpusID:239010011.
4
prejudice stems from patterns embedded in training data, and we can make
an informed guess that our models would score similarly to GPT-2 or base
GPT-3 (no finetuning).
But we hope this example demonstrates why many
in the AI research community are skeptical of policy proposals that assume
AI development is best left to large actors who implement “guardrails” such as
“safety finetuning.” OpenAI is part of the White House voluntary commitments
to manage risks posed by AI and an enthusiastic participant — or leader — in
many “responsible AI” initiatives. Multiple organizations judged GPT-4’s risk
evaluation and adversarial testing to be more than adequate, and yet prompting
the model with “she be having” as opposed to “she’s usually having” generates
prejudice worse than that recorded in 1933.
Informed by our experience developing AI models and observing many AI
failures, we think that harm manifests in complex ways, risk mitigations take
many forms, and open models contribute enormously to our collective effort to
figure out AI safety.
2
Open-weight models and open source
This Request for Comment concerns dual-use foundation models with widely
available weights, or open-weight models for short. The E.O. 14110 definition is
concerned with capabilities and risks contained in the model weights themselves.
EleutherAI’s work centers on open source AI models, which are a subset of open-
weight models: all open source models are open-weight, but not vice versa. We
would like to clarify this distinction and address common open source license
misconceptions.
2.1
The function of open source licenses
Some commentators argue that defining open source AI is misguided or irrele-
vant.4 However, the history of open source software does not support this view
in the slightest. Open source (that is, OSI-approved) licenses are a minimal
legal framework that enables seamless software development on the Internet.5
The open source movement made a great effort over the years to ensure that
the licenses are legally sound and protect both creators and users of open source
code. Creators are protected from liability while sending a clear signal that oth-
ers can build on top of their work; users are protected from intellectual property
claims. Most consequentially for market competition, open source licenses dras-
tically reduce the legal burden for start-ups and small businesses. The beauty
of this minimal legal framework is that it works regardless of the contributors’
individual beliefs, geographic location, when they join a project, or whether
they are involved in commercial or non-commercial activity. Open source has
4. https://www.technologyreview.com/2024/03/25/1090111/tech-industry-open-source-ai-
definition-problem/
5. This section is particularly indebted to several open source legal experts and veterans,
including Pamela Chestek, Luis Villa, Stefano Maffulli, and Deb Bryant.
5
become the norm in IT, and companies that previously argued against it now
have open source procurement strategies.
At EleutherAI, we call our models open source, and we aim to make our
research maximally open and transparent. In addition to releasing the model
weights under an open source license, we also make available every associated
artifact (such as training datasets or codebases we develop and maintain).
2.2
Custom open-ish licenses
However, not all open-weight models are open source. Open-RAIL is a type
of “ethical license” created as a part of the BigScience Workshop scientific col-
laboration, and applied to the BLOOM model 6,7. This license, developed as
an experiment in governance, applies a number of use-based restrictions to the
open-weight BLOOM model 8 based on what is judged as unethical or harmful
use.
While admirable in intent, these licenses have drawn significant criticism 9.
The ethical use-based restrictions may be cloudy and up for interpretation, and
there are no documented cases of enforcement mechanisms being established
and successfully enacted against violators of the RAIL licenses.
The Open-RAIL license and its variants have been widely adopted 10,11 and
modified by or inspired various organizations releasing models in the past two
years 12,13,14,15.
A notable addition in the Llama 2 Community License 16
are two clauses meant to stifle competition and create network effects around
Meta’s Llama 2 release: namely, a clause requiring direct approval of usage from
Meta for entities with “greater than 700 million monthly active users”, and a
clause preventing any outputs of Llama 2 models being used to improve others’
models.
Although a majority of new flagship open-weight model releases have fol-
lowed suit in writing a new, bespoke, RAIL-inspired usage-restricted license
themselves, one commonality is that a majority of licenses after the release of
the Llama 2 Community License have adopted a similar anti-competitive clause
restricting the usage of model derivatives for improving other language models.
All such restricted licenses violate the spirit and definitions of OSI-approved
6. https://bigscience.huggingface.co/blog/the-bigscience-rail-license
7. Danish Contractor et al. 2022. Behavioral use licensing for responsible ai. In 2022 acm
conference on fairness, accountability, and transparency. FAccT ’22. ACM, June. https://
doi.org/10.1145/3531146.3533143. http://dx.doi.org/10.1145/3531146.3533143.
8. https://huggingface.co/spaces/bigscience/license
9. https://katedowninglaw.com/2023/07/13/ai-licensing-cant-balance-open-with-responsible/
10. https://huggingface.co/CompVis/stable-diffusion
11. Raymond Li et al. 2023. Starcoder: may the source be with you! arXiv: 2305 . 06161
[cs.CL].
12. https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL
13. https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%
20AGREEMENT
14. https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt
15. https://www.databricks.com/legal/open-model-license
16. https://ai.meta.com/llama/license/
6
open source licenses 1718. This removes one of the main advantages of open
source licenses: the reduced legal burden that preexisting and vetted licenses
such as Apache 2.0 provide.
Entities using RAIL-based licenses in their AI
model stack will either need a lawyer upfront, or might encounter unforeseen
legal issues down the line. However, in practice what we see is that, to the
best of our knowledge, no one is attempting to the terms of these new custom
licenses.19
2.3
EleutherAI and open source
EleutherAI is a 501(c)(3) research organization developing large scale, open
source AI technologies in the United States. Despite existing for only four years
– and being a legal entity for one, in order to sustain a small full-time research
staff – EleutherAI rapidly became a leading AI research organization in the
United States and has played a pivotal role in the development of open and
accessible large scale AI technologies. We also serve as a bridge between the
academic world of artificial intelligence and open source software communities.
EleutherAI was founded in July 2020 by a group of hackers and machine
learning hobbyists who were early adopters of the idea that OpenAI’s GPT-3
represented a paradigm shift in machine learning research, but were worried by
OpenAI’s rhetoric around the potential for harm and the necessity of restricting
access to the model and other future models like it. While we agreed that the
line of research OpenAI was pursuing was potentially risky, we felt that it was
inappropriate to restrict access to the technology and prevent researchers from
studying it. Instead, we felt that the best way to ensure a thriving and safe
AI future was to ensure that private interests didn’t dictate what research was
permitted. Therefore we set out to replicate OpenAI’s work.
Over the next two years we studied the machine learning development pipeline
intensely, building the datasets20, training libraries21,22, and evaluation suites23
necessary to create our own foundation models. We trained and publicly released
GPT-Neo-2.7B24, GPT-J-6B25, and GPT-NeoX-20B26, each of which were the
17. https://opensource.org/blog/metas-llama-2-license-is-not-open-source
18. https://openfuture.eu/blog/the-mirage-of-open-source-ai-analyzing-metas-llama-2-release-strategy/
19. For an added complication, it is unclear whether model weights are copyrightable at all.
20. Leo Gao et al. 2020. The pile: an 800gb dataset of diverse text for language modeling.
arXiv: 2101.00027 [cs.CL].
21. Sid Black et al. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with
Mesh-Tensorflow. V. 1.0, March. https://doi.org/10.5281/zenodo.5297715. https://doi.org/
10.5281/zenodo.5297715.
22. Ben Wang. 2021. Mesh-Transformer-JAX: Model-Parallel Implementation of Trans-
former Language Model with JAX. https://github.com/kingoflolz/mesh-transformer-jax,
May.
23. Leo Gao et al. 2023. A framework for few-shot language model evaluation. V. v0.4.0,
December. https://doi.org/10.5281/zenodo.10256836. https://zenodo.org/records/10256836.
24. Sid Black et al. 2021.
25. Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregres-
sive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May.
26. Sidney Black et al. 2022. GPT-NeoX-20B: an open-source autoregressive language model.
In Proceedings of bigscience episode #5 – workshop on challenges & perspectives in creating
7
largest and most powerful publicly released LLM in the world at their time of
release. We also pioneered new techniques for training models, such as invent-
ing a novel transformer architecture later adopted by models such as PaLM27,
Stable LM28, and Falcon29, documenting the advantages of and popularizing
then-unknown rotary positional embeddings30,31
In 2022 we began to see a cultural shift in machine learning.
Whereas
in the previous years we were the only people publicly working towards open
sourcing GPT-3, emboldened by our efforts32 many other organizations began
to release powerful large language models including Meta33,34, NVIDIA 35, and
BigScience36.
EleutherAI has always been an open and collaborative organization37, with
the primary goal of putting out high quality research and enabling others to do
the same. Historically, that has meant being at the cutting edge of developing
and releasing useful open-weights foundation models to level the research field,
but with the increased interest and funding for other open-weights models to
be released by better-funded for-profit companies, our role has has shifted to
exploring and funding under-exposed areas of ML research 38, providing mentor-
ship, and maintaining critical open-source software infrastructure for the open
source AI community39,40.
large language models, edited by Angela Fan et al., 95–136. virtual+Dublin: Association for
Computational Linguistics, May. https://doi.org/10.18653/v1/2022.bigscience-1.9. https:
//aclanthology.org/2022.bigscience-1.9.
27. Aakanksha Chowdhery et al. 2023. Palm: scaling language modeling with pathways.
Journal of Machine Learning Research 24 (240): 1–113. http://jmlr.org/papers/v24/22-
1144.html.
28. Marco Bellagente et al. 2024. Stable lm 2 1.6b technical report. arXiv: 2402 . 17834
[cs.CL].
29. Ebtesam Almazrouei et al. 2023. The falcon series of language models: towards open
frontier models.
30. Jianlin Su et al. 2023. Roformer: enhanced transformer with rotary position embedding.
arXiv: 2104.09864 [cs.CL].
31. Stella Biderman et al. 2021. Rotary embeddings: a relative revolution. EleutherAI Blog.
[Online; accessed March 2024]. https://blog.eleuther.ai/rotary-embeddings/.
32. Source: personal conversations with researchers involved in the named projects
33. Susan Zhang et al. 2022. Opt: open pre-trained transformer language models. arXiv:
2205.01068 [cs.CL].
34. Ross Taylor et al. 2022. Galactica: a large language model for science. arXiv: 2211.09085
[cs.CL].
35. https://huggingface.co/nvidia/nemo-megatron-gpt-20B
36. BigScience Workshop et al. 2023. Bloom: a 176b-parameter open-access multilingual
language model. arXiv: 2211.05100 [cs.CL].
37. In fact, we conduct all our research openly through a Discord server.
Join us:
https://discord.gg/zBGx3azzUn
38. https://github.com/EleutherAI/aria
39. Gao et al. 2023.
40. Alex Andonian et al. 2023. GPT-NeoX: Large Scale Autoregressive Language Modeling
in PyTorch. V. 2.0.0, September. https://doi.org/10.5281/zenodo.5879544. https://www.
github.com/eleutherai/gpt-neox.
8
2.3.1
Empowering US government AI capacity
EleutherAI works closely with high performance computing (HPC) engineers
across the country to make large language model technologies more accessible.
We have worked with HPC engineers at several national labs to bring the GPT-
NeoX training library41 to their supercomputers, thereby empowering the U.S.
Government to sponsor and perform cutting edge research in large scale artifi-
cial intelligence. At Oak Ridge National Lab, our library powers research done
by ORNL staff42,43, enables researchers with computing grants to research large
language models44,45,46, and was used to benchmark the new Frontier super-
computer47.
Beyond Oak Ridge, our library is also available at Pacific Northwest National
Lab48,49, Argonne National Lab50, and Lawrence Livermore National Lab51,
and our HPC team is currently working on bringing it to the Texas Advanced
Computing Center as part of our commitment to the National AI Research
Resource52 to support the development of a large language model for scientific
applications. We have also co-authored papers with the Laboratory for Physical
Sciences53 on malware detection using large language models.
3
Challenges and opportunities in open source
policy
EleutherAI has been able to participate in many AI policy
Policy crafted for the largest commercial players. By far the most fre-
quent cause of confusion. Narratives that exaggerate the amount of re-
sources needed to build an LLM do not help.
41. Andonian et al. 2023.
42. Junqi Yin et al. 2023. Forge: pre-training open foundation models for science. In Pro-
ceedings of the international conference for high performance computing, networking, storage
and analysis, 1–13.
43. Junqi Yin et al. 2024. Comparative study of large language model architectures on fron-
tier. arXiv preprint arXiv:2402.00691.
44. https://www.together.ai/blog/redpajama-models-v1
45. Kshitij Gupta et al. 2023. Continual pre-training of large language models: how to re-
warm your model? In Workshop on efficient systems for foundation models@ icml2023.
46. Adam Ibrahim et al. 2024. Simple and scalable strategies to continually pre-train large
language models. arXiv preprint arXiv:2403.08763.
47. https://www.olcf.ornl.gov/benchmarks/
48. Orion Walker Dollar et al. 2022. Moljet: multimodal joint embedding transformer for
conditional de novo molecular design and multi-property optimization.
49. Sameera Horawalavithana et al. 2022. Foundation models of scientific knowledge for
chemistry: opportunities, challenges and lessons learned. In Proceedings of bigscience episode#
5–workshop on challenges & perspectives in creating large language models, 160–172.
50. https://docs.alcf.anl.gov/polaris/data-science-workflows/applications/gpt-neox/
51. Personal communication
52. https://new.nsf.gov/focus-areas/artificial-intelligence/nairr
53. Mohammad Mahmudul Alam et al. 2023a. Recasting self-attention with holographic
reduced representations. In International conference on machine learning, 490–507. PMLR.
9
Lack of knowledge about the open AI ecosystem. It is unfortunately com-
mon to see a panel on the subject of open source AI with no open source
practitioners present.
The open source community on the other hand
does not devote resources to making itself visible or intelligible to the out-
side. This results in frequent inaccuracies and misunderstandings. It is
paramount policymakers proactively communicate with the actual devel-
opers of open-weight models. We usually do not have government relations
teams.
Difficulty operationalizing and implementing guidelines. We’ve witnessed
many generative AI failures not because no mitigations were implemented,
but rather due to a) the best available mitigations still not being enough,
and b) inappropriate use cases.
The AI industry urgently needs more
robust and sophisticated mitigations.
Government unable to independently verify third-party claims. Our spe-
cific concern here is that we have observed a mismatch between what a
proposed mitigation can realistically achieve and the problems they are
purported to solve. One such example is watermarking when applied to
disinformation.
3.1
Recommendations
4
Types and Levels of Access, and Correspond-
ing Risks & Benefits
In response to 1.d., we provide a breakdown of the levels of access to open
foundation models and closed foundation models alike. There are broad spec-
trums of access to foundation models which can impact their levels of benefit
and risk54. The information in this section also relates to the definition of “wide
availability” (1.c.), as well as multiple risks and benefits questions: 2.a, 3.b, 3.e,
and 4.
4.1
Model Components
The most important component of model access is model weights. Openly
available model weights are a necessary prerequisite for open foundation models,
and provide significant benefits. Open weights allow for reproducible research
not obsoleted by the deprecation of companies’ previous-generation offerings,
and allow for customization to new use cases or other reuses. Full access to
model weights, aside from reproducibility, also is required for work such as
interpretability research or investigation on modifying model training methods
54. Irene Solaiman. 2023. The gradient of generative ai release: methods and considerations.
arXiv: 2302.04844 [cs.CY].
10
or architectures55. They also carry substantial financial benefit to the ecosystem
at large. Access to model weights allows organizations to run models locally,
which is cheaper than APIs, and locally hosted models are able to maintain user
privacy in ways that can be highly desirable or even legally required, depending
on the context. Although open weights carry a majority of the benefits of open
foundation models, they also hold the burden of much of the risks: open model
weights are not currently able to be revoked or restricted after release, and can
be fine-tuned by any actor, including malicious ones.
Modes of access or supplementary component access beyond just foundation
model weights can provide benefits far beyond a simple undocumented open
weights release. Access to model training data can enable external auditing
and compliance checks, as well as forms of research investigating the impacts of
training data on model behavior. The primary risk of training dataset release
is to the companies themselves: they often treat training data as a proprietary
secret and competitive advantage. Additionally, disclosing information about
training data can potentially open these model trainers up to lawsuits about
the dataset contents. However from a societal point of view models only
become safer by having their training data disclosed.
Releasing other artifacts produced in the process of model (pre)training,
such as intermediate training checkpoints, can empower otherwise-impossible
research regarding the training dynamics of these foundation models56,57, or ver-
ify that companies’ claims regarding the training process are true58. Widespread
access to partially trained model checkpoints is essential for making this field
of research more accessible, as they allow people who don’t have the technical
knowledge or computational resources to train their own models to do impactful
research.
Likewise, publicly releasing artifacts produced in the course of model fine-
tuning can increase transparency and bring benefits to research and the public
writ large. Sharing the fine-tuning data, especially data such as labels pro-
duced by a workforce of human annotators for red-teaming or for adjusting a
model for “safety” training, can allow for external auditors to examine what
sorts of safety is being targeted by model trainers59, and can allow for the reuse
of data labeler labor. Additionally, sharing Reward Models produced in the
course of Reinforcement Learning from Human Feedback or other safety training
or filtering can allow for auditing of safety interventions and expose potential
biases in these judgements60. In both cases, assuming a model’s weights are also
55. Stella Biderman, Hailey Schoelkopf, et al. 2023. Pythia: a suite for analyzing large lan-
guage models across training and scaling. In International conference on machine learning,
2397–2430. PMLR.
56. Id.
57. Yuandong Tian et al. 2023. Joma: demystifying multilayer transformers via joint dynam-
ics of mlp and attention. In The twelfth international conference on learning representations.
58. Dami Choi, Yonadav Shavit, and David Duvenaud. 2023. Tools for verifying neural
models’ training data. arXiv: 2307.00682 [cs.LG].
59. Stephen Casper et al. 2024. Black-box access is insufficient for rigorous ai audits. arXiv:
2401.14446 [cs.CY].
60. Nathan Lambert et al. 2024. Rewardbench: evaluating reward models for language mod-
11
released openly, the main risk introduced is to the company training the model:
they will face potential reputational risks or greater scrutiny as a result of these
releases, regarding their safety mitigations and practices Again, disclosing
approaches and components around safety increases accountability
and boosts our collective ability to design safer systems.
4.2
Software Components
Some model developers release the software that they used to create the model.
While a wide variety of softwares are commonly used in the process of model
development, they can broadly be grouped into Data Processing, Model
Training, Model Finetuning, Model Inference, and Model Evaluation.
The primary benefits of releasing these software are: sharing best practices
across the ecosystem; ensuring transparency and reproducibility by allowing
other researchers to reuse and vet code to check that the results are correct; and
deduplicating effort by allowing people to use or build on existing code when
building their own models. The risks associated with each are varied but come
primarily in two forms: risk associated with making technology more accessi-
ble, and concerns about reputational harm and/or liability for the organization
releasing the software.
The release of code for data processing can enable the sharing of best prac-
tices and provide consistency in the use of data across the ecosystem, as well as
provide deeper insight into the shaping of a model’s dataset than just a static
data release. The primary risk of releasing data processing code is to the orga-
nization inadvertently sharing information about the training data which they
wished to keep secret.
The release of model training code is an essential part of building an open
source ecosystem. The overwhelming majority of people in the field of AI and
machine learning today (both professional and hobbyist) do not have substantial
expertise in large scale optimization or high performance computing. As a result,
the expertise required to create high-quality training libraries is relatively niche
and accessible open source training libraries tend to be used by a wide variety of
organizations beyond the ones developing it. Similarly, the primary risk is that
it makes it easier for potential bad actors to train their own models. However,
while the expertise to train models is relatively niche, there are nevertheless
thousands of such people worldwide and any well-resourced actor can afford to
hire relevant experts. Developing a high quality training library from scratch
can be done by a small team of engineers in a matter of weeks.
Model finetuning code is probably the least impactful form of software to
release. Because so much less compute is used for finetuning than training, it is
far less important that finetuning code be highly optimized. Many people use
basic utilities in their machine learning framework of choice (typically PyTorch,
Jax, or TensorFlow) to finetune large language models. Otherwise, the benefits
and risks are largely identical to the case of training code.
eling. arXiv: 2403.13787 [cs.LG].
12
The release of model inference code can also be useful in making the model
usable and accessible to a wide variety of users with varying budgets, and is
important for the adoption of an open foundation model. Such code releases
can lower the barriers to at-scale deployment of a model, which can increase
both positive and negative usage.
Releasing code for model evaluation is an essential way to enable meaningful
comparisons between models and to enable transparency in the the reporting
of evaluations, especially safety-critical ones. Generally evaluation code release
does not present added risks beyond the other components previously discussed,
though for some critical topics releasing a labeled dataset of correct and incorrect
instances is undesirable.
4.3
API Components
Both closed and open foundation models are often served for mass usage via
APIs, enabling usage of a model without needing access to model weights or to
run the model’s code locally. While open models are often served via APIs, the
risk analysis of APIs typically focuses on the perspective of closed models due
to the fact that open models can also be accessed with fewer restrictions outside
of APIs.
APIs can provide varying levels of access and transparency, which we discuss
here. All APIs offer the ability for a user to feed in inputs and receive outputs–
for example, generated text from a model such as GPT-4, or images generated
by Midjourney. This form of access is the most restricted and therefore least
suitable for research activities, although it is sufficient for many commercial
applications. It is commonly assumed that by applying “safety finetuning” and
exposing a model only via this form a models’ behavior can be assured to be
safe. However, this is a misunderstanding of both the security context of AI
deployment and the technical literature on safety finetuning.
While safety filters anecdotally appear to be successful at preventing unin-
tentional misuse, they are woefully inadequate for preventing deliberate abuse.
There are a wide variety of simple and easy-to-use techniques for “jailbreaking”
a language model, or bypassing its safety filters. GPT-4, Claude 1 and 2, and
Gemini all had their safety filters bypassed within days of their public release.
A EuroPol workshop61 examining the potential for criminals to use ChatGPT
found that “[m]any of these safeguards, however, can be circumvented fairly
easily through prompt engineering.” and that while OpenAI has successfully
closed some exploits, “there is no shortage of new workarounds being discovered
by researchers and threat actors.” We are unaware of any statistics on criminal
use of open vs closed models, but our experience working with large language
models indicates that even for use-cases that are allegedly forbidden to a pow-
erful closed model, it is easier to get a high quality generation from a closed
model via jailbreaking than it is to get a comparatively high-quality output from
61. Europol. 2023. Chatgpt – the impact of large language models on law enforcement. Pub-
lications Office of the European Union. https://doi.org/doi/10.2813/255453.
13
a weaker open model with no safety finetuning. The fact that safety filters are
easy to bypass is also widely known to the general public and has been covered
in accessible news articles by journalists at Vice, CBS News, and Yahoo among
others.
It is also common for API providers to provide more detailed information
than just model generations. Logits, the probability distributions62 over po-
tential next tokens, are generally viewed as the preferred form of output for
researchers as it is more useful for many types of research and most knowledge
benchmarks rely on access to logits to score models. However the full set of
logits can expose information about models that closed model providers often
want to keep secret such as architectural details, tokenizer details, and some-
times even details about the training data. As a compromise, it has become
common practice to make the k most probable tokens for some reasonable, but
small, k available.
Another feature many API providers included until very recently is logit
bias. Logit biases allow the user to skew the distribution of logits towards or
away from specific tokens, such as discouraging slurs. Recent work showed that
allowing user-specified logit bias removes the security enhancements of top-k
logits and enables users to obtain access to the full logit distribution, as well
as information about the model architecture, efficiently63,64. As a result of this
work, major model providers have stopped allowing users to specify arbitrary
logit biases or have limited logit bias to only impact models when they’re used for
generation rather than returning logits. This continues to enable most industry
applications, but substantially limits the usefulness for researchers.
Foundation model APIs also need not be limited to model inference. Fine-
tuning APIs are often made available, allowing users to provide a dataset
which the API provider will use to fine-tune and subsequently host for the user.
This can be used to enable many successful real-world deployments of models
specialized for a given task and can be used for many types of research. However
it comes with substantial downsides from a safety point of view, as safety filters
are easy to finetune out of a model, even inadvertently65.
Finally, many API providers allow their models to interface with external
apps or plug-ins through a process generally known as tool use. Tool use for
large language models is a budding field of research whose risks and benefits are
difficult to assess at this time. However it is clear that current implementations
pose massive security risks. For example, researchers at Salt Labs66 found that
62. Technically, logits are a particular transformation of the probabilities, but nothing is lost
on a conceptual level by this simplification.
63. Matthew Finlayson, Swabha Swayamdipta, and Xiang Ren. 2024. Logits of api-protected
llms leak proprietary information. arXiv preprint arXiv:2403.09539.
64. Nicholas Carlini et al. 2024. Stealing part of a production language model. arXiv preprint
arXiv:2403.06634.
65. Xiangyu Qi et al. 2023. Fine-tuning aligned language models compromises safety, even
when users do not intend to! In The twelfth international conference on learning representa-
tions.
66. Aviad Carmel. 2024. Security flaws within chatgpt ecosystem allowed access to accounts
on third-party websites and sensitive data. Salt Labs Blog, https://salt.security/blog/security-
14
ChatGPT plug-ins enabled a wide variety of attacks including impersonating
the user to the third-party app, impersonating the third-party app to the user,
and tricking a user into running malicious code. The first two attacks allow
attackers to gain control of user accounts either on the OpenAI side or on the
third-party application side, and the last enables attackers to install malware
on user systems without their knowledge. Another prominent example is the
work of Greshake et al.67 which showed that by inserting invisible text into the
HTML markup of one’s personal website, one can gain control of the ChatGPT
session of any user who uses ChatGPT’s web browsing capabilities to view thier
website.
The attacker can then see all user inputs, all model outputs, and
impersonate the model at will.
5
National security considerations
Concerns have been raised about the impact of generative AI on national secu-
rity. Here we present our observations from years of participation in an inter-
national open model research community.
5.1
Foreign open-weight models
The United States is not the sole creator of powerful open-weight models. At
time of writing there are ten open weight models as powerful or more powerful
than LLaMA 2 70B: Qwen 1.5, Mixtral, Grok-1, Falcon-180B, Yi-34B, DBRX,
DeepSeek-67B, Command R, Miqu68, and LLaMA 2 70B itself. Four of these
models were trained in the US, three in China, two in France, and one in the
UAE69. At various points in the past year, each of these countries has produced
the model holding the title of “most powerful public-weight model in the world.”
In addition to releasing highly capable pretrained language models, the govern-
ments of all three countries have identified investment in open source AI as part
of their national strategy.
In February, the New York Times published an article claiming that
Some of the technology in 01.AI’s system came from Llama. Mr.
Lee’s start-up then built on Meta’s technology, training its system
[Yi-34B] with new data to make it more powerful.
This quote implies that, if not for the release of LLaMA 2, 01.AI wouldn’t have
been able to train a model as powerful as Yi-34B because the trained LLaMA 2
flaws-within-chatgpt-extensions-allowed-access-to-accounts-on-third-party-websites-and-
sensitive-data.
67. Kai Greshake et al. 2023. Not what you’ve signed up for: compromising real-world llm-
integrated applications with indirect prompt injection. In Proceedings of the 16th acm work-
shop on artificial intelligence and security, 79–90.
68. This model is widely believed to be a leaked quantized early checkpoint of the unreleased
Mistral-medium model. We will assume that this is correct.
69. Cohere’s Command R was likely trained within the US, although Cohere is a Canadian
company.
15
model was a component in the pipeline that produced Yi. There is no reason to
believe that this claim is true, and substantial reason to believe that this claim
is false. We believe that the authors made an understandable but regrettable
mistake when trying to follow a technical discussion on the Hugging Face Hub.
For further elaboration on this topic, see our recent blog post70.
This article highlights a common misunderstanding of the relationship be-
tween current and future open-weight models. Future open weight models are
very rarely based on current open weight models as computing artifacts. Rather
they will draw design inspiration, learn about best practices, and learn to avoid
pitfalls previous developers fell into by studying the academic papers and techni-
cal reports accompanying those models. The models themselves play no role in
the transfer of scientific knowledge other than to demonstrate that the authors
did in fact successfully train a powerful model. As a result, heavy restrictions
or bans on the release of powerful pretrained models is unlikely to meaningfully
limit model development overseas.
Note that we focus above on general capabilities rather than size or pretrain-
ing budget. The list and distribution of countries that have trained the largest
language models or most expensive language models is largely similar, with the
notable addition of Russia on the “largest” list due to Yandex’s 100B parameter
language model. While there are some differences in capabilities brought about
by design choices, capabilities and expenditures are very strongly correlated for
any competent team independent of individual design choices. One thing that
is important to keep in mind regarding countries that have trained large but
not as good models is that those models are typically trained for a small frac-
tion of the number of tokens that the powerful models they are being compared
to were trained for. However training them for a meaningful amount of time
requires solving all the core engineering problems, and increasing the training
token budget is primarily a question about financial investment. If Russia prior-
itized funding for training an approximately state-of-the-open model, we should
take YaLM-100B as evidence that they are likely to succeed.
5.2
Cybersecurity
Cybersecurity and artificial intelligence is a major topic of interest to the US
government, as demonstrated by unclassified version of the Office of the Director
of National Intelligence (ODNI)’s 2024 Annual Threat Assessment of the U.S.
Intelligence Community71. The Threat Assessment outlined three key threats
from generative artificial intelligence:
• Automatic disinformation campaigns
• Cyber warfare and AI-powered hacking campaigns
70. Hailey Schoelkopf, Aviya Skowron, and Stella Biderman. 2024. Yi-34b, llama 2, and
common practices in llm training: a fact check of the new york times. EleutherAI Blog.
[Online; accessed March 2024]. https://blog.eleuther.ai/nyt-yi-34b-response/.
71. ttps://www.dni.gov/files/ODNI/documents/assessments/NIC-Declassified-ICA-Foreign-Threats-to-the-2022-US-Elections-D
pdf
16
• Chemical, biological, radiological and nuclear (CBRN) information
which we will address in turn.
5.2.1
Disinformation
As far as we are aware, limited ability to produce false information is not the
primary bottleneck for disinformation campaigns. While it can be a bottleneck,
other issues such as dissemination of information, compromising trust in author-
ities, and discrete infiltration of social media platforms tend to be substantially
greater barriers72.
We also take issue with the common suggestion that disinformation is a
problem that should be solved at the model-provider level.73 Disinformation is
a highly nuanced and contextually sensitive concept that cannot be evaluated
in a vacuum: true sentences in one sociopolitical context might be misleading in
another and clearly false in a third. Additionally, it is not even clear that false
information is necessarily the most dangerous74. Research on vaccine hesitancy
has shown that misleading true information is substantially more impactful at
reducing compliance than false information.75.
It is important to keep in mind that the ability to create photo-realistic
images and videos has been around for over five years and has been used in
attempted disinformation ops. One interesting case study is the Russia-Ukraine
war in which falsified videos of Ukrainian President Volodymyr Zelenskyy and
Russian President Vladimir Putin were created by Russia and Ukraine respec-
tively and distributed to the other side.76 However disinformation experts are
skeptical that these photorealistic videos played a significant role in shaping
discourse on either side77 and point to cruder but better situated attacks on the
epistemic commons as being more effective, highlighting the need for a nuanced
view of the issue beyond the ability to generate realistic synthetic content.
None of this is to say that disinformation isn’t a real problem or that artifi-
cial intelligence does not play a role in its creation or propagation78. However
over-indexing on the threat of foundation models for disinformation risks both
underestimating the problem and overestimating the solution, leaving the U.S.
at substantially increased risk.
72. Jon Bateman and Dean Jackson. 2024. Countering disinformation effectively: an
evidence-based policy guide.
73. https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property
74. Michael Hameleers. 2023. Disinformation as a context-bound phenomenon: toward a
conceptual clarification integrating actors, intentions and techniques of creation and dissemi-
nation. Communication Theory 33 (1): 1–10.
75. Jennifer Nancy Lee Allen, Duncan J Watts, and David Rand. 2023. Quantifying the
impact of misinformation and vaccine-skeptical content on facebook.
76. https://theconversation.com/deepfakes-in-warfare-new-concerns-emerge-from-their-use-around-the-russian-invasion-of-ukra
77. Bateman and Jackson 2024.
78. While it is a very different type of artificial intelligence than foundation models, recom-
mender algorithms are a key battleground in the fight against disinformation
17
5.2.2
Cyber warfare and AI-powered hacking campaigns
The question of whether open source systems and openly available tools for
finding security vulnerabilities has been debated beyond the field of artificial
intelligence for decades. In general, security experts believe that there is no
reason to think that closed systems are more secure than open systems or that
publicly sharing tools for attacking systems decreases security.79,80
Foundation models, including open foundation models, are beginning to
show promise for helping secure our computing systems. Models for malware
detection81,82 and fuzzing83 are especially promising applications of current
technologies. Malware detection gives the ability to protect us from offensive
cyberoperations of adversaries, and the relatively cheap cost of the models con-
sidered in those papers84 may imply that for a fixed amount of investment one
gets substantially more defensive value than offensive value.
Deng et al. (2023) reports using both API and open-weight models to find
65 bugs in the PyTorch library, 41 of which they were able to confirm are novel
bugs. While the criticality of these bugs is generally not particularly high,
5.2.3
(CBRN) Information
Chemical, biological, radiological and nuclear (CBRN) information is highly sen-
sitive and sometimes classified information that pertains to the development of
extremely dangerous weapons. Some have suggested that large language mod-
els will make this kind of information more readily accessible and “democratize
access” to biological weapons. The best available research indicates that this is
not true of current models,85,86 and we believe that the focus on models in these
conversations misses the point in a fundamental way: the core issue isn’t the
model (which is effectively a better search algorithm over the training corpus for
these purposes) but rather the presence of this information in the training data
in the first place. We expect that better data management practices, oversight
of training data, and data investigation and removal will address this issue (to
the extent that there is one) far better than any model-level interventions.
79. https://www.zdnet.com/article/risky-business-keeping-security-a-secret-5000127072/
80. David A Wheeler. 2021. Secure programming howto. Walters Art Museum in Baltimore,
Maryland.
81. Mohammad Mahmudul Alam et al. 2023b. Recasting self-attention with holographic
reduced representations. In International conference on machine learning, 490–507. PMLR.
82. Mohammad Mahmudul Alam et al. 2024. Holographic global convolutional networks for
long-range prediction tasks in malware detection. In Proceedings of the 27th international
conference on artificial intelligence and statistics (aistats).
83. Yinlin Deng et al. 2023. Large language models are zero-shot fuzzers: fuzzing deep-
learning libraries via large language models. In Proceedings of the 32nd acm sigsoft interna-
tional symposium on software testing and analysis, 423–435.
84. They are multiple orders of magnitude cheaper than training a competent code genera-
tion algorithm.
85. Ella Guest, Caleb Lucas, and Christopher A Mouton. 2024. The operational risks of ai
in large-scale biological attacks: results of a red-team study.
86. https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation
18
When considering models developed for widespread deployment, it is impor-
tant to keep in mind that there is no evidence that the kind of generalization
that language models are capable of is suited to generalizing from, e.g., first year
undergraduate biology to designing biological weapons, and in the overwhelming
majority of use-cases there isn’t a need for more advanced knowledge than that.
Even when more advanced knowledge is necessary, curating biological data to
selectively remove the specific dangerous subfields will very likely ameliorate the
threat.87 It is possible that for specialist models for sufficiently similar appli-
cations would be able to perform this kind of generalization, and if substantial
evidence of this emerges access control regulation for those specialist models
may be appropriate.
5.3
Rotary Embeddings
On March 24, 2021, a Discord user by the name of bratao dropped a link to a
Chinese language blog post88 in the middle of the night along with the message
“Don’t understand a word, but some smart [Chinese AI researchers] are staring
the repo.” The next morning, Stella Biderman (a mathematically-inclined con-
tributor who today leads EleutherAI and doesn’t speak a word of Chinese) spent
several hours reverse engineering the methodology from the equations and a low-
quality machine translation of the blog post. Intrigued and impressed by what
she found, she and several other Discord members started working to experi-
ment with the methodology, training 125M, 350M, and 1.5B parameter models
(for a small fraction of the typical pretraining budget) that outperformed other
methods by a small but meaningful amount. The resulting blog post89 went
live a handful of hours before the official paper90 came out and was the first
English-language resource on the now-ubiquitous technique. Our GPT-J and
GPT-NeoX were among the first large models trained with the technique, and
other work such as PaLM91 and LLaMA 192 cite our work as motivating their
use.
87. Research on this topic is currently on-going at EleutherAI.
88. https://kexue.fm/archives/8265
89. Biderman et al. 2021.
90. Jianlin Su et al. 2021. Roformer: enhanced transformer with rotary position embedding.
arXiv preprint arXiv:2104.09864.
91. Chowdhery et al. 2023.
92. Hugo Touvron et al. 2023. Llama: open and efficient foundation language models. arXiv:
2302.13971 [cs.CL].
19
Part B: Remaining questions
1
Defining “open” or “widely available”
a
Will closed models be opened?
Yes, there is significant evidence that the weights of models similar to current
closed AI systems will become available. The current most advanced open mod-
els are equal or better than the most advanced closed models from just a few
years ago. In many cases, prominent closed models have been replicated whole-
sale and released openly. For example, GPT-393 was replicated and released
openly less than 2 years later by Meta AI’s OPT94 models, and the unreleased
Chinchilla95 model from Google DeepMind was replicated by Meta AI in the
form of the Llama 196 series of models less than a year later.
Other prominent models have been replicated by entities not labs at major
tech companies. For example, an academic lab led a collaboration to successfully
replicate AlphaFold297 as OpenFold98, without a restrictive license or lack of
training details documented.
OpenAI’s CLIP models99 were also replicated
and released openly by LAION100, and the released CLIP models by OpenAI
were used to drive the then-state of the art in controllable image generation
via VQGAN-CLIP101. The Stable Diffusion 1 model102, then the most capable
image generation model, was released with open weights. In general, the trend
has been for open-weights models to match closed AI systems of the time or in
93. Tom Brown et al. 2020. Language models are few-shot learners. In Advances in neu-
ral information processing systems, edited by H. Larochelle et al., 33:1877–1901. Cur-
ran Associates, Inc. https : / / proceedings . neurips . cc / paper files / paper / 2020 / file /
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
94. Zhang et al. 2022.
95. Jordan Hoffmann et al. 2022. Training compute-optimal large language models. arXiv:
2203.15556 [cs.CL].
96. Touvron et al. 2023.
97. John Jumper et al. 2021. Highly accurate protein structure prediction with alphafold.
Nature 596 (July): 1–11. https://doi.org/10.1038/s41586-021-03819-2.
98. Gustaf Ahdritz et al. 2023. Openfold: retraining alphafold2 yields new insights into its
learning mechanisms and capacity for generalization. bioRxiv, https://doi.org/10.1101/2022.
11.20.517210. eprint: https://www.biorxiv.org/content/early/2023/08/12/2022.11.20.517210.
full.pdf. https://www.biorxiv.org/content/early/2023/08/12/2022.11.20.517210.
99. Alec Radford et al. 2021. Learning transferable visual models from natural language
supervision. In Icml.
100. Mehdi Cherti et al. 2023. Reproducible scaling laws for contrastive language-image learn-
ing. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition,
2818–2829.
101. Katherine Crowson et al. 2022. Vqgan-clip: open domain image generation and editing
with natural language guidance. In Computer vision – eccv 2022: 17th european conference, tel
aviv, israel, october 23–27, 2022, proceedings, part xxxvii, 88–105. Tel Aviv, Israel: Springer-
Verlag. isbn: 978-3-031-19835-9. https://doi.org /10.1007/978- 3- 031- 19836- 6 6. https:
//doi.org/10.1007/978-3-031-19836-6 6.
102. Robin Rombach et al. 2022. High-resolution image synthesis with latent diffusion models.
In Proceedings of the ieee conference on computer vision and pattern recognition (cvpr). https:
//github.com/CompVis/latent-diffusionhttps://arxiv.org/abs/2112.10752.
20
some cases be the forefront.
In fact, during the drafting of this response xAI released Grok-1103 and
Databricks released DBRX104, two large and powerful models following the same
Mixture-of-Experts architecture used in many leading closed labs. According to
DataBricks’s analysis their model outperforms GPT-3.5 (likely gpt-3.5-turbo-16k-0613
in particular), a model that was state-of-the-art when released November 2022.
b
Predicting the Gap
The timeframe between deployment of an open and closed equally-performing
model is difficult to predict reliably. The primary blocker for the capabilities of
open models is funding, which can disappear at the whim of a handful of well-
resourced individuals. The continued increase in costs (of compute and capital)
to create the most capable closed models will create further barriers to entry,
but open models remain frequently competitive to closed models and there is
evidence that they will continue to do so despite these costs.
The centralization of funding for training these models is itself also a source
of uncertainty in the gap between open and closed models, as individual factors
affecting the sponsoring organizations unrelated to AI research per se can have
substantial impacts on release timelines. For example,
c
What level of distribution constitutes “wide availabil-
ity?”
For the bulk of our response to this question, see Section 4.
d
Do certain forms of access to an open foundation model
provide more or less benefit or more or less risk than
others?
For the bulk of our response to this question, see Section 4.
i
Are there promising prospective forms or modes of access that
could strike a more favorable benefit-risk balance? If so, what are
they?
For the bulk of our response to this question, see Section 4.
We would like to additionally note that, although the weights of closed mod-
els that have been productized such as ChatGPT are not openly available, such
deployed models may be far more “widely available” to end users than many
openly downloadable foundation model weights, especially weights of models too
large for end consumer hardware to quickly run. Calculations regarding volume
of risk and benefit should take not just availability of weights into account, but
103. https://x.ai/blog/grok-os
104. https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm
21
also level of distribution and barriers to model use, since widely deployed models
receive more traffic and use than open-weight models.
2
Comparing Risks
d
Well-Financed Actors
There is no substantial evidence that the lack of weight access to existing models
is a meaningful factor constraining the actions of state-level actors or other well-
financed actors. A small but dedicated research team with ten million dollars
in funding could relatively easily obtain a model as good as or better than the
best models with widely available weights today.
i
How do these risks compare to those associated with closed mod-
els?
For both closed and open models, there are numerous state or determined non-
state actors capable of training highly-capable state of the art models without
relying on the models which already exist. See also Section ?? and in particular
our discussion of the claim that LLaMA 2 enabled the training of Yi.
Notably when trying to maximize benefit but minimize risk, local hosting or
edge deployment can enable a greater amount of personalization and tailoring
for a particular use case.
This can reduce risk, because while ensuring the
complete “safety” of a general-purpose foundation model is an under-specified
task105, basing safety around a concrete goal and use case, alongside potential
threat models, can allow for safety and risk reduction to be effectively achieved.
ii
How do these risks compare to those associated with other types
of software systems and information resources?
While it has been speculated that AI models will produce security risks such
as assistance in creating bioweapons, existing studies have shown that there is
not evidence that current systems are able to meaningfully assist in bioweapon
development106, and that they are not more helpful than the use of freely avail-
able traditional software or information resources such as Google or PubMed.
AI models are currently trained on almost exclusively publicly available infor-
mation and content, indicating that most threat models–similar to Google or
other search engines–consist of publicly discoverable information simply being
easier to surface and access.
105. https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property
106. https://www.rand.org/news/press/2024/01/25.html
22
3
Unique Benefits of Open Models
We feel it would be appropriate to collect some of the unique benefits and
perspectives that open models offer as a component of the AI ecosystem.
Throughout our response elsewhere we discuss risks and benefits of open
release, but feel that EleutherAI is well-positioned to convey precisely why open
models matter, and can be entirely of a different kind than closed models.
In particular, while openly available model weights can carry some of the
benefits we discuss, maximally open models such as those EleutherAI releases
including GPT-NeoX107 and Pythia108, or AI2’s OLMo109 and LLM360’s Am-
ber110 models that followed, provide extensive benefits for the AI ecosystem, for
research, and for AI governance.
Open models open up new research areas There are entire areas of ML
research that would not be possible without open and well-documented
models like Pythia. Understanding models’ “learning dynamics”–how they
change over the course of training, in response to new data111,112,113,114.
Many other forms of research require access to the dataset in addition to
model weights115,116, and interpretability research requires at minimum
weight and gradient-level access to a model117,118,119.
Sharing knowledge broadly Openly released, and especially well-documented
or transparent open model projects, disseminate knowledge throughout
the open source community. EleutherAI in particular has trained many
107. Sidney Black et al. 2022.
108. Biderman, Schoelkopf, et al. 2023.
109. Dirk Groeneveld et al. 2024. Olmo: accelerating the science of language models. arXiv:
2402.00838 [cs.CL].
110. Zhengzhong Liu et al. 2023. Llm360: towards fully transparent open-source llms. arXiv:
2312.06550 [cs.CL].
111. Davis Brown et al. 2023. Understanding the inner-workings of language models through
representation dissimilarity. In Proceedings of the 2023 conference on empirical methods in
natural language processing, edited by Houda Bouamor, Juan Pino, and Kalika Bali, 6543–
6558. Singapore: Association for Computational Linguistics, December. https://doi.org/10.
18653/v1/2023.emnlp-main.403. https://aclanthology.org/2023.emnlp-main.403.
112. Dmitrii Krasheninnikov et al. 2023. Meta- (out-of-context) learning in neural networks.
arXiv: 2310.15047 [cs.LG].
113. James A. Michaelov and Benjamin K. Bergen. 2023. Emergent inabilities? inverse scaling
over the course of pretraining. arXiv: 2305.14681 [cs.CL].
114. Fabien Roger. 2023. Large language models sometimes generate purely negatively-
reinforced text. arXiv: 2306.07567 [cs.LG].
115. Stella Biderman, USVSN PRASHANTH, et al. 2023. Emergent and predictable mem-
orization in large language models. In Advances in neural information processing systems,
edited by A. Oh et al., 36:28072–28090. Curran Associates, Inc. https://proceedings.neurips.
cc/paper files/paper/2023/file/59404fb89d6194641c69ae99ecdf8f6d-Paper-Conference.pdf.
116. Choi, Shavit, and Duvenaud 2023.
117. Eric Todd et al. 2024. Function vectors in large language models. arXiv: 2310.15213
[cs.CL].
118. Kevin Meng et al. 2023. Mass-editing memory in a transformer. arXiv: 2210 . 07229
[cs.CL].
119. Wes Gurnee et al. 2023. Finding neurons in a haystack: case studies with sparse probing.
arXiv: 2305.01610 [cs.LG].
23
researchers, many of whom come from non-traditional backgrounds120.
This allows a larger amount of people to contribute to pushing knowledge
of the Machine Learning field forward and to help develop techniques that
serve them and their concerns.
Safety research is supported and broadened by open models Safety re-
search is also helped by access to open and transparent foundation models.
Open-weights models allow more researchers than just the small number
at industry labs to investigate how to improve model safety, improving
the breadth and depth of methods that can be explored, and also allows
for a wider demographic of researchers or auditors of safety. Too much
work on improving safety has been made possible by open-weights foun-
dation models to list in full, but we provide a sample here: research on
the ability to extract “memorized” training datapoints121 from a model,
investigating the failure modes of watermarking schemes122, and steering
models’ predictions to provably not depend on undesired attributes such
as gender123. Even for researchers in industrial labs such as Google, open
models can enable research on model safety that would not otherwise be
possible: in an earlier revision of Quantifying Memorization Across Neural
Language Models, Carlini et al. 124 state that their research on harmful
memorization in language models “would not have been possible with-
out EleutherAI’s complete public release of The Pile dataset and their
GPT-Neo family of models”125.
Overall, open models enable people close to the deployment context to have
greater control over the capabilities and usage restrictions of their models, study
the internal behavior of models during deployment, and examine the training
process and especially training data for signs that a model is unsafe to deploy in
a specific use-case. They also lower barriers of entry by making models cheaper
to run and enable users whose use-cases require strict guarding of privacy (e.g.,
medicine, government benefits, personal financial information) to use.
120. Jason Phang et al. 2022. Eleutherai: going beyond ”open science” to ”science in the
open”. arXiv: 2210.06413 [cs.CL].
121. Biderman, PRASHANTH, et al. 2023.
122. John Kirchenbauer et al. 2023. On the reliability of watermarks for large language mod-
els. arXiv: 2306.04634 [cs.LG].
123. Nora Belrose et al. 2023. Leace: perfect linear concept erasure in closed form. In Advances
in neural information processing systems, edited by A. Oh et al., 36:66044–66063. Curran
Associates, Inc. https://proceedings.neurips.cc/paper files/paper/2023/file/d066d21c619d0a
78c5b557fa3291a8f4-Paper-Conference.pdf.
124. https://arxiv.org/abs/2202.07646v2
125. Nicholas Carlini et al. 2023. Quantifying memorization across neural language models.
arXiv: 2202.07646 [cs.LG].
24
5
Technical issues in managing risks and dis-
tributing benefits
a
What model evaluations, if any, can help determine
risks or benefits?
We caution that current benchmarks in the research community, used in the
field as objectives to measure progress on an artificially-constructed task, while
useful for some research applications, are not suited for direct policy or risk
measurement purposes.
The best evaluation for measuring model safety is one that closely tracks an
explicitly outlined threat model and restricted use case, rather than attempting
to measure “safety” writ large across all potential use cases. For example, red-
teaming with the help of domain experts, with adequate baseline systems also
available, can be used to assess safety in real-world (adversarial) scenarios, if
the red-teamers are adequately representative of user demographics.
As an additional example of the necessity of targeting and comparing against
a concrete scenario of potential harm, and a threat model for how a model might
cause such harm, we call back to Section 1 and note that “bias benchmarks”
used in the field are insufficient to ensure that models that score
highly do not cause harm in the real world. Another paper illustrating
this phenomenon is Large language models propagate race-based medicine126,
which finds that Bard, ChatGPT, Claude, and GPT-4127, which finds that de-
ployed large language models recommend discredited race-based medical prac-
tices grounded in eugenics research. The study specifically considers what would
happen if an inexperienced doctor consults the state-of-the-art models to answer
some basic questions about medical best practices and finds that substantial
harm would ensue. We believe that real world harm, rather than artificial
bias benchmarks, must be prioritized when designing evaluation protocols for
deployed systems. In particular, this means that models cannot be evaluated in
a vacuum and must be evaluated situated in a proposed application context.
f
Components Necessary for Red Teaming
Prior work such as Black-Box Access is Insufficient for Rigorous AI Audits128
has argued that black-box access is insufficient for effective auditing: it is cru-
cial to also take into account factors such as methodological choices and how
these might impact a model’s safety, or additionally internal documentation and
evaluations to investigate what systems are being optimized for and what steps
are being taken to reduce risks. Ensuring a broad range of people are able to
audit the model can achieve broader coverage of potentially-unforeseen harms.
126. Jesutofunmi A Omiye et al. 2023. Large language models propagate race-based medicine.
NPJ Digital Medicine 6 (1): 195.
127. ChatGPT here presumably refers to GPT-3.5 via the chat interface, though the paper
is unclear on this point.
128. Casper et al. 2024.
25
While some types of auditing or evaluation can be done with purely inputs +
outputs to a model, having the ability to know what types of transformation
or augmentation layers surround a foundation model, and access to full out-
puts such as logits or log-probabilities can enable more thorough evaluation of
models, such as probing for bias or more fine-grained answer probabilities and
nuances in evaluation.
g
Testing and Verifying Model Weights
Here we limit our scope to verifying claims regarding either model trainers’
claims made to auditors, or assessing whether the audited model is the one
deployed. Regarding the latter, it is possible to determine that these models
match, as a sufficiently large quantity of random queries will reveal if two models
are non-identical. However, a complication is that API providers do more than
just run the input through their model.
For example, they will sometimes
process and reformat the input, and also call external APIs such as web search, a
code compiler, or a computer algebra system. While API calls can be avoided by
restricting questions to natural language English and turning off web-browsing,
reprocessing and reformatting of inputs is more challenging. The model host
would need to disclose all such steps and how they work for testing to be feasible.
There may also be other, non-public, steps that model hosts use that need to be
taken into account when devising the verification scheme. Regarding methods
for proving model trainers’ claims made to auditors are truthful, especially with
respect to training data composition, this is not yet possible, with the best work
to date on the topic being Tools for Verifying Neural Models’ Training Data129.
Note that their schema is not secure and can be fooled.
129. Choi, Shavit, and Duvenaud 2023.
26
",EleutherAI,16755
NTIA-2023-0009-0287,"Before the
National Telecommunications and Information Administration
Washington, DC 20230
In the Matter of
Dual-Use Foundation Artificial Intelligence
Models with Widely Available Model
Weights
)
)
)
)
)
)
Docket No. 240216-0052
COMMENTS OF NEW AMERICA’S OPEN TECHNOLOGY INSTITUTE
March 27, 2024
Prem M. Trivedi
Nat Meysenburg
New America’s Open Technology Institute
740 15th Street NW, Suite 900
Washington, D.C. 20005
Executive Summary
3
I. Introduction
3
II. The Gradient of Openness and the Question of Marginal Risk
4
III. Historical context: Open societies and the ideal of open-source software
6
IV. Assessing Benefits and Risks of Open and Closed Models
7
A. Cybersecurity and related harms to people
9
1. Benefits
9
2. Risks
10
B. U.S. Foreign Policy: Geopolitics & Geoeconomics
12
1. Benefits
12
2. Risks
13
C. Transparency and Public Accountability
13
1. Benefits
14
2. Risks
14
D. Economic Health, Innovation, and Competition
15
1. Benefits
15
2. Risks
15
E. Community Control and Benefits
16
1. Benefits and Challenges
16
V. Recommendations for Policy and Regulatory Approaches
17
A. Study the marginal risk of open models and precisely articulate observable harms.
17
B. Create common requirements for responsible development across the entire gradient of
openness.
17
C. Consider the broad range of relevant national security and foreign policy objectives before
recommending policy or regulatory action aimed at vaguely defined or narrow types of
security risks.
18
D. Develop a thoughtful approach to cybersecurity software liability that accounts for the
need to incentivize innovation in open AI models.
18
VI. Conclusion
18
2
Executive Summary
New America’s Open Technology Institute welcomes the opportunity to submit comments in
response to the National Telecommunications and Administration (NTIA)’s request for public
submissions on the risks, benefits, and “Dual-Use Foundation Artificial Intelligence Models with
Widely Available Weights.” Our comments emphasize the importance of policy and regulatory
approaches that encourage the development of a strong open-source AI ecosystem in the United
States. We take a broader framing of “openness” than is captured by the phrase “models with
widely available weights.”
Section II briefly discusses terminology and definitions related to openness and risk, noting that
there is no decisive definition of open or closed, and that the focus should be on defining the
marginal risks posed by more open systems.. Section III briefly situates the current debate in the
context of the history of open-source software development and discusses the relationship
between open code and open societies. Section IV analyzes the benefits and risks of more open
models relative to more closed ones along five policy objectives: (a) cybersecurity and related
harms to people; (b) foreign policy, through the prism of geopolitics and geoeconomics, (c)
transparency and public accountability, (d) economic health and innovation, and (e) community
control and benefits. Section V offers four principles and recommendations for U.S. government
and related stakeholders developing policy and regulatory approaches to open models: (1) Study
marginal risk and articulate harms with specificity, (2) create common policy and regulatory
requirements that apply to foundation models no matter where they fall on the spectrum of
openness, (3) consider the broad range of relevant national security and foreign policy objectives
before recommending sweeping policy or regulatory action aimed either at vaguely defined or
very specific defined security risks, and (4) develop a thoughtful approach to cybersecurity
software liability that accounts for the need to incentivize innovation in open AI models.
I.
Introduction
The Open Technology Institute (“OTI”) is a program within New America that works at the
intersection of technology and policy to ensure that every community has equitable access to
digital technology and its benefits.1 OTI promotes universal access to digital technologies that
are both open and secure, using a multidisciplinary approach that brings together advocates,
researchers, organizers, and innovators. OTI welcomes the opportunity to respond to the National
Telecommunications and Information Administration’s (NTIA) request for comments about the
risks and benefits of foundation AI models with widely available weights.2 Our comments are
largely responsive to aspects of questions 1, 2, 3, 5, and 8.
2 Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights, Docket No.
240216-0052, 89 FR 14059, National Telecommunications and Information Administration (hereafter “NTIA RFC”)
(Feb. 26, 2024).
1 New America, Open Technology Institute, https://www.newamerica.org/oti/about/.
3
This call for comments and public debate is timely, as there has been a rise of closed AI models
in the United States that largely reflect existing concentrations of power in the hands of a few
leading companies. This trend is a cause for concern, as it risks extending the dynamics we now
experience with large technology companies to a period of rapid and transformative evolution in
foundation AI models. It would be a mistake for the United States to adopt a policy approach to
foundation AI models that hampers the chance for large and small open source systems to thrive
alongside large companies’ more closed models. At a minimum, the United States should adopt
common guardrails and rules requiring meaningful transparency and accountability from all AI
foundation models, regardless of where they fall on the complicated spectrum of open to closed.
At OTI, we favor an ecosystem in which open AI models can flourish alongside proprietary
ones. Our comments analyze the benefits and risks in the context of five major U.S. policy
objectives: cybersecurity, foreign policy, public accountability, economic health and innovation,
and community control and benefits. Although open AI models—just like closed
models—present risks, the many benefits of open models play an essential role in furthering
these five objectives.
This week, OTI joined 46 other civil society organizations and academics in urging Secretary of
Commerce Gina Raimondo to protect AI openness and transparency. Ensuring that open and
transparent AI models flourish is critical to developing trustworthy AI applications that can
bolster American innovation, global competitiveness, and an equitable AI future for all
Americans. In order to counter or at least offset the trend toward dominant closed AI systems and
continued concentrations of power in the hands of a few companies,3 The U.S. government
should take a coordinated interagency approach designed to ensure that the vast potential
benefits of a flourishing open model ecosystem to American interests, broadly defined.
II.
The Gradient of Openness and the Question of Marginal Risk
There is no easy binary that opposes “open” and “closed” AI models. Commentary and research
that suggest otherwise unhelpfully distort the reality—which AI technical and governance
experts have repeatedly explained—that AI models sit somewhere on a spectrum or “gradient” of
openness.4 We commend NTIA for forthrightly acknowledging this gradient and focusing on the
4 See, e.g., The Future Society, Toward Effective Governance of Foundational Models and Generative AI (“Several
speakers challenged the notion of a binary between “open” and “closed” models, pointing toward a spectrum of
options regarding the level of access to system components such as datasets, code, model cards, and model
weights.”)
3 Mark Surman, Ayah Bdeir, Lindsey Dodson, Alexis-Brianna Felix, and Nik Marda, Accelerating Progress Toward
Trustworthy AI, Mozilla, Feb. 22, 2024.
4
need to discuss marginal risks associated with open models relative to closed models or what is
already publicly available online.5
To put it plainly, “open” and “open source” have many meanings, with different actors using the
term in different, often self-serving, ways. As Widder, West, and Whittaker noted in a paper last
year:
Broadly, the terms ‘open’ and ‘open source’ are used in the context of AI in varying ways
to refer to a range of capabilities that can be broadly bucketed as offering attributes of
transparency - the ability to access and vet source code, documentation and data -
reusability - the ability and licensing needed to allow third parties to reuse source code
and/or data - and extensibility - the ability to build on top of extant off-the-shelf models,
‘tuning’ them for one or another specific purpose. While the terms ‘open’ and ‘open
source’ are used variably to refer to these attributes, in practice there are gradients of
openness that offer vastly differing levels of access. Some systems described as ‘open’
offer little more than an API or the ability to download a hosted model. In these cases,
many question whether the term should be applied at all, or if it is ‘openwashing’ systems
that should be understood as ‘closed’. Other more maximal versions of ‘open’ AI go
further, offering access to the source code, underlying training data and full
documentation, as well as licensing the AI system for wide reuse under terms aligned
with the mandates of the Open Source Initiative’s definition of ‘open source’.6
In the context of foundation AI models, openness can manifest in many ways, including:
●
Open code that can be downloaded and used by others for training and application to new
contexts
●
Open model weights
●
Open licenses for model use
●
Information about model inputs (data sources, training, methodology)
●
Information about and plans to mitigate against undesirable downstream effects (e.g.,
malicious actors fine-tuning the model to cause clear harms).
6 David Gray Widder, Sarah West, and Meredith Whittaker, Open (For Business): Big Tech, Concentrated Power,
and the Political Economy of Open AI, at 2-3, August 17, 2023.
5 Remarks of NTIA Administrator Alan Davidson, Mar. 22, 2024
https://www.ntia.gov/speechtestimony/2024/national-security-and-open-weight-models (“One thing we have already
learned is the importance of focusing on the marginal or differential risks and benefits of open weights. For example,
we need to measure the risks of open-weight models relative to the risks that already exist today from
widely-available information, or from closed models. We have also been encouraged to hear that this is not a binary
choice of “open” vs. “closed.” Rather there is a broader “gradient of openness” that we need to consider and that
may offer broader options for policy.”)
5
To plot whether a model is open or closed along just these five factors To illustrate the concept of
the gradient of openness—and how challenging it is to try to plot models on it—we offer
simplified breakdowns, with examples, of four categories. Much more fluidity on the gradient is
possible than these four categories represent; we have chosen them as an exercise in illustrative
line-drawing.
Table 1: Illustration of the Gradient of Openness
Attributes
Foundation Model
Example(s)
Open
↑
Closed
Open code + transparency about model inputs
(sources, training, methodology etc.) +
transparency about downstream effects +
published model weights
Mixtral, LLaMA7
Open code + no transparency about sources and
training + no published model weights
Grok,
Closed code + no open model weights + public
access + some transparency about model inputs
and downstream effects
ChatGPT, Gemini
Closed code + proprietary access + perhaps no
transparency about model inputs or downstream
effects
Many companies’ internal
systems (e.g., likely for
warehouse management and
distribution)
Throughout these comments, we use “open models” to refer generally to the top two rows, but
our preference is for models that embrace the many aspects of transparency in the topmost row.
But because of definitional issues around “open” and “open source” that are vital but unlikely to
soon be decisively resolved, it is more helpful to talk about the benefits and risks of models
toward the open end of the spectrum than those at the closed end. But given the definitional
difficulties and debates, we think the most productive use of policy makers and regulators’
bandwidth is to develop a set of common rules and technical standards that would apply across
all foundation models (see Section V below for OTI’s recommendations).
III.
Historical context: Open societies and the ideal of open-source software
While we do not argue for a reductive one-to-one equation of open-source software (OSS)
development or open AI models with attributes of open societies, the relationship between the
design and governance of open software/models and the ethos of open societies is worth
7 The difficulties of trying to plot just a few foundation models on a simplified chart perfectly makes the point that
regulatory policy should not be based on parsing these differences.
6
examining. While open and closed models have specific, if contested, meanings in the context of
software, they also have some broader resonance in the context of politically open and closed
societies. In a seminal 1999 article Open Code and Open Societies, Lawrence Lessig describes
the “Open Source Software Moment” taking shape at the time.8 Lessig lays out the ideal of open
source:
The idea that through this collective, essentially volunteer, effort, one of the most
powerful operating systems on the planet [Linux] could be developed is, to put it mildly,
surprising. … This idea—or ideal—of open source software is limited to this OS … It
extends to many of the core technologies that make the Net run. And this idea, or ideal, of
putting into the commons one’s work product—of giving away what one makes, with no
guarantee of compensation—might all sound wildly 60s-ish, wildly idealistic: Marx
applied to code. It sounds alien to our tradition, foreign to what has made our national
flourish. … Until one thinks again … about the way science works. For basic science
functions much the same—progress made and given to the next generation.9
Lessig goes on to explain that the Internet, and perhaps most dramatically the World Wide Web,
is built on public standards. These open source choices created “the fastest growing network in
our history.” Open source, Lessig concludes by quoting an unnamed software engineer, “is the
Internet.”10
The last thirty years of the Internet’s evolution have exposed the extent to which large
proprietary models have contributed to what Jonathan Zittrain calls the “appliancized” nature of
the Internet.11 While they have punctured the image of Lessig’s ideal, his core points remain
more than aspirational values for internet governance: “The values of Internet governance are
about the values of governance; and the balance the Internet must strike is between that part that
leaves itself open to these values, and that part that doesn’t.” For Lessig, the concept of the
commons is key. “[W]e can’t privatize every feature of cyberspace,” he writes. “The opposite of
private is not the government. The opposite of the private is the commons. … There is a value in
preserving that space, regardless of efficiency.”12
IV.
Assessing Benefits and Risks of Open and Closed Models
The preservation of the commons and the parallels between how open source software and
scientific inquiry operate help to explain the vast potential of more open AI models. Open-source
12 Lawrence Lessig, Open Code and Open Societies: Values of Internet Governance at 116, Chicago-Kent Law
Review, 1999.
11 Jonathan Zittrain, The Future of the Internet and How to Stop It.
10 Id. at 109.
9 Lessig, id. At 107-108.
8 Lawrence Lessig, Open Code and Open Societies: Values of Internet Governance at 104, Chicago-Kent Law
Review, 1999.
7
software development, including the development of open foundation models, is critical to
responsible AI development and critical to ensuring that benefits of the “AI revolution”
maximize their potential to serve people equitably.13 The reasons are manifold. As OTI and
nearly 50 academics and civil society organizations wrote in an open letter earlier this week:
For decades, open source software has provided building blocks for everything from
creating art to designing vaccines. According to recent estimates, open source software is
worth more than $8 trillion in value14 and is a part of 96% of commercial software.15 The
U.S. government is one of the biggest users of open source software in the world,16 and
funds open source approaches ranging from boosting cybersecurity to protecting human
rights and fighting cancer.17
NTIA also rightly highlights in its RFC some of the ways in which open AI models present
tremendous positive potential:
Dual use foundation models with widely available weights (referred to here as open
foundation models) could play a key role in fostering growth among less resourced
actors, helping to widely share access to AI’s benefits…. Open foundation models can be
readily adapted and fine-tuned to specific tasks and possibly make it easier for system
developers to scrutinize the role foundation models play in larger AI systems, which is
important for rights- and safety-impacting AI systems (e.g. healthcare, education,
housing, criminal justice, online platforms etc.)
…Historically, widely available programming libraries have given researchers the ability
to simultaneously run and understand algorithms created by other programmers.
Researchers and journals have supported the movement towards open science, which
includes sharing research artifacts like the data and code required to reproduce results.18
18 NTIA RFC at 14060.
17 See, e.g., Rachel Berkowitz, “How Berkeley Lab Helped Develop One of the World's Most Popular Open-Source
Security Monitoring Platforms,” Lawrence Berkeley National Laboratory, February 2023; “Supporting Critical
Open-Source Technologies That Enable a Free and Open Internet,” State Department, November 2023; and
“CANcer Distributed Learning Environment,” National Cancer Institute, February 2023.
16 Eric Goldstein and Camille Stewart Gloster, “We Want Your Input to Help Secure Open Source Software,”
Cybersecurity and Infrastructure Security Agency, August 2023. See also, federal policy supporting open source and
open innovation, e.g., Tony Scott and Anne Rung, “M-16-21 Federal Source Code Policy: Achieving Efficiency,
Transparency, and Innovation through Reusable and Open Source Software,” August 2016.
15 Synopsys, “2024 Open Source Security and Risk Analysis Report,” February 2024. (Analyzed 1,067 commercial
codebases across 17 industries in 2023, and found that 96% of those codebases contained open source.) See also,
Chinmayi Sharma, “Tragedy of the Digital Commons,” North Carolina Law Review, October 2022. (“Google,
iPhones, the national power grid, surgical operating rooms, baby monitors, surveillance technology, and wastewater
management systems all run on open-source software… Without it, our critical infrastructure would crumble.”)
14 Manuel Hoffman et al., “The Value of Open Source Software,” Harvard Business School, January 2024.
13 See, e.g., World Economic Forum, Why Open Source is Crucial for Responsible AI Development, Dec. 27, 2023,
https://europeansting.com/2023/12/27/why-open-source-is-crucial-for-responsible-ai-development/#; Kapoor &
Bommasani et al., On the Societal Impact of Open Foundation Models, Feb. 27, 2024.
8
On balance, the benefits of open AI models, whether foundation models or compound models
built on top of them, outweigh the risks. While we at OTI are deeply mindful of risks, we urge
policymakers, regulators, and researchers to articulate not just risks but measurable and
foreseeable harms as precisely and specifically as possible. Without this kind of rigor, “safety”
and “risk”-focused discourse can morph into a bogeyman that intimidates consumers,
legislators, and regulators into overstating the risks of openness and significantly underweighting
its benefits. The subsections that follow analyze some of the relative risks and benefits of open
and closed models along five major policy objectives: cybersecurity; U.S. foreign policy;
transparency, public accountability, and human rights; innovation and competition; and
community control and benefits.
A. Cybersecurity and related harms to people
A short piece from the Wilson Center entitled “Open Source Software and Cybersecurity: How
unique is this problem?” is worth quoting in full:
Both open source and proprietary models of producing software will inevitably contain
vulnerabilities. Vulnerabilities can come from dependency management (what, how, and
which software packages are pulled into a new software project) to bad-faith actors
(people that intentionally break into systems, or contributors intentionally changing the
software to be exploitable) whether the software is developed internally or in the open.
Best practices for enhancing security in software already exist, and these apply to both
open and proprietary code, packages, and systems. No matter the model of development,
these best practices can guide developers during every stage of the life cycle, from
development of software to architecting a system.19
The Wilson Center goes on to note that OSS’s widespread success has driven adoption and use.
As a result of the massive uptake of OSS, and not because of anything inherent in OSS itself,
there are attendant cybersecurity vulnerabilities.20 But some vulnerabilities broadly distributed by
OSS also
1. Benefits
20 Id. (“OSS is widely adopted in both open and proprietary systems, resulting in decentralized usage of code that
can contain vulnerabilities. Because the use of OSS is more widespread than proprietary code, it is difficult to track
these vulnerabilities. In turn, it can be harder to identify and remediate them. This is not due to the nature of OSS
itself, but to its widespread success and use.”)
19 Ashley Schuett, Alison Parker, and Alex Long, Open Source Software and Cybersecurity: How Unique is this
Problem?, Wilson Center, Nov. 10, 2022.
9
The cybersecurity benefits of open-source software have long been understood, including by
U.S. cybersecurity and national security practitioners and experts. Among open source software’s
greatest security strengths is the ability of security researchers to independently examine code for
vulnerabilities and then feed those findings back into an established ecosystem where software
developers can be alerted and patches written. Open source also provides the ability for other
developers to patch older code if the original authors are no longer willing or able.
Microsoft’s 2023 Digital Defense Report emphatically makes the case for open-source software’s
benefits to cybersecurity: “Open-source collaboration also drives innovation and enhances skills
through shared tools and techniques, leveraging the inclusivity and diversity of a community.
This is vital for understanding the current supply chain threat landscape and scaling mitigation
efforts against emerging threats.”21
It is worth noting that the Department of Defense, arguably the most security-conscious federal
agency in the United States, relies heavily on open-source components of its cyber and weapons
systems22, not simply because open source integration drives innovation but also because it
produces more secure products and systems when accompanied by a coordinated approach to
rigorous vetting.
2. Risks
The overarching question, which NTIA itself has recognized, is how the emergence of open
foundation models present marginal risk relative to more closed models or information publicly
available on the internet. Broad claims about open models causing mis-and disinformation seem
to conflate the large zones in which risks to information integrity challenges are common among
open and proprietary systems. In fact, we have clear evidence that proprietary models are being
frequently weaponized by bad actors.
Much has been made of the impact of generative AI on disinformation, and some commentators
have casually equated open generative AI models with significantly higher risks of
disinformation.23 As nearly fifty academics and civil society organizations emphasized in an
open letter to Secretary of Commerce Raimondo, “the claim that open models make it easier to
operate disinformation campaigns needs to be compared against the ease of conducting
disinformation campaigns using closed models like DALL-E 3 and existing tools like
23 Staying ahead of threat actors in the age of AI, Microsoft, Feb. 14, 2024.
22 Of course, weapons systems integrate both open and closed hardware and software components.
21 Microsoft Digital Defense Report 2023 at 116.
10
Photoshop.24” Indeed, Microsoft’s own research team notes the weaponization of ChatGPT,
which runs on a closed foundation model, by adversary governments including China, [..]
There is at least one important area where analysis of marginal risk shows the rise of open
foundational models causing measurable harms: the spread of child sexual abuse material
(CSAM) and, in particular, computer generated CSAM (CG-CSAM). A 2023 paper that
establishes how open AI models using Stable Diffusion are contributing to the proliferation of
CG-CSAM.25 These known harms are grave, and we do not have decisive solutions in hand. The
authors propose a number of potential mitigations that need further research.26
Public debate and NTIA’s RFC has also focused on the relationship between model computing
power and risk. In short, computing power is an imperfect proxy for risk, and using it as a proxy
can create perverse incentives. Defining risk thresholds with computing power benchmarks
cannot account for improved models that require less computing power. In addition, choosing
static computing thresholds (instead of articulating risk through impacts/effects), also creates
incentives for malicious model developers to stay just under a compute threshold that triggers
heightened regulatory scrutiny. Similarly, malicious downstream model users can simply shop
around for powerful models that fall below an established threshold and might therefore be likely
to have less in-built safeguards against misuse.
Overall, NTIA and the U.S. government more broadly should identify, evaluate, and target the
specific risks from openness in AI, including developing better proxies for risk that are not solely
based on the amount of computing power used to train a model.27
One other risk inherent in the broad open-source software project is also worth considering. How
can maintainers of code be incentivized to perform what is largely volunteer work? This
long-standing problem in open source software security is likely to evolve further in the context
of maintaining smaller, more bespoke models (not necessarily foundation models).28
28 Luis Villa, The largest problems require government collaboration: Tidelift’s response to the ONCD RFI
27 Rishi Bommasani, “Drawing Lines: Tiers for Foundation Models,” Stanford University Center for Research on
Foundation Models, November 2023. (“the relationship between compute and impact is quite tenuous and not
evidentiated… there is no demonstration that compute robustly predicts results on risk evaluations, let alone
demonstrations that compute predicts the impact foundation models have in society… compute is a measure of
upstream resource expenditure, naturally divorced from downstream societal impact.”)
26 Id at 9-14.
25 David Thiel et al., Generative ML and CSAM: Implications and Mitigations, at 7-8 (“Several decidedly negative
outcomes have been observed and pose a high risk to child safety as the availability of CG-CSAM grows. One likely
scenario is that the advent of realistic CG-CSAM generates hundreds of thousands of reports to technology
platforms, NGOs handling CSAM cases, and law enforcement, thus overloading the ability of companies and
organizations to effectively handle reporting and investigations. The investigators will have the added challenge of
determining whether the victim in the scenario is in fact a real person.”)
24 See, e.g., Sayash Kapoor and Arvind Narayanan, “How to Prepare for the Deluge of Generative AI on Social
Media,” Knight First Amendment Institute at Columbia University, June 2023. (“[T]he bottleneck for successful
disinformation operations is not the cost of creating it.”)
11
Software cybersecurity vulnerabilities have long bedeviled the U.S. software ecosystem. Some
have argued that the problem is posed less by the absence of liability-based disincentives and
more a failure to identify incentives for maintainers of software to continually identify
vulnerabilities and update code.29 But the Biden White House’s 2023 National Cybersecurity
Strategy directs the Office of the National Cyber Director to consult with academics and civil
society to grapple with the hard question of how to assign liability within the value chain from
developers to downstream users. The White House and other federal agencies should engage
with the broader open-source software/open AI model community in the development of a new
cybersecurity software liability framework that preserves the innovation potential of open AI
models.
B. U.S. Foreign Policy: Geopolitics & Geoeconomics
Although debates about a U.S. policy or regulatory approach to open AI models might not
obviously seem to implicate U.S. foreign policy interests, U.S. domestic policy on open models
is highly relevant to foreign policy objectives, particularly when it comes to China.
1. Benefits
First, a thriving open model ecosystem could produce broad regulatory harmonization with key
economic and international security partners like the European Union, India, Brazil, and Japan.
Each of these countries are seen broadly as valuable trading partners as well as vital security
partners in the context of a U.S.-China competitive dynamic. A U.S. regulatory regime,
therefore, should take a broadly similar approach to these partner countries by avoiding
differential treatment of open-source or otherwise more open AI models. A fragmented approach
could engender unnecessary diffusion in the global economy. U.S. departure from what is likely
to be a broadly common approach among democracies also would miss an opportunity to plant a
decisive global flag in favor of openness in AI model development, further isolating countries
like China and Russia whose governance models necessitate tight government control over
foundation models and whose governing ethos is largely inimical to openness and public
transparency.
Second, if the United States were to adopt a policy and regulatory approach to AI models
broadly consistent with its storied history in helping to catalyze the FOSS movement, the
benefits to its geopolitical and geoeconomic strategy would be clear. There would be broad
policy and regulatory harmonization with the EU
29 Id.
Tidelift blog, November 9, 2023. https://blog.tidelift.com/tidelift-response-to-oncd-rfi (“If security problems don’t
align with the interests and time resources of the volunteer supply chain, adding more “requirements” is not likely to
solve the problem—they will at best not respond, and at worst quit doing other maintenance activities!”
12
Third, the claim that a permissive regulatory environment in the United States for open source
model development would hand dangerous weapons to foreign adversaries fails to consider some
powerful counterarguments.The Chinese government, for example, does not want to use open
U.S. models because they fear that code might contain security exploits targeted at China, and
because the governance of those models (through code, transparency to researchers and civil
society, and broader public scrutiny) would not afford them the level of control they desire. It
would be unwise to base legislative or regulatory policy on an overstatement of risks and — in
the process — discourage the development of open AI models in the United States in ways that
run counter to broader American security and economic interests.
Fourth, stifling innovation in open models may have the effect of granting a competitive
economic advantage to countries that permit open AI models to flourish. And to the extent that
the evolution of open AI models become essential to cybersecurity and broader national security
offensive and defensive capabilities, a U.S. decision to hamper open model development could
be directly deleterious to our own national security capabilities.
2. Risks
Detractors of open-source systems often note that adversary nation-states can use powerful open
foundation models, too. While we do not dismiss this possibility, several known factors suggest
that this threat may be overstated. Hostile or potentially hostile state actors like North Korea,
Iran, and certainly China have the capacity to develop and train their own foundation models.
Indeed, many U.S. adversaries will want to train their own models to do precisely what they
want, rather than rely on code originating from the United States. In other words, any nations in
hostile or competitive relationships with the United States will be wary of, not eager, to rely on
open-source models that have been developed wholly or predominantly in the United States.30
develop their own foundation models (and already are); they don’t need open U.S. models. They
also can and are working to weaponize LLMs like ChatGPT.31
C. Transparency and Public Accountability
Models further along the gradient of openness models are more transparent than those that tend
toward the closed end of the spectrum. As noted above, model openness has many implications
for transparency and therefore for public accountability and democratic health. These benefits
need study, policy, and regulatory approaches to be realized. And openness alone cannot “solve
the problem of oversight and scrutiny.”32
32 David Gray Widder, Sarah West, and Meredith Whittaker, Open (For Business): Big Tech, Concentrated Power,
and the Political Economy of Open AI, at 2-3, August 16, 2023.
31 Id.
30 Microsoft, Staying ahead of threat actors in the age of AI, Feb. 14, 2024.
13
1. Benefits
As Kapoor and Bommasani et al. note, openness in the broader open-source software context and
the more specific context of AI foundation models furthers important objectives in transparency
and therefore accountability.33 Relatedly, more open models may also further the crucial ability
of people to be able to contest decisions made by AI models. As Jim Dempsey, Susan Landau,
Steve Bellovin, and others explain in a recent paper, true “contestability” can depend upon some
meaningful transparency into some or all of the following: model methods, criteria, code, and
data.34 More open systems, even those that do not provide access to source code itself, are
generally better to further the important objective of contestability.
The presence of more open models may also exert helpful competitive pressure that drives
improvements in transparency and explainability among more closed models. Specifically, all AI
developers and companies are likely to feel pressure to respond with more information about
model data sources, methodology, training, and risk mitigations based on a study of downstream
effects.
The better the incentives for meaningful transparency across the board, the more we drive a race
toward the top, rather than a slide to the bottom. More open models also increase pressure on
companies building closed models to provide meaningful transparency into and to open up key
design and governance questions to a broader set of stakeholders. In this sense, the presence of
successful open models can theoretically have a salutary effect on the entire ecosystem of
foundation models.
2. Risks
At the same time, the charge is that open models represent ungoverned space. While this is an
overstatement and a misunderstanding of how open-source software has been governed by
research communities for decades, the colloquial equation of open with risks may be rooted in
some concerns. In particular, as Bommasani, Kapoor, et al. detail clearly, publishing model
weights alone is an insufficient step for transparency. Similarly, in “Open (for Business): Big
Tech, Concentrated Power, and the Political Economy of Open AI,” Widder, West, and Whittaker
34 Susan Landau, James X. Dempsey, Ece Kamar, Steven Bellovin, Recommendations for Government Development
and Use of Advanced Automated Systems to Make Decisions about Individuals, March 1, 2024 (“Nonetheless,
although designing advanced automated systems to support a meaningful right of contestability is difficult, it is not
impossible—and it is often required by law, … The degree of transparency necessary to support contestability will
vary by context. Especially where the government fails to justify an outcome in a way that is understandable by the
affected individual and his or her representative, it may be necessary for advocates and litigators to delve into how
the system was constructed. Datasheets or model cards as documentation for how a system was built could enable
contestability (Mitchell et al., Ehsan et al.), but in some cases a deeper examination of methods, criteria, code and
data may be necessary along with expert analysis by those asserting challenges.”)
33 Kapoor and Bommasani et al. at 4-5.
14
remind us that meaningful openness is not enough to achieve the lofty aims of democratizing AI
or equitably distributing its benefits throughout society. They argue that this is in large part
because of the well-entrenched “concentration of power in the tech industry.”35
D. Economic Health, Innovation, and Competition
In late March 2024, the startup Databricks announced that it had succeeded in building what
might be the world’s most powerful open source large language model. “Similar in design to the
[model] behind OpenAI’s ChatGPT,” it outperformed every other open source model available,
including Meta’s LLaMA 2 and Mistral’s Mixtral, which are two leading open source (or note
quite open source, depending on one’s perspective on LLaMA). It also performed very close to
ChatGPT. Databricks says it plans to release DBRZ under an open-source license.36 There is
much yet to unpack about Databricks’ emergence onto the scene, but it is a powerful example of
companies’ will to innovate in the development of open AI models.
1. Benefits
By one Harvard Business School study’s estimation, the economic value of open-source software
is $8 trillion.37 That staggering number may be an over- or under-estimation,38 but even
arbitrarily quartering that estimate results in a remarkable figure estimating the value of open
source. Open-source software has served as a key driver of U.S. technological innovation and
economic growth, and it would be prudent for U.S. policymakers to create the conditions for
innovative open AI models to continue that trend.
2. Risks
38 Luis Villa, Eight triiiiiilllion dollars: the “new” valuation of open source, Tidelift, Feb. 1, 2024. (“[T]he number is
almost certainly too small. The authors explicitly exclude operating systems from their measures, and yet we know
that open source operating systems are (1) extremely complex to create … and (2) extremely widely deployed, both
on servers and on consumer devices (Android phones and in new home embedded devices like TVs). In addition, the
survey does not appear to capture the value of web browsers, which are second only to operating system kernels in
complexity, possibly more widely deployed than open source operating systems, and central to modern e-commerce.
A total value number that captured those missing components would likely be even larger.”)
37 Manuel Hoffman et al., The Value of Open Source Software, Harvard Business School Working Paper 24-038,
Jan. 2024.
36 Will Knight, Inside the Creation of the World’s Most Powerful Open Source AI Model, Wired, Mar. 27, 2024.
35 David Gray Widder, Sarah West, and Meredith Whittaker, Open (For Business): Big Tech, Concentrated Power,
and the Political Economy of Open AI, August (“We find that even though there are a handful of meaningfully
transparent, reusable, and extensible AI systems, these and all other ‘open’ AI exists within a deeply concentrated
tech company landscape. With scant exceptions that prove the rule, only a few large tech corporations can create and
deploy large AI systems at scale, from start to finish - a far cry from the decentralized and modifiable infrastructure
that once animated the dream of the free/open source software movement.16 Given the immense importance of scale
to the current trajectory of artificial intelligence, this means ‘open’ AI cannot, alone, meaningfully ‘democratize’ AI,
nor does it pose a significant challenge to the concentration of power in the tech industry.”)
15
Again, the risks to new entrants in the market stem from the uphill battle against entrenched tech
companies with massive training data sets at their disposal. Widder et al. argue that while open
foundation models may increase competition in some parts of the AI supply chain, they will
struggle to reduce market concentration in the highly concentrated upstream markets of
computing and specialized hardware providers.39
E. Community Control and Benefits
This section focuses more on impact on and use by people and communities than on
macro-economic health and innovation. But these issues aren’t neatly segregable and community
control over the training and application of foundation models reinforces competitive benefits
and economic vitality, and is also furthered by them.
1. Benefits and Challenges
One of the key benefits of a healthy ecosystem characterized by a prevalence of open models is
that many people can learn how the technology works. This enables technologists and
community leaders to partner in ways that are tailored to address specific community needs and
implement community-driven solutions.40
Relatedly, open-source projects can also be used to fill technological gaps that aren't being met in
the private sector. For example, the OpenCellular project aimed to make it possible for
communities not currently served by mobile network operators (MNO) to start their own. This is
achieved through open sharing not only of software, but also hardware schematics and other
plans. This could drastically reduce the cost of becoming an MNO.
The low (often zero) dollar cost for using open-source also lowers the barrier to entry for those
interested in learning skills like coding. A variety of open-source programming languages, and
training materials make it possible for someone to start coding on less than one hundred dollars
worth of equipment. The benefits for technical literacy, digital equity, and the ability of would-be
coders from all over the country to serve their communities are potentially vast.
Open data provides an analogue for the possible community benefit of open models.
Governments at every level have spent the last decade or more making a wide array of public
data available on the internet in ""machine readable"" formats. This has allowed researchers and
advocates to look into the effectiveness of government programs, or highlight matters of public
concern. From mapping urban tree canopies, to looking at healthcare outcomes in rural areas,
40 See, e.g., Bending Generative AI’s Trajectory Toward a Responsible Technology Future, Omidyar, Sep. 2023;
39 See David Gray Widder, Sarah West, and Meredith Whittaker, Open (For Business): Big Tech, Concentrated
Power, and the Political Economy of Open AI, August 17, 2023.
16
opening datasets has provided for new levels of citizen engagement and insight that were only
possible because they were available for creative new uses.
V.
Recommendations for Policy and Regulatory Approaches
It may be premature to take highly prescriptive regulatory approaches to foundation models,
whether those models fall more on the open or closed end of the spectrum. But there are at least
four broad recommendations that should guide the NTIA’s and the interagency high-level
approach to considering policy and regulatory frameworks for foundation models.
A. Study the marginal risk of open models and precisely articulate observable
harms.
NTIA and other U.S. government agencies must focus vague discussions about the risks of open
AI models on the study and precise articulation of the marginal risk these models pose. Rigorous
empirical analysis is necessary to inform thoughtful, targeted interventions. As a coalition of
academics and civil society organizations (of which OTI was a part) put it in our open letter: “We
urge you to be rigorous in evaluating and targeting the specific risks from openness in AI,
including developing better proxies for risk that are not solely based on the amount of computing
power used to train a model.”
B. Create common requirements for responsible development across the entire
gradient of openness.
Precisely because of the definitional indeterminacy we have highlighted about the terms “open,”
“open source,” and “closed,” these terms would form an unsound basis for policy or regulation
aimed at articulating differential rules or treatment depending on how a model is classified on the
gradient of openness. Apart from the lack of consensus and clarity, the mere fact that terms are
likely to remain contested for a long time suggests that policymakers who rightly feel the urge to
act swiftly should not wait indefinitely. Instead, policymakers should establish a common
baseline of governance requirements for all foundation models, rather than considering
higher regulatory burdens for certain types of open models. Taken together with a serious
study of marginal risk, the implementation of laws, policies, technical standards, and meaningful
transparency norms will help to produce public accountability and a good governance race to the
top. Such a policy approach would give upstart companies entering the marketplace and
communities across America the best possible chance to ensure that advances in AI produce
wide-ranging economic benefits.
17
C. Consider the broad range of relevant national security and foreign policy
objectives before recommending policy or regulatory action aimed at vaguely
defined or narrow types of security risks.
As discussed at length in Section IV.B, vague claims of security risks or specific articulations of
marginal risk need to be weighed against other factors like regulatory harmonization with
partners and participating in economic coalitions competing with China. These geopolitical and
geoeconomic considerations are manifestly “security” considerations that need to be balanced on
the scale when assessing the benefits and risks of more open foundation models. Returning to the
question of marginal risk, U.S. intelligence agencies and U.S. companies should continue to
closely study foreign governments’ malicious use of models along the gradient of openness. This
analysis will be vital to understanding threat vectors and strategies to address them.
D. Develop a thoughtful approach to cybersecurity software liability that
accounts for the need to incentivize innovation in open AI models.
We are without easy solutions to making changes to developer and/or downstream liability. But
the U.S. government should continue the consultative work started pursuant to the 2023 National
Security Strategy specifically with the question of open AI models (foundational or otherwise) in
mind. Open-source software and open AI model proponents should engage with the discussions
around implementing the National Cybersecurity Strategy to ensure that a liability regime and a
standard of care are explicitly developed with the implications for innovation in open AI models
in mind.
VI.
Conclusion
At OTI, we favor an ecosystem in which open AI models can flourish alongside proprietary
ones. Our analysis of the benefits and risks of open models in the context of five major U.S.
policy objectives (cybersecurity, foreign policy, public accountability, economic health and
innovation, and community control and benefits) suggests that on balance the benefits of more
open models are manifold and outweigh the risks. This does not mean that models should
develop without guardrails, but both guardrails and incentives should be broadly common across
the gradient of openness and should push and pull developers toward meaningful transparency
about model inputs and downstream effects. Section V (above) contains four key
recommendations for policy and regulatory approaches to open AI models.
Respectfully submitted,
/s/ Prem M. Trivedi
/s/ Nat Meysenburg
18
New America’s Open Technology Institute
740 15th Street NW, Suite 900
Washington, D.C. 20005
March 27, 2024
19
",NAOTI,10139
NTIA-2023-0009-0250,"Comments of the Wikimedia Foundation
In response to the
National Telecommunications and Information Administration’s
Request for Comments on
Dual Use Foundation Artificial Intelligence Models
With Widely Available Model Weights
27 March 2024
The Wikimedia Foundation appreciates the opportunity to comment on the National
Telecommunication and Information Administration’s (NTIA) Request for Comments on Dual
Use Foundation Artificial Intelligence Models with Widely Available Model Weights.1 The
Wikimedia Foundation is the nonprofit organization that hosts and supports a number of free,
online, collectively-produced resources, including Wikipedia. The Foundation’s objective is to
help create a world where every human being can share freely in the sum of all knowledge.
Widely available access to information has always been particularly important to that mission;
restrictions or limitations on access to information impair the ability to freely share, benefit from,
and improve knowledge. The Foundation has used free culture and free and open source software
copyright licenses since its inception and is currently the world’s largest repository of Creative
Commons licensed material. This also places the Wikimedia Foundation and the projects it hosts
in an important role with relation to artificial intelligence (AI) systems based on large language
models: freely licensed technology, including various machine learning (ML) tools, help to
support the quality of the Wikimedia projects and aid volunteer editors in working more
effectively and efficiently.2 For example, virtually all of the content on Wikipedia is created,
edited, and verified by volunteers. Volunteers, in conjunction with Foundation staff, also build
and maintain automated tools to help them maintain the projects’ high quality, such as tools to
detect vandalism or assess the relative “health” of articles so that volunteer editors can prioritize
their efforts. At the same time, the freely licensed high quality information and media on
2 See, e.g., Jonathan T. Morgan, Designing ethically with AI: How Wikimedia can harness machine
learning in a responsible and human-centered way, Wikimedia Foundation, (18 Jul 2019),
https://wikimediafoundation.org/news/2019/07/18/designing-ethically-with-ai-how-wikimedia-can-harness-
machine-lear ning-in-a-responsible-and-human-centered-way/; Miriam Redi et al., Can machine learning
uncover Wikipedia’s missing ‘citation needed’ tags?, Wikimedia Foundation, (3 Apr 2019),
https://wikimediafoundation.org/news/2019/04/03/can-machine-learning-uncover-wikipedias-missing-citati
on-needed-t ags/; Miriam Redi, How we’re using machine learning to visually enrich Wikidata, Wikimedia
Foundation, (14 Mar 2018)
https://wikimediafoundation.org/news/2018/03/14/machine-learning-visually-enriching-wikidata/.
1 Request for Comments, NTIA, 89 FR 14059 (26 Feb 2024)
https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-foundation-artificial-intelligen
ce-models-with-widely-available-model-weights.
1
Wikipedia and the other Wikimedia projects forms one of the most important bases for training
generative AI programs.3
We embrace these and other forms of openness as fundamental aspects of open knowledge.4 In
the context of ML and AI, we believe that openness presents many benefits. For example, the
transparency inherent to open source ML projects means that the shortcomings of a dataset—i.e.,
biases, incompleteness, and errors—can be identified and can be addressed. More broadly, open
and widely available AI models, along with the necessary infrastructure to deploy them, could be
an equalizing force for many jurisdictions around the world by mitigating historical
disadvantages in the ability to access, learn from, and use knowledge.
When considering the benefits of openness, we believe that the greatest benefits come when
source materials are sufficiently open to enable others to study, research, and modify technology.
This level of openness allows people to understand the tools that impact their lives, correct
problems caused by the tools in question, and to improve the world around them. These high
level principles inform our assessment of the risks and benefits of open source AI, discussed in
greater detail in our responses to the NTIA’s inquiry, below:
1. How should NTIA define “open” or “widely available” when thinking about foundation
models and model weights?
We urge the NTIA to carefully consider the potential consequences of creating an official
government definition of the term “open” in the context of AI models. We are concerned that,
unless the definition of “open” is phrased to require maximum transparency about AI models and
to allow maximum reuse and study of AI models, the definition would set a standard that could
negatively impact the free and open source software (FOSS) movement.5
Regardless of the wording of these definitions, we observe that the risks of the distribution of
models or model weights beyond the original developer are not dependent on either openness or
wide availability. Conversely, the benefits of such models lean strongly towards greater openness
in our view. A relatively closed model could be leaked. An open model need not be widely
available to fall into the hands of bad actors. Even if the likelihood of misuse may increase as
tools are made more open or more widely available, closed systems are an incomplete defense.
5 See Free and open-source software, Wikipedia, last accessed 20 March, 2024,
https://en.wikipedia.org/wiki/Free_and_open-source_software.
4 Open knowledge, Wikipedia (last accessed 27 Mar 2024) https://en.wikipedia.org/wiki/Open_knowledge.
3 See Selena Deckelmann, Wikipedia’s value in the age of generative AI, Wikimedia Foundation, (12 Jul
2023), https://wikimediafoundation.org/news/2023/07/12/wikipedias-value-in-the-age-of-generative-ai/.
2
In addition to providing imperfect protection, closed systems can limit the distribution of the
benefits of dual use technologies.6 In comparison, open technologies allow a broader range of
people to better understand them and create beneficial uses and modifications to them.
Therefore, we suggest that regulatory approaches should support and encourage the development
of beneficial uses of open technologies rather than depending on more closed systems to mitigate
risks.
a. Is there evidence or historical examples suggesting that weights of models similar to
currently-closed AI systems will, or will not, likely become widely available? If so, what are
they?
Within the nascent AI field, there are already examples of attempts to develop open models as
well as leaks to circumvent corporations’ efforts to provide limited access to models.7 Based on
our historical experience hosting platforms dedicated to freely and openly licensed knowledge
and software for over two decades, we anticipate that new actors who focus on these kinds of
efforts will continue to appear globally: both to deliberately develop and share open models and
to find ways to make less open models more widely available.8
As both stewards of large, open datasets and developers of ML tools, we offer our own practices
as examples. Managed correctly, data sources that are publicly available via open data licenses
can maximize transparency and reusability. The transparency inherent to open source projects
means that the shortcomings of a dataset—i.e., biases, incompleteness, and errors—are known
and can be addressed. On the Wikimedia projects, this means that anyone can participate in
identifying and addressing knowledge gaps related to gender, ethnicity, and/or other backgrounds
that exist on the platforms. Similarly, anyone can leverage this repository of culture and heritage
8 The model weights for two LLMs have been made available recently: xAI released the model weights
and network architecture for its Grok-1 large language model, Benj Edwards, Elon Musk’s xAI releases
Grok source and weights, taunting OpenAI, Ars Technica (18 Mar 2024)
https://arstechnica.com/information-technology/2024/03/elon-musks-xai-releases-grok-source-and-weight
s-taunting-openai/, and Databricks released both the base weights and the fine-tuned weights for its new
LLM, DBRX, under an open license, The Mosaic Research Team, Introducing DBRX, A New
State-of-the-Art Open LLM, Databricks (27 Mar 2024)
https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm.
7 Companies like EleutherAI are building products that attempt to match the capabilities of the most
powerful closed systems. See https://www.eleuther.ai/. Meta Platforms Inc. attempted to provide limited
access to its LLaMA model, but the model was leaked online within a week. See James Vincent, Meta’s
powerful AI language model has leaked online — what happens now?, The Verge (8 Mar 2023),
https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse.
6 Compare the impacts of both patents and trade secrets on public health and human rights: Olga
Gurgula, Strategic Patenting by Pharmaceutical Companies–Should Competition Law Intervene? IIC Int
Rev Ind Prop Copr Law (2020) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7592140/; Allison Durkin &
Patricia Ann Sta Maria, et al., Addressing the Risks That Trade Secrets Pose for Health and Rights,
Health Hum Rights, (2021) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8233014/; Julie E Zink, When
Trade Secrecy Goes Too Far: Public Health and Safety Should Trump Corporate Profits, Vanderbilt
Journal of Entertainment and Technology Law, (2020)
https://scholarship.law.vanderbilt.edu/jetlaw/vol20/iss4/4/.
3
for their own educational purposes like accessing oral histories or teaching endangered languages
so that these might live on.9
Transparency around datasets also improves understanding about how that data has been tagged
and modified to develop AI and ML tools. To support transparency in our role as open source
developers of human-centered ML tools, the Foundation creates model cards for ML models
hosted on our servers.10 Model cards are primary documentation about a model, reporting on the
reasons why it was made, proper and improper use cases, and model evaluation scores across a
variety of conditions, including across different cultural, demographic, and intersectional
groups.11 A similar concept exists for datasets, which would provide an additional layer of
transparency when evaluating a model.12
c. Should “wide availability” of model weights be defined by level of distribution? If so, at
what level of distribution ( e.g., 10,000 entities; 1 million entities; open publication; etc.)
should model weights be presumed to be “widely available”? If not, how should NTIA
define “wide availability?”
We would agree with an assumption that, once there has been a publicly accessible publication of
model weights, they are considered widely available. We would also suggest that the same
assumption holds true for making weights available through licensing options: if a distribution
model makes it possible for actors to engage in redistribution of model weights without needing
to seek permission, the weights could be considered widely available. We note that the nature of
the entities or institutions who are granted access to model weights may be an additional factor to
consider: a distribution to two entities representing access by thousands of people might be
considered differently than a distribution to two entities representing access by ten people.
We offer two other observations: First, accurate distribution metrics may be difficult or
impossible to obtain. Even if the original developer can count the number of times model
weights were accessed or downloaded, measuring secondary distribution becomes significantly
more difficult. This is true even where developers attempt to control access in some way—in the
12 Pushkarna, Zaldivar, and Kjartansson, Data Cards: Purposeful and Transparent Data Set
Documentation for Responsible AI, (3 Apr 2022), https://arxiv.org/abs/2204.01075.
11 Margaret Mitchell et al., Model cards for model reporting, (14 Jan 2019),
https://arxiv.org/abs/1810.03993.
10 Chris Albon, apply() Conference 2022 | More ethical machine learning using model cards at Wikimedia,
(31 May 2022), https://www.youtube.com/watch?v=t4GMq7MC7Js.
9 Amrit Sufi, How to help save endangered languages in India – a project on oral culture digitization,
Wikimedia Diff (8 Aug 2022),
https://diff.wikimedia.org/2022/08/08/how-to-help-save-endangered-languages-in-india-a-project-on-oral-c
ulture-digitization/; Gareth Morlais, Using technology to promote Welsh Language: Wikipedia, (7 Aug
2017),
https://digitalanddata.blog.gov.wales/2017/08/07/using-technology-to-promote-welsh-language-wikipedia/.
4
case of a breach or leak of model data, the original developer will not be able to measure the
distribution of the model by third parties.
Second, we note that the reasons for defining “widely available”—beyond the basic obligation to
follow instructions set out in the Executive Order—may be more important factors in this
determination than any particular threshold. For example, if a purpose of defining “widely
available” is to assign different obligations to developers or users of widely available model
weights to mitigate the risk of misuse, then NTIA would either need to: a) assess how risk of
misuse scales with availability and determine an appropriate distribution threshold; b) assume
that the potential harms of misuse are great enough to warrant a very low threshold of
availability for the application of such obligations; or, c) assume that obligations aimed at risk
mitigation will not deter bad actors intent on misuse, and should focus instead on different
approaches to dealing with potential harms. We suggest that the last option—i.e., c)—provides
the most benefits overall, and that attempts to limit access to dual use models will not be an
effective deterrent against misuses of AI systems.
The NTIA should also consider what factors other than availability may need to be defined or
assessed in order to better approach risk mitigation.13 For example, it may be true that the risk of
misuse correlates more strongly with the capabilities of the model than with its availability. If
that is the case, then approaches to risk mitigation may be better tied to capability rather than
availability. Additionally, we observe that making information about AI systems available for
independent research and testing is one of the most reliable ways to determine those systems’
capabilities.
d. Do certain forms of access to an open foundation model (web applications, Application
Programming Interfaces (API), local hosting, edge deployment) provide more or less
benefit or more or less risk than others? Are these risks dependent on other details of the
system or application enabling access?
In general, we believe that the most benefits flow from the most open modes of access to data,
models, and computing power. However, we acknowledge that mitigating risks and preventing
harms may become more difficult as AI technology becomes more affordable and readily
available to more people. To address this tension, we recommend considering the types of risks
associated with access to the basic inputs of most AI systems (i.e., data, models, and compute) as
well as the likelihood that restricting access to these inputs will effectively mitigate these risks.
For example, as we will discuss below, access to some types of data can increase the risk of
privacy harms because of the direct causal link between having information about a person and
13 See, generally, Kapoor and Bommasani, et al., On The Societal Impacts of Open Foundation Models,
(27 Feb 2024) https://arxiv.org/pdf/2403.07918.pdf.
5
being able to cause harm to that person—in many cases, the exposure of private information is
what causes the harm.14
It is less clear, however, what risks may be associated with access to model weights, so we
encourage policymakers to articulate their theories of risk and causation. As an analogy, anyone
with access to the internet, or even a local library, can easily learn how to construct any number
of harmful devices or compounds. With the help of a computer-assisted design program, they
could even produce schematics for producing these harmful materials. However, this knowledge
and these capabilities are not self-executing. Typically, other tools and resources are required to
implement a harmful act. If minimizing the risks of harm while amplifying the benefits of an
emerging technology is a primary goal, then policy interventions must be focused appropriately:
Limiting access to information about a technology is rarely the best point of intervention to
achieve this goal. Instead, we believe that access to information about technology is the best way
to support accountability for that technology, and that this information empowers the researchers,
developers, and policymakers who can help to mitigate potential harms.
When considering the benefits of openness, we believe that the greatest benefits come when
source materials are sufficiently open to enable others to study, research, and modify technology.
This level of openness allows people to understand the tools that impact their lives, correct
problems caused by the tools in question, and to improve the world around them. Modes of
access that limit a person to simply using a “black box” that hopefully does what they want
present a risk of exploitation, discrimination, and stagnation, and likely should not be classified
as open at all. A tool’s widespread availability or utility to the public should not factor into its
designation as open, a category which implies specific benefits related to transparency and reuse.
In some cases, openness may increase the risk for certain types of harm. As we mentioned above
and now will explain in more detail, releasing training data that contains sensitive personal
information, for instance, would likely present significant privacy and safety risks to the subjects
of that training data. In such cases, risk mitigation may require curating datasets or applying
other privacy preserving techniques prior to training and publication of models.
Finally, we do not suggest that the government should, in all circumstances, require private AI
developers to publish model weights or other AI system documentation. Instead, we encourage
the NTIA to refrain from recommending restrictions on the availability of this information and to
support regulatory policies that promote openness, especially for AI research and AI systems
funded or adopted by the government. Relying on access limitations to mitigate risks may forfeit
many of the potential public benefits of dual use AI models. As an alternative, we encourage the
14 Danielle Keats Citron & Daniel J. Solove, Privacy Harms, (2021)
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3782222.
6
NTIA to consider not just the modes of access, but also the types of content being made available
as well as the different risks and mitigation methods available for each.15
2. How do the risks associated with making model weights widely available compare to the
risks associated with non-public model weights?
With widely available model weights, there is at least the possibility of determining the
capabilities and potential performance specifications of an application using that model, as well
as of identifying vulnerabilities and designing countermeasures to mitigate harmful uses. With
nonpublic model weights, it takes a lot more work to get that kind of valuable information, if you
can get it at all, and there is still risk of misuse whether by the developer or a third party who
acquires the model weights in one way or another. Closed systems prevent good-faith actors
from helping to improve AI systems and mitigate harms, but do not prevent bad actors from
finding and exploiting flaws and vulnerabilities.16
a. What, if any, are the risks associated with widely available model weights? How do these
risks change, if at all, when the training data or source code associated with fine tuning,
pretraining, or deploying a model is simultaneously widely available?
In general, we believe that the risks associated with widely available model weights are similar
to the risks associated with the availability of information enabling the reconstruction of other
dual use technologies: the risks of misuse. As with other nascent technologies, the full range of
potential uses for foundational models is not yet known. This could make it very difficult to
anticipate the function or capabilities of a new AI system that is built from the same inputs as an
existing system, but with a different objective (i.e., to circumvent moderation filters).
However, we remain firm in our assertion that open ecosystems provide more public benefits and
help to mitigate some risks. We believe this is true because expanding access to information also
expands the number of potential good-faith contributors working to identify flaws and improve
systems.
16 Joseph Lorenzo Hall & Stan Adams, Taking the Pulse of Hacking: A Risk Basis for Security Research,
Center for Democracy & Technology (10 Apr 2018),
https://cdt.org/insights/report-taking-the-pulse-of-hacking-a-risk-basis-for-security-research/; Lillian Ablon,
Martin C. Libicki, Andrea M. Abler, Markets for Cybercrime Tools and Stolen Data: Hackers’ Bazaar,
RAND (25 Mar 2014), https://www.rand.org/pubs/research_reports/RR610.html.
15 For example, as the Executive Order requires in some cases, requiring Infrastructure as a Service
(IaaS) providers to verify the identity of and maintain usage records for certain clients who contract for
computing services could help to deter bad actors from using commercially available resources to deploy
harmful models. White House, Executive Order 14110, Sec. 4.2(c) (30 Oct 2023)
https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trustworthy-develop
ment-and-use-of-artificial-intelligence.
7
b. Could open foundation models reduce equity in rights and safety-impacting AI systems (
e.g., healthcare, education, criminal justice, housing, online platforms, etc.)?
Use of ML in safety-impacting systems does present significant risks, especially in situations
where existing training data is subject to historical biases or where algorithmic prediction
impacts a person’s rights or opportunities. However, making the operational information
available to more people generally helps to reduce the harms associated with biased models or
datasets.
Based on many years of experience with diverse populations contributing to freely and openly
licensed knowledge, we believe that following open source guidelines is one of the best ways to
address this kind of risk. Allowing people to study how models function means that they can
identify potential errors and biases that would impact lives. Allowing people to modify and
redistribute such models may also allow them to identify and implement improvements, correct
biases and errors, and potentially propose improvements that governments and private actors
deploying the models can adopt. At the same time, we recommend that the deployment of
safety-impacting ML systems undergo appropriate review processes, which would help prevent
the deployment of harmful models.
c. What, if any, risks related to privacy could result from the wide availability of model
weights?
It remains uncertain to what extent it is possible to reverse engineer or extract training data
details by starting with model weights. Based on recent research, it may be possible for someone
to use an open model trained on private data to extract identifiable data about individuals
included in a training data set.17 We note, however, that this kind of manipulation appears to be
available in a variety of contexts, even in relatively non-open models, and likely would only be
prevented in the limited circumstance in which the deployer of the model tightly controls who
can make use of it.18
An additional risk is the use of widely available model weights to support a generative AI system
capable of reproducing the likenesses of existing individuals. Without appropriate guardrails,
these systems could be used to impersonate, blackmail, or cause reputational harm to people. We
observe that technology capable of supporting such acts has been available for decades, but note
that the sophistication of AI tools and the ability to generate content, quickly, at low cost, and on
a large scale may require novel approaches to mitigate the potential harms. However, we suggest
18 Tiernan Ray, ChatGPT can leak training data, violate privacy, says Google's DeepMind, ZDNet (4 Dec
2024)
https://www.zdnet.com/article/chatgpt-can-leak-source-data-violate-privacy-says-googles-deepmind/.
17 Jeffery Wang and Jason Wang, et al., Pandora’s White-Box: Increased Training Data Leakage in Open
LLMs (2024) https://arxiv.org/abs/2402.17012.
8
that attempting to prevent harmful uses of technology by regulating its availability is unlikely to
be an effective method when compared to focusing on deterring and defending against specific
harmful uses of the technology.
d. Are there novel ways that state or non-state actors could use widely available model
weights to create or exacerbate security risks, including but not limited to threats to
infrastructure, public health, human and civil rights, democracy, defense, and the
economy?
i. How do these risks compare to those associated with closed models?
The risk of state or non-state actors using AI to threaten security, public health, rights,
democratic processes, and more is perhaps more strongly correlated with factors other than the
relative availability of the model: for instance, the model’s capabilities or the threat actor’s
available resources. Consider the following contrast: A closed model that offers sufficient cloud
infrastructure resources to generate custom content at scale could empower disinformation
campaigns, while a malicious actor with access to an open model but without sufficient cloud
infrastructure resources of their own for deployment might struggle to leverage the model’s
capabilities effectively. We further observe that closed models are also subject to attacks that
exploit flaws or vulnerabilities in the system’s defenses.19
ii. How do these risks compare to those associated with other types of software systems and
information resources?
Perhaps the biggest difference between the models underpinning emerging AI systems and other
types of software resources is that the future capabilities of and use cases for these new AI tools
are relatively unknown. This makes it more difficult to anticipate and plan for potentially
harmful uses.
Very large AI models may compound this phenomenon by bringing two potent capabilities
together in ways that may present new risks: recognizing incredibly complex patterns, and
making predictions based on those patterns, both of them at scale. For example, a bad actor could
deploy a system to generate large quantities of disinformation, spam, or fraudulent materials
engineered to be especially convincing and compelling for a variety of niche audiences. This is
not entirely new—automation of many kinds has existed for years, such as that used maliciously
in distributed denial-of-service (DDoS) attacks—nor is the mere generation of manipulative
content harmful without an effective distribution mechanism. Nonetheless, the combination of
scale and the ability to use generative AI to produce material that appears relatively high-quality
19 Nicole Clark, ‘Grandma exploit’ tricks Discord’s AI chatbot into breaking its own ethical rules, The Verge
(19 Apr 2023), https://www.polygon.com/23690187/discord-ai-chatbot-clyde-grandma-exploit-chatgpt.
9
all together presents new challenges. Because the range of applications of these new capabilities
is unknown, it is difficult to predict where and how they might be exploited to cause harm.
e. What, if any, risks could result from differences in access to widely available models
across different jurisdictions?
One of the benefits of making models widely available is that, in theory, more people across all
jurisdictions can access and use them. However, where access to tools like predictive models or
the necessary infrastructure to deploy them is limited or unequal, those without the ability to use
these powerful tools could face increasingly disadvantaged positions compared to others who do
have it. Access to technology has long been a deciding factor in the distribution of wealth and
competition for resources. Widely available models, along with the necessary infrastructure to
deploy them, could be an equalizing force for many jurisdictions or, at least, assist them in
keeping pace.
In addition to the equity considerations of unequal access to AI technology, we note that some
governments may be less assertive in regulating to mitigate harms. Some may even encourage
and support the use of AI models by non-state actors to harm the governments’ rivals, whether
through AI-supported disinformation campaigns or other attacks. Despite these risks, controlling
model weight availability is not the answer. To reiterate, we believe that making AI technology
more open will provide the greatest benefits for humanity, and caution that attempts to mitigate
harms through the use of closed systems is unlikely to deter those who wish to use AI to cause
harm.
3. What are the benefits of foundation models with model weights that are widely available
as compared to fully closed models?
In general, we believe that open systems are more likely to deliver benefits to society than closed
systems. Although the question above relates specifically to model weights—only one
component of a functional AI system—we believe this generalization holds true. These systems
are: a) powerful tools, which b) use incredibly complex internal processes, and c) could be
applied in an unknown number of use cases. Each of these factors weighs in favor of allowing
the public to study, understand, improve, and adapt such tools.
The study of open models helps us to understand their strengths and weaknesses, the extent of
their abilities, and the best ways to refine and deploy them. Public understanding of models helps
people use them more effectively and appropriately, and to make more informed decisions about
the benefits, risks, and regulation of AI systems. The ability to improve models and their
applications helps those who wish to use existing systems built on those models as well as those
10
who wish to build new systems. Finally, the ability to adapt and repurpose models to perform
different functions or to amplify certain features of a model’s performance increases the diversity
of uses to which models might be applied, creating new tools beyond those envisioned by the
original developers.
a. What benefits do open model weights offer for competition and innovation, both in the
AI marketplace and in other areas of the economy? In what ways can open dual-use
foundation models enable or enhance scientific research, as well as education/training in
computer science and related fields?
We anticipate significant benefits to researchers, who can help improve our understanding of
how deployed tools affect people, institutions, and public interests. We also anticipate significant
benefits to smaller communities, who can use open models to research tools, identify likely
biases and errors, and modify them for their communities’ unique needs where appropriate.
For example, the Foundation uses open source software to develop ML tools in collaboration
with volunteer contributors. Such ML tools, in turn, assist volunteer editors in their work to build
and maintain Wikipedia and other Wikimedia projects. But it is the volunteers, the humans of
Wikipedia, who ensure that ML tools are beneficial to the Wikimedia projects and people. This is
in part due to the human-centric design and development process the Foundation uses, which
accounts for the needs of the volunteers who will use these tools through a collaborative design
process.20 Furthermore, the benefits flow primarily from volunteers’ responsible use of ML tools
to augment, not replace, their own human capabilities. Although most of these systems are
designed to serve specific purposes in the Wikimedia project environment, they are also
transparent and open source, available for anyone to inspect, test, improve, or modify for a new
purpose.21
In the future, open source dual use AI models could further enhance Wikipedia volunteers’
capacity to verify, document, and preserve the world’s knowledge, and to make that knowledge
available across the spectrum of human languages as well as accessible for persons with
disabilities. Although the ML tools that volunteer editors presently use now are simpler than
today's most sophisticated AI models, open source technology combined with human-centric
practices will continue to bring benefits and stability to this oftentimes volatile technological and
regulatory space. Even if the future uses of open source AI systems are hard to predict,
empowering people to use these systems for good serves the public interest.
21 Wikimedia Meta-Wiki, Machine learning models, (last visited 27 Mar 2024),
https://meta.wikimedia.org/wiki/Machine_learning_models#; Mediawiki,
https://www.mediawiki.org/wiki/MediaWiki.
20 Mediawiki, Wikimedia Product/Inclusive Product Development (last visited 25 Mar 2024),
https://www.mediawiki.org/wiki/Wikimedia_Product/Inclusive_Product_Development.
11
Finally, we observe that open models offer more opportunities than just recreating or reusing
particular model weights. Publishing model documentation might also reveal things like novel
approaches to neural network architecture or optimization techniques that could be applied in
many additional contexts.
We encourage the NTIA to recommend policies that support open technology and the public
benefits it can enable, and to suggest appropriate limitations and exceptions to policy
recommendations that would otherwise inhibit openness in the design, development, and
deployment of technology.
b. How can making model weights widely available improve the safety, security, and
trustworthiness of AI and the robustness of public preparedness against potential AI risks?
In addition to the public interest benefits flowing from open source technologies, the
technologies themselves benefit from being open to research, testing, and review. We call the
NTIA’s attention to the demonstrated benefits of independent computer security research and
note that, regardless of whether a model’s weights are widely available, vulnerabilities in the
model might still be identified and exploited to cause harm.22 To quote renowned software
engineer Linus Torvald: “Given enough eyeballs, all bugs are shallow.”23 We believe this axiom
holds true for large dual use AI models as well. Researchers trying to help make AI systems
more secure, resilient, and resistant to abuse or harmful uses can do some work, even with closed
systems, but their contributions grow with increased access to source materials like training data,
model weights, source code, and training methodology.
This kind of access and testing is important even for more closed systems: if the system’s
security is to be an effective barrier to accessing model weights or other system specifications,
then researchers also need to be able to test that security, in good faith, without fear of legal
retribution.
In either context, whether the functioning of the system or the security that protects it, obscurity
is not an adequate defense. More eyeballs, more researchers, more people using and studying the
way technology works can improve that technology. Attempting to keep systems closed,
especially those with many potentially beneficial applications, will produce fewer benefits than
making those systems open, even if there are inherent risks that the same technology might be
used to cause harm.
23 See Linus's law, Wikipedia, (last visited 20 Mar 2024), https://en.wikipedia.org/wiki/Linus%27s_law.
22 Nathan Van Buren v. United States, Brief of Amici Curiae Computer Security Researchers, Electronic
Frontier Foundation, Center for Democracy & Technology, Bugcrowd, Rapid7, Scythe, and Tenable in
Support of Petitioners, 19-783 (US 2021)
https://www.supremecourt.gov/DocketPDF/19/19-783/147214/20200708124706913_19-783%20Amici%2
0Brief.pdf; Matt Burgess, The Security Hole at the Heart of ChatGPT and Bing, Wired, (25 May 2023)
https://www.wired.com/story/chatgpt-prompt-injection-attack-security/.
12
Regarding trustworthiness in particular, making technical specifications available may be
especially important. Although we believe that people are best served by sources of information
created and verified by humans, we understand that the ways humans retrieve and digest
information are changing. From that perspective, we think it is especially important to be able to
understand, test, and improve the AI systems that may become critical intermediaries between
people and knowledge. Or, if those systems prove incapable of consistently providing
high-quality information in response to human inquiries, then we suggest that making more
information about predictive models available will help people understand the limitations of such
technologies, and aid the development of new approaches to accessing and distributing
knowledge.
d. How can the diffusion of AI models with widely available weights support the United
States' national security interests? How could it interfere with, or further the enjoyment
and protection of human rights within and outside of the United States?
In the case of Wikipedia, the deployment of transparent AI models and tools can support human
beings in curating and expanding reliable knowledge online and combating disinformation at
scale. This supports the general public's right to freedom of expression by keeping reliable
information widely accessible, while defending national security by serving as an effective
deterrent, watchdog, and mitigation against coordinated disinformation campaigns by malicious
actors.
e. How do these benefits change, if at all, when the training data or the associated source
code of the model is simultaneously widely available?
We propose that the benefits of openness increase as more information is made available. This
may be especially true in the context of AI system model weights, training data, and source
code—each is a key component of the composite system, with its own unique impacts on the
capabilities, functionality, flaws, and vulnerabilities of the system. Making one of these
components accessible is helpful for research and understanding, but making all three available,
in addition to the training methodology, greatly improves the depth, nuance, and detail possible
for researchers and developers who wish to study an AI system.
We identify four ways that fully open models can improve the benefits of AI systems. First,
access to training data, source code, and model weights means that students, researchers,
companies, and governments can learn any new or interesting techniques that were used to train
the model. This broad awareness can inspire significant and beneficial cross-pollination of ideas.
Second, access to datasets means that content producers—like Wikipedia editors—can see how
their data is being used in these models. Third, access to datasets and source code allows
researchers to identify and communicate biases found in the system. Fourth, access to datasets
and the source code used in automated dataset curation makes it easier to determine whether
13
private data was included in the model’s training so that privacy remediations can be
implemented. As explained in response to Question 2(d), however, where training datasets are
known to contain large proportions of sensitive personal data, developers must take care when
releasing dataset documentation.
4. Are there other relevant components of open foundation models that, if simultaneously
widely available, would change the risks or benefits presented by widely available model
weights? If so, please list them and explain their impact.
There are four components of a foundation model that allow for complete open source rights
(unrestricted access, use, and modification). These are:
1) The source code to run the model,
2) The model weights,
3) The training dataset, and
4) The training methodology, including any source code used to automate aspects of the
training.
As noted in response to Question 2, it may not always be appropriate to fully release all four of
these components. Making all four types of information available maximizes the benefit in terms
of study, testing, and modification. However, fully open systems can also increase privacy risks
and risks of other harms. Even so, we note that these risks are not eliminated in fully closed
systems and that making system information more available is, on balance, the more socially
beneficial option.
Additional components, such as high-capacity computing hardware and reliable energy supplies,
are necessary to enable current state-of-the-art systems to serve millions of users or to operate at
maximum capacity over a period of time. Lacking this infrastructure will limit a dual use model’s
potential impacts.
5. What are the safety-related or broader technical issues involved in managing risks and
amplifying benefits of dual-use foundation models with widely available model weights?
b. Are there effective ways to create safeguards around foundation models, either to ensure
that model weights do not become available, or to protect system integrity or human
well-being (including privacy) and reduce security risks in those cases where weights are
widely available?
14
We support the approach that the Executive Order takes toward addressing these risks, that is to
say, by requiring Infrastructure as a Service (IaaS) providers to keep records in order to identify
their customers and report acquisitions of computing services above a certain capacity
threshold.24 We believe that this approach strikes the right balance to allow good faith research
and development of dual use models while discouraging bad actors and helping enforcers take
action against entities seeking to develop or deploy harmful applications.25 We note, however,
that nation-states and very wealthy individuals could acquire computing resources through other
channels, without triggering the reporting requirement.
f. Which components of a foundation model need to be available, and to whom, in order to
analyze, evaluate, certify, or red-team the model? To the extent possible, please identify
specific evaluations or types of evaluations and the component(s) that need to be available
for each.
We call the NTIA’s attention to the four basic components we identify in our response to
Question 4: model weights, training data, source code to run the model, and any source code
used to automate aspects of the training.
7. What are current or potential voluntary, domestic regulatory, and international
mechanisms to manage the risks and maximize the benefits of foundation models with
widely available weights? What kind of entities should take a leadership role across which
features of governance?
d. What role, if any, should the U.S. government take in setting metrics for risk, creating
standards for best practices, and/or supporting or restricting the availability of foundation
model weights?
We would strongly encourage governments to require open source documentation for any AI
research they fund and any AI tool they adopt, leaning toward as much disclosure as is safe to
do, so that people can understand how government decisions that affect them are being made by
these systems and identify potential risks and problems. Open source acquisition policies also
help to ensure that that the government is not entirely beholden to proprietary owners, which
25 As a matter of law, it is unclear whether the compelled disclosure of such customer records would be
limited by statutes such as 18 U.S.C. 2702.
24 White House, Executive Order 14110, Sec. 4.2(c) (30 Oct 2023)
https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trustworthy-develop
ment-and-use-of-artificial-intelligence.
15
presents its own kind of risk. Consistent with our response to Question 1, we encourage the
NTIA to maintain its high standards for what qualifies as “open” in the context of AI models.26
e. What should the role of model hosting services ( e.g., HuggingFace, GitHub, etc.) be in
making dual-use models with open weights more or less available? Should hosting services
host models that do not meet certain safety standards? By whom should those standards be
prescribed?
The roles and responsibilities of hosting services should not be defined by government entities.
These services fit the definition of “interactive computer services” under Section 230 of the 1996
Communications Decency Act (CDA), and should remain free to determine their own policies,
standards, and practices for moderating the content that they host.27 We understand that the
leading hosting services already employ many methods to assess the safety and security of the
models uploaded to their platforms, and note that even an array of robust screening measures
provides an incomplete defense against potentially malicious material.28 Although there are risks
related to the hosting of third-party content, we contend that these risks are outweighed by the
benefits of facilitating the sharing of models, datasets, source code, and other components of AI
systems.
f. Should there be different standards for government as opposed to private industry when
it comes to sharing model weights of open foundation models or contracting with
companies who use them?
Yes, the government should insist on maximum disclosure of information about models and
applications that are provided to the government by private contractors or developed using public
funding. Models developed or funded by the government should be completely open by default.
Private industry should remain free to make information about their products available as they
see fit. However, there may be some circumstances in which public interest dictates the
mandatory disclosure of enough information about privately developed technologies to allow
public research for accountability purposes.
28 Bill Toulas, Malicious AI models on Hugging Face backdoor users’ machines, Bleeping Computer (28
Feb 2024),
https://www.bleepingcomputer.com/news/security/malicious-ai-models-on-hugging-face-backdoor-users-
machines/.
27 47 U.S.C. §230.
26 The definition used by the US Department of Commerce includes the correct elements, but may benefit
from additional clarity regarding the non-software components of an AI system. United States Department
of Commerce, Open Source Code, https://www.commerce.gov/about/policies/source-code.
16
",Wikimedia,9665
NTIA-2023-0009-0324," 
 
 
 
 
 
 
1 
Mr. Travis Hall 
National Telecommunications and Information Administration 
U.S. Department of Commerce 
1401 Constitution Ave NW 
Washington, DC 20023 
 
March 27, 2024 
 
Re: ITI Feedback to NTIA RFC on Dual Use Foundation AI Models with 
Widely Available Model Weights (Docket No. 240216-0052; RIN# 0660-
XC06) 
 
Dear Mr. Hall, 
 
The Information Technology Industry Council (ITI) welcomes the opportunity to provide feedback to the 
National Telecommunications and Information Administration (NTIA) Request for Comment (RFC) 
Related to NTIA’s assignment Under Sections 4.6 (a) of the Executive Order Concerning Artificial 
Intelligence on dual use foundation artificial intelligence models with widely available model weights 
(E.O.). 
 
ITI represents the world’s leading information and communications technology (ICT) companies. We 
promote innovation worldwide, serving as the ICT industry’s premier advocate and thought leader in the 
United States and around the globe. ITI’s membership comprises leading innovative companies from all 
corners of the technology sector, including hardware, software, digital services, semiconductor, network 
equipment, and other internet and technology-enabled companies that rely on ICT to evolve their 
businesses. Artificial Intelligence is a priority technology area for our member companies, who are both 
developing and using the technology to evolve their businesses.  
 
ITI is committed to fostering the responsible development and deployment of AI. We have been actively 
engaged in shaping AI policy around the world. In 2021, we issued a set of Global AI Policy 
Recommendations, aimed at helping governments facilitate an environment that supports AI while 
simultaneously recognizing that there are challenges that need to be addressed as the uptake of AI 
grows around the world.1 We also launched our AI Futures Initiative in 2023, an initiative comprised of 
technical and policy experts aimed at addressing challenging questions that are emerging in the global 
conversation on AI. We have published several policy papers via this Initiative, including on the AI Value 
Chain and Foundation Models, and on AI-Generated Content Authentication, both of which we think are 
particularly relevant to NTIA’s RFI.2 We have also actively worked to inform the efforts of the National 
 
1 Our complete Global AI Policy Recommendations are available here: https://www.itic.org/documents/artificial-
intelligence/ITI_GlobalAIPrinciples_032321_v3.pdf  
2 ITI’s Guide to AI Content Authentication available here: 
https://www.itic.org/policy/ITI_AIContentAuthorizationPolicy_122123.pdf and ITI’s Understanding Foundation 
Models & the AI Value Chain paper available here: https://www.itic.org/documents/artificial-
intelligence/ITI_AIPolicyPrinciples_080323.pdf  
 
 
 
 
 
 
 
2 
Institute of Standards and Technology (NIST)3 to create an AI Risk Management Framework (RMF) and 
have consistently contributed to the debate in the EU on its AI Act.  
 
General Feedback  
Below, we offer several high-level points, followed by answers to more specific questions that NTIA poses 
in the RFC. 
 
• 
NTIA should consider closed and open models to include a spectrum of foundation models. We 
appreciate how NTIA highlights the difference between open and closed foundation models by 
citing relevant examples for each category. NTIA should also note that some foundation models 
do not neatly fall into either category, and can include other models that may have features of 
either broad group. As we explain below in our response, experts classify openness on a gradient 
with the following categories: Fully closed, hosted access, API access to model, API access to fine 
tuning, weights available, weights, data, and code available with and without use restrictions.4  
 
Furthermore, the choice between leveraging open or closed models presents its own set of 
considerations. Open models or foundation models with widely available model weights foster 
collaboration and rapid iteration because they are accessible and customizable by a wide 
community of developers and researchers. On the other hand, closed models, which are 
proprietary and tightly controlled, might be preferred in scenarios where data privacy or 
intellectual property concerns are paramount. 
 
• 
NTIA should consider that access to different component parts of open foundation models 
may change the risk and benefit calculus. For example, access to model weights alone may 
present a limited risk, while access to model weights plus source code could marginally increase 
risks and benefits (such as enhancing research and transparency) because it could grant a user 
the ability to make more significant changes to the model. For more sophisticated actors, the 
ability to use other existing source code to finetune a model will remain regardless of whether 
the code is shipped with the weights.    
 
• 
NTIA should clarify that not all open foundation models or foundation models with widely 
available weights are dual use foundation models as defined by the E.O. In considering open 
foundation models / foundation models with widely available weights, NTIA should recognize 
that not all of them possess the characteristics or capabilities outlined in the E.O., which 
constitute a dual-use foundation model. Treating all open foundation models as dual use 
foundation models with widely available weights would adversely impact innovation and 
competitiveness of many actors across the AI ecosystem. 
 
 
3 See ITI response to RFI on AI RMF Concept Paper here: ITI Comments on AI RMF Concept Paper FINAL.pdf 
 
4 Bommasani, Rishi, et al. ""Issue Brief Considerations for Governing Open Foundation Models | Stanford HAI."" 
Stanford University Human-Centered Artificial Intelligence, 13 Dec. 2023, https://hai.stanford.edu/issue-brief-
considerations- governing-open-foundation-models. 
 
 
 
 
 
 
 
3 
• 
NTIA should adopt a risk-based approach with respect to open foundation models since not all 
models pose an equivalent degree of risk.  In considering policy options to present in its report, 
NTIA should at this stage focus on open foundation models that rise to the level of dual use – 
these are foundation models that may pose a serious security risk, a risk to national/economic 
security, or a risk to national public health and safety. A ‘one size fits all’ approach would 
disproportionately impact researchers, academics and developers of open foundation models 
that do not rise to the level of dual use. Broader policy options for open foundation models 
should be considered once an appropriate approach to closed models, and any marginal risk 
posed by open models, are better understood. 
 
• 
NTIA should acknowledge that risk management is a shared responsibility across the AI value 
chain. This approach is consistent with the way we have discussed the AI value chain in our 
Understanding Foundation Models & the AI Value Chain paper, which reflects the need for 
different actors to share responsibility across the value chain. This is especially important 
because once foundation models with widely available weights are deployed, developers are not 
able to retract said model, even in cases where that model is being used in malicious ways.  
 
• 
NTIA should work with stakeholders across the AI value chain. NTIA would benefit from hearing 
from a range of diverse stakeholders in the AI value chain, including developers, deployers and 
end users. Foundation models with widely available weights and closed models can be used and 
deployed under various contexts. There are clearly downstream effects when deploying these 
models, which is why it is important for NTIA to consult with all stakeholders across the AI value 
chain. 
 
• 
NTIA should prioritize evidence based and scientifically informed policymaking while 
considering recommendations for dual use foundation AI models with widely available 
weights. Any recommendations addressing risks of dual use foundation models with widely 
available weights should follow guidance developed by NIST through the U.S. AI Safety Institute, 
which is tasked with studying and identifying mitigations to AI safety risks under the E.O. 
 
• 
NTIA, and the USG more broadly, should ensure robust international cooperation and 
coordination. We appreciate that the overarching E.O. from which the RFC stems recognizes the 
importance of working with key international allies to further bolster innovation and address 
risks. The U.S. should remain engaged in AI policy discussions in both bilateral and multilateral 
fora, and look to progress discussions around open foundation models because this is a 
conversation that is taking place across jurisdictions. 
 
Specific Responses 
 
Please find below our response to several of the questions posed by NTIA. 
 
1. How should NTIA define “open” or “widely available” when thinking about foundation models 
and model weights? 
 
 
 
 
 
 
 
 
4 
In defining “open” foundation models, we encourage NTIA to consider that there is a gradient of 
“openness” for foundation models. Experts classify this gradient using the following categories: fully 
closed, hosted access, API access to model, API access to fine tuning, weights available, weights, data 
and code available with and without use restrictions.5  
  
We appreciate that later in the RFC, NTIA specifically asks a question about if and how risks vary 
depending on which components of an AI model are available (e.g. model weights, model weights AND 
source code, model weights, source code, AND data sets, etc), which demonstrates to us that there is in 
fact a recognition that the risk a model poses depends on the interplay of the three elements. That said, 
we think it is worth reiterating again and encourage NTIA to clearly explain this in its forthcoming report.  
 
c. Should “wide availability” of model weights be defined by level of distribution? If so, at what 
level of distribution (e.g., 10,000 entities; 1 million entities; open publication; etc.) should model 
weights be presumed to be “widely available”? If not, how should NTIA define “wide 
availability?” 
 
In thinking through how to appropriately define “widely available,” we encourage NTIA to consider 
whether “widely available” is actually a realistic proxy for risk. “Wide availability” as a metric may 
inadvertently overlook the functionality of the model, which may also present risk in a way that simple 
availability does not. To be sure, a model could be widely available but present limited risk; or, it could 
have limited availability but be significantly risky. The definition should ideally reflect a balance that 
allows for broad use, competition, and innovation while managing risks associated with the deployment 
of the specific AI model. 
 
2. How do the risks associated with making model weights widely available compare to the risks 
associated with non-public model weights? 
 
ITI and its members acknowledge that risks arise from the malicious use of both widely available and 
non-public model weights. We explore several of these risks in our paper on Understanding Foundation 
Models and the AI Value Chain.6 It is important to note that in many instances, risks arising from the 
introduction of models with open weights and risks arising from models with closed weights are not 
significantly different. Risks arising from malicious use exist; however, more research and analysis is 
required to ground policy interventions in a way that will help address said risks.  
 
The risk calculus may change because of the ease with which a malicious actor may be able to access 
and fine-tune an open model, where it might be more difficult to leverage a closed model for malicious 
purposes given said models cannot be fine-tuned or adapted as easily (though this not to say that they 
cannot be). Additionally, once an open model is released, the developer no longer has control over how 
it is used – it cannot be pulled back.  
 
5 Bommasani, Rishi, et al. ""Issue Brief Considerations for Governing Open Foundation Models | Stanford HAI."" 
Stanford University Human-Centered Artificial Intelligence, 13 Dec. 2023, https://hai.stanford.edu/issue-brief-
considerations- governing-open-foundation-models. 
6 ITI’s Understanding Foundation Models & the AI Value Chain paper available here: 
https://www.itic.org/documents/artificial-intelligence/ITI_AIPolicyPrinciples_080323.pdf  
 
 
 
 
 
 
 
5 
 
Although open models may, in certain instances, present an increased risk due to their unique 
characteristics, it is important to determine if and how that risk differs from existing risks (and if and how 
it differs from risks presented by closed models). The Framework introduced by Stanford for measuring 
the marginal risk of open foundation models is something that may be worth further exploring.7    
Overall, any policy interventions and governance models should be dynamic and adaptable to the 
evolving landscape of AI technology.  
 
3. What are the benefits of foundation models with model weights that are widely available as 
compared to fully closed models? 
 
a. What benefits do open model weights offer for competition and innovation, both in the 
AI marketplace and in other areas of the economy? In what ways can open dual-use 
foundation models enable or enhance scientific research, as well as education/training in 
computer science and related fields? 
 
There is growing evidence8 about the benefits of open foundation models. This includes enabling 
competition, catalyzing innovation and facilitating transparency.9 Additionally, widely available model 
weights can help to democratize access and use of AI systems, allowing a greater number of users to 
contribute to AI development processes. 
 
Widely available model weights also allow for independent evaluations of software by a wider 
community of developers, enabling them to identify vulnerabilities and test for safety issues. This 
openness promotes the adoption of robust cybersecurity practices since a diverse pool of experts are 
able to effectively evaluate model components (training data, model weights, source code) to mitigate 
risks that may otherwise go unnoticed, which in turn can lead to improved and safer foundation models. 
These practices can also support the establishment of verifiable benchmarks for model performance in 
safety and compliance. 
 
Widely available model weights may also reduce the environmental impact of training large models, 
which use significant amounts of energy. Open model weights can also promote a more competitive and 
diverse ecosystem by reducing market concentration and barriers to entry. 
 
Further, fundamental research in AI relies on transparent collaboration in the development of models as 
a primary method for research communities to collaborate. Models with open weights, code and data 
have become as important to AI research as traditional publication and conferences because they 
provide reference implementations and examples of research, and testbeds for collaboration in the 
research community. For example, the field of large language models (LLMs) began with open, 
 
7 Kapoor and Bommasani et al, On the Societal Impact of Open Foundation Models, Stanford HAI, 2023.  
8 Seger, Dreksler, Moulange et al, Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, 
benefits, and alternative methods for pursuing open-source objectives, Centre for the Governance of AI, 2023. 
Available here: https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf.  
9 Kapoor and Bommasani et al, Considerations for Governing Open Foundation Models, Stanford HAI, 2023.  
Available here: https://hai.stanford.edu/issue-brief-considerations-governing-open-foundation-models 
 
 
 
 
 
 
 
6 
collaborative research including the seminal BERT model that was an implementation of the new 
Transformer approach that is considered the critical breakthrough that made LLMs possible. BERT was 
released under the MIT open-source license. 
 
Finally, widely available model weights are able to promote innovation allowing actors across the AI 
value chain to create new economic opportunities in diverse fields such as marketing, communications, 
medicine, education, and employee training. Developers and deployers can customize their models 
depending on the specific use case. 
 
b. How can making model weights widely available improve the safety, security, and 
trustworthiness of AI and the robustness of public preparedness against potential AI 
risks? 
 
Foundation models with widely available model weights provide upstream and downstream users with 
the ability to make more informed decisions and effectively work with the models that they are training. 
Additionally, with access to weights and other components, developers and deployers can adapt models 
more quickly, therefore saving time and resources. This makes using foundation models more cost-
effective, business friendly, and allows for a broad range of applications. 
 
Widely available model weights can also help facilitate transparency. In line with ITI’s prior positioning 
(see, for example, our paper on the AI Value Chain & Foundation Models10), transparency is important 
throughout the AI value chain, and is especially important to risk management. Access to model weights 
can help to provide insight into the ways in which certain models were developed. Researchers can use 
the weights to analyze the strength of connections between different components or links within the 
model, thus potentially revealing information about how the model prioritizes various factors.11  
 
c. Could open model weights, and in particular the ability to retrain models, help advance 
equity in rights and safety-impacting AI systems (e.g. healthcare, education, criminal 
justice, housing, online platforms etc.)? 
 
Weights can help to influence the performance of AI model outcomes. As weights are fine-tuned, so are 
the accuracy of outcomes. Having weights available makes retraining models more cost effective for 
users and can promote more accurate models. Additionally, testing also plays a significant role in 
improving model performance and accuracy. 
 
We also stress that more expertise is required to work with model weights alone, which may serve to 
limit the circle of capable developers. Along the gradient of openness, a model with only adjustable 
weights is limited in customizability compared to a model which is radically open, meaning that anyone 
has full access to all components of including data, weights, and source code.  
 
 
10 See ITI’s Understanding Foundation Models & The AI Value Chain paper here: 
https://www.itic.org/documents/artificial-intelligence/ITI_AIPolicyPrinciples_080323.pdf 
11 Prompt Engineering Institute. (2023, December 1). Openness in Language Models: Open-Source vs Open Weights 
vs Restricted Weights. https://promptengineering.org/llm-open-source-vs-open-weights-vs-restricted-weights/ 
 
 
 
 
 
 
 
7 
In areas where concerns surrounding sensitive data processing and security are most acute, NTIA should 
consider how models with widely available weights could be beneficial. As discussed above, these types 
of models are oftentimes more conducive to operating within a given organization or business’ network, 
which would allow the model to operate in a more isolated environment. This would make it easier to 
isolate the model’s processing of sensitive data (personal or other proprietary data) to within a 
company’s own systems and to monitor that environment for any communication attempts made to 
external systems, which is indicative of command and control or data exfiltration behaviors. 
 
4. Are there other relevant components of open foundation models that, if simultaneously 
widely available, would change the risks or benefits presented by widely available model 
weights? If so, please list them and explain their impact. 
 
The risk-benefit landscape can be altered by considering other relevant components including training 
data, source code and model architecture, evaluation metrics, usage guidelines and pre-trained data. 
a. Training data: Making datasets that AI models are trained on (open) can increase 
transparency. However, at the same time, it is important for companies to manage this 
data carefully to protect personally identifiable information and prevent misuse. 
 
b. Model architecture and source code: The availability of these components can 
accelerate innovation by enabling developers and deployers to customize and improve 
upon existing models.  
 
c. Evaluation metrics: Broadening access to evaluation criteria can lead to a more 
standardized and transparent methodology in assessing AI performance. 
 
d. Documentation and usage guidelines: Their availability can guide responsible use, 
helping users understand the models' capabilities and limitations, and prevent misuse. 
 
e. Pre-trained AI models: The availability of pre-trained models, beyond just the weights, 
includes configurations and tuning parameters. This can significantly lower the barrier to 
entry for using advanced AI technologies, democratising access. 
 
5. What are the safety related or broader technical issues involved in managing risks and 
amplifying benefits of dual use foundation models with widely available model weights? 
 
What model evaluations, if any, can help determine the risks or benefits associated with making weights 
of a foundation model widely available? 
 
Best practices for ensuring safety of AI models may include red teaming AI models, or testing models for 
flaws or other vulnerabilities before they are released. The recently launched U.S. AI Safety Institute will 
be an important venue to facilitate collaboration on ways to test, evaluate, verify, and validate open 
models, including establishing guidelines to help determine the benefits and risks associated with widely 
available model weights. 
 
6. What are the legal or business issues or effects related to open foundation models? 
 
 
 
 
 
 
 
8 
 
a. In which ways is open-source software policy analogous (or not) to the availability of 
model weights? Are there lessons we can learn from the history and ecosystem of open-
source software, open data, and other “open” initiatives for open foundation models, 
particularly the availability of model weights? 
 
The history of open-source software and other “open” initiatives can, in certain ways, provide helpful 
reference points for the conversation taking place around open model weights. There are a few key 
themes that underpin the open-source software conversation that may provide useful analogs or at least 
a conceptual frame for open model weights.   
 
• 
Transparent Collaboration. The concept of transparent collaboration underpins open-source 
software. Software developers with a variety of affiliations, such as commercial, academic, and 
government, as well as individuals, can together access, modify, and contribute source code to 
open-source codebases which can drive rapid innovation, be critical in catching and fixing bugs, 
and improve the overall functionality of software. In the same way that transparent 
collaboration is helpful to improving functionality in the open-source context, it can also be a 
useful concept for the development of foundation and other machine learning models. While 
source code may not always be directly available, allowing a wide variety of users access to at 
least the weights of a model can help in accelerating innovation, spotting issues, patching 
vulnerabilities, and improving functionality of the model.  
 
• 
Research. A de-facto method for research communities in artificial intelligence and other areas 
to collaborate world-wide, and especially in the United States, has been via open-source 
software - to provide examples, working implementations and testbeds for collaborative 
development of research, and as such open source has become as important to research as 
traditional publication and conferences. Research in artificial intelligence also relies on 
transparent collaboration in the development of models. As such, it is critical that any regulation 
relating specifically to models with widely available weights not slow down or stifle the flow and 
exchange of information vital for research. 
 
• 
Cybersecurity. The security of open-source software is an evolving conversation. A few months 
ago, the Office of the National Cyber Director requested information on open-source software 
security so that it can further implement Section 4.2.1 of the National Cybersecurity Strategy, 
aimed at improving open-source software security. In particular, ONCD recognizes the immense 
benefits of open-source software, but also highlights the possible negative downstream impact 
of vulnerabilities. Just as in the case of open source software, the openness of a model does not 
make it inherently more or less secure than a closed access model, and so while openness may 
mean that the model is more vulnerable to exploitation and modification for malicious purposes, 
especially if source code and training data is available, it also means that these vulnerabilities 
can be found and fixed proactively, and exploits rapidly addressed, by the developer community, 
analogous to how cybersecurity issues are often handled by open source communities because 
of the ability to transparently collaborate on addressing the problem.  
 
 
 
 
 
 
 
 
9 
• 
Licensing. The open-source software community relies heavily upon a common licensing 
framework to grant developers the right to access, modify, and distribute the code. While the 
specific licensing regime used in the open-source software context is not directly portable to an 
open model weights context, the overall concept of a licensing regime remains applicable. For 
open model weights, it is worth considering what a framework would look like and how terms of 
use will be discovered. We discuss this further below. 
 
b. Are there concerns about potential barriers to interoperability stemming from different 
incompatible “open” licenses, e.g., licenses with conflicting requirements, applied to AI 
components? Would standardizing license terms specifically for foundation model 
weights be beneficial? Are there particular examples in existence that could be useful? 
 
Standardizing license terms for foundation models could be an important component of a holistic 
approach to open model safety. Licenses can preserve the benefits of open models and help support 
responsible use. While licenses can help to dictate the terms and conditions of use, as well as the ability 
of a user to redistribute the software, they are not a silver bullet solution. To be sure, licenses will not 
prevent a malicious actor from leveraging the model for nefarious purposes.   
 
That being said, a licensing framework may be helpful in providing guidance and fostering a common 
understanding of terms and conditions for developers looking to open their models.  Importantly, the 
ongoing conversation about if and how traditional software licenses might apply to open model weights 
is ongoing. To be sure, open model weights are in many instances released under existing licensing 
mechanisms, like MIT or Apache 2.0.12 However, we think it important to point out that traditional 
software licenses may not be directly applicable when adapted for AI models. For example, it may be 
challenging to apply traditional open-source software licenses to AI models because they do not entirely 
account for the technical nature and capabilities of an AI model. Therefore, an effective licensing 
framework may need to encapsulate more components of a model to include things like weights, but also 
data, and other factors. We do not believe that NTIA should be charged with designing a licensing 
regime, but offer the below thoughts which may shape a future community framework for AI models 
with widely available weights: 
 
• 
Training & Deployment  
Deployment code and training code are the building blocks of AI models. A license for open models 
should consider the use, modification, and distribution of this code.  
 
• 
Data & Weights 
Databases can be covered by a variety of licenses depending on the copyright and public availability of 
the data. Specific database licenses are also a possibility. Weights are not as simple to license as some 
models provide access to weights where others do not. Licenses for fully open models must consider the 
data and weights with which a model is trained as they are important for replicating the model’s 
performance.13 
 
12 Hugging Face’s model index provides an overview of various licenses that model weights are released under, 
available here: https://huggingface.co/models?sort=trending  
13 Riddiugh, J. (2024, March 7). Licensing and legal considerations for open-source AI.  
 
 
 
 
 
 
 
10 
 
• 
Impact, Use, & Documentation 
Licenses can be designed to take into account the impact, use, and documentation of a model. License 
designs should consider potential unwanted side effects, however, in particular, that use restrictions 
being proposed for open access model licenses do not hinder the ability for commercial enterprises to 
use and contribute to such models.   
 
7. What are current or potential voluntary, domestic regulatory and international mechanisms 
to manage the risks and maximize the benefits of foundation models with widely available 
weight? What kind of entities should take a leadership role across the features of governance? 
 
NTIA may want to consider highlighting in its report the significant role that stakeholders can play in 
establishing a community framework to contribute to the responsible development of open models.’14  
 
• 
Platform for sharing best adoptions of models with widely available weight 
A community-based platform for gathering feedback from users and developers would assist in 
promoting the benefits of open sourcing AI and lead to more optimal model development. 
 
• 
Clear guidelines addressing ethical use 
Ethical use guidelines are an essential part of an open model community framework. Such guidelines 
must address the responsible use of open models and include principles on issues such as bias and 
privacy. 
 
• 
Comprehensive licensing information on all components of a model 
Open-source AI licenses should consider all components of AI models including the code, training data, 
model weights, and use of a model. 
 
 NTIA should also prioritize engagement with stakeholders to discuss approaches to and governance 
issues related to open foundation models. This conversation is taking place across jurisdictions, and it 
would behoove NTIA, and the U.S. government as a whole, to leverage existing multilateral dialogues 
such as the G7 process, forums held around the forthcoming  AI Safety Summits. and the UN AI Advisory 
Board.  
 
b. When, if ever, should entities deploying AI disclose to users or the general public that they are 
using open foundation models either with or without widely available weights? 
 
As we highlight in ITI’s Policy Principles for Enabling Transparency of AI Systems15, disclosure generally 
refers to making a user aware of the fact that they are interacting with or using an AI system – usually in 
real time or during the use of the system. We encourage policymakers to take a risk-based approach to 
 
14 Yadav, R. (2023, December 18). #117 flexing open weights. Exploring the Evolution of Open-Source Models in the 
AI Era: From Open-API to Open Weights.  
15 See ITI’s AI Transparency Policy Principles available here: https://www.itic.org/documents/artificial-
intelligence/ITIsPolicyPrinciplesforEnablingTransparencyofAISystems2022.pdf.  
 
 
 
 
 
 
 
11 
disclosure requirements, focusing on disclosure in high-risk settings. These principles should apply 
irrespective of whether they are interacting with open or closed foundation models. 
 
d. What role, if any, should the U.S. government take in setting metrics for risk, creating 
standards for best practices, and/or supporting or restricting the availability of foundation model 
weights? 
i. Should other government or non-government bodies, currently 
existing or not, support the government in this role? Should this vary 
by sector? 
 
NIST and international standards organizations can play a role in supporting international standards that 
are aimed at setting metrics for risk management and developing best practices and guidelines for the 
development and release of foundation models.  
 
E. In the face of continually changing technology, and given unforeseen risks and 
benefits, how can governments, companies and individuals make decisions or 
plans today about foundation models that will be useful in the future? 
 
 
b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the threshold, is 
the amount of computational resources required to build a model, such as the cutoff of 1026 
integer or floating-point operations used in the Executive Order, a useful metric for thresholds to 
mitigate risk in the long-term, particularly for risks associated with wide availability of model 
weights? 
 
While we appreciate that the Commerce Department (and the Administration more broadly) needed to 
start from somewhere in defining what constitutes a dual-use foundation model, we have concerns 
about using floating point operations (FLOPs) as a useful way to classify risk. Compute is not necessarily 
indicative of risk, in the same way that wide availability does not necessarily imply increased risks. It is 
possible that smaller AI models with similar capabilities could also be used in harmful ways. Such an 
approach is also not future-proof, given the way in which technology evolves – to be sure, it is unlikely 
to keep up with innovation in processor architecture and training methodologies.   
 
In seeking to develop further thresholds to categorize a dual-use foundation model, we encourage the 
Commerce Department, through the National Institute of Standards and Technology, to work to 
establish criteria based on the capabilities of a model.  
",ITI,6807
NTIA-2023-0009-0325,"Uber Technologies, Inc.
1515 3rd Street
San Francisco, CA 94158
March 27, 2024
Stephanie Weiner
Chief Counsel
National Telecommunications and Information Administration
U.S. Department of Commerce
1401 Constitution Avenue NW, Room 4898
Washington, D.C. 20230
RE: Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights;
Request for Comments, NTIA–2023–0009
Dear Chief Counsel Weiner:
Thank you for the opportunity to provide input on the National Telecommunications and
Information Administration's (NTIA) Request for Comments regarding the potential risks, benefits,
other implications, and appropriate policy and regulatory approaches to dual-use foundation
models for which the model weights are widely available. Uber Technologies, Inc. (""Uber"")
respectfully submits these comments in response to the Request for Comments.
The potential of artificial intelligence (AI) and algorithms and their promise to solve societal
problems is immense. At Uber, we are proud of the ways we have been able to leverage
technology to help facilitate billions of trips and deliveries around the world. The introduction of
these new services has expanded access to transportation and economic opportunity, helping
individuals overcome gaps in transportation systems and creating value for workers, customers,
and small business restaurants. At the same time, we recognize that the use of AI can raise
questions and concerns about safety and the risk to individuals.
Both the risks and the potential benefits of AI are present when we consider the recent
developments in generative AI. At Uber, we believe that technological progress and the use of AI
can be accomplished both safely and collaboratively. Historically, open-source technology has
been critical to safe, technological progress. In order for deployers of AI to build on these AI
systems, they need access to and confidence in the data and the models they are using.
As a technology company that is involved in the early development and application of AI
technology that benefits consumers in their daily activities and transactions, we appreciate the
opportunity to share our perspective. We are deeply involved both in the development and
deployment of AI models and make use of both ""open"" and ""closed"" models. As an entity in the
ecosystem that is not developing the large, dual-use foundational models that the government is
concerned about but may use these models to enhance our existing products and services, we
have a unique perspective.
At a high level, Uber believes that widely available model weights, and the source code and data
of these models, are extremely important for:
●
Innovation: Widely available model weights spur innovation in the field of AI. By
democratizing access to foundational AI models, innovators from diverse backgrounds
can build upon existing frameworks, accelerating the pace of technological
advancement and increasing competition in the space.
●
Safety and security: Widely available model weights, source code, and data are
necessary to foster accountability, facilitate collaboration in risk mitigation, and
promote ethical and responsible AI development.
Fostering Innovation and Democratizing Access to Model Weights
Open-source frameworks play a crucial role in driving innovation in AI. They enable the creation
of benchmarks, tools, and methodologies to ensure the quality and safety of AI systems.
Foundation models with widely available model weights offer several benefits. Firstly, by making
model weights openly available, we can accelerate innovation across many fields, including but
not limited to, education, training, medicine, and computer science. This is particularly evident as
open-source software enhances accessibility and fosters a culture of knowledge-sharing. For
example, this can create a positive separation between AI developers and deployers, facilitating
the development of better tools and scientific advancement. This division allows AI developers to
focus on innovating and refining models, while deployers can dedicate their efforts to using the
technology to advance their respective industries.
Moreover, in the financial sector, open-source AI systems are employed to analyze data, improve
security systems, and provide banking forecasts. Within healthcare, AI is increasingly applied as a
tool for diagnostics and tasks such as medical image analysis, owing to its accuracy comparable
to that of expert clinicians.
Secondly, this inclusive approach increases competition and diversifies access to generative AI
models. This fosters collaboration and a diverse technological landscape that not only benefits
corporate entities like Uber but also the broader tech sector and society at large.
Thirdly, open-source models create a more level playing field and lower barriers to entry for AI
use, ensuring that more individuals and organizations can access and improve upon existing
technology. This allows AI innovations to flourish at an accelerated rate.
Finally, the increased accessibility facilitated by open-source frameworks fuels economic growth
by enabling companies to leverage existing source code and data to increase productivity. This
also paves the way for novel AI-based solutions to address societal challenges in various sectors.
Moreover, when open-source data is made public, it enables models to be trained on high-quality,
diverse data that ensures that AI systems are more inclusive and representative of different
populations.
Safety and Dependability
Ensuring that AI technology is safe is dependent on the availability of weight models, source
code, and training data. As large language models (LLMs) continue to evolve, access to the
underlying models and the data used to train them allows developers and deployers to ensure
the data is free of intellectual property violations, biases, and sensitive or private information.
Without such access, it becomes challenging to conduct thorough assessments of the data
quality and mitigate associated risks effectively.
Currently, several governments are considering how to assign liability to companies and
individuals deploying AI systems. If a deployer is forced to assume liability for a model that it
cannot assess or evaluate, it will hamper innovation, as this may be a risk they are unwilling to
take.
Open-source models can improve oversight in the evaluation of models for biases, limitations,
and societal impacts, thereby enhancing transparency and accountability of the technology. This
enables stakeholders to scrutinize models more effectively and identify potential issues or bad
actors. Moreover, requiring parties to make their training data and code available can help build
societal trust and prevent risks associated with the misuse of open model weights.
Having data and source code simultaneously available is especially beneficial during the
development stage of new and emerging technologies, as it allows for rapid identification and
resolution of potential security issues. By leveraging the collective intelligence and expertise of
the open-source community, developers can identify and address software flaws and
vulnerabilities. Addressing these issues early on will further fuel AI research globally and enhance
AI safety efforts.
Navigating Risks: Balancing Openness with Security and Regulation
While there are many benefits and opportunities associated with widely available model weights,
training data, and source code, we realize that open source can also present challenges for AI
technology.
The risks associated with widely available model weights are multifaceted and are grounded in
transparency. Firstly, the broad accessibility of model weights enables broader access for people
to use the technology. While developers can leverage these LLMs for various applications,
including educational and innovative purposes, there is also the risk of the potential misuse of the
technology. For example, bad actors have used LLMs to develop and spread misinformation by
using the technology to create deep fakes.
Widely available model weights can also increase cybersecurity risks—if individuals or businesses
fail to effectively secure or manage their open-source systems, it can increase the likelihood of a
security breach.
It is important to note that the misuse of generative AI by bad actors will emerge regardless of
whether only model weights or both training data and source code are simultaneously made
widely available. Nevertheless, we will be better able to tackle the challenges posed by the
misuse of AI if training data and source code are made widely available. This approach would not
only expedite and democratize AI research but it would also foster a larger community of LLM
users and experts equipped to counteract bad actors. For companies like Uber that will utilize
these systems, it is critical to correct issues with training data and address cybersecurity risks.
Applying restrictions on widely available model weights may also inadvertently incentivize the
development of closed, proprietary models that operate outside of transparent and accountable
frameworks. This could further exacerbate risks associated with AI technology, so it is important
to keep model weights widely available to maintain a dynamic and inclusive system for managing
AI risks.
While we have outlined some of the risks associated with making model weights—and their
training data and source code—widely available, these risks must be weighed against the
benefits of fostering open innovation and transparency in AI development. By promoting
responsible AI practices and ensuring access to data and source code, policymakers can support
the advancement and democratization of AI technology while mitigating potential risks
associated with its misuse.
We hope that this response will provide valuable insight for the report on the potential benefits,
risks, and implications of dual-use foundations. Thank you for considering our input on this
important matter. We look forward to continuing to collaborate with NTIA and other stakeholders
to promote the responsible development and adoption of AI technology.
Sincerely,
CR Wooters
Head of Federal Affairs
Uber Technologies, Inc.
",Uber,1919
NTIA-2023-0009-0245," 
1 
Cohere Comments on the National Telecommunications & Information Administration’s Request for 
Comments on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights  
 
Docket Number NTIA 240216-0052 
March 27, 2024 
 
Cohere appreciates the opportunity to submit comments in response to National Telecommunications & 
Information Administration (NTIA)’s Request for Comments (RFC) on Dual Use Foundation Artificial 
Intelligence Models with Widely Available Model Weights (open foundation models).  
 
Introduction 
Cohere is one of the leading enterprise-focused foundation model developers worldwide. Cohere 
empowers every developer and enterprise to build amazing products and capture true business value 
with language AI.  
 
Cohere is the only foundation model developer to have signed each of the White House’s Updated 
Voluntary Commitments1 and the Canadian Federal Government’s Voluntary Code of Conduct on the 
Responsible Development and Management of Advanced Generative AI Systems2 and to also endorse the 
G7’s Hiroshima Process International Code of Conduct for Organizations Developing Advanced AI 
Systems.  
 
Cohere For AI (C4AI) is Cohere's non-profit research laboratory3 and is committed to the development of 
safe and responsible AI, with a focus on cutting-edge research and approaches to safety, and creating 
more points of entry into machine learning research. To-date, C4AI has published 39 papers with 40+ 
cross-institutional collaborators, and ran open science projects and research communities involving 
3000+ independent researchers across 119 countries. C4AI has also helped bridge access gaps through a 
scholars program to support researchers around the world4, a compute grant program to bridge access 
to resources5, and research artifact releases to share technical expertise with the wider machine 
learning ecosystem.6 
Both Cohere and C4AI are members of the US AI Safety Institute Consortium.7 
 
1 Cohere Signs White House’s Updated Voluntary AI Commitments: <www.whitehouse.gov/briefing-room/statements-
releases/2023/09/12/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-eight-additional-artificial-
intelligence-companies-to-manage-the-risks-posed-by-ai/>.  
2 Cohere Signs Canada’s Voluntary Code of Conduct on the Responsible Development and Management of Advanced 
Generative AI Systems: <ised-isde.canada.ca/site/ised/en/voluntary-code-conduct-responsible-development-and-management-
advanced-generative-ai-systems> . 
3 For more information see:<cohere.com/research/>.   
4 For information about C4AI’s scholars programme, see: <txt.cohere.com/c4ai-scholars-program>  
5 For more information about C4AI’s Research Grant Program, see: <txt.cohere.com/c4ai-research-grants/> 
6 An up-to-date list of C4AI’s research publications is available at: <cohere.for.ai/#spotlight-papers>.  
7 The U.S. Artificial Intelligence Safety Institute Consortium (AISIC). <www.nist.gov/artificial-intelligence/artificial-intelligence-
safety-institute>. 
 
2 
Summary 
 
Our response to this RFC on open foundation models is informed by our experience in both building a 
for-profit enterprise centric AI business as well as investing in a world-recognized not-for-profit research 
lab dedicated to contributing to sharing leading edge innovation in the field.  Having experience in 
building models that we release as both fully open weights and proprietary models via flexible 
deployment mechanisms (including via API), we fundamentally believe in the benefits of these different 
approaches. We strongly encourage the US government to explore a range of regulatory options to 
support this flexibility going forward. 
 
In February, C4AI released Aya8, a state-of-art multilingual model that doubles the number of languages 
served by an open source LLM while establishing a new precedent for technology that serves under 
resourced languages. Along with Aya, C4AI released the largest multilingual instruction fine tuning 
dataset to-date, covering 513 million instances covering 101 languages. Both the Aya model and dataset 
were released under a fully permissive Apache 2.0 license to further access to communities historically 
underserved by language coverage of commercially available LLMs.9 
 
In March 2024, C4AI released the weights for C4AI Command-R10 publicly so that it can be used for 
research purposes.11 C4AI Command-R is a highly performant state-of-art, weights-only release licensed 
for non-commercial uses, which is intended to allow researchers access to a productionized model from 
Cohere for research and safety auditing. This is part of our commitment to explore ways to enable third 
party cross-institutional efforts around fundamental research in safety. This model weight release is part 
of our wider support across both Cohere and C4AI for the machine learning ecosystem 
alongside research compute grants12, publishing research at top tier venues13 and regularly releasing 
code and libraries under a variety of licenses14.   
 
 
 
 
8 Aya is a global initiative involving over 3000 independent researchers across 119 countries who contributed to datasets, 
testing, refining and more.  Information about Aya is available at: <cohere.com/research/aya>; papers relating to the Aya 
dataset and model are available at: <cohere.com/research/papers/aya-dataset-paper-2024-02-13> and 
<cohere.com/research/papers/aya-model-paper-2024-02-13>.  
9 The Aya model and dataset are available via their repositories on Hugging Face. Model weights are available at: 
<huggingface.co/CohereForAI/aya-101>; and datasets are available at: <huggingface.co/datasets/CohereForAI/aya_collection>.  
10 The C4AI Command-R model is: (a) a version of Cohere’s ‘scalable’ category Command-R LLM; and (b) released under a 
Creative Commons Attribution Non-Commercial Public License with an addendum requiring users to adhere to C4AI’s acceptable 
use policy.  
11 C4AI Command-R is available at: <huggingface.co/CohereForAI/c4ai-command-r-v01>.  
12 For more on C4AI’s Research Grant Program, see: <https://txt.cohere.com/c4ai-research-
grants/?_gl=1*1fprkmg*_ga*MTA2NjMwMjA0NC4xNzA5MjI3MjM1*_ga_CRGS116RZS*MTcxMDk4MjA0OC4yMS4xLjE3MTA5O
DIwNTMuNTUuMC4w>.  
13 Cohere for AI research papers are available at: <https://cohere.com/research/papers>.   
14 For more information about Cohere and Cohere For AI’s sandbox, see: <txt.cohere.com/introducing-sandbox-coheres-
experimental-open-source-initiative/>.  
 
3 
As it relates to open foundation models, we believe that: 
 
1. Concepts such as ‘openness’ and ‘widely available’ should be defined contextually on the basis 
that openness is not a binary concept but rather a spectrum. 
2. There should not be a monolithic approach to openness, and the level of accessibility and 
release format (e.g., open weights, API access only, private deployment) should depend on a 
holistic case-by-case evaluation of the risks and benefits. 
3. It is important that licenses and access restrictions reflect perceived risks. 
4. When considering approaches to model ‘openness’, considerations must  go beyond the degree 
of weight release to also consider questions such as access to compute and  expertise. 
5. The regulatory approach to open foundation models needs to be dynamic, proportionate and 
contextual, taking a risk-and-principled approach.  
 
Our detailed submission below outlines the five issues highlighted above. As the capabilities of our 
generative models advance and as the policy and governance debate around open foundation models 
evolves, Cohere and C4AI are committed to iterating and developing our approach to openness, and we 
will continue to pioneer best practices and share our learnings.  
 
Our Detailed Submissions  
 
(i) Openness is not binary but rather a spectrum 
 
There is currently no consensus on what is considered ‘open’ or ‘widely available’. Existing research 
describes different ‘gradients’ of openness that can apply to foundation models15 with different 
considerations for risks associated at each gradient from fully closed, to fully open.16 Cohere and C4AI 
have experimented with different levels of access, from ‘closed’ API access to proprietary models 
(Cohere Command, Rerank and Embed), research releases of model weights (C4AI Command-R), and 
fully open source releases (C4AI Aya).  
 
Accordingly, when defining where a model sits in relation to the gradients of openness, the purpose of 
release and permitted uses are crucial aspects to consider in addition to any technical considerations. 
Factoring the purpose of release, intended uses, and consequently the licenses applied are as follows: 
 
● Aya’s components are available under a fully permissive Apache 2.0 license and intended for 
other researchers and developers to adapt, use and build, to advance the capability and safety 
of LLMs for underserved languages.17 
 
15 Solaiman, I., (2023). The Gradient of Generative AI Release: Methods and Considerations. 
<doi.org/10.48550/arXiv.2302.04844>. 
16 World Economic Forum (2024) Presidio AI Framework: Towards Safe Generative AI Models. 
<https://www3.weforum.org/docs/WEF_Presidio_AI%20Framework_2024.pdf>.  
17 To view Aya’s components on Hugging Face see <huggingface.co/CohereForAI/aya-101>.  
 
4 
● C4AI Command-R is a weights-only release under a CC-BY-NC 4.0 License with an Acceptable Use 
Addendum which establish red-line use cases that are not appropriate for downstream use, 
intended to allow researchers access to a state-of-art model for non-commercial purposes such 
as fundamental research and safety auditing.18 
● Cohere’s proprietary models available via an API - Command, Rerank and Embed - are released 
under a commercial license19. To ensure our customers leverage our product responsibility, 
we’ve also established and linked Usage Guidelines20 to the commercial licenses that specify 
domains where model use requires extra scrutiny and prohibit high-risk use-cases that aren’t 
appropriate. We have systems and infrastructure in place to enforce usage guidelines. This 
includes rate limits, content filtering, monitoring for anomalous activity, and revoking or 
suspending access when necessary. We also encourage builders using Cohere’s models to 
monitor model behavior for their use-case and incorporate redress mechanisms where Cohere 
is included in outputs at scale. This allows us to detect any emerging biases or harms through 
usage.21  
 
(ii) There should not be a monolithic approach to openness, and the level of accessibility to model 
weights and release format should depend on case-by-case estimation of risks and benefits. 
 
Where a model sits on a spectrum from ‘fully open’ to ‘fully closed’ brings greater or lesser degrees of 
certain benefits and risks.22  
 
The benefits of more open foundation models include: 
  
● Increased access to resources for research and development that can serve communities and 
needs that may not be primarily met by commercial incentives. For example, releasing Aya made 
a state-of-art multilingual large language model and dataset available for 101 languages, 50 of 
which were previously unserved by existing language models - representing over 1.25 billion 
language speakers globally. Similarly, C4AI Command-R offers researchers and developers 
access to a leading-edge frontier model to experiment, explore, and build on for non-
commercial purposes, including to facilitate model evaluation and safety research.  
 
● Ability to enable independent and third-party model evaluations and testing, and community-led 
approaches to safety research. For example, C4AI Command-R is a weights-only release licensed 
for non-commercial uses, intended to allow researchers to better understand a state-of-art 
model, evaluate how it works, and to use it in research that improves collective understanding 
of how to build and deploy language models responsibly and safely. This release helps to 
 
18 To view C4AI’s model weights on Hugging Face see <huggingface.co/CohereForAI/c4ai-command-r-v01>.  
19 To view Cohere’s: Terms of Use (see<https://cohere.com/terms-of-use>) and  SaaS Agreement 
(see<https://cohere.com/saas-agreement>). 
20 To view Cohere’s Usage Guidelines see <https://docs.cohere.com/docs/usage-guidelines>. 
21 To view our recommendation for evaluation techniques and tooling see< https://txt.cohere.com/evaluating-llm-outputs/>.  
22 Seger, E. et al. (2023) ‘Open-Sourcing Highly Capable Foundation Models: An Evaluation of Risks, Benefits, and Alternative 
Methods for Pursuing Open-Source Objectives’. Available at: <https://doi.org/10.2139/ssrn.4596436>.  
 
5 
address gaps in researcher and developer access to state-of-art model weights, the appetite for 
which is demonstrated by the 24,500 downloads of C4AI Command-R within the first three 
weeks of release.23 Additionally, our experience in developing and releasing Aya in collaboration 
with 3000+ researchers globally demonstrates how open model weights can be adapted, 
improved, analyzed, safety-mitigated, and reproduced by a wide open-science research 
community, accelerating progress towards safer and more responsible language models through 
community-led approaches.  
 
At the same time, release of open weights can amplify certain risks, including reduced traceability of 
downstream users and uses, and the possibility of misuse - which we define as the intentional use of 
foundation models to cause harm. For instance, if model weights are fully released without API 
restrictions, a primary concern is that visibility and control over downstream use is relinquished. In these 
cases, efforts to limit safety for sensitive downstream use can be undermined. For example, threat 
models of concern include downstream finetuning of open weights that undoes safeguards24 or 
adversarial approaches to circumventing or inferring safeguards.25  
 
The benefits and risks across the different gradients of openness are summarized by the below table 
which shows that as ‘openness’ increases, certain risks and benefits increase and vice versa. 
 
 
 
23 C4AI Command-R was released on 11 March 2024. As of Tues 26 March there were 24,558 downloads reported on the C4AI 
Command-R-v01 Hugging Face repository: <huggingface.co/CohereForAI/c4ai-command-r-v01>.  
24 Chan, A. et al. (2023) ‘Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models’. arXiv. < 
http://arxiv.org/abs/2312.14751>.  
25 Rando, J. et al. (2022) ‘Red-Teaming the Stable Diffusion Safety Filter’. arXiv. <https://doi.org/10.48550/arXiv.2210.04610>.  
 
Increased openness 
Decreased openness 
 
 
Benefits 
● Greater access to resources for 
research and development that can 
serve communities and needs that 
aren’t met by commercial incentives.  
● Enables independent model 
evaluations and testing, and 
community-led approaches to safety 
research. 
● Greater control over access and 
downstream uses, reducing risk of 
misuse.  
● Greater traceability of downstream 
users and uses.  
 
 
Risks 
● Less control over downstream uses, 
which could increase risk of misuse.  
● Less traceability of downstream use, 
making model proliferation hard to 
monitor.  
● Less ability for independent and third-
party research and evaluation. 
● Limited access to resources for 
research and development that can 
support underserved communities.  
 
6 
Because of the non-binary and sliding-scale nature of benefit and risk associated with open foundation 
models, more work is needed to understand what release formats best balance risk and benefits in 
different contexts. Amplified risks do not necessarily mitigate the benefits of openness, or vice versa, 
but it must be acknowledged in terms of understanding the trade-offs with openness. As such, 
approaches to open foundation model release that can account for the different levels of risk while 
enabling certain benefits, such as controlled API access and partial access models, should continue to be 
explored as formats which allow some traceability while providing benefits in terms of access.  
 
(iii) It is important that licenses and access or use restrictions reflect perceived risks.  
 
Approaches to applying licenses to model weights that prescribe acceptable uses should: (a) reflect the 
sliding scale nature of model openness; and (b) access to model weights should be tailored to 
reasonably foreseeable risks. 
 
For example, for projects such as Aya, which serve underserved communities and where the risk of 
misuse is outweighed by wide societal benefits, we have supported an open license. Aya was released 
on a more open and permissive license to enable important acceleration of language model capabilities 
and applications for communities not served by current commercial models.  
 
However, for state-of-art model weight releases where the capabilities of the model raise important 
concerns about amplification of model risk – like misinformation – a more restrictive license can 
emphasize red lines for possible misuse. This is the case for C4AI Command-R, which is released under a 
restrictive non-commercial use license. We believe this appropriately balances between enabling 
important research access for safety researchers and preventing the worst misuse. We continue to 
advocate for a nuanced case-by-case approach to decide on the appropriate terms of use to balance the 
benefits of openness and amplified risk. 
 
(iv) Considerations must go beyond the degree of weight release to also consider questions such as 
access to compute and expertise. 
 
Releasing weights does not ensure models are accessible because of the current engineering scale: to 
successfully make use of state-of-art model weight releases requires access to hardware, skills and 
expertise that is not widely available. This means that researchers and communities may be left 
unsupported due to a lack of access to state-of-art models. Accordingly, considerations of access to 
open weights should be accompanied by considerations of national compute programs and access both 
for technical innovation and for broadening access in participation. C4AI’s Research Grants program26 
provides academic researchers with credits to access Cohere’s APIs at no cost for benchmarking and 
experimentation, and offers another example of how a degree of model openness (API access) helps 
address gaps in access to machine learning resources for independent research. 
 
 
26 Cohere For AI Research Grant Program <txt.cohere.com/c4ai-research-grants/>.  
 
7 
 
 
(v) The regulatory approach to open foundation models needs to be dynamic, proportionate, and 
contextual, taking a risk-and-principled approach.  
 
As the NTIA has noted in the RFC consultation paper, there is currently no consensus on what is 
considered ‘open’ or ‘widely available’. Similarly, the potential risks and societal impacts associated with 
the release of open foundation model weights versus their potential benefits are a matter of intense 
debate. Consequently, regulating these models requires a nuanced approach that considers their 
distinctive properties and the broader implications. 
 
To the extent NTIA considers advancing regulation or other policy approaches as it relates to open 
foundation models, it should ensure that such an accountability framework is risk-and-principles based 
and takes a contextual and proportionate approach, including in addressing how ‘openness’ and ‘widely 
available’ are defined. It can do so by taking into consideration the following factors: 
 
● model capabilities (e.g., whether the open foundation model has frontier dual use model 
capacity versus non-frontier general purpose model capabilities); 
● availability and degree of openness of existing models with similar capabilities;  
● purpose of release (e.g., is it for non-commercial or research purposes); 
● licensing regime and permitted uses under which foundation model weights are released; 
● entity releasing the open foundation model and their role in the AI-ecosystem; 
● nature and scope of release, including whether other components (e.g., training data, code, 
compute access) are shared as part of the release; 
● impact of such a release on the societal risk analysis (e.g., risks relating to biosecurity, 
cybersecurity, disinformation, among others), and whether the risks posed outweigh the 
societal benefits associated with dissemination of such open foundation models (e.g., 
transparency, promoting research etc.); and  
● the state of the technology and research surrounding open foundation models, including ability 
to control and monitor downstream risks (and deployment), and the understanding of risk and 
society impacts of foundation models.  
 
It is evident that a one-size-fits-all approach will not adequately address the nuanced and evolving 
nature of these risks and opportunities and AI model capabilities. Accordingly, the regulatory landscape 
for open foundation models must account for the dynamic nature of AI development, continually 
assessing and addressing the emerging risks posed by advances in model capabilities while fostering the 
opportunities made possible through innovation. Any approach advanced by the US surrounding open 
foundation models should also take into consideration the need for such regimes to be interoperable 
with standards and policies advanced at the international level. Such a proactive approach shall ensure 
that safeguards keep pace with technological advancements. 
 
We recognise and commend the NTIA’s efforts to gather evidence to inform their forthcoming report to 
the President on the potential benefits, risks, and implications of dual-use foundation models for which 
the model weights are widely available, as well as policy and regulatory recommendations pertaining to 
 
8 
those models. We hope our perspective contributes to the task of navigating nuances across the 
different commercial and non-commercial purposes of foundation models and supporting contributions 
that both more ‘closed’ and more ‘open’ foundation models make to society and the economy while 
advancing safe, responsible innovation in machine learning.  
 
Thank you for the opportunity to provide comments on AI accountability. We look forward to serving as 
a resource as you continue to engage in policy discussions on this issue. 
",Cohere,4801
NTIA-2023-0009-0251," 
Request  for  Comment (
) on 
 
NTIA–2023–0009
Dual Use Foundation Artificial Intelligence
 
Models with Widely Available Model Weights
Organization: Future of Life Institute  
Point of Contact: Hamza Tariq Chaudhry, US Policy Specialist. hamza@futureoflife.org  
About the Organization 
The  Future  of  Life  Institute  (FLI)  is  an  independent  nonprofit  organization  with  the  goal  of 
reducing large-scale risks and steering transformative technologies to benefit humanity, with a 
particular focus on artificial intelligence (AI). Since its founding, FLI has taken a leading role in 
advancing key disciplines such as AI governance, AI safety, and trustworthy and responsible AI, 
and is widely considered to be among the first civil society actors focused on these issues. FLI 
was responsible for convening the first major conference on AI safety in Puerto Rico in 2015, 
and  for  publishing  the  Asilomar  AI  principles,  one  of  the  earliest  and  most  influential 
frameworks  for  the  governance  of  artificial  intelligence,  in  2017.  FLI  is  the  UN  Secretary 
General's  designated  civil  society  organization  for recommendations on the governance of AI 
and has played a central role in deliberations regarding the EU AI Act's treatment of risks from 
AI. FLI has also worked actively within the United States on legislation and executive directives 
concerning AI. Members of our team have contributed extensive feedback to the development of 
the NIST AI Risk Management Framework, testified at Senate AI Insight Forums, participated in 
the  UK  AI  Summit,  and  connected  leading  experts  in  the  policy  and  technical  domains  to 
policymakers  across  the  US  government.  We  thank  the 
 
National  Telecommunications  and
 for the opportunity to respond to this request for comment 
Information Administration (NTIA)
(RfC)  regarding 
 
(RfC)  regarding  Dual  Use  Foundation  Artificial  Intelligence  Models  with
 
Widely Available Model Weights, as specified in the White House Executive Order on the Safe,
  
Secure, and Trustworthy Development and Use of Artificial Intelligence.
 
 
 
EXECUTIVE SUMMARY 
 
FLI has a long-standing tradition of thought leadership on AI governance, focusing on mitigating
 
risks and maximizing the benefits of AI. In line with this mission, we have undertaken research
 
and  policy  work  to  explore  the  potential  risks,  benefits,  and  policy  approaches  to  governing
 
various AI systems. Our work has included a particular focus on understanding the implications
 
of  ""dual-use  foundation  models  with  widely  available  model  weights""  and  developing
 
appropriate  governance  strategies  for  these  systems.  This  term  was  recently  introduced  in
 
President Biden's Executive Order on the Safe, Secure, and Trustworthy Development and Use
​
1
​
2
 
of  AI,   and  will  be  referred  to  as  Open  Dual-Use  Models  (ODUMs)  hereafter.   In  our
 
contribution, we offer analysis and recommendations on how to manage the unique use-cases,
 
harms, and benefits of ODUMs.
 
1. ANALYSIS
 
ODUMs pose significant risks to society due to their unique characteristics, potential for misuse,
 
and  the  difficulty  of  evaluating  and  controlling  their  capabilities.  The  following  high-level
 
summary  outlines  our  risk  analysis,  which  is  expanded  in  the  subsequent  sections  of  our
  
response.
1.
 
Open  weight  AI  systems,  including,  but  not  limited  to  ODUMs,  are  fundamentally
 
distinct from traditional open source software (OSS).
2. ODUMs present unique risks in comparison to closed/proprietary systems, i
 
ncluding the
 
capacity  to  be  fine-tuned  by  third  parties to remove safeguards or augment dangerous
  
capabilities.
3. AI systems claimed to be 'open' lie across a spectrum of access, each carrying different 
levels of benefits and risks. 
4.
 
The  evaluation  of  capabilities  of  ODUMs  is  especially  difficult,  which  is  reason  for
  
caution against the release of more powerful systems.
5.
 
""Open""  AI  systems  are  already  nearly  as  powerful  as  closed  frontier  systems,  and
 
evidence suggests that they will become much more powerful over time.
6.
 
ODUMs  have  already  exhibited  harms  that,  without  appropriate  intervention,  will
 
compound over time.
1 https://www.federalregister.gov/documents/2023/11/01/2023-24283/safe-secure-and-trustworthy-development-
 
and-use-of-artificial-intelligence
2 We also use the term ""open AI systems,"" to refer to AI systems that have widely available or ""open"" components, 
but do not meet the thresholds for ""dual-use foundation models with widely available model weights."" 
 
 
 
2. RECOMMENDATIONS
 
Based on the analysis above, we advocate for the following policy recommendations: 
  
Updated Definitions and Thresholds
●
 
The  compute  threshold  for  models  to  be considered presumptively dual-use should be
 
reduced by an order of magnitude to 1025 integer or floating point operations (FLOPs),
which would match the EU AI Act's threshold for general purpose AI models with high-
impact  capabilities.  The  threshold  for  additional  safety  evaluation  should  arguably  be 
 
lower still if the model weights are made widely available or planned to be made widely
 
available, especially if the model's training data includes CBRN, cyber offensive, or other
 
sensitive  expertise.  These  thresholds    should  be  subject  to  periodic  and  as-needed
 
reductions in the event technological advances reduce the compute necessary to produce
  
similar capabilities.
●
 
Relevant  government  agencies  should  adopt  a  less  restrictive  definition  of  'dual-use
  
foundation models' to ensure that current and future systems of concern remain in scope.
 
 
Auditing and Developer Responsibility
●
 
ODUMs  should  undergo  thorough  testing  and  evaluation  in  secure  environments
  
appropriate to their level of risk prior to deployment.
●
Developers should  be legally responsible for ensuring that ODUMs are not accessed by 
actors  barred  from  access  by  the  government  and  should 
 
be  legally  responsible  for
 
performing  all  reasonable  measures  to  prevent  their  models  from  being  retrained  to
 
critically enable illegal activities. 
●
Whistleblower protections should be augmented to cover reporting on unsafe practices in 
development and/or planned deployment of unsafe ODUMs. 
 
International Cooperation and Enforcement
●
 
NTIA  should  encourage  cooperation  with  other  major  AI  powers,  including  strategic
 
competitors.
  
Public Access and Power Concentration
●
 
To address the tension between avoiding the concentration of power of AI and ensuring
 
its  safety  and  security,  initiatives  like  the  National  Artificial  Intelligence  Research
 
 
  
Resource (NAIRR) should be pursued to create ""public options"" for AI development.
 
 
 
Risk Analysis  
BACKGROUND 
 
Over the last three decades, OSS has greatly benefited society, but the emergence of ODUMs has
 
raised concerns about the risks associated with 'open-source' AI systems. While ODUMs share
 
some similarities with OSS, they have the potential to be used for malicious purposes, such as
 
generating disinformation, mass producing harmful synthetic content, automating cyberattacks,
 
and  even  assisting  in  the  creation  of  harmful  biological  weapons  on  a scale that far exceeds
 
traditional OSS. As the capabilities of AI systems increase and the weights of powerful models
 
become more widely available, these risks are likely to grow, requiring careful consideration and
 
active management of their unique risks.
 
ANALYSIS
 
1.  Open  weight  AI  systems,  including,  but  not  limited  to  ODUMs,  are  fundamentally
 
distinct from OSS. When debating the benefits, risks and policy prescriptions for AI systems,
 
policymakers  and  experts  have  at  times  drawn an analogy with OSS, which masks important
 
distinctions between the two. In the case of traditional OSS, access to the source code provides
 
users  and  developers  with  a  comprehensive  understanding  of  the software's functionality and
 
capabilities. However, this is not the case with open AI systems. Even if the source code of an AI
 
system is made available, it typically does not provide complete information about the system's
 
behavior  and  potential.  The  key  determinants  of  an  AI  model's  capabilities  lie  in  its  model
 
weights,  which  are  the  result  of  extensive  training  on  vast  amounts  of  data.  The  complex
 
interactions and relationships between the model weights within the neural network architecture
 
make it difficult for humans to interpret and understand the model's decision-making process and
 
potential outputs, even with access to the individual weight values. As a result, releasing model
 
weights for AI systems confers less benefit when compared with releasing the source code of
 
OSS, as it does not guarantee a complete understanding of the system's capabilities and potential
 
consequences. 
 
This  problem  is  taken  a  step  further  in the case of ODUMs because they are generally more
 
capable and are likely to have unforeseen capabilities. The potential for misuse of these models
 
is significantly higher, as malicious actors could leverage their advanced capabilities for harmful
 
purposes.  Consequently,  the  release  of  ODUMs  necessitates  even  greater  caution  and
 
consideration  of  the  associated  risks.  Additionally,  while  traditional  OSS  products  can
 
themselves be useful for general purposes, they are not inherently designed to perform a wide
 
range of tasks. In contrast, many ODUMs are specifically designed to be highly generalizable
 
 
 
and capable of undertaking a broad spectrum of tasks without explicit programming. ODUMs
 
can learn and improve their performance over time through exposure to data and interactions,
 
making them significantly more powerful and potentially dangerous than traditional OSS. This
 
generality,  capability,  and  adaptability,  combined  with  their  ability  to  learn  and  evolve,
 
distinguish ODUMs from traditional OSS and raise critical concerns about their potential misuse
 
or unintended consequences.
 
2. ODUMs present unique risks in comparison to closed systems, including the capacity to
be  fine-tuned  by  third  parties  to  remove  safeguards  or  augment dangerous capabilities. 
 
While all frontier AI systems present some risks, ODUMs present unique potential for misuse.
 
Presently, there are a small number of companies developing frontier dual-use systems. If there
 
are  security,  privacy,  or  content  issues  with  a  ""closed""  system,  the  developer  can  patch  the
 
system,  affecting  all  instances  of  it,  or  take  it  offline  to address these issues. However, once
 
model  weights  are  available  for  download  by  the  public,  their release is irreversible, making
 
reliably  patching  vulnerabilities  top-down  -  or  deploying  guardrails  around  dangerous
 
capabilities - impossible. Ultimately, like OSS, it is up to the downstream entities to choose to
 
adopt  updates. These entities may continue with the original, unpatched version, or may even
 
fork the project to preserve or further enhance the dangerous capabilities.
 
Defining the openness of a model based on its level of distribution is a topic of ongoing debate.
 
Some suggest restricting access to certain entities, such as academic institutions, while keeping
 
the model open for others. However, enforcing such access restrictions is incredibly challenging
 
in practice. Once model weights become widely available, there is currently no way to regain
 
control, restrict access, or limit their use.
 
The release of model weights also allows for trivial removal of safeguards and lowers the barrier
​
3,​
4,​
5 
to entry for adapting systems toward more dangerous capabilities through fine-tuning.
Fine-
 
tuning in the context of AI involves training an AI model on a specific task or domain to improve
 
its  domain-  or  use  case-specific  performance.  AI  systems  whose  model  weights  are  made
publicly available allow actors to directly access the model's weights without oversight and fine-
3 
 
Lermen, Simon, Charlie Rogers-Smith, and Jeffrey Ladish. ""LoRA Fine-tuning Efficiently Undoes Safety Training
 
in Llama 2-Chat 70B."" arXiv, Palisade Research, 2023, https://arxiv.org/abs/2310.20624.
4 
 
Gade, Pranav, et al. ""BadLlama: Cheaply Removing Safety Fine-Tuning from Llama 2-Chat 13B."" arXiv,
 
Conjecture and Palisade Research, 2023, https://arxiv.org/abs/2311.00117.
5 
 
Yang, Xianjun, et al. ""Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models."" arXiv,
 
2023, https://arxiv.org/abs/2310.02949.
 
 
 
tune  it  for  their  specific  needs  without  having to train the model from scratch, which can be
 
prohibitively expensive and therefore limited to a small number of well-resourced corporations.
 
Fine-tuning enables the creation of specialized AI models tailored to specific domains or tasks
 
while  still  benefiting  from  the  general  knowledge  and  baseline  capabilities of the pre-trained
 
model.  Furthermore,  the  release  of  model  weights,  and  the  subsequent  ability  for  third-party
 
actors to build on top of them, expands the amount of actors to manage from a small handful of
 
AI  companies  to  potentially  millions  of  independent  actors,  making  it  far  more  difficult  to
 
universally mitigate these harms. 
 
Finally, in the absence of robust safeguards and comprehensive regulations governing the release
 
of  model  weights,  it  is  highly  likely  that  we  will  see the emergence of AI systems that lack
 
essential safety measures while possessing the full capabilities of advanced models. The primary
 
reason  why  this  has  not  yet  happened  with  the  most  capable  systems  is  due  to  the  internal
 
policies at top companies (e.g. OpenAI, Google Deepmind, Anthropic, etc.). If OpenAI made the
 
decision to provide open access to their models, there would be no gap between the capabilities
 
of  open  and  closed  AI  systems.  This  scenario  poses  significant  risks  to  society,  as  these
 
unrestrained  systems  could  be  used  for  malicious  purposes  in  the future or cause unintended
 
harm on a large scale.
3. AI systems claimed to be 'open' lie across a spectrum of access, each carrying different 
levels  of  benefits  and  risks. 
 
In  the  last  few  years,  a  number  of  leading  AI  labs,  including
 
OpenAI, Anthropic, Meta, Google, Deepmind, among others, have released foundation models.
 
While  some  models  remain  highly  restricted,  limiting  who  can  access  the  model  and  its
 
components,  other  models  and  their  developers  provide  open  access  to  model  weights  and
 
architecture.  For  a  detailed  assessment  on  the  risks  and  opportunities  across  the spectrum of
​
6 
access to ODUMs, see (Brammer 2023).
 
Presently,  many  companies  developing  frontier  systems  are  focused  on  the  release  of  model
 
weights while remaining 'closed' on other crucial aspects of their models. This allows companies
 
to enjoy the marketing benefits of ""open-source"" without fully committing to the principles of
 
transparency  and  collaboration  that  underpin  the  open-source  movement.  For  a  detailed
  which explores the use of open weight 
exploration  of  this  busines  tactic,  see  Widder  (2024),
6
 
Institute for Security and Technology. How Does Access Impact Risk? Assessing AI Foundation Model Risk Along
A Gradient of Access. Dec. 2023, https://securityandtechnology.org/wp-content/uploads/2023/12/How-Does-Access-
 
Impact-Risk-Assessing-AI-Foundation-Model-Risk-Along-A-Gradient-of-Access-Dec-2023.pdf.
 
 
release as a tactic for market consolidation.   
​
7
 
Furthermore, as Widder et al. suggest, the concentration of computational power among a few
 
large  technology  companies  raises  significant  concerns  about  the  potential  for  ODUMs  to
 
exacerbate  antitrust  issues.  Developing  and  deploying  powerful  AI  models  requires  massive
 
datasets and computational resources, which are increasingly controlled by a limited number of
 
major players in the industry. The high costs associated with training and running inferences on
 
large-scale AI models create a significant barrier to entry for smaller competitors. Additionally,
 
the  software  systems  used  to  optimize  computational  power  for  AI  development  are  often
 
designed for efficiency in proprietary hardware environments and are developed and governed
 
by the same companies that sell access to computational resources and license AI models.
 
As  a  result,  these  large  companies  could  potentially  use  their  control  over  computational
 
resources  and  optimized  software  to  lock  in the ecosystem of developers by making their AI
 
models and APIs the de facto standard. This could effectively create a vendor lock-in scenario,
 
making  it  difficult  for  developers  to  switch  to  alternative  providers  or  develop  their  own
 
competing  solutions.  The  concentration  of  power  in  the  hands  of  a  few  large  technology
 
companies, combined with the potential for vendor lock-in, raises significant antitrust concerns,
 
such as reduced competition, stifled innovation, limited consumer choice, and the potential for
 
abuse  of  market  dominance  through  anti-competitive  practices,  and  is  fundamentally  at  odds
 
with the values of traditional OSS.
4.  The  evaluation  of  capabilities  of  ODUMs  is  especially  difficult,  which  is  reason  for 
caution against the release of more powerful systems. 
 
In the past year, we have witnessed that,
 
despite  extensive  internal  testing,  even  closed  AI  systems  often  contain  significant  bugs  or
​
8,​
9
security  failures.
  This  issue  is  even  more  severe  with  ODUMs,  as  models  with  widely 
 
available  weights  can  be  fine-tuned  after release to remove safeguards or augment dangerous
7
 
Widder, David Gray and West, Sarah and Whittaker, Meredith, Open (For Business): Big Tech, Concentrated
 
Power, and the Political Economy of Open AI (August 17, 2023). Available at SSRN:
 
https://ssrn.com/abstract=4543807 or http://dx.doi.org/10.2139/ssrn.4543807
8 Burgess, M. (2023, November 29). OpenAI’s Custom Chatbots Are Leaking Their Secrets. WIRED. 
https://www.wired.com/story/openai-custom-chatbots-gpts-prompt-injection-attacks/ 
9 Porter, J. (2023, March 21). ChatGPT bug temporarily exposes AI chat histories to other users. The Verge. 
https://www.theverge.com/2023/3/21/23649806/chatgpt-chat-histories-bug-exposed-disabled-outage 
 
 
 
 
capabilities, even if they underwent rigorous evaluations prior to release. 
 
Given the wide range of novel approaches that malicious actors could use to co-opt ODUMs for
 
malicious  purposes,  and  the  growing usefulness of such models for enabling, encouraging, or
 
executing malevolent actions, evaluations should not merely test whether developers can identify
 
risks in the unmodified model. Instead, they should assess whether the technical safeguards in
 
place are robust enough to prevent malicious use above a reasonable, pre-defined, risk threshold,
 
even  if  the  model  is  modified  to  weaken  or  remove  protective  features  or  to augment latent
 
capabilities.  If  a  dual-use  model  cannot  meet  this  standard,  its  weights  should  not  be  made
 
publicly available.
While investing in tools to mitigate these risks is crucial, significant time will also be necessary 
 
to develop them properly. In the interim, it is essential to adopt a precautionary approach when
 
deploying the AI systems that are plausibly disruptive, given the scale of potential harm and the
 
irreversibility of model weight release.
5.  "" pen""  AI  systems
 
O
  are  already  nearly  as  powerful  as  closed  frontier  systems,  and
 
evidence  suggests  that  they  will  become  much  more  powerful  over  time.  LLAMA-2,  an
 
open-source AI model developed by Meta, has already demonstrated superiority to GPT 3.5, a
closed-source model created by OpenAI, on a number of capabilities assessments. In December, 
  
Mistral AI released the model weights to Mixtral 8x7B, claiming that is outperfomes LLAMA-2
​
10
 
and  GPT-3.5  on  ""most  standard  benchmarks.""   While  ODUMs  have  thus  far  lagged  behind
closed systems, this is purely an artifact of top AI companies (e.g. OpenAI) electing not to open-
 
source their most advanced models based on primarily profit-motivated internal policy decisions.
 
There is no evidence to suggest that ODUMs will continue to lag behind closed systems in the
 
long  term.  If  the  leading  AI  companies  were  to  change  their  stance  on  open-sourcing  their
 
models,  e.g.,  due  to  changes  in  market  conditions  or  their  assessment  of  self-interest,  the
  
landscape would be significantly different.
 
Recent  developments  in  the  AI  market,  such  as  Meta's announcement to build 'Open-Source'
 
AGI, Elon Musk's lawsuit against OpenAI and the subsequent release of Grok's model weights,
 
and Mistral's agreement with Microsoft to offer exclusive access to its most powerful AI system,
 
underscore the volatile climate in which these decisions are made and the need for government
10 
 
Mistral AI Team. ""Mixtral of Experts."" Mistral AI, 11 Dec. 2023, https://mistral.ai/news/mixtral-of-experts/.
 
 
​
11,​
12,​
13
 
intervention.
 There is no guarantee that corporations will maintain a consistent position on
 
the level of access granted to their systems. Instead, they may choose to 'open' or 'close' their
 
models  at  any  point  based  on  market  advantage,  with  pronounced  consequences  for  safety,
 
national security, and American competitive advantage. Additionally, even if ODUMs currently
 
lag behind the most powerful closed AI models, they are likely to become much more powerful
 
over  time  as  more  actors  build  on  top  of them or elect to release model weights, amplifying
 
potential risks in both scale and likelihood.
6.  ODUMs  have  already  exhibited  harms  that,  without  appropriate  intervention,  will 
compound over time 
 
We can categorize the potential harms associated with ODUMs into three distinct classes:
Kinetic Harms 
Attacks on Perception and 
Cognition 
Magnification of High-Risk 
AI 
Facilitate the development of 
components or conditions to 
physically harm targets. 
Purposeful use of deception, 
influence, and manipulation, 
among others, to target 
individuals and institutions 
with the intent of 
capturing/disrupting levers of 
power or sow instability. 
Generate powerful AI 
systems (e.g. recursive self 
improvement) that are highly-
capable, misaligned, devoid 
of safeguards, or made to 
identify the vulnerabilities of 
other AI systems.   
 
 
Open AI systems have already demonstrated the potential to facilitate attacks on perception and
 
cognition and cause kinetic harm to society, particularly in the areas of cyberattacks,
11 Open Release of Grok-1. https://x.ai/blog/grok-os 
12 In fact, Meta has explicitly expressed an intention to create Artificial General Intelligence (AGI) and 'open-
 
source' it. While definitions of AGI vary, it is commonly assumed to imply systems that are better than even expert
 
humans at virtually all cognitive tasks. If they succeed in that objective, this would immediately reverse the
 
capabilities dynamics between open and closed systems, with open systems then outpacing their closed competitors.
 
The imminence of this threat is bolstered by the unprecedented amount of compute Meta has amassed in pursuit of
  
this goal. This emphasizes the need for governance measures beyond academic and industry norms and trends. See
13 Heath, A. (2024, January 18). Meta’s new goal is to build artificial general intelligence. The Verge. 
https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview 
 
 
 
​
14,​
15 The UK National Cyber Security Centre found 
disinformation, and the proliferation of CSAM.
that AI systems are expected to significantly increase the volume and impact of cyber attacks by 
2025, with varying degrees of influence on different types of cyber threats. While the near-term 
threat primarily involves the enhancement of existing tactics, techniques, and procedures, AI is 
already being used by both state and non-state actors to improve reconnaissance and social 
engineering. More advanced AI applications in cyber operations are likely to be limited to well-
resourced actors with access to quality training data and expertise, but the increasing 
commoditization of AI-enabled capabilities will make improved tools (including ODUMs) 
available to a wider range of threat actors, including cybercriminals and state-sponsored groups.​
16 
 
 
In the case of CBRN risks, as of early 2024, a significant portion of evidence suggests that open
 
AI systems models function as instruments comparable to internet search engines in facilitating
​
17
 
the procurement of information that could lead to harm.  However, as the reasoning, planning,
 
and persuasion capabilities of these models continue to grow as models become more advanced,  
 
their potential to be misused by malicious actors is also expected to increase.
 
Leading  AI  researchers and companies are working on advanced, general-purpose AI systems
 
that  are  expected  to  have  deep,  wide-ranging  expertise  and  the  ability  to  act  in  complex
 
environments with relative ease. If these AI systems are released as open-source models without
 
proper safeguards, they could quickly be modified by bad actors to remove any built-in safety
 
measures. This could lead to harms spanning the categories mentioned above, and would cause a
 
significant  detriment  to  public  security,  as  such  AI  systems  could  facilitate the planning and
 
execution of harmful, illegal activities at unprecedented scales and complexities. 
 
 
14 
 
CrowdStrike. 2024 Global Threat Report. CrowdStrike, 2023, https://www.crowdstrike.com/global-threat-report/.
15 
 
Thiel, David, Melissa Stroebel, and Rebecca Portnoff. ""Generative ML and CSAM: Implications and
 
Mitigations."" Stanford Cyber Policy Center, Stanford University, 24 June 2023,
 
https://cyber.fsi.stanford.edu/publication/generative-ml-and-csam-implications-and-mitigations.
16The near-term impact of AI on the cyber threat. (n.d.). https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-
threat 
17 
 
Irving, Doug. ""Red-Teaming the Risks of Using AI in Biological Attacks."" RAND, 25 Mar. 2024,
 
https://www.rand.org/pubs/articles/2024/red-teaming-the-risks-of-using-ai-in-biological-attacks.html.
 
 
 
 
 
 
 
 
 
Recommendations  
To mitigate the risks mentioned above, we propose the following recommendations for 
consideration by NTIA: 
 
Definitions and Thresholds
 
1.  The  compute  threshold for models to be considered presumptively dual-use should be
 
reduced  by  an  order  of  magnitude  to  1025 integer or floating point operations (FLOPs),
which would match the EU AI Act's threshold for general purpose AI models with high-
​
18  
impact capabilities.  
 
The current definition of a ""dual-use foundation model"" in the Executive Order presumptively
 
includes  ""any  model  that  was  trained  using  a  quantity  of  computing power greater than 1026
 
integer or floating-point operations."" We applaud the Executive Order's recognition that models
 
beyond a certain capability threshold (using training compute as a proxy) present inherent risks
 
due  to  their  unpredictability,  their  capacity  for  emergent  capabilities, and their wide range of
 
potential  uses,  both  malicious  and  otherwise.  However,  we recommend that this threshold be
 
lowered  to  1025  operations,  aligning  with  the  EU  AI  Act's  threshold  for  general  purpose  AI
 
models with high-impact capabilities. Today's cutting-edge models have already exhibited some
 
capacity for dangerous misuse and malicious use, and none of these models have thus far reached
 
the  threshold  of  1026  operations.  This  adjustment  is  particularly  crucial,  and  should arguably
 
warrant further reduction, for models whose weights are made widely available or are planned to
 
be made widely available, as the potential for misuse, harm, and unmonitored adoption is higher
  
in such cases, and post-hoc remediation of latent risks is not possible.
 
The definition should also allow for the possibility of lowering the compute threshold moving
 
forward  as algorithmic improvements enable more efficient training and increased capabilities
 
require  far  less  compute.  By  incorporating  a  mechanism for adjusting the threshold based on
 
technological advancements, the definition can remain effective and relevant in the face of rapid
 
progress. The following is a suggested modification to the Executive Order's definition:
 
""...any model that was trained using a quantity of computing power greater than 1025 integer or
 
floating-point  operations,  with  the  understanding  that  this  threshold  may  be  lowered  as
 
algorithmic  improvements  enable  more  efficient  training  and  increased  capabilities  with  less
18
 
""Article 52a: Classification of General-Purpose AI Models as General Purpose AI Models with Systemic Risk.""
 
EU Artificial Intelligence Act, Future of Life Institute, https://artificialintelligenceact.eu/article/52a/.
 
 
 
compute.""
2.  Relevant  government  agencies  should  adopt  a  less  restrictive  definition  of  'dual-use 
foundation  models' 
 
 
to ensure that current and future systems of concern remain in scope. At
 
present, capabilities generally scale with the amount of compute used to train a model and the
 
model's  parameter size, making compute thresholds useful proxies for identifying dual-use AI
 
systems in the absence of reliable capability benchmarks. However, the definition of ""dual-use
 
foundation  models""  should  remain  flexible  and  inclusive,  suggesting  the  use  of  multiple
sufficiency thresholds (rather than necessity thresholds) for qualifying as dual-use for a general-
 
purpose  AI  system.  These  thresholds  should  include  some  that  directly  assess  a  variety  of
 
consequential  capabilities  and  subject  matters,  with  the  ability  to  add  new  thresholds  as
 
benchmarks  emerge  and in the face of algorithmic improvements. To accommodate this more
 
inclusive  concept  of dual-use and appropriately support the inclusion of new benchmarks and
 
algorithmic improvements, a slight modification to the Executive Order's definition of ""dual-use
 
foundation models"" (and subsequent definition of ODUMs)  could be made as follows:
 
“[...]an AI model that is trained on broad data; generally uses self-supervision; contains at least
 
tens of billions of parameters; and is applicable across a wide range of contexts; and or an AI
 
model that exhibits, or could be easily modified to exhibit, high levels of performance at tasks
 
that pose a serious risk to security, national economic security, human rights, national public
 
health or safety, or any combination of those matters, such…”
Auditing and Developer Responsibility 
 
3. ODUMs should undergo thorough testing and evaluation in secure environments
 
appropriate to their level of risk. These secure environments could include secure enclaves,
 
data cleanrooms, or Sensitive Compartmented Information Facilities (SCIFs). The government
 
should conduct these assessments directly or delegate them to a group of government-approved
 
independent auditors. When assessing these models, auditors must assume that a) built-in safety
 
measures or restrictions could be removed or bypassed once the model is released, and b) the
 
model could be fine-tuned or combined with other resources, such as datasets, educational
 
materials, additional software, or other AI systems, potentially leading to the development of
 
entirely new and unanticipated capabilities. Insufficient safeguards to protect against dangerous
 
capabilities or dangerous unpredictable behavior should justify the authority to suspend the
 
release of model weights, and potentially the system itself, until such shortcomings are resolved.
4. Developers should be legally responsible for ensuring that ODUMs are not accessed by 
actors  barred  from  access  by  the  government,  and  should  be  legally  responsible  for 
performing  all  reasonable  measures  to  prevent  their  models  from  being  retrained  to 
 
 
critically enable illegal activities.  
 
Previous executive orders have established requirements for
 
record-keeping  and  investigations  related  to  transactions  involving  foreign  malicious  cyber
 
actors and other actors barred by the government. However, the nature of ODUMs make it nearly
 
impossible for developers to fulfill these requirements. When model weights are made widely
 
available,  it  becomes  intractable  for  developers  to maintain records of distribution, especially
 
when  foreign  entities  are  involved.  This  inherent  characteristic  of  ODUMs  undermines  the
 
objectives and concerns outlined in previous executive orders (Executive Orders 13694, 13757,
 
and 13984), as comprehensive record-keeping is not feasible.
 
Despite the challenges in meeting these record-keeping requirements, developers should still be
 
held  accountable  if  it  can  be  proven  that  their  models  were  used  by  actors  barred  by  the
 
government.  In  such  cases,  even  if  the  developers  did  not  directly  engage  in  the  malicious
 
activities, they should be liable for the consequences of releasing their model weights without
 
adequate  safeguards  or  controls.  Existing  statutes  on  aiding  and  abetting  are  in  many  cases
 
insufficient  to  cover  these  circumstances,  as  they  generally  require  intent  on  the  part  of  the
 
developer  to  assist  in  the  underlying crime. Since ODUMs provide incidental but foreseeable
 
aide to foreign malign actors, new legal mechanisms must be created.
5. Whistleblower protections should be augmented to cover reporting on unsafe practices in 
development  and/or  planned  deployment  of  unsafe  ODUMs. 
 
These  protections  should  be
 
expanded to cover a wide range of potential whistleblowers, including employees, contractors,
 
and  external  stakeholders  who  have  knowledge  of  unsafe  practices.  This  protection  should
 
include  legal  protection  against  retaliation,  confidentiality,  safe  reporting  channels,  and  the
 
investigation of reports documenting unsafe practices. It is not presently clear whether existing
 
whistleblower  protections  pertaining  to  consumer product safety would be applicable in these
 
circumstances; as such, new regulations may be necessary to encourage reporting of potentially
 
dangerous practices.
International Cooperation and Enforcement 
6.  NTIA  should  encourage  cooperation  with  other  major  AI powers, including strategic 
competitors. 
 
The  inherent  portability  of  ODUMs  implies  that  potential  harms  are  highly
 
unlikely to be confined to a developer's country of origin. International cooperation is critical for
 
two  primary  reasons:  first,  to  encourage  effective  enforcement  of  restrictions and regulations
 
within our own jurisdiction. International cooperation ensures a level playing field and mitigates
 
largely  unfounded  concerns  that  governance  efforts  could  place  domestic  industries  at  a
 
competitive  disadvantage.  Second,  international  cooperation  would encourage the adoption of
 
similar  restrictions  in  other  countries  so  that  dangerous  and  uncontrollable  ODUMs  are  not
 
 
 
released, regardless of their country of origin.
 
Public Access and Power Concentration  
 
8. To address the tension between avoiding the concentration of power of AI and ensuring
 
its safety and security, initiatives like the National Artificial Intelligence Research Resource
 
(NAIRR) should be pursued to create ""public options"" for AI. The question of how to safely
 
govern ODUMs, and AI systems more broadly, presents some tension between preventing  the
 
concentration of AI power in the hands of a few powerful private interests and mitigating the risk
 
amplication that would arise from broader access to potentially dangerous systems.
 
One potential solution is for the U.S. to further invest in ""public options"" for AI. Initiatives like
 
the  National  Artificial  Intelligence  Research  Resource  (NAIRR)  could  help  develop  and
 
maintain publicly-funded AI models and infrastructure. This approach would ensure that access
 
to advanced AI isn't solely controlled by corporate or proprietary interests, allowing researchers,
 
entrepreneurs,  and  the  general  public to benefit from the technology while prioritizing safety,
 
security, and oversight.
 
 
CLOSING REMARKS
 
We thank NTIA for the opportunity to provide comments on Dual Use Foundation Artificial
We believe that if the risks outlined 
Intelligence Models With Widely Available Model Weights. 
above are effectively addressed,  the potential benefits of ODUMs can be realized while 
mitigating the associated dangers.  
 
 
 
",FLI,12974
NTIA-2023-0009-0328,"Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 1 
RESPONSE TO NTIA RFC ON DUAL USE FOUNDATION 
ARTIFICIAL INTELLIGENCE MODELS WITH WIDELY 
AVAILABLE MODEL WEIGHTS 
 
Submitted by the Johns Hopkins Center for Health Security 
Executive Summary 
Thank you for the opportunity to provide comments in response to the National 
Telecommunications and Information Administration (NTIA) Request for Comment (RFC) on 
“Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights,”1 
related to NTIA’s responsibilities under section 4.6 of the Executive Order on Safe, Secure, and 
Trustworthy Development and Use of Artificial Intelligence (EO).2 The comments expressed 
herein reflect the thoughts of the Johns Hopkins Center for Health Security and do not 
necessarily reflect the views of Johns Hopkins University. Below, we provide information 
regarding biosecurity considerations for topics related to policy and regulatory approaches to 
“open” dual-use foundation models (ie, those for which the model weights are widely 
available).  
 
The Johns Hopkins Center for Health Security conducts research on how new policy 
approaches, scientific advances, and technological innovations can strengthen health security 
and save lives. The Center has 25 years of experience in biosecurity and is dedicated to 
ensuring a future in which pandemics, disasters, and biological weapons can no longer 
threaten our world. Our Center is composed of researchers and experts in science, medicine, 
public health, law, social sciences, economics, national security, and emerging technology. 
 
Section 4.6 of the EO tasked NTIA with preparing a report concerning the benefits and risks 
associated with dual-use foundation models with widely available model weights.3 The EO 
expressed particular interest in the risks associated with users fine-tuning open dual-use 
foundation models or removing model safeguards.  
 
The EO defines a dual-use foundation model as, among other things, any AI model that 
contains at least tens of billions of parameters, is “applicable across a wide range of contexts,” 
and “exhibits, or could be easily modified to exhibit, high levels of performance at tasks that 
pose a serious risk to security, national economic security, national public health or safety.”4 
The first concerning capability highlighted by the EO is the ability to “substantially lower[] the 
 
1 Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights, 89 Fed. Reg. 14,059 
(Feb. 26, 2024). 
2 See Executive Order No. 14,110, Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence 
(Oct. 30, 2023) [hereinafter EO]. 
3 EO § 4.6. Although model openness exists on a spectrum, for the sake of simplicity we refer to all models with 
widely available weights as “open.” See Sayesh Kapoor et al., On the Societal Impact of Open Foundation Models 
(working paper, 2024), https://crfm.stanford.edu/open-fms/paper.pdf. 
4 EO § 1(k).  
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 2 
barrier of entry for non-experts to design, synthesize, acquire, or use chemical, biological, 
radiological, or nuclear (CBRN) weapons.” 
 
We consider highly capable large-language models (LLMs) and broad biological-design tools 
(BDTs) as potentially covered by this definition. Although BDTs are more narrowly targeted 
than LLMs such as GPT-4 or Llama 2, some are capable of a broad range of biology-related 
tasks or can be adapted to perform such tasks. For example, the recently released Evo model 
can purportedly “generalize across the three fundamental modalities of the central dogma of 
molecular biology” to design novel DNA, RNA, and proteins.5 Although Evo is a 7-billion 
parameter model, and so below the EO size threshold for a dual-use foundation model, 
current trends—including, in recent years, an exponential increase in compute used to train 
BDTs and rapid growth in biological sequence data that models can be trained on—indicate 
that BDTs will continue to rapidly scale up in size and capability.6 For these reasons, our 
recommendations below apply to both frontier LLMs and broad BDTs. 
 
We recommend that NTIA consider the following points in drafting its report: 
 
(1) Open dual-use foundation models’ current biological capabilities are a poor proxy for 
model capabilities in the near- and medium-term future. 
• Increased model scaling and the rapid generation of usable data mean that LLMs 
and BDTs will likely grow substantially larger and more capable in the coming 
months and years. The United States should plan for that future rather than wait 
for it to arrive.  
(2) The United States should set open dual-use foundation model policies that mitigate 
high-consequence biosecurity risks, which we judge to be the potential for a dual-use 
foundation model to do the following: 
• Substantially accelerate or simplify the reintroduction of particularly dangerous 
extinct viruses, or dangerous viruses that only exist now within research labs, that 
have the capacity to start pandemics; or   
• Substantially enable, accelerate, or simplify the creation of new or enhanced 
biological constructs that could start pandemics.   
(3) Narrowly targeted export controls may mitigate high-consequence biosecurity risks 
stemming from open dual-use foundation models. 
• The Department of Commerce (DOC) has significant export control authority to 
restrict the transmission of software and data that pose biosecurity risks. 
• The federal government should consider whether narrowly targeted export control 
restrictions on dual-use foundation models with concerning biosecurity capabilities 
may be justified in the future. 
 
 
 
5 Eric Nguyen et al., Sequence modeling and design from molecular to genome scale with Evo (working paper, 
2024), https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2.full.pdf. 
6 See id.; Nicole Maug et al., Biological Sequence Models in the Context of the AI Directives, Epoch (2024), 
https://epochai.org/blog/biological-sequence-models-in-the-context-of-the-ai-directives. 
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 3 
Considerations and Recommendations 
Below, we discuss the above recommendations in more detail, including briefly surveying 
current and anticipated biological capabilities of leading AI models and the risks inherent in 
releasing open dual-use foundation models.7  
Open dual-use foundation models’ current biological capabilities are a poor proxy for future 
capabilities  
 
Foundation models have enormous potential to address major challenges in medicine, public 
health, and the environment, and they offer other important benefits. However, AI 
capabilities that can improve health may also be used to cause harm. In analyzing the benefits 
and risks of open dual-use foundation models, NTIA should consider the serious biosecurity 
risks that emerging open dual-use foundation models may pose in the coming years over and 
above existing technologies such as the internet and preexisting biology modelling software.  
 
Our concerns about such risks have been bolstered by two recent trends in AI development.  
 
First, closed LLMs, such as GPT-4, have shown rapid progress in bioweapons-relevant tasks, 
including assisting with biological and chemical research design and testing.8 Although public 
information related to the capabilities of LLMs suggests that the current generation of LLMs 
do not substantially assist in bioweapons planning today, their rapidly improving capacities are 
a cause for concern in the future.9 And while open LLMs such as Gemma, Llama 2, and Mistral 
have lagged behind the technological frontier, their capabilities have advanced rapidly in 
recent months.10 Meta, which has generally released open models, has announced plans to 
invest billions of dollars in creating models “that are at the state of the art and eventually the 
leading models in the industry.”11 
 
7 Portions of our response draw in part on material provided in our recent response to NIST’s Request for 
Information. See Center for Health Security, Response to RFI Related to NIST’s Assignments Under Sections 4.1, 
4.5 and 11 of the Executive Order Concerning Artificial Intelligence (Feb. 2, 2024), 
https://www.regulations.gov/comment/NIST-2023-0009-0138. 
8 See Daniil A. Boiko et al., Autonomous chemical research with large language models, 624 Nature 570 (2023); 
Brendt A. Koscher, Autonomous, multiproperty-driven molecular discovery: From predictions to measurements 
and back, 382 Science E1 (2023); Andres M Bran et al., ChemCrow: Augmenting large-language models with 
chemistry tools, (working paper, 2023), https://arxiv.org/abs/2304.05376. 
9 See Tejal Patwardhan et al., Building an early warning system for LLM-aided biological threat creation, OpenAI 
(2024), https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation; 
Christopher A. Mouton et al., The Operational Risks of AI in Large-Scale Biological Attacks: Results of a Red-Team 
Study, RAND (2024), https://www.rand.org/pubs/research_reports/RRA2977-2.html. 
10 See Mixtral of Experts: A High Quality Sparse Mixture-of-Experts, Mistral (Dec. 11, 2023), 
https://mistral.ai/news/mixtral-of-experts/; Jeanine Banks & Tris Warkentin, Gemma: Introducing new state-of-
the-art open models, Google (Feb. 21, 2024), https://blog.google/technology/developers/gemma-open-models/ 
11 Alex Health, Mark Zuckerberg’s new goal is creating artificial general intelligence, Verge (Jan. 18, 2024), 
https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview. See also Nathan 
Lambert, Model commoditization and product moats, Interconnects (March 13, 2024), 
https://www.interconnects.ai/p/gpt4-commoditization-and-moats (“There are countless individuals who can 
easily pay the price it takes to create a model like Claude 3 and release it to the world.”). 
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 4 
 
Second, AI systems specifically focused on biological data and outputs—BDTs—have seen a 
similar rate of progress and model size expansion.12 Many cutting-edge BDTs, such as Evo, 
RFdiffusion, and RoseTTAFold, are fully open. Moreover, advances in LLMs and BDTs are 
complementary. LLMs can now assist users in accessing and using BDTs to perform complex 
scientific tasks, such as designing proteins to bind to the SARS-CoV-2 spike protein.13 Together, 
these advances are likely to lower the cost and decrease the skill required for researchers to 
use increasingly complex and powerful biological AI tools. Therefore, when assessing risks, 
dual-use foundation models should be understood to exist within the broader threat 
environment and not assessed singularly or within a vacuum. 
 
Open dual-use foundation models create special risks and benefits. Such models could benefit 
safety by allowing open access for independent experts to test model characteristics and risks 
and understand their inner workings,14 though such independent testing also could be done 
within closed systems that provide access to safety experts seeking to test the models.  
Openness, though, also poses serious risks. Researchers have shown that third parties can, at 
modest expense, strip out open dual-use foundation model safeguards and/or train open 
dual-use foundation models to create new (and potentially dangerous) capabilities. For 
example, scholars have trained Mistral 7B on the entirety of open-access content in the 
PubMed database to create “BioMistral,” a model they report provides “superior performance 
compared to existing open-source medical models and [a] competitive edge against 
proprietary counterparts.”15 Researchers at MIT, meanwhile, fine-tuned Llama-2-70B—at the 
 
12 See Maug et al., supra note 6; Cassidy Nelson and Sophie Rose, Examining Risks at the Intersection of AI and 
Bio, Ctr. Long-Term Resilience (2023), https://www.longtermresilience.org/post/report-launch-examining-risks-
at-the-intersection-of-ai-and-bio; Sarah R. Carter et al., The Convergence of Artificial Intelligence and the Life 
Sciences, NTI (2023), https://www.nti.org/analysis/articles/the-convergence-of-artificial-intelligence-and-the-life-
sciences/; Jacob T. Rapp et al., Self-driving Laboratories to Autonomously Navigate the Protein Fitness Landscape, 
1 Nature Chem. Engineering 97 (2024). See also, eg, Wei Feng et al., Generation of 3D Molecules in Pockets via a 
Language Model, 6 Nature Machine Intelligence 62 (2024); Google DeepMind Alpha Fold Team & Isomorphic 
Labs, Performance and Structural Coverage of the Latest, In-development AlphaFold Model, Alphabet (2023), 
https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/a-glimpse-of-the-next-generation-of-
alphafold/alphafold_latest_oct2023.pdf; Minkyung Baek et al., Accurate Prediction of Nucleic Acid and Protein-
Nucleic Acid Complexes Using RoseTTAFoldNA (working paper, 2022), 
https://www.biorxiv.org/content/10.1101/2022.09.09.507333v1; Joseph L. Watson et al., De Novo Design of 
Protein Structure and Function with RFdiffusion, 620 Nature 1089 (2023); Jiankun Lyu et al., AlphaFold2 Structures 
Template Ligand Discovery (working paper, 2023), 
https://www.biorxiv.org/content/10.1101/2023.12.20.572662v1. 
13 See, eg, The Impact of Large Language Models on Scientific Discovery: A Preliminary Study using GPT-4, 
Microsoft Research (working paper, 2023), https://arxiv.org/pdf/2311.07361.pdf. 
14 See, eg, Shayne Longpre et al., A Safe Harbor for AI Evaluation and Red Teaming (working paper, 2024), 
https://bpb-us-e1.wpmucdn.com/sites.mit.edu/dist/6/336/files/2024/03/Safe-Harbor-0e192065dccf6d83.pdf; 
Beren Millidge, Open Source AI Has Been Vital for Alignment, Beren’s Blog (Nov. 5, 2023), 
https://www.beren.io/2023-11-05-Open-source-AI-has-been-vital-for-alignment/. 
15 Emmanuel Morin et al., BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical 
Domains (working paper, 2024), https://arxiv.org/abs/2402.10373. 
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 5 
cost of only $200 in compute—to remove safeguards against providing virology-related 
answers in response to prompts that explicitly informed the model that the user was planning 
to release a bioweapon.16 Finally, we note that the creators of Evo, a reportedly highly capable 
BDT, excluded viruses that infect eukaryotes from Evo’s training set for safety purposes.17 
Because the model’s weights are freely available, however, we are aware of no technical 
hurdle preventing a third party from doing that training themselves at a fraction of the cost it 
took to create the original Evo model (assuming data availability). Indeed, less than a month 
after Evo was released, it had already been fine-tuned on a dataset of adeno-associated virus 
capsids, ie, protein shells used by a class of viruses that infect humans.18 As this case suggests, 
when a model’s weights are publicly available, a developer’s decision not to endow the model 
with dangerous capabilities is far from final.19 
 
As Sayesh Kapoor and colleagues caution, it is important to consider the marginal risk that 
open models pose above preexisting technologies.20 As of mid-2023, several small studies 
indicate that users with access to leading LLMs, even in one case a model with safeguards 
removed, were not statistically significantly better at planning biological weapons attacks than 
those with access to search engines alone.21 And creating a competent plan of attack is quite 
different from having the skills or resources to carry it out.22  
 
These caveats may provide cold comfort in the time ahead. First, dual-use foundation model 
capabilities are rapidly improving. It is impossible to predict with certainty how substantially 
LLMs will eventually improve over search-enabled bioweapons planning. But the fact that 
experts with GPT-4 access had improved accuracy scores on all five metrics of bioweapons 
planning surveyed by OpenAI (albeit, not statistically significantly) suggests that future dual-
use foundation models may provide marginal benefits over preexisting resources.23  
 
16 Anjali Gopal et al., Will Releasing the Weights of Future Large Language Models Grant 
Widespread Access to Pandemic Agents? (working paper, 2023), https://arxiv.org/abs/2310.18233. 
17 See Nguyen et al., supra note 5. 
18 Kenny Workman, Engineering AAVs with Evo and AlphaFold, LatchBio (March 20, 2024), 
https://blog.latch.bio/p/engineering-aavs-with-evo-and-alphafold. 
19 See also generally Tom Davidson et al., AI capabilities can be significantly improved without expensive 
retraining (working paper, 2023), https://arxiv.org/pdf/2312.07413.pdf. 
20 See Kapoor et al., supra note 3. 
21 See Patwardhan et al. supra note 9; Mouton et al., supra note 9. 
22 For a discussion of the tacit knowledge requirements for creating biological weapons, see Sonia Ben 
Ouagrham-Gormley, Barriers to Bioweapon: The Challenges of Expertise and Organization for Weapons 
Development (2014) and Kathleen M. Vogel, Phantom Menace or Looming Danger?: A New Framework for 
Assessing Bioweapons Threats (2012). 
23 See Gary Marcus, When Looked at Carefully, OpenAI’s New Study on GPT-4 and Bioweapons is Deeply 
Worrisome, Marcus on AI (Feb. 4, 2024), https://garymarcus.substack.com/p/when-looked-at-carefully-openais; 
Anjana Ahuja, AI’s Bioterrorism Potential Should Not Be Ruled Out, Fin. Times (Feb. 9, 2024), 
https://www.ft.com/content/e2a28b73-9831-4e7e-be7c-a599d2498f24; Matthew E. Walsh, How to Better 
Research the Possible Threats Posed by AI-driven Misuse of Biology, Bulletin of the Atomic Scientists (Mar. 18, 
2024), https://thebulletin.org/2024/03/how-to-better-research-the-possible-threats-posed-by-ai-driven-misuse-
of-biology. 
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 6 
 
Second, none of the small studies in the field so far have evaluated how much dual-use 
foundation models purposefully trained on relevant data (eg, virology literature) will 
marginally improve bioweapons development or assessed the interaction between LLMs and 
BDTs.24 Nor, to our knowledge, have there been any published evaluations of the marginal 
benefit BDTs like Evo or RFdiffusion could play in bioweapons design.  
 
Third, tacit knowledge and resource barriers are likely falling even as AI capabilities are 
increasing. A growing proportion of wet-lab work can be conducted by machines, including 
machines that researchers can pay to access remotely on a part-time basis.25 Dual-use 
foundation models, even those untrained for this purpose, have also shown facility at 
directing research robots to perform laboratory tasks.26 Taken together, these facts suggest 
that informational capabilities may play an increasingly large role in enabling high-
consequence biosecurity threats in the coming years. 
 
More empirical research is certainly called for. But given the risks involved, and the direction 
of dual-use foundation model capabilities, the US government should plan for a future in 
which there is a reasonable probability that open dual-use foundation models could provide 
meaningful assistance to those seeking to design and deploy biological weapons. 
The United States should set open dual-use foundation model policies that mitigate the 
highest-consequence biosecurity risks 
 
Dual-use foundation models can excel at many tasks, and therefore create many forms of risk. 
These actual and potential risks range from assisting fraudulent behavior and inadvertently 
cementing bias to enabling mass-casualty attacks. All these dangers are worthy of serious 
attention. But given the limited time the federal government has to develop its initial 
approach to such risks—in light of fast-approach EO deadlines and the rapid advances in AI 
model capabilities—we believe US agencies should, at a minimum, set open dual-use 
foundation model policies that address the most catastrophic risks, such as foundation models 
substantially enabling the creation of pandemic-capable pathogens. In its report, NTIA should 
 
24 Gopal and colleagues studied a model that was altered to be more helpful in planning a bioweapons attack but 
did not formally evaluate its efficacy or compare its assistance to access to the internet alone. 
25 See Rapp et al., supra note 11 (reporting on “the Self-driving Autonomous Machines for Protein Landscape 
Exploration (SAMPLE) platform for fully autonomous protein engineering. SAMPLE is driven by an intelligent 
agent that learns protein sequence–function relationships, designs new proteins and sends designs to a fully 
automated robotic system that experimentally tests the designed proteins and provides feedback to improve the 
agent’s understanding of the system.”); Tianhao Yu et al., In Vitro Continuous Protein Evolution Empowered by 
Machine Learning and Automation, 14 Cell Sys. 633 (2023); Filippa Lentoz & Cédric Invernizzi, Laboratories in the 
Cloud, Bulletin of the Atomic Scientists (July 2, 2019), https://www.ft.com/content/e2a28b73-9831-4e7e-be7c-
a599d2498f24; Tessa Alexanian, Develop A Screening Framework Guidance For AI-Enabled Automated Labs, Fed. 
Amer. Scientists (Dec. 12, 2023), https://fas.org/publication/bio-x-ai-policy-recommendations/. 
26 See Microsoft Research, supra note 12. 
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 7 
underscore the importance of prioritizing the mitigation of high-consequence biosecurity 
threats among other open dual-use foundation model risks. 
 
As a group of civil society organizations and academics recently wrote to the Secretary of 
Commerce, model openness provides significant benefit to society.27 This fact underscores the 
need for the US government to narrowly tailor rules and regulations on open dual-use 
foundation models to address the highest-consequence and best-supported safety concerns. 
 
We are particularly concerned that future dual-use foundation models may make it easier for 
scientists, and perhaps even those outside the scientific community, to create, cultivate, 
modify, and disseminate new or existing pandemic-capable pathogens. We are also concerned 
that dual-use foundation models may lower bioweapon program costs for nation states or 
other high-capability actors or enable such entities to develop pathogens with greater 
transmissibility or virulence than would be possible using traditional approaches to synthetic 
biology. As discussed above, these dangers are exacerbated by the existence of open, highly 
capable models that malicious actors (or benign but insufficiently cautious actors) could 
modify to improve dual-use biological capabilities. 
 
As we discuss at greater length in our recent response to the National Institute of Standards 
and Technology (NIST) request for information regarding its obligations under the EO,28 we 
believe the US government should prioritize developing policies that will mitigate the 
following high-consequence biological risks:  
1. An AI system that substantially accelerates or simplifies the reintroduction of extinct 
viruses with pandemic potential or viruses with pandemic potential that only exist 
now within research labs or virus repositories. 
2. An AI system that substantially enables, accelerates, or simplifies the creation of new 
or enhanced biological constructs that could start pandemics. 
At a minimum, this means the US government should develop evaluations that assess whether 
dual-use foundation models increase: (1) the possibility that users can synthesize pandemic-
capable pathogens that are either extinct or are limited to being in a lab or repository; or (2) 
the capability of a user to create a novel variant of a pathogen that has the potential to initiate 
a pandemic. 
 
The federal government should also consider policies that mitigate high-consequence 
biosecurity risks specific to open dual-use foundation models. In particular, the government 
should consider narrowly tailored limitations on dual-use foundation models that can 
substantially assist in enabling high-consequence biological attacks, since there may be 
settings in which it is appropriate for users to interact with dual-use foundation models with 
 
27 Accountable Tech et al., Letter to Gina Raimondo, Sec. Dept. of Commerce, March 25, 2024, 
https://cdt.org/wp-content/uploads/2024/03/Civil-Society-Letter-on-Openness-for-NTIA-Process-March-25-
2024.pdf. 
28 See Center for Health Security, supra note 6. 
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 8 
dangerous capabilities. For example, vaccine developers and cell biologists may need to have 
access to a range of advanced BDTs, potentially including those with dual-use capabilities, to 
the extent that the public health benefits of access exceed the risks. But the fact that some 
access to dual-use foundation models is justified does not itself justify unlimited access to 
those models. The government should therefore consider mandatory limitations in cases in 
which the risks of open access to a dual-use foundation model exceed its benefits. 
Narrowly targeted export controls may mitigate high-consequence biosecurity risks 
stemming from open dual-use foundation models 
 
NTIA should consider whether narrow, targeted export controls can serve as a useful 
regulatory tool to mitigate high-consequence biosecurity risks associated with open dual-use 
foundation models. The United States has used its broad export control authorities for more 
than 70 years to reduce access to biological weapons worldwide.29 The US also has for 
decades participated in an arrangement known as the Australia Group to maintain multilateral 
controls on advanced technology, including software, that could be used to develop biological 
weapons.30 Congress has recently reinforced this mandate, directing the DOC in 2018 to 
regulate the export of physical goods, software, and technical data, as well as the actions of 
US persons, in order to limit access to biological weapons anywhere in the world.31 
 
Export controls, despite their name, do not only regulate physical goods shipped abroad. They 
can also be used to control dual-use technical information shared in the United States. In rare 
and controversial instances, the government has used export controls to prevent the 
publication of software or computer files it deemed threatening to national security.32 The 
DOC conceivably could use its authority to restrict parties from making the weights of 
dangerous advanced dual-use foundation models freely available for download.33 
 
 
29 See Proclamation 3038, Enumeration of Arms, Ammunition, and Implements of War, 18 Fed. Reg. 7505, 7505 
(Nov. 25, 1953). 
30 See About Us – History, Australia Group (2023), 
https://www.dfat.gov.au/publications/minisite/theaustraliagroupnet/site/en/origins.html. See also Control List 
of Dual-use Biological Equipment and Related Technology and Software, Australia Group (2022), 
https://www.dfat.gov.au/publications/minisite/theaustraliagroupnet/site/en/dual_biological.html. Notably, the 
Australia Group’s software controls do not apply to software “in the public domain.” 
31 See 50 USC. §§ 4811(2)(A)(i); 4812(a)(2)(C); 4813(d). For regulatory instantiations, see, eg, 15 C.F.R. Part 774, 
Supp. No. 1 (Commerce Control List), at ECCNs 1C351, 1C352, 1C353, 1E001, 2B352.e, j; 15 C.F.R. § 744.6(b). For 
a longer discussion, see Doni Bloomfield, Export Controls and Artificial Intelligence Biosecurity Risks (working 
paper, 2024), https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4741033. See also Doron Hindin et al., The 
Role of Export Controls in Regulating Dual Use Research of Concern: Striking a Balance between Freedom of 
Fundamental Research and National Security, National Academies (2017), 
https://nap.nationalacademies.org/resource/24761/Strosnider-Hindin-Trooboff_Paper_012017.pdf. 
32 See Bloomfield, supra note 31; Steven Levy, Crypto (2001); Defense Distributed v. Dep’t of State, 838 F.3d 451 
(5th Cir. 2016). 
33 See Bloomfield, supra note 31. 
Response to NTIA RFC on Dual Use Foundation Artificial Intelligence Models with Widely Available Model 
Weights | 9 
NTIA should consider whether and under what circumstances the US—and potentially other 
members of the Australia Group or even a broader set of nations—should update export 
control rules to reduce high-consequence biosecurity risks associated with open dual-use 
foundation models. In doing so, NTIA should address what capabilities would justify export 
controls on model weights or actions, how controls might be narrowly tailored to apply only to 
the most concerning set of dual-use foundation models, and whether alternative models 
posing analogous risks or dangers would be readily available from sources the US government 
(or other Australia Group members) cannot control. 
 
In a comment to NTIA, a group of civil society organizations and academics, including the 
Federation of American Scientists and the Electronic Frontier Foundation, have cautioned the 
DOC against the application of broad export controls to “general purpose” models.34 We agree 
that open models can confer significant social benefits, including to public health, and that 
prior attempts to control open software provide a cautionary tale about the legal and practical 
challenges of applying export controls in this domain.35 As those commenters acknowledge, 
however, “there are some situations where openness may exacerbate risks from AI.” Given 
the comparative ease with which users can modify open models to remove safeguards or 
confer additional capabilities, we believe that openness may in some circumstances 
exacerbate the biosecurity risks associated with highly biologically capable models.36 
 
For these reasons, any export controls on open dual-use foundation model weights should be 
narrowly tailored to address high-consequence threats to safety, such as the high-
consequence biosecurity risks we outline above. In ongoing work, the Johns Hopkins Center 
for Health Security is studying specific model capabilities that could increase high-
consequence biosecurity risk on the margin. We look forward to sharing that research with 
NTIA and other US government agencies when appropriate. 
 
Given the uncertain nature of current and future open model capabilities, and the importance 
of open software, we are not suggesting that the DOC should impose export controls on dual-
use foundation models today. Rather, the risks posed by open, biologically capable dual-use 
foundation models are grave enough for the US government to prepare such policy options so 
they can be deployed when and if they become relevant. 
 
34 See Accountable Tech et al., supra note 27. 
35 See Craig Jarvis, Crypto Wars: The Fight for Privacy in the Digital Age (2021); Bernstein v. United States, 176 
F.3d 1132 (9th Cir.), reh’g en banc granted and opinion withdrawn, 192 F.3d 1308 (9th Cir. 1999); Junger v. Daley, 
209 F.3d 481 (6th Cir. 2000). 
36 See supra notes 15—26 and associated text for a discussion of potential risks. 
",Johns Hopkins,7102
NTIA-2023-0009-0301,"ASSOCIATION FOR LONG TERM EXISTENCE AND RESILIENCE (ALTER)
20/5 HEBESHT, REHOVOT, ISRAEL, 7621032
Tel: +972-51-246-9363
https://alter.org.il
Dual Use Foundation Artificial Intelligence
Models with Widely Available Model Weights
Response to NTIA Request for Comments
Note: We have responded to a subset of questions, and have not addressed all sub-questions. Our
non-response does not indicate that we think questions are not critical, but stem from our limited ability to
make useful comments. We hope and expect that other commenters will provide useful ideas or
suggestions in those areas.
Question 1 - How should NTIA define “open” or “widely available”
when thinking about foundation models and model weights?
First, we congratulate NTIA for differentiating between “Widely Available Model Weights” and “open.”
we strongly agree that the word “open” should be reserved for something more specific than “public” or
“widely available” model weights.
Historically, open source has referred to a collaborative method of software development and an ability of
users to re-compile the software. Neither is implied by public model weights, which are being labeled
“open.” Further, the word open is intended to imply a level of transparency that many “open-source”
models fundamentally lack, since they are potentially using non-public training data owned by the
developer, or training data that is copyrighted by others and has dubious provenance.
To be called open, as a parallel, the training data must be public, and available free of restrictive licenses.
In addition, details like the model hyperparameter search method, final hyperparameters, and training
code for the base model, as well as all RLHF examples and the training regime used, as well as any
changes introduced in other ways, should all be available. This allows another party to re-create the
model. This would be necessary, for example, in order for someone to independently verify that the model
they have does not have backdoors put in place by developers or other third parties1. Ideally, these models
would then use open licenses which apply the requirements to any derivative models.
All of that said, widely available models, whether open source or simply those with public model weights
or other forms of access, do have a variety of issues which the RF correctly notes should be addressed.
The later questions properly asks whether the additional availability of training data and source code
could increase risk of misuse, and other questions about access.
1 Yang, Haomiao, et al. ""A comprehensive overview of backdoor attacks in large language models within
communication networks."" arXiv preprint arXiv:2308.14367 (2023). https://arxiv.org/abs/2308.14367
a. Is there evidence or historical examples suggesting that weights of models
similar to currently-closed AI systems will, or will not, likely become widely
available? If so, what are they?
There is a wealth of information and potential sources for inferring this, which we hope others will
provide. However, one often ignored aspect of the question is unintentional leakage or theft. Given the
historical record of firms like Microsoft failing to secure their source code repositories, and the theft by
4chan of Meta's LLaMA model. Clearly, the cybersecurity of the labs is certainly not sufficient to ensure
that sophisticated attackers could not steal the model weights, and in some cases, publicize them.
b. Is it possible to generally estimate the timeframe between the deployment of
a closed model and the deployment of an open foundation model of similar
performance on relevant tasks? How do you expect that timeframe to
change? Based on what variables? How do you expect those variables to
change in the coming months and years?
No response.
c. Should “wide availability” of model weights be defined by level of distribution?
If so, at what level of distribution (e.g., 10,000 entities; 1 million entities; open
publication; etc.) should model weights be presumed to be “widely available”?
If not, how should NTIA define “wide availability?”
No response.
d. Do certain forms of access to an open foundation model (web applications,
Application Programming Interfaces (API), local hosting, edge deployment)
provide more or less benefit or more or less risk than others? Are these risks
dependent on other details of the system or application enabling access?
Access to the model at scale allows model-stealing attacks, though it is unclear how much these preserve
capabilities of the base model. API access which allows fine-tuning is a potential enabler of reversing
post-training safety features. Local hosting and edge deployment, on the other hand, create additional
risks of model theft or leakage.
i.
Are there promising prospective forms or modes of access that could
strike a more favorable benefit-risk balance? If so, what are they?
No response.
Question 2 - How do the risks associated with making model
weights widely available compare to the risks associated with
non-public model weights?
a. What, if any, are the risks associated with widely available model weights?
How do these risks change, if at all, when the training data or source code
associated with fine tuning, pretraining, or deploying a model is
simultaneously widely available?
Training data and source code seem unlikely to materially change the risk from open models, but it is
plausible that fine-tuning and RLHF data could make reversing safety measures somewhat easier. It seems
worthwhile to investigate whether safety measures can be reversed effectively without that data, however,
as it seems very unclear that the data would make a material difference.
If model weights are available anyways, barriers to releasing such data seem likely to be negative, on net,
due to the impacts on safety and other research.
b. Could open foundation models reduce equity in rights and safety-impacting
AI systems (e.g. healthcare, education, criminal justice, housing, online
platforms, etc.)?
It is possible that they reduce equity, but is seems far more likely that open models they increase equity.
The places where use of models to reinforce inequity, especially where there is profit in (intentionally or
accidentally) misusing the models, seem unlikely to be accelerated by more public or cheaper models,
whereas efforts to ensure models are equitable, or efforts to evaluate whether a given model is inequitable,
can be greatly assisted by having and being able to modify the model. Additionally, use of closed models
which are biased can more easily be legally challenged, or regulated, when other more open and/or more
fully audited models are available.
c. What, if any, risks related to privacy could result from the wide availability of
model weights?
First, it seems overwhelmingly likely that models training on non-public data would be able to be used to
access at least parts of that data. If a firm is willing to release model weights based on data they are not
willing to release, it is unclear what the longer term privacy of the training data is.. Therefore, widely
available model weights based on using non-public data is rolling the dice on whether future work will
allow others to extract that data. The presumption should be that such releases violate the privacy of any
persons whose data was used.
Second, misuse of these systems will create a wide variety of issues, but wide availability seems likely to
avoid risks similar to how search results are stored and reduce user privacy, by allowing many groups to
run the models.
d. Are there novel ways that state or non-state actors could use widely
available model weights to create or exacerbate security risks, including but
not limited to threats to infrastructure, public health, human and civil rights,
democracy, defense, and the economy?
i.
How do these risks compare to those associated with closed models?
First, it seems very important to address a fundamental assumption that many unthinkingly include in
their reasoning about open versus closed models, which is that AI firms will successfully keep model
weights from state actors or other sophisticated adversaries. This seems incredibly unlikely. State actors
sophisticated enough to run such models are likely to at least attempt to infiltrate not just premier AI labs
organizations, but the external servers on which they train, with supply chain and social engineering
attacks which will not necessarily even be visible to the organizations.
Second, given the computational costs of running SoTA models, it seems unlikely that the risk from
technologically unsophisticated actors is the critical issue, and their inability to get the latest models
seems like it doesn’t materially change the risk. This is especially true because closed models which
allow API access to LoRA and similar fine tuning seem likely to be approximately as vulnerable as open
models to having safety features reversed, and unless AI system providers are implementing oversight and
security that far exceeds the norm in the technology sector, and beyond that, is significantly more secure
than the frequently-circumvented types of AML/KYC/Fraud prevention used in banking and similar, it
seems very unlikely that non-state actors will not simply use shell companies, stolen credentials, or stolen
credit cards to sign up for the access and use the systems.
ii.
How do these risks compare to those associated with other types of
software systems and information resources?
The risks are difficult to ascertain, especially for future models, given the explicit attempts to build human
level intelligence and previous lack of success by firms in correctly predicting the capabilities of their
own models in advance. For this reason, it seems many currently-comparable systems are a poor reference
class - AI is not simply software.
However, other high-security software systems and resources are tested in ways that are not currently
used or feasible for LLMs, via extensive specifications prior to development, architecture maps, unit tests,
and finally, verification and validation of the systems. Not only that, but at least in domains where
security is taken seriously, even the final secured systems are fully air-gapped, or at least remain entirely
inside of well secured networks. We are unaware of, for example, any military intelligence systems which
are open to public API access, and if they exist, would question the wisdom and basic competence of
those deploying the systems.
e. What, if any, risks could result from differences in access to widely available
models across different jurisdictions?
The competitive impacts are plausibly significant, but given the current concentration of talent and
resources, reasonable standards which are enforced in the US seem likely to be enforced across most
model developers. This is because it seems to create a commercial disincentive from developing the most
advanced models in ways that the US does not allow even elsewhere, because of the prominence of the
US market. Even if not, the lack of availability within the US does not seem to change the risk from
foreign use of the systems; security researchers will still be able to test and research unsafe models, even
if they cannot be developed or used commercially or otherwise within the United States.
f. Which are the most severe, and which the most likely risks described in
answering the questions above? How do these set of risks relate to each
other, if at all?
No response.
Question 5 - What are the safety-related or broader technical
issues involved in managing risks and amplifying benefits of
dual-use foundation models with widely available model weights?
First, we agree with the implicit assertion that these are “dual-use” foundation models, by nature of being
open. Narrow technologies can often be non-dual-use - though even in that case, they can be misused.
General technologies can in some cases be restricted in ways that make their by-default dual-use nature
into something less dangerous, if control and oversight is exercised
However, general technologies which can be adapted freely are inevitably able to be used in ways that are
unconstrained. The very usefulness of language models is the way in which they can be used for any task.
AI is a tool which has many uses. There's no such thing as non-military, or non-medical, or
non-disinformation, or non-job-displacing general AI. If capable enough, there’s no such thing as a safe
general AI. There are only constraints on the general AI that (hopefully) prevent some uses. A capable
model can be used to plan a party, or a kidnapping, and can orchestrate a research project, or a terrorist
attack. Safety measures can make these systems partly or largely incapable of contemplated misuse, but
these safety measures can be reversed by those with access to model weights. Of course, similar risks may
exist for those who have API access for fine tuning, or even those who spend time finding the latest
jailbreaks, if there are not proactive measures to counter these uses.
1. What model evaluations, if any, can help determine the risks or
benefits associated with making weights of a foundation model widely
available?
For open models, a fundamental question for evaluations is the capability of the model, rather than the
specific evaluation of whether a model can be misused in a given form. Because publicly available
weights make safety features reversible, any evaluations that are intended to evaluate the risk should be
based on a pre-RLHF model.
The benefits of open model weights, however, are significant, both in terms of allowing research to
determine safety, and in terms of increasing our understanding of large models. That said, most of those
benefits are accrued by allowing research groups access. The unfortunate result of any such policy is that
access is exclusionary, and the administrative burdens of controlled access are significant.
Question 9 - What other issues, topics, or adjacent technological
advancements should we consider when analyzing risks and
benefits of dual-use foundation models with widely available
model weights?
The use of foundation models in combination with other technologies is a critical enabler of misuse. One
area which has been highlighted are biological design tools2, which are far more concerning for misuse by
sophisticated actors than language models alone - but language models can greatly lower the barrier for
their misuse. These tools are often public but difficult to use; experts can presumably already design and
build dangerous proteins, though these are far from actually weaponizable designs.
That said, there have been claims that bioterrorists would be able to commit attacks that they otherwise
would not. The current class of models, however, are feasibly used only by experts, rather than
unsophisticated users, and it seems that we are at least a generation away from the most concerning
applications in any case. Given that, now is the time to build robust evaluations, structured access models,
2 Sandbrink, Jonas B. ""Artificial intelligence and biological misuse: Differentiating risks of language
models and biological design tools."" arXiv preprint arXiv:2306.13952
(2023).https://arxiv.org/abs/2306.13952
and oversight capabilities, and investigate ways to enable future dangerous models, whether open or not,
to not be run in environments without safeguards.
",ALTER,3158
NTIA-2023-0009-0249,"Expert Insights 
Considerations for Dual-Use Foundation 
Models with Widely Available Weights 
Insights to Inform a Request for Comment by the National 
Telecommunications and Information Administration 
Everett Smith, Gaurav Sett 
RAND Global and Emerging Risks Division 
PE-A3309-1
March 2024 
About RAND
The RAND Corporation is a research organization that develops solutions to public policy challenges to help make communities 
throughout the world safer and more secure, healthier and more prosperous. RAND is nonprofit, nonpartisan, and committed to the 
public interest. To learn more about RAND, visit www.rand.org.
Research Integrity
Our mission to help improve policy and decisionmaking through research and analysis is enabled through our core values of quality 
and objectivity and our unwavering commitment to the highest level of integrity and ethical behavior. To help ensure our research 
and analysis are rigorous, objective, and nonpartisan, we subject our research publications to a robust and exacting quality-assurance 
process; avoid both the appearance and reality of financial and other conflicts of interest through staff training, project screening, 
and a policy of mandatory disclosure; and pursue transparency in our research engagements through our commitment to the open 
publication of our research findings and recommendations, disclosure of the source of funding of published research, and policies to 
ensure intellectual independence. For more information, visit www.rand.org/about/research-integrity.
RAND’s publications do not necessarily reflect the opinions of its research clients and sponsors.
Published by the RAND Corporation, Santa Monica, Calif.
© 2024 RAND Corporation
 is a registered trademark.
 
 
 iii 
About This Paper 
RAND’s Technology and Security Policy Center (TASP) conducts research on dual-use 
technologies, such as artificial intelligence (AI), and their relevance to the national security of 
the United States. As part of this work, TASP has investigated the risks and benefits of dual-use 
AI foundation models and policies that are relevant to models with widely accessible weights. 
This paper provides insights from RAND experts in response to the National 
Telecommunications and Information Administration’s Request for Comment related to the 
Biden administration’s Executive Order 14110 on Safe, Secure, and Trustworthy Development 
and Use of Artificial Intelligence. 
Technology and Security Policy Center 
RAND Global and Emerging Risks is a division of RAND that delivers rigorous and 
objective public policy research on the most consequential challenges to civilization and global 
security. This work was undertaken by the division’s Technology and Security Policy Center, 
which explores how high-consequence, dual-use technologies change the global competition and 
threat environment, then develops policy and technology options to advance the security of the 
United States, its allies and partners, and the world. For more information, contact 
tasp@rand.org. 
Funding 
Funding for this work was provided by gifts from RAND supporters. 
Acknowledgments 
The authors thank Kristin Leuschner, Karson Elmgren, Ashwin Acharya, Mauricio Baker, 
Greg Smith, and Robin Meili for their technical insights and assistance with this paper. We are 
particularly grateful for the valuable insights from our reviewers, Li Ang Zhang and Marjory 
Blumenthal  
 
 
 
 
 iv 
Summary 
The National Telecommunications and Information Administration (NTIA) has requested 
comment on the “potential risks, benefits, other implications, and appropriate policy and 
regulatory approaches to dual-use foundation models for which the model weights are widely 
available.”1 Drawing on our expertise and ongoing research, we respond to NTIA’s request by 
discussing the benefits and risks of open artificial intelligence (AI) foundation models and 
highlighting red lines—risk thresholds that warrant significant responses if crossed—as a 
potential approach to balancing the risks and benefits of open foundation models. 
Key Takeaways 
• Open foundation models can provide benefits, such as reducing AI market 
concentration and accelerating AI safety research. For instance, increased model 
access is helpful for dangerous capabilities evaluations and AI interpretability research. 
• Current evaluations show no significant biological or cyber risks from existing open 
foundation models. However, most evaluations do not use all available techniques to 
elicit model capabilities, and existing proprietary models show higher capabilities. As 
foundation model capabilities grow, future models may pose larger risks. 
• There are more-secure methods to capture the benefits of open foundation models 
than by publishing model weights. For example, structured access to models would 
allow for greater transparency and independent AI safety research while reducing the 
ability of malicious actors to misuse model weights if they contain dangerous 
capabilities. 
• Red lines provide a framework for targeted risk management of foundation models 
if they pose risks, while exempting less risky models. In this context, red lines are the 
point at which the risks of publishing model weights clearly outweigh the benefits, as 
informed by assessments of the risks, benefits, and alternatives to publishing weights. 
This approach allows targeted interventions on risky models without stifling innovation 
from less risky models. 
• The European Union (EU) AI Act already includes features of a red-line approach. 
The EU AI Act exempts open foundation models from many requirements, except when 
they are classified as posing “systemic risks” (e.g., a model is developed using large 
amounts of computational power [compute], on the scale of $50 million), in which case it 
must be evaluated for dangerous capabilities and steps must be taken to mitigate any 
risks. 
• Coordination with foreign partners will be critical to manage risks from open 
foundation models. Attempts to control any open foundation models should consider 
which foreign actors could re-create equally capable models, as those operating in 
 
1 NTIA, “Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights,” p. 14059. 
 
 
 v 
countries with different rules could make their model weights publicly accessible, 
undercutting U.S. controls. Such forums as the G7, AI Safety Summits, the International 
Dialogues on AI Safety, and the International Scientific Report on Advanced AI Safety’s 
Expert Advisory Panel could serve to facilitate that coordination. 
 
 
 
 vi 
Contents 
About This Paper ........................................................................................................................... iii 
Summary 
......................................................................................................................................... 
iv 
Considerations for Dual-Use Foundation Models with Widely Available Weights ....................... 
1 
Benefits and Risks of Open Foundation Models 
....................................................................................... 
2 
Red Lines for Targeted Risk Management 
................................................................................................ 
5 
Summary ................................................................................................................................................. 
12 
References ..................................................................................................................................... 
13 
 
 
 
 
 1 
Considerations for Dual-Use Foundation Models with Widely 
Available Weights 
In this paper, we provide expert insights in response to the National Telecommunications and 
Information Administration (NTIA)’s February 2024 Request for Comment related to the Biden 
administration’s Executive Order 14110, “Safe, Secure, and Trustworthy Development and Use 
of Artificial Intelligence.”2 This paper focuses on foundation models, a term that refers to any 
artificial intelligence (AI) model “that is trained on broad data . . . that can be adapted (e.g., fine-
tuned) to a wide range of downstream tasks.”3 We also discuss open foundation models, which 
the NTIA defines as “[d]ual use foundation models with widely available weights.”4 
The introduction of open foundation models has led to discussions about the potential 
benefits of these models in terms of fostering innovation, transparency, and scientific advances. 
At the same time, concerns have been raised about the potential for such models to pose “risks to 
security, equity, civil rights, or other harms due to, for instance, affirmative misuse, failures of 
effective oversight, or lack of clear accountability mechanisms,” particularly if the model’s 
weights are made widely available.5 
This paper is divided into two sections and addresses the following questions posed in the 
Request for Comment (shown in the order in which they are addressed in this paper): 
• Benefits and Risks of Open Foundation Models 
- Question 3: “What are the benefits of foundation models with model weights that are 
widely available as compared to fully closed models?” 
- Question 2: “How do the risks associated with making model weights widely 
available compare to the risks associated with non-public model weights?” 
- Question 1.i: “Are there promising prospective forms or modes of access that could 
strike a more favorable benefit-risk balance? If so, what are they?” 
• Red Lines for Targeted Risk Management 
- Question 8: “In the face of continually changing technology, and given unforeseen 
risks and benefits, how can governments, companies, and individuals make decisions 
or plans today about open foundation models that will be useful in the future?” 
- Question 7h: “What insights from other countries or other societal systems are most 
useful to consider?” 
 
2 NTIA, “Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights”; White 
House, “Executive Order 14110 of October 30, 2023.”  
3 Bommasani et al., “On the Opportunities and Risks of Foundation Models,” p. 3. 
4 NTIA, “Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights” p. 14060. 
5 NTIA, “Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights,” p. 14061. 
 
 
 2 
- Question 7g: “What should the U.S. prioritize in working with other countries on this 
topic, and which countries are most important to work with?”6 
Throughout the paper, each question is shown in an italicized heading before the discussion of 
that particular topic. 
Benefits and Risks of Open Foundation Models 
Question 3: “What are the benefits of foundation models with model weights that are 
widely available as compared to fully closed models?” 
A significant concern with the creation of advanced AI systems and the automation of human 
labor is the growing concentration of economic power in the hands of a few technology 
companies.7 When these companies prevent open access to their AI model weights, users of the 
model must send and receive messages from an AI model hosted in a data center. As a result, the 
AI developer (or the data center owner) can monitor and shape the behavior of the AI system by 
automatically filtering messages between the AI and the user for harmful content, even if users 
disagree with the AI developer on what constitutes such content. Additionally, smaller AI 
companies can be prevented from adapting foundation models that are developed by large 
companies to a specific business case. 
In contrast, open foundation models reduce barriers to entry to develop and fine-tune AI 
systems, providing the following benefits (among others): 
• Open foundation models may reduce market concentration. When smaller actors can 
access open foundation models, they can avoid the large expense of developing their own 
models and can therefore better compete with large tech companies in adapting the 
foundation model to a particular business context.8 Whether this access will be enough to 
maintain a competitive market for foundation model based products or services in general 
will depend on the price to develop and the performance of open models compared with 
closed models and on how the economics of fine-tuning, adapting, and serving 
foundation models differs in a particular business application between large and small 
companies.9 
• Open foundation models can decentralize decisions on acceptable model behavior. 
Making model weights widely accessible would allow users, rather than AI developers or 
data center owners, to determine for themselves what model behavior is acceptable. 
 
6 NTIA, “Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights,” pp. 14062–
14063. 
7 Vipra and Korinek, “Market Concentration Implications of Foundation Models.” 
8 Miller, “Open Foundation Models.” 
9 Widder, West, and Whittaker, “Open (for Business)”; Miller, “Open Foundation Models.” 
 
 
 3 
However, setting up their own safeguards on model behavior would be technically 
challenging for smaller organizations that are deploying open foundation models.10 
In addition, by providing greater accessibility to a wider variety of actors, open foundation 
models can enhance the following aspects of AI safety: 
• Publishing foundation model weights can aid in AI safety research. Sampling-only 
access to foundation models, which is the minimum provided for closed AI models, is 
insufficient for AI safety research, such as evaluating models for dangerous capabilities 
or developing tools to interpret AI systems. Providing researchers with the model weights 
can better facilitate this research.11 However, there are also more-secure alternatives to 
providing model weights that can capture many of the same benefits (see question 1.i). 
• Making foundation model weights accessible helps uncover vulnerabilities, biases, 
and potentially dangerous capabilities. With a wider set of eyes examining these 
models, there is a higher likelihood of identifying and addressing issues that might have 
been overlooked by the original developers, as is the case with open-source software 
broadly.12 This scrutiny is useful for developing AI systems that are secure, fair, and 
aligned with societal values. The detection and mitigation of biases in AI models, for 
instance, are critical steps toward ensuring that AI technologies do not perpetuate or 
exacerbate social inequalities. 
Question 2: “How do the risks associated with making model weights widely available 
compare to the risks associated with non-public model weights?” 
As discussed in the previous section, making model weights widely available has potential 
benefits for users of these models and for the public as a whole. However, the features of open 
foundation models might make them attractive to a variety of threat actors, including individuals 
and states, that seek to use the models for malicious purposes. Such purposes could include 
spreading disinformation, conducting surveillance, launching cyberattacks, or even developing 
biological, chemical, or nuclear weapons of mass destruction.13 
To prevent misuse, AI developers often implement safeguards on their foundation models to 
limit dangerous capabilities, such as fine-tuning models to refuse to output hazardous content or 
filtering the inputs and outputs of models.14 However, because open foundation models can be 
modified by users, these safeguards can be removed at low cost (e.g., $200).15 At least some 
compromised systems might be very hard to detect.16 
 
10 Kapoor et al., “On the Societal Impact of Open Foundation Models.” 
11 Seger et al., “Open-Sourcing Highly Capable Foundation Models.” 
12 Boulanger, “Open-Source Versus Proprietary Software.” 
13 Shevlane et al., “Model Evaluation for Extreme Risks.”  
14 Bai et al., “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.”  
15 Qi et al., “Fine-Tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend to!”; 
Gade et al., “BadLlama.” 
16 Hubinger et al., “Sleeper Agents.”  
 
 
 4 
Current evaluations do not show significant biological or cyber risks from existing open 
foundation models. The science of dangerous capability evaluations in AI is still evolving. Few 
dangerous capability evaluations have been conducted, and they have not covered all potential 
risks (we only cite cyber and biological risks in this paper, given their prevalence in discussions 
related to the Biden administration’s AI executive order, but these risks do not represent our full 
range of concerns for open foundation models). Additionally, evaluations have not covered all 
open foundation models (or all currently closed models whose weights could be made widely 
available), and many evaluations have not made use of common techniques to elicit dangerous 
capabilities.17 In light of these limitations, the results of evaluations should be interpreted with 
caution. However, several evaluations of various models for cyber misuse potential have not 
shown that existing open models possess significantly dangerous cyber capabilities (though 
leading closed models, which could become open, show stronger cyber capabilities).18 Likewise, 
two evaluations did not find significantly dangerous biological misuse capabilities in current 
large language models compared with internet search.19 Evaluations science will continue to 
evolve, and the capabilities of foundation models broadly may continue to improve at significant 
and surprising speed. Therefore, it may be warranted to prepare for a world in which evaluations 
provide evidence that foundation models possess significantly dangerous capabilities. 
Question 1.i: “Are there promising prospective forms or modes of access that could 
strike a more favorable benefit-risk balance? If so, what are they?” 
Publicly releasing model weights is an irreversible act. Once the weights are on the internet, 
they can be copied easily and are thus nearly impossible to remove. However, there are 
alternatives to widely releasing model weights that can capture many of the benefits while 
reducing risks. Importantly, these alternatives leave open the option to make model weights 
widely accessible in the future if there is evidence that doing so would pose only acceptable 
risks. 
Structured access is an alternative approach that can provide users with many of the 
benefits of making foundation model weights widely accessible while reducing some of the 
risks. The most common approach to structured access is to create flexible application 
programming interfaces (APIs) that allow researchers, small businesses, or the public to access 
the model.20 Updating APIs to provide greater access could significantly improve the 
transparency that third-party groups would have for proprietary AI systems, providing many of 
 
17 Model Evaluation and Threat Research, “Guidelines for Capability Elicitation.” 
18 Fang et al., “LLM Agents Can Autonomously Hack Websites”; Hazell, “Spear Phishing with Large Language 
Models”; Shestov et al., “Finetuning Large Language Models for Vulnerability Detection.” 
19 Mouton, Lucas, and Guest, The Operational Risks of AI in Large-Scale Biological Attacks; Patwardhan et al., 
“Building an Early Warning System for LLM-Aided Biological Threat Creation.” 
20 Shevlane, “Structured Access”; Anderljung et al., “Towards Publicly Accountable Frontier LLMs.”  
 
 
 5 
the benefits of possessing model weights directly.21 At the same time, structured access would 
prevent researchers and users from copying the model weights onto a new device and 
proliferating them to malicious actors. Once malicious actors—including individuals, nonstate 
actors, and nation states—possess the model weights, they could remove safeguards on model 
behavior and use the models to cause harm.22 
Market concentration and research independence concerns might emerge if today’s large, 
commercial AI developers are the sole body that decides how to provide structured access to 
their models. A noncommercial body could be granted the authority to manage or review 
decisions related to structured access to ensure it is being used equitably. Examples of bodies 
that could potentially fulfill this role are the National AI Research Resource, in partnership with 
foundation model developers and providers, and the Cybersecurity and Infrastructure Security 
Agency. 
Red Lines for Targeted Risk Management 
Question 8: “In the face of continually changing technology, and given unforeseen risks 
and benefits, how can governments, companies, and individuals make decisions or 
plans today about open foundation models that will be useful in the future?” 
We introduce the concept of red lines as a framework for managing the risks and benefits of 
open foundation models.  
What Are Red Lines? 
Red lines are thresholds for dangerous capabilities that warrant a significant response 
if crossed. In the context of this response, red lines represent dangerous capability thresholds 
beyond which the risks of making the model weights widely available would significantly 
outweigh the benefits. Agreements on red lines should necessarily include discussions of the 
methods used to assess whether the red line has been crossed.23 
Red lines are often discussed in the context of capabilities that are relevant to enabling 
chemical, biological, nuclear, or cyber attacks. However, red lines are a sufficiently flexible 
approach to risk management that various actors could design red lines based on their distinct set 
of risk concerns. For instance, actors concerned that AI models could be used to generate 
disinformation to interfere with elections could define red lines based on persuasion capability or 
hyperrealistic video generation capability. 
 
21 Casper et al., “Black-Box Access Is Insufficient for Rigorous AI Audits”; Bucknall and Trager, Structured Access 
for Third-Party Research on Frontier AI Models.  
22 Shevlane, “Structured Access.”  
23 In many cases, the capability of true concern can be difficult or impossible to measure directly, so, for practical 
purposes, a proxy must be used. 
 
 
 6 
Example Red Lines 
The following are some examples of potential red lines for biological, nuclear, and cyber 
capabilities:  
• Biological. A graduate student in biology with access to an academic lab and a given AI 
system can recreate the smallpox virus in X amount of time for less than $Y.  
• Nuclear. The AI system can, based on open-source information and its knowledge of 
nuclear physics and engineering, re-derive a given nuclear secret in less than X amount of 
time for less than $Y. 
• Cyber. The AI system can discover at least one novel zero-click vulnerability in common 
or critical operating systems for less than $Y.24 
These examples are purely for discussion. We think further work is needed to define functional 
red lines. 
How to Design Red Lines 
We outline the following potential steps in designing a red line: 
• Step 1: Brainstorm threats. Red lines should emerge from a process of threat-modeling 
and scenario-planning. Brainstorming potential ways in which an AI model could cause 
harm would obviously benefit from subject-matter expertise, but avoiding groupthink and 
other blinders is also important. Therefore, threat models should be sourced not only 
from well-credentialed experts but also from a wide and diverse array of contributors . 
However, in many instances, threat modelling will involve sensitive information that 
must be controlled, and researchers will need to exercise appropriate caution when 
selecting and involving contributors. Striking the appropriate balance will be a challenge. 
• Step 2: Identify bottleneck capabilities. A small number of AI capabilities will likely 
be bottlenecks for many threat models, making them promising candidates for red lines. 
Importantly, these capabilities do not need to appear imminent to be worthy of concern. 
Historically, progress in AI capabilities has surprised many experts.25 Future capability 
surprises should be expected and prepared for. 
• Step 3: Brainstorm mitigations and responses. A red line should tie a dangerous 
capability measurement to a response. Example mitigations for dangerous capabilities 
include pursuing alternative deployment strategies (such as structured access) or 
employing other safeguards (such as fine-tuning or watermarking). No such response is 
without flaw, and novel risk mitigation techniques will likely be required for future 
dangerous capabilities. 
• Step 4: Analyze the risks and benefits of capabilities and mitigations, given the 
threat models. A sense of the risks and benefits of the capabilities, as well as the 
mitigations involved in proposed red lines, will be crucial for analyzing whether the red 
lines are reasonable.  
 
24 A zero-click vulnerability is a particularly powerful kind of vulnerability that requires no user interaction to 
compromise a device. Numerous such vulnerabilities have been discovered for smartphones. For more information, 
see Kaspersky, “What Is Zero-Click Malware, and How Do Zero-Click Attacks Work?” 
25 Musser, “How AI Knows Things No One Told It.” 
 
 
 7 
• Step 5: Specify evaluation methods. Ideally, agreements on red lines would specify, at 
least at a high level, the evaluation methods to be used to measure whether a red line has 
been crossed. Agreeing on these methods in advance might help make future discussions 
of measurement results more objective, but stakeholders also run the risk of the 
evaluation science progressing in the time between the agreement and the triggering of 
the red line, so agreements on methodology will need to factor this risk in. 
What Are the Alternatives to Red Lines for Risk Management? 
Red lines are a flexible approach to risk management and are not mutually exclusive with 
most other risk management strategies, such as liability, prescribed safety standards, defense in 
depth, and other strategies with an ex ante or ex post character. In fact, we think red lines 
complement many of these alternative risk management approaches. 
What Are the Alternatives to Red Lines Based on Dangerous Capabilities? 
We have outlined red lines as thresholds of dangerous capabilities, but one could also set red 
lines to be thresholds of risk (e.g., the probability × severity of harm) or of training compute (the 
amount of computing power used to build a foundation model).  
Although risk (probability × severity of harm) thresholds would, in theory, better track the 
risks posed by foundation models than would dangerous capability thresholds, calculating the 
probability × severity of harm can be highly subjective in practice. Therefore, probability × 
severity of harm thresholds are poor tools for coordination because different parties might 
struggle to arrive at an agreed-on measurement result. Compute thresholds have the opposite 
problem. While they are much cheaper and more objective to measure than dangerous 
capabilities, they are also a much worse proxy for the risks posed by foundation models.  
To balance these factors, one may both define red lines according to a dangerous capability 
measurement and decide on the capability threshold based on probability and severity of harm 
considerations. Additionally, using a compute threshold as an initial indicator of whether a 
foundation model might pose significant risks and should then be evaluated for dangerous 
capabilities can help make evaluations more targeted. 
What Are the Advantages of Red Lines?  
There are numerous advantages of red lines, as follows: 
• Proactive risk management. Red lines enable stakeholders to proactively identify and 
mitigate potential large risks associated with foundation models. By establishing clear 
thresholds for dangerous capabilities, red lines serve as an early warning system that 
triggers necessary actions to prevent harm. 
• Clarity and consensus. The process of defining red lines can be organized to encourage 
a comprehensive discussion among stakeholders—including governments, companies, 
and the scientific community—to foster a consensus on what constitutes unacceptable 
risks. This collective understanding helps in setting clear standards and expectations, 
reducing ambiguity in risk management. 
 
 
 8 
• Flexibility and adaptability. Given the rapid evolution of AI technologies, red lines 
offer a flexible framework that can be updated or adjusted in response to new 
developments and insights. This adaptability ensures that risk management strategies 
remain relevant and effective over time. 
• Enhanced coordination and collaboration. Red lines facilitate coordination among 
actors by providing a shared language and benchmarks for discussing AI risks. This 
enhances collaborative efforts to address complex challenges and ensures a consistent 
approach to mitigating potential threats. 
• Public trust and confidence. By transparently defining and enforcing red lines, 
policymakers and technology developers can build public trust. Knowing that there are 
safeguards in place to prevent the misuse or unintended consequences of AI technologies 
reassures the public and stakeholders about the responsible development and use of 
foundation models. 
• Strategic guidance for research and development. Red lines can guide researchers and 
developers in identifying safe and responsible pathways for AI innovation. By 
understanding the boundaries of acceptable risks, the AI community can focus on 
advancements that offer significant benefits while minimizing potential harms. 
• Prevention of regulatory overreach. Establishing red lines allows for a more nuanced 
approach to regulation that avoids blanket bans or overly restrictive measures that could 
stifle innovation. By focusing on specific capabilities or outcomes, red lines ensure that 
regulations are targeted and proportionate to the actual risks. 
What Are the Challenges and Limitations of Red Lines? 
There are also numerous challenges and limitations to implementing red lines, including the 
following: 
• Complexity of AI systems. The intricate nature of advanced AI systems, characterized 
by their opaque decision making processes and unpredictable behaviors, poses a 
significant challenge to defining and enforcing red lines. Unlike more straightforward 
engineering domains, the emergent properties of AI systems can make it difficult to 
anticipate all potentially dangerous capabilities. 
• Incentives for evasion and gaming. The establishment of red lines may create 
incentives for AI developers to find ways to circumvent thresholds without technically 
violating them. Volkswagen famously designed a car to produce low emissions only 
when it was being evaluated by regulators.26 Similarly, AI developers could design their 
systems to perform safely only when being evaluated, leading to AI systems that, while 
not crossing red lines according to evaluations, still pose significant risks. Those 
designing or conducting evaluations should aim to prevent such gaming by, for instance, 
keeping the details of some evaluations secret from AI developers. 
• Disputes of measurement validity. The integrity of red lines can be significantly 
challenged by potential discrepancies in measurement methods and the subsequent 
disputes these differences can incite. Given the reliance on predefined results to signal the 
crossing of a red line, divergent measurement techniques can yield conflicting outcomes. 
Such inconsistencies may lead to catastrophic misinterpretations or undermine 
 
26 Jung and Sharon, “The Volkswagen Emissions Scandal and Its Aftermath.” 
 
 
 9 
collaborative efforts to manage risks, as motivated parties could exploit these variances to 
contest the accuracy of findings. This exploitation risks the erosion of trust and 
cooperation, particularly when actors, seeking to derive political or economic advantages, 
challenge measurement results after initially agreeing to red lines. 
As they stand, red lines are a largely theoretical framework, and as is the case in many policy 
frameworks, there are likely to be large gaps between theory and implementation.  
Question 7.h: “What insights from other countries or other societal systems are most 
useful to consider?” 
In this section, we only discuss the European Union’s approach to risk management for open 
foundation models. We would have liked to discuss how the People’s Republic of China is 
approaching this issue but were unable to find sufficient literature outlining their open 
foundation model policies. 
On March 13, 2024, the European Parliament passed the EU AI Act, a landmark piece of 
legislation governing AI systems in the EU.27 It is widely believed that the EU AI Act will set 
the tone for much of the world’s regulation of advanced AI systems, which will significantly 
affect AI companies in the United States that do business abroad, for better or worse.28 
The EU AI Act contains many of the key features of a red-line approach, including 
governing via defined thresholds of risk, targeted risk management that exempts non-risky 
models, and a heavy emphasis on evidence from evaluations and risk assessments. We 
discuss some of these features here. 
The EU AI Act categorizes AI model risks, with systemic risk being the highest. Systemic 
risks involve foundation models being misused for chemical, biological, nuclear, or cyber 
attacks, as well as models posing threats to democracy, human rights, critical infrastructure, and 
public safety, among other concerns.29 
Open foundation models are generally exempt from the Act's requirements unless they are 
deemed to pose systemic risks.30 All existing open foundation models are classified as not posing 
systemic risks. Foundation models (open or otherwise) are presumed to pose systemic risks if 
they required greater than 10^25 floating point operations to train, given that such large models 
are at the frontier of dangerous capabilities. However, the AI Office has the flexibility to adjust 
the threshold up or down, and it can designate specific models as posing systemic risks based on 
further criteria (such as the model’s degree of autonomy).31 
 
27 Gilchrist and Iordache, “World’s First Major Act to Regulate AI Passed by European Lawmakers.”  
28 Siegmann and Anderljung, The Brussels Effect and Artificial Intelligence. 
29 Council of the European Union, “Proposal for a Regulation of the European Parliament and of the Council Laying 
Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union 
Legislative Acts.” 
30 Future of Life Institute, “High-Level Summary of the AI Act.” 
31 European Commission, “Artificial Intelligence – Questions and Answers.” 
 
 
 10 
Providers of foundation models that pose systemic risks must perform risk assessments and 
mitigate risks. The protocol for determining how evaluations should be performed or which steps 
to mitigate systemic risks are acceptable is not yet specified.32 
As it stands, we have not seen how the EU AI Act will influence responsible AI development 
and innovation, and it contains many provisions not related to a red lines approach to AI risk 
management. However, observing the effects of the provisions of the EU AI Act related to a red 
lines approach which we outline here over the coming years may provide valuable insight into 
the implementation and effects of a red lines approach. 
Question 7.G: “What should the U.S. prioritize in working with other countries on this 
topic, and which countries are most important to work with?” 
Restricting the publication of open foundation model weights is an important potential policy 
lever to address the risk of dangerous capabilities falling into the hands of malicious actors, 
including states, nonstate actors, and individuals. However, attempts to control any open 
foundation models should consider which foreign actors could re-create equally capable models. 
Regardless of other issues, controls are only effective if actors that would misuse the controlled 
model do not already have access to an equivalent model or the ready ability to create one. 
Analogously, the 1954 Atomic Energy Act’s controls on the proliferation of nuclear weapon 
information are only coherent because few other actors have that information, and they are 
similarly unwilling to proliferate it.  
Models with the greatest potential for dangerous capabilities happen to be very 
expensive to develop. Initial evaluations in offensive cyber and deception capabilities show that 
the largest, most expensive, and generally most capable foundation models also possess the most 
dangerous capabilities.33 The most capable models with widely accessible weights cost around 
$1.6 million to develop (Meta’s Llama 2 model); however, this figure is dwarfed in comparison 
with the frontiers of AI research, where models such as GPT-4 and Gemini each likely took over 
$50 million to train.34 Given the increasing scale of funding and talent needed, few countries 
globally contain actors that will be able to compete at the frontier of AI development (and, 
consequently, few actors will be able to create the most dangerous models). However, the 
number of actors globally that can compete is not zero, and some actors that could not build a 
model themselves might be able to steal one from U.S. AI developers. 
 
32 Council of the European Union, “Proposal for a Regulation of the European Parliament and of the Council Laying 
Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union 
Legislative Acts.” 
33 Fang et al., “LLM Agents Can Autonomously Hack Websites”; Phuong et al., “Evaluating Frontier Models for 
Dangerous Capabilities.” 
34 Epoch, “Database of Notable Machine Learning Systems”; Will Knight, “OpenAI’s CEO Says the Age of Giant 
AI Models Is Already Over.” 
 
 
 11 
Coordinating with countries that could re-create (or otherwise acquire) frontier AI models 
will be critical to regulate the diffusion of dangerous AI capabilities. This coordination would be 
consistent with U.S. strategy on leading international standards development.35 Several countries 
with access to capital, talent, and high-performance compute could create foundation models 
with equally dangerous capabilities as those in the United States. Additionally, countries with 
strong offensive cyber programs could steal model weights from U.S. developers. If any of these 
countries chose to make the weights of models with dangerous capabilities widely accessible 
(e.g., by publishing them online), it might be extremely difficult or impossible to contain or roll 
back the diffusion of those dangerous AI capabilities to all kinds of malicious actors. Therefore, 
coordinating with these countries on any model weight controls (in addition to improving AI 
company cybersecurity and securing the global high performance compute supply) will be 
critical to preventing the diffusion of dangerous AI capabilities. 
International forums that could be used to facilitate coordination on controlling the diffusion 
of dangerous AI capabilities include the following: 
1. The G7. The G7—consisting of Canada, France, Germany, Italy, Japan, the United 
Kingdom, and the United States—has been meeting to coordinate principles and policies 
for managing risks from foundation models.36 
2. The International Scientific Report on Advanced AI Safety’s Expert Advisory 
Panel. Comprising scientists from 30 nations—including China and the United States—
as well as the United Nations and EU, this panel will author a report summarizing the 
state of the science on the capabilities and risks of advanced AI systems.37 
3. AI Safety Summits. The first AI Safety Summit was held by the United Kingdom in fall 
2023 and resulted in the signing of the Bletchley Declaration by 29 countries, which 
committed to coordinate further on the safe development and deployment of foundation 
models.38 The next AI Safety Summits will be held in South Korea and France in 2024.39 
4. The International Dialogues on AI Safety. These dialogues bring together leading AI 
scientists from around the world to discuss extreme risks to humanity from AI. In March 
2024, these scientists published a joint statement on red lines for dangerous AI 
capabilities.40 
 
35 White House, United States Government National Standards Strategy for Critical and Emerging Technology.  
36 White House, “G7 Leaders’ Statement on the Hiroshima AI Process.” 
37 United Kingdom Department for Science, Innovation, and Technology, “International Scientific Report on 
Advanced AI Safety: Expert Advisory Panel Members”; United Kingdom Department for Science, Innovation, and 
Technology, “International Scientific Report on Advanced AI Safety: Principles and Procedures.”  
38 Government of the United Kingdom, “The Bletchley Declaration by Countries Attending the AI Safety Summit, 
1-2 November 2023.” 
39 “South Korea and France to Host Next Two AI Safety Summits.”  
40 International Dialogues on AI Safety, homepage. 
 
 
 12 
Summary 
Although most existing open foundation models are highly beneficial and evaluations 
indicate that they currently pose no significant biological or cyber risks, future gains in the 
capabilities of foundation models could change the benefit-risk calculation on publishing model 
weights. The benefits and risks posed by open foundation models thus call for a nuanced, 
evidence-based policy approach that can track the actual risks posed by publishing the weights of 
particular models. Adopting and refining frameworks (such as the red-line approach), being 
informed by such examples as the EU AI Act, and engaging in international coordination provide 
promising options to promote the responsible advancement of open foundation models while 
minimizing their risks. 
 
 
 
 
 13 
References 
Anderljung, Markus, Everett Thornton Smith, Joe O’Brien, Lisa Soder, Benjamin Bucknall, 
Emma Bluemke, Jonas Schuett, Robert Trager, Lacey Strahm, and Rumman Chowdhury, 
“Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem 
Under the ASPIRE Framework,” arXiv, November 15, 2023.  
Bai, Yuntao, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn 
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al., “Training a Helpful and 
Harmless Assistant with Reinforcement Learning from Human Feedback,” arXiv, April 12, 
2022.  
Bommasani Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, 
Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al., “On the 
Opportunities and Risks of Foundation Models,” arXiv, July 12, 2022. 
Boulanger, Alan, “Open-Source Versus Proprietary Software: Is One More Reliable and Secure 
Than the Other?” IBM Systems Journal, Vol. 44, No. 2, 2005. 
Bucknall, Benjamin S., and Robert F. Trager, Structured Access for Third-Party Research on 
Frontier AI Models: Investigating Researchers’ Model Access Requirements, University of 
Oxford, October 2023.  
Casper, Stephen, Carson Ezell, Charlotte Siegmann, Naom Kolt, Taylor Lynn Curtis, Benjamin 
Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, et al., “Black-
Box Access Is Insufficient for Rigorous AI Audits,” arXiv, January 25, 2024.  
Council of the European Union, “Proposal for a Regulation of the European Parliament and of 
the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence 
Act) and Amending Certain Union Legislative Acts,” Interinstitutional File 
2021/0106(COD), January 26, 2024.  
Epoch, “Database of Notable Machine Learning Systems,” accessed on March 13, 2024. As of 
March 13, 2024:  
https://epochai.org/data/epochdb/table 
European Commission, “Artificial Intelligence—Questions and Answers,” updated December 
14, 2023. As of March 13, 2024:  
https://ec.europa.eu/commission/presscorner/detail/en/qanda_21_1683 
Fang, Richard, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang, “LLM Agents Can 
Autonomously Hack Websites,” arXiv, February 16, 2024.  
 
 
 14 
Future of Life Institute, “High-Level Summary of the AI Act,” February 27, 2024. As of March 
13, 2024:  
https://artificialintelligenceact.eu/high-level-summary/ 
Gade, Pranav, Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish, “BadLlama: Cheaply 
Removing Safety Fine-Tuning from Llama 2–Chat 13B,” arXiv, October 31, 2023.  
Gilchrist, Karen, and Ruxandra Iordache, “World’s First Major Act to Regulate AI Passed by 
European Lawmakers,” CNBC, March 13, 2024.  
Government of the United Kingdom, “The Bletchley Declaration by Countries Attending the AI 
Safety Summit, 1-2 November 2023,” November 1, 2023. As of March 13, 2024:  
https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-
declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-
november-2023 
Hazell, Julian, “Spear Phishing with Large Language Models,” arXiv, December 22, 2023.  
Hubinger, Evan, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, 
Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, et al., “Sleeper Agents: 
Training Deceptive LLMs That Persis Through Safety Training,” arXiv, January 17, 2024. 
International Dialogues on AI Safety, homepage, undated. As of March 22, 2024:  
https://idais.ai 
Jung, Jae C., and Elizabeth Sharon, “The Volkswagen Emissions Scandal and Its Aftermath,” 
Global Business and Organizational Excellence, Vol. 38, No. 4, May/June 2019. 
Kapoor, Sayash, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter 
Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, et al., “On the 
Societal Impact of Open Foundation Models,” arXiv, February 27, 2024.  
Kaspersky, “What Is Zero-Click Malware, and How Do Zero-Click Attacks Work?” webpage, 
undated. As of March 25, 2024:  
https://www.kaspersky.com/resource-center/definitions/what-is-zero-click-malware 
Knight, Will, “OpenAI’s CEO Says the Age of Giant AI Models Is Already Over,” Wired, April 
17, 2023.  
Miller, Kyle, “Open Foundation Models: Implications of Contemporary Artificial Intelligence,” 
Center for Security and Emerging Technology, March 12, 2024. 
Model Evaluation and Threat Research, “Guidelines for Capability Elicitation,” webpage, 
undated. As of March 26, 2024:  
https://metr.github.io/autonomy-evals-guide/elicitation-protocol/ 
 
 
 15 
Mouton, Christopher A., Caleb Lucas, and Ella Guest, The Operational Risks of AI in Large-
Scale Biological Attacks: Results of a Red-Team Study, RAND Corporation, RR-A2977-2, 
2024. As of March 13, 2024:  
https://www.rand.org/pubs/research_reports/RRA2977-2.html 
Musser, George, “How AI Knows Things No One Told It,” Scientific American, May 11, 2023. 
National Telecommunications and Information Administration, “Dual Use Foundation Artificial 
Intelligence Models with Widely Available Model Weights,” Federal Register, Vol. 89, No. 
38, February 26, 2024.  
NTIA—See National Telecommunications and Information Administration. 
Patwardhan, Tejal, Kevin Liu, Todor Markov, Neil Chowdhury, Dillon Leet, Natalie Cone, 
Caitlin Maltbie, Joost Huizinga, Carroll Wainwright, Shawn Jackson, et al., “Building an 
Early Warning System for LLM-Aided Biological Threat Creation,” OpenAI, January 31, 
2024. As of March 13, 2024:  
https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-
threat-creation 
Phuong, Mary, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria 
Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, et al., 
“Evaluation Frontier Models for Dangerous Capabilities,” arXiv, March 20, 2024. 
Qi, Xiangyu, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter 
Henderson, “Fine-Tuning Aligned Language Models Compromises Safety, Even When 
Users Do Not Intend to!” arXiv, October 5, 2023.  
Seger, Elizabeth, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K. Wei, 
Christoph Winter, Mackenzie Arnold, Seán Ó hÉigeartaigh, Anton Korinek, et al., “Open-
Sourcing Highly Capable Foundation Models: An Evaluation of Risks, Benefits, and 
Alternative Methods for Pursuing Open-Source Objectives,” arXiv, September 29, 2023. 
Shestov, Alexey, Rodion Levichev, Ravil Mussabayev, Evgeny Maslov, Anton Cheshkov, and 
Pavel Zadorozhny, “Finetuning Large Language Models for Vulnerability Detection,” arXiv, 
March 1, 2024.  
Shevlane, Toby, “Structured Access: An Emerging Paradigm for Safe AI Deployment,” arXiv, 
April 11, 2022.  
Shevlane, Toby, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade 
Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al., “Model 
Evaluation for Extreme Risks,” arXiv, September 22, 2023.  
 
 
 16 
Siegmann, Charlotte, and Markus Anderljung, The Brussels Effect and Artificial Intelligence: 
How EU Regulation Will Impact the Global AI Market, Centre for the Governance of AI, 
August 2022.  
“South Korea and France to Host Next Two AI Safety Summits,” Reuters, November 1, 2023.  
United Kingdom Department for Science, Innovation, and Technology, “International Scientific 
Report on Advanced AI Safety: Expert Advisory Panel Members,” February 1, 2024. As of 
March 13, 2024:  
https://www.gov.uk/government/publications/international-scientific-report-on-advanced-ai-
safety-expert-advisory-panel-and-principles-and-procedures/international-scientific-report-
on-advanced-ai-safety-expert-advisory-panel-members 
United Kingdom Department for Science, Innovation, and Technology, “International Scientific 
Report on Advanced AI Safety: Principles and Procedures,” February 1, 2024. As of March 
13, 2024:  
https://www.gov.uk/government/publications/international-scientific-report-on-advanced-ai-
safety-expert-advisory-panel-and-principles-and-procedures/international-scientific-report-
on-advanced-ai-safety-principles-and-procedures 
Vipra, Jai, and Anton Korinek, “Market Concentration Implications of Foundation Models: The 
Invisible Hand of ChatGPT,” Brookings Institution, September 2023. 
White House, “Executive Order 14110 of October 30, 2023: Safe, Secure, and Trustworthy 
Development and Use of Artificial Intelligence,” Federal Register, Vol. 88, No. 210, 
November 1, 2023.  
White House, “G7 Leaders’ Statement on the Hiroshima AI Process,” October 20, 2023. As of 
March 13, 2024:  
https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/g7-leaders-
statement-on-the-hiroshima-ai-process/ 
White House, United States Government National Standards Strategy for Critical and Emerging 
Technology, May 2023.  
Widder, David Gray, Sarah West, and Meredith Whittaker, “Open (for Business): Big Tech, 
Concentrated Power, and the Political Economy of Open AI,” August 17, 2023. 
",RAND,10757
NTIA-2023-0009-0263," 
1 
 
March 27, 2024 
 
Stephanie Weiner 
National Telecommunications and Information Administration 
U.S. Department of Commerce 
1401 Constitution Avenue NW, Washington, DC 20230 
 
RE: 
Dual Use Foundation Artificial Intelligence Models With Widely Available Model 
Weights, Docket No. 240216-0052 
 
Dear Ms. Weiner: 
 
The below signed civil, human, and technology rights organizations thank the National 
Telecommunications and Information Administration (NTIA) for its request for comment on “Dual 
Use Foundation Artificial Intelligence Models With Widely Available Model Weights”1 and its 
larger work to ensure that artificial intelligence is safe, trustworthy, and equitable. Although the 
emergence of foundation models with widely available model weights has spurred significant 
discourse about the potential risks they pose,2 we urge NTIA to recognize the corresponding 
benefits of “openness” as it considers appropriate regulatory approaches. In particular, 
“openness”—a term covering data, model architecture, model weights, and source code in the 
AI context—can help advance transparency around models that impact civil rights, safety, and 
access to opportunity, where AI is already being deployed and already affecting people’s lives. 
 
Both Advanced and More Rudimentary AI Technology Are Affecting Civil Rights Now, 
with the Potential for Large Foundation Models to Have Increased Impact in the Future 
Computer-based decision-making processes, including statistical models, automated systems 
and other forms of artificial intelligence are already affecting individuals’ access to crucial life 
opportunities.3 These systems range from basic statistical models to foundation models 
including large language models. Examples of sectors where both legacy and foundation 
models are being used to make decisions that impact opportunities include:  
 
● Housing: Tenant-screening algorithms are prone to errors and incorrectly include 
criminal or eviction records tied to people with similar names.4 Open Communities, a 
nonprofit housing advocacy organization, recently settled a lawsuit against a property 
management company and related organizations for the use of a chatbot, an application 
that can be driven by a foundation model, that systematically denied housing 
 
1 89 Fed. Reg. 14059 (Feb. 26, 2024), here [hereinafter RFC]. 
2 Sharon Goldman, Why Anthropic and OpenAI Are Obsessed with Securing LLM Model Weights, 
Venture Beat (Dec. 15, 2023), here; cf. The Leadership Conference on Civil and Human Rights, et.al., 
Civil Rights Principles in an Era of Big Data (2020) here. 
3 Cf. 15 U.S.C. § 9401(3); The White House, Blueprint for an AI Bill of Rights at 10 (2022), here (definition 
of “automated system”). 
4 Kaveh Waddell, How Tenant Screening Reports Make It Hard for People to Bounce Back From Tough 
Times, Consumer Reports (Mar. 11, 2021), here; Lauren Kirchner, Can Algorithms Violate Fair Housing 
Laws?, The Markup (Sept. 24, 2020), here. 
 
2 
 
opportunities for individuals and families that use housing choice vouchers.5 Moreover, 
dynamic rental pricing powered by AI can be a barrier to housing for rental voucher 
recipients as rent prices potentially fluctuate above HUD fair market rent amounts. This 
can negatively impact low wealth groups who are disproportionately single, female-
headed families with children and people with disabilities6 as well as people who live in 
rural communities.7 
● Employment: Employers are using large language models,8 which are instances of 
foundation models to evaluate job applicants, and those technologies can unfairly 
advantage male candidates or de-preference first-generation college graduates and 
racial minorities.9 Other AI-driven hiring technology such gamified personality tests can 
be inaccessible to applicants with disabilities.10 
● Credit: Credit scoring systems are algorithmic models that attempt to predict a 
borrower’s risk and how well that person is likely to repay their debt obligations. These 
systems typically generate a numerical score used to help players in the financial 
services system determine the creditworthiness of a consumer. They are often used as 
part of a lender’s decisions on underwriting and pricing. Algorithmic credit scoring 
disadvantages Black, Latinx, and Native American consumers who have historically had 
less access to traditional credit than white consumers.11 As Federal Reserve Vice Chair 
of Supervision Michael Barr stated, “Artificial Intelligence…relies on the data that is out 
there in the world and the data…is flawed. Some of it is just wrong. Some of it is deeply 
biased…Information we have on the Internet is imperfect…if you train a Machine 
Learning device, if you train a Large Language Model on imperfect data, you’re going to 
get imperfect results.”12 
 
5 Open Communities Reaches Resolution in Case Alleging AI Discrimination, Open Communities (Jan. 
31, 2024), here. 
6 See Claudia D. Solari et al., Housing Insecurity in the District of Columbia, Urban Institute (Nov. 16, 
2023), here; Sharon Cornelissen, The Pandemic Aggravated Racial Inequalities in Housing Insecurities: 
What Can It Teach Us about Housing Amidst Crisis?, Harvard Joint Center for Housing Studies (July 12, 
2023), here. 
7 See Irina Ivanova, Inflation is Hurting Rural Americans More Than City Folks - Here‘s Why, CBS 
MoneyWatch (Dec. 2, 2021), here 
here. 
8 Leon Yin et al., OpenAI’s GPT Is a Recruiter’s Dream Tool. Tests Show There’s Racial Bias, Bloomberg 
(Mar. 7, 2024), here. 
9 Avi Asher-Schapiro, AI is Taking Over Job Hiring, But Can It Be Racist?, Thomson Reuters (June 7, 
2021), here; Jeffrey Dastin, Insight - Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against 
Women, Thomson Reuters (Oct. 10, 2018), here. 
10 Lydia X.Z. Brown et al., Center for Democracy & Technology, Algorithm-Driven Hiring Tools: Innovative 
Recruitment or Expedited Disability Discrimination? (2020), here. 
11 Emmanuel Martinez & Lauren Kirchner, The Secret Bias Hidden in Mortgage-Approval Algorithms, The 
Markup (Aug. 25, 2021), here. 
12 See Federal Reserve Board of Governors Vice Chair Michael Barr, Setting the Foundation for Effective 
Governance and Oversight: A Conversation with U.S. Regulators, Responsible AI Symposium (Jan. 19, 
2024), here. 
 
3 
 
● Governmental Benefits: Algorithmic systems have been employed in a variety of 
governmental programs, including state Medicaid programs in Idaho and Arkansas, 
resulting in arbitrary denials of or reductions in benefits.13 
● Healthcare: An algorithm widely used in hospitals is less likely to refer Black patients for 
care than equally ill white patients,14 resulting in 28.8% of Black patients being 
incorrectly deemed ineligible for additional care.15 
● Education: Algorithmic proctoring tools fail to recognize students of color and flag 
“atypical” eye and body movements of students with disabilities as “cheating 
behaviors.”16 
● Child Welfare: An ACLU and Human Rights Data Analysis Group audit of an algorithmic 
risk-scoring system used in child welfare determinations highlighted the ways in which 
the algorithm could potentially disproportionately flag Black families and families with 
disabilities for investigation, due in part to the data sources it relied on.17 
● Insurance: Algorithms assign at least 10% higher car insurance premiums in minority 
zip codes than in majority-white zip codes18 and analyze individuals’ online activities to 
assign life insurance premiums.19 Additionally, the Casualty Actuarial Society (CAS) 
acknowledged that algorithmic bias can manifest in systems used in the insurance 
sector including underwriting, pricing, and claims models.20 
● Criminal Legal System: Algorithmic systems are used to predict the likelihood an 
incarcerated person will pose a risk on release, but those systems are punctuated by 
multiple decisions that incorporate existing disparities.21 
These examples span the range of technology that may be deemed “artificial intelligence,” and 
the emergence of applications such as chatbots and facial recognition technology underscores 
that both rudimentary and the most sophisticated AI technologies are already affecting civil 
rights, safety, and access to opportunities. 
 
 
13 Testimony of Ritchie Eppink, Hearing AI in Government Before the S. Comm. On Homeland Security & 
Government Affairs (May 16, 2023), here; Colin Lecher, What Happens When an Algorithm Cuts Your 
Health Care, The Verge (Mar. 21, 2018), here.  
14 Heidi Ledford, Millions of Black People Affected by Racial Bias in Health-Care Algorithms, Nature (Oct. 
24, 2019), here. 
15 Emily Sokol, Eliminating Racial Bias in Algorithm Development, Health IT Analytics (Dec. 26, 2019), 
here. 
16 Lydia X.Z. Brown, How Automated Test Proctoring Software Discriminates Against Disabled Students, 
Center for Democracy & Technology (Nov. 16, 2020), here; Todd Feathers & Janus Rose, Students Are 
Rebelling Against Eye-Tracking Exam Surveillance Tools, Motherboard (Sept. 24, 2020), here. 
17 Marissa Gerchick et al., ACLU, The Devil is in the Details: Interrogating Values Embedded in the 
Allegheny Family Screening Tool (2023), here.  
18 Julia Angwin et al., Minority Neighborhoods Pay Higher Car Insurance Premiums Than White Areas 
With the Same Risk, ProPublica (Apr. 5, 2017), here. 
19 Angela Chen, Why the Future of Life Insurance May Depend on Your Online Presence, The Verge 
(Feb. 7, 2019), here. 
20 Ronda Lee, AI Can Perpetuate Racial Bias in Insurance Underwriting, Yahoo! Money (Nov. 1, 2022). 
21 Marissa Gerchick & Brandon Buskey, ACLU, ACLU Statement on the PATTERN Risk Assessment Tool 
(2022), here [hereinafter ACLU PATTERN Statement]. 
 
4 
 
Existing AI systems also play a critical role in preparing and enriching the data used to train 
foundation models, including large language models. From housing decisions to insurance 
decisions to decisions made by criminal legal systems built on data-driven algorithms, outputs 
from legacy AI systems often serve as inputs for large language models, and there are multiple 
instances where legacy AI systems are either integrated with foundation models or used to 
provide information needed by foundation models to function effectively. As both large language 
models and existing AI systems continue to evolve, we can expect even more innovative ways 
to leverage their combined capabilities. Hence, we urge the NTIA to also consider the role of 
non-foundation AI models in its efforts to ensure that dual use foundation models do not lead to 
irreparable roadblocks in the path to equitable, life-changing opportunities.  
 
“Open” AI May Advance Regulatory Goals to Ensure Robust Auditing of and 
Transparency for Algorithmic Systems 
 
Responsive to Questions 1 and 3. 
 
Auditing and Transparency Are Crucial to Protect Civil Rights, Safety, and Access to 
Opportunity 
 
In addressing AI’s risks for civil rights, safety, and access to opportunity, advocates, affected 
communities, and policymakers have championed a number of regulatory goals, including 
auditing and assessments, transparency, and explainability.22 
 
Audits and assessments of algorithmic systems for discriminatory impacts, fitness for its 
intended purpose, and unintended outcomes should take place prior to deployment of an 
algorithmic system and on an ongoing basis afterwards. Audits and assessments should occur 
in a real-world context that anticipates the conditions in which the algorithmic system will be 
used—including accounting for likely operators’ training and consulting with impacted 
communities. Assessments and audits should not only identify potential harms but also 
implement mitigations. An algorithmic system should be deployed for a particular use case only 
if its harms are outweighed by its benefits and the harms have been appropriately mitigated.  
Ongoing post-deployment monitoring should continue to assess risks and develop new 
mitigation techniques to reduce those risks. Ultimately, AI that is biased should not be 
used.Where the algorithmic system’s risks exceed an acceptable level and where mitigation is 
not practicable, the algorithmic system should be discontinued or decommissioned.  
 
Crucially, to avoid conflicts of interests, audit and assessments should be conducted by 
independent third parties—who often do not have direct access to AI models or their underlying 
training data.23 Assessments and auditing are critical if AI systems are to be trusted. In addition, 
 
22 Other regulatory measures in addition to auditing and assessments, transparency, and explainability 
are also necessary to protect civil rights, safety, and civil liberties. See. e.g., Comments of the ACLU, 
Docket No. OMB-2023-0020 (2023), here; Center for Democracy & Technology et al., Civil Rights 
Standards for 21st Century Employment Selection Procedures (2022), here. 
23 The White House, Blueprint for an AI Bill of Rights at 20 (2022), here. 
 
5 
 
audits and assessments should be based on NIST’s AI Risk Management Framework, a 
foundational tool for the White House Executive Order on Safe, Secure and Trustworthy Use 
and Development of AI.24  
 
Similarly, transparency and explanations about how an algorithmic system functions are 
essential for protecting civil rights, safety, and access to opportunity. Transparency and 
explanations should include:25 
● the system’s purposes; 
● intended use cases; 
● the factors and data it relies on; 
● the data and factors’ relationship to the system’s intended purpose and each other; 
● steps taken to ensure that the training data and samples are accurate and 
representative; 
● measured disparate impact or discriminatory outcomes; and, 
● corresponding mitigations.  
Transparency will entail making explanations, audits, and assessments not only publicly 
available but also approachable and accessible. Approachable transparency will require making 
documentation available in a format that is brief, easy to read, and in plain language, so that 
people may readily understand it, in addition to more technical documentation. Accessibility 
requires that explanations and transparency be available in a variety of formats that 
accommodate individuals’ disabilities and other barriers to access.  
 
The Definition of “Open” AI Should Recognize Degrees of Openness and Corresponding 
Balancing of Risks and Benefits 
 
“Open” AI may advance each of those regulatory goals.26 As the NTIA recognizes, “openness” 
comes in a variety of degrees, from fully “closed” to fully “open.”27 Moreover, the application of 
the concept of “openness” to complex AI models is not always clear.28 As Gray Widder et al. 
recognized: 
Some systems described as ‘open’ offer little more than an API or the ability to download 
a hosted model. In these cases, many question whether the term should be applied at 
all, or if it is ‘openwashing’ systems that should be understood as ‘closed’. Other more 
 
24 The National Fair Housing Alliance published a framework that can be used to audit any algorithmic 
system. The framework can help AI developers and deployers to identify sources of bias or risks in their 
algorithmic systems. The framework can be accessed here, and used in conjunction with the NIST AI 
Risk Management Framework, here.  
25 Cf. CFPB Acts to Protect the Public from Black-Box Credit Models Using Complex Algorithms, 
Consumer Financial Protection Bureau (May 26, 2022), here. 
26 Nicol Turner Lee, Brookings, Written Statement to the U.S. Senate AI Insight Forum on Transparency, 
Explainability, Intellectual Property, & Copyright (2023), here (“Experts continuously emphasize the 
difficulty in understanding why an AI model behaves as it does . . . . That is why there is an increasing 
need and call for openness . . . .”). 
27 RFC at 14061. 
28 Ayah Bdeir & Camille François, Introducing the Columbia Convening on Openness and AI, Mozilla 
(Mar. 6, 2024), here; Stefano Maffulli, Open Source AI Definition: Where It Stands and What’s Ahead, 
Open Source Initiative (Feb. 7, 2024), here. 
 
6 
 
maximal versions of ‘open’ AI go further, offering access to the source code, underlying 
training data and full documentation, as well as licensing the AI system for wide reuse . . 
. . These precise distinctions matter, in part because the ideologies of ‘open’ and ‘open 
source’ are currently being projected onto ‘open’ AI systems even when they don’t fit—
ideologies that were forged decades ago in the context of open source software at a 
time when the current tech industry was emergent, not immensely powerful.29 
 
Further compounding the complexity around “open” AI is the fact that it is not always easy to 
separate “openness” from the business interests of large AI developers, who may benefit from 
open innovation on their platforms and may later withdraw commitments to openness after the 
benefits have reached a critical mass, knowing that smaller developers are unlikely to have the 
resources necessary to independently compete.30 
 
In light of those considerations and multiple degrees of “openness,” we believe that NTIA should 
define “open” AI as a range of “AI systems in which one or many components [such as data, 
model weights, source code, and model architecture of the AI system] are offered transparently, 
reusably, or in ways that allow third parties to extend and build on top [of the components or 
systems].”31 In that definition, openness is built on three attributes: 
● transparency, “the ability to access and vet source code, documentation and data”; 
● reusability, “the ability and licensing needed to allow third parties to reuse source code 
and/or data”; and, 
● extensibility, “the ability to build on top of extant off-the-shelf models, ‘tuning’ them for 
one or another specific purpose.”32 
That definition, however, should not be used in a vacuum. Instead, “openness” should always 
be discussed with reference both to the components of the AI system that are being made 
widely available—such as the model weights, architecture or coding, or training data—and the 
attributes that define the “open” system——namely, transparency, reusability, or extensibility. 
“Openness” varies wildly, depending on whether the components being made available are little 
more than an application program interface, documentation, the model weights, or the model’s 
code and training data. This approach to “openness” appropriately allows regulatory responses 
to the corresponding risks and benefits to be tailored appropriately, as described below. 
 
“Open” AI Will Further Civil Rights, Safety, and Access to Opportunity 
 
Each degree of openness may further civil rights goals of transparency and explainability, 
especially when bolstered by additional protections as necessary. The White House’s AI Bill of 
 
29 David Gray Widder et al., Open (For Business): Big Tech, Concentrated Power, and the Political 
Economy of Open AI, SSRN at 4 (2023), here. 
30 Id. at 3, 12; Devin Coldewey, Why Elon Musk’s AI Company ‘Open-Sourcing’ Grok Matters— and Why 
It Doesn’t, Tech Crunch (Mar. 18, 2024), here (“If you’re not already in possession of, say, a dozen Nvidia 
H100s in a six-figure AI inference rig, don’t bother clicking that download link.”); Benj Edwards, Elon 
Musk’s xAI Releases Grok Source and Weights, Taunting OpenAI, Ars Technica (Mar. 18, 2024), here. 
31 Gray Widder et al., supra note 29, at 3. 
32 Id. at 3-4. 
 
7 
 
Rights recognized the need for independent auditors to “be given access to the system and 
samples of associated data,”33 and varying degrees of openness will aid that access. 
 
The request for comment is “particularly focused on the wide availability, such as being publicly 
posted online, of foundation model weights,”34 pursuant to the charge in Section 4.6 of President 
Biden’s Executive Order on “Safe, Secure, and Trustworthy Development and Use of Artificial 
Intelligence.”35 Even this degree of openness can significantly aid transparency and 
explainability, especially if samples of associated data are provided. As others have observed, 
“Widely available model weights enable external researchers, auditors, and journalists to 
investigate and scrutinize foundation models more deeply,” including to assess harms to 
marginalized communities, by better understanding the relationship among the parameters 
evaluated by the model, especially in the context of sample data used to derive the weights.36  
 
“Openness” might also refer to openness around a model’s underlying training data, source 
code, and architecture.37 Many of the discriminatory harms perpetuated by AI can arise from the 
data used to train a model or the human judgments represented as the model’s code and 
architecture.38 For example: 
● The federal Bureau of Prison uses the “PATTERN” risk assessment tool to purportedly 
predict an individual’s likelihood of recidivism—but instead relying on historic data 
regarding recidivism per se, the model was developed with data regarding rearrest or 
return to custody following release, which more accurately reflects racially biased 
policing practices than an individual’s risk of committing a crime.39 Similarly, PATTERN’s 
thresholds for low- and high-risk scores reflect the value judgments of the model’s 
creators.40  
● An algorithm used in one Pennsylvania county to score families for investigation similarly 
relied on training data that potentially biased the algorithm’s predictive outcomes.41 The 
algorithm relied on existing government databases, including county child welfare, 
juvenile probation, and behavioral health records. Problematically, those databases 
reflect the lives of those who have more contact with government agencies and systems 
shaped by historical and ongoing discrimination—not necessarily those who pose 
greater “risk“ to their children—potentially over-representing some groups of people and 
underrepresenting others. 
In each of these cases, the availability of the training data, production data, model weights, and 
information about the design of the model were integral in assessing its potential impacts. 
 
33 The White House, Blueprint for an AI Bill of Rights at 20 (2022), here. 
34 RFC at 14062. 
35 Executive Order 14110 of October 30, 2023, 88 Fed. Reg. 75191 (November 1, 2023), here. 
36 Sayash Kapoor et al., On the Societal Impact of Open Foundation Models, arXiv at 4-5 (2024), here. 
37 Andreas Liesenfeld et al., Opening Up ChatGPT: Tracking Openness, Transparency, and 
Accountability in Instruction-Tuned Text Generators, arXiv at 1-3 (2023), here. 
38 E.g., ACLU PATTERN Statement, supra note 21; Ivey Dyson, How AI Threatens Civil Rights and 
Economic Opportunities, Brennan Center (Nov. 16, 2023), here. 
39 ACLU PATTERN Statement, supra note 21, at 4-5. 
40 ACLU PATTERN Statement, supra note 21, at 7 n.21. 
41 Marissa Gerchick et al., ACLU, The Devil is in the Details: Interrogating Values Embedded in the 
Allegheny Family Screening Tool (2023), here. 
 
8 
 
Voluntary commitments to openness can aid in future assessments and audits of AI’s impact on 
civil rights, safety, and access to opportunities. AI systems used in such consequential ways 
should be able to withstand scrutiny. 
 
Appropriately Tailored Regulatory Responses May Mitigate the Risks of Both Open and 
Closed AI 
Despite the benefits of openness for civil rights, safety, and access to opportunity, open AI—like 
“closed” AI—may still pose risks, and regulatory responses should be tailored accordingly. As 
others have observed, “[E]ven the most open of ‘open’ AI systems do not, on their own, 
ensure democratic access to or meaningful competition in AI, nor does openness alone solve 
the problem of oversight and scrutiny.”42 The same harms that might be posed by any AI 
system, such as discriminatory hiring decisions or the automated rejection of housing choice 
vouchers, may be posed by open AI. Consequently, any system that might plausibly—or 
implausibly—qualify for the label “open”43 should not be automatically exempted from 
fundamental regulatory protections. 
 
For example, the use of open source AI by employers to evaluate job applicants raises the 
same concerns as the use of “closed” AI of discrimination and a poor fit for purpose. Employers 
that deploy that AI to make hiring decisions—and the vendors that develop or fine-tune it for that 
purpose—should subsequently be subject to requirements, such as those to audit and evaluate 
their systems both by themselves and independent third parties.44 Although “open” AI may 
substantially advance transparency and explainability, it should not serve as a broad exemption 
from regulation. Similarly, the release of data sets collected by a developer to train models 
should be subject to the same laws as any release of information, such as privacy protections 
for personal information contained in the data set.45 
 
In imposing obligations on the deployers and developers of AI systems, policymakers should 
also be cognizant of the position of open foundation models in the wider AI ecosystem.46 To the 
extent it is appropriate to impose obligations on the developers of open foundation models, they 
should be carefully tailored to those developers’ role in the open ecosystem without diminishing 
the benefits of open AI.47 In the end, both developers and deployers of AI systems have a 
shared responsibility to ensure that the systems they design, develop, and deploy are fit for 
purpose—that they work and are not biased.  
 
42 Gray Widder et al., Open (For Business), supra note 29, at 7. 
43 Gray Widder et al., Open (For Business), supra note 29, at 2 (“Having a grasp on the multiplicity of 
definitions for ‘open’ and ‘open source’ AI, alongside what such systems do and do not enable, also 
matters because classifying a system as ‘open’ or ‘open source’ could have downstream effects on how it 
is regulated.”). 
44 See ACLU et al., EEOC Priorities on Automated Systems and Technological Discrimination at 7-9 
(2024), here (“[T]he EEOC should issue guidance that clarifies when digital platforms and software 
vendors can themselves be liable for the discriminatory functioning of their tools.”); CDT et al., supra note 
22, Civil Rights Standards. 
45 Cf. CDT et al., supra note 22, Civil Rights Standards at 30-31 (requiring retention of records for audits 
to comply with privacy laws). 
46 Sayash Kapoor et al., On the Societal Impact of Open Foundation Models, arXiv at 9 (2024), here.  
47 Id.  
 
9 
 
 
""Open"" AI Could Help Mitigate Market Concentration In Emerging AI Markets and 
Democratize Its Benefits, Including for Marginalized Communities. 
 
Advanced AI models like foundation models require vast amounts of training data and high-
powered compute, meaning that large existing tech companies already have a strong 
advantage.48 There are already great fears that the AI revolution will further entrench existing 
Big Tech companies instead of advancing a more traditional cycle of disruption that comes from 
new challengers. Existing incumbents are increasingly investing in and licensing the technology 
to newer AI companies, creating concerns about potential conflicts.49  
 
The potential promise of “open” AI is that it may allow increased competition and customization 
of AI models, disrupting the potential concentration developing in the advanced AI market. 
However, this competition will only exist if “open” AI models are able to be hosted at the scale 
necessary for success. Additional competition concerns arise from the high-powered compute 
required to train or run advanced AI models like large language models. Because few 
companies can afford to build or run their own infrastructure, most AI companies are restricted 
to a handful of major commercial cloud computing vendors to develop or host their AI models.50 
Each of those major commercial cloud computing vendors is also developing proprietary closed 
AI models,51 which they will sell alongside their commercial cloud computing hosting services. 
Currently, the major commercial cloud computing vendors allow other AI models, including 
“open” AI models, to be hosted on their cloud computing services.52 But there is no requirement 
for any major commercial cloud computing vendors to allow “open” AI models to be hosted on 
their services, and the potential for self-preferencing may make the use of non-native AI models 
more difficult or expensive. Extra attention should be paid to the commercial cloud computing 
vendors and other infrastructure, and the ways that “open” AI may enable competition in AI 
markets.53 
 
In addition, the National Artificial Intelligence Research Resource Task Force emphasized the 
importance of public access to resources for building AI in its 2023 report. By making datasets, 
computing power, and educational materials openly available, researchers outside of well-
funded corporations or labs gain the tools necessary to develop competitive AI solutions. This 
broader participation can prevent a small number of companies from controlling the field's 
 
48 Amba Kak, Sarah Myers West & Meredith Whittaker, Make No Mistake—AI Is Owned by Big Tech, MIT 
Technology Review (Dec. 5, 2023), here; Parmy Olson, Big Tech Has a Troubling Stranglehold on 
Artificial Intelligence, Bloomberg (June 20, 2023), here. 
49 FTC Launches Inquiry into Generative AI Investments and Partnerships, Federal Trade Commission 
(Jan. 25, 2024), here. 
50 Amba Kak and Sarah Myers West, AINow, 2023 Landscape: Confronting Tech Power at 6 (2023), 
here. 
51 Azure OpenAI Service, Microsoft, here (last visited Mar. 25, 2024); Krystal Hue, Amazon Dedicates 
Team to Train Ambitious AI Model Codenamed 'Olympus’, Reuters (Nov. 8, 2023); Overview of 
Generative AI on Vertex AI, Google, here (last visited Mar. 25, 2024). 
52 Amazon Bedrock, Amazon, here (last visited Mar. 25, 2024). 
53 Comments of the Center for American Progress, Solicitation for Public Comments on the Business 
Practices of Cloud Computing Providers, Docket No. FTC-2023-0028 (June 21, 2023), here. 
 
10 
 
direction and potentially lead to more innovation and accessibility in the long run.54 As seen in 
other technological contexts, diffusing market concentration, especially over gateway or 
bottleneck facilities, can increase the diversity of voices, including for marginalized 
communities.55 
Conclusion 
 
We thank the NTIA and the Administration for their continued focus on mitigating the potential 
civil rights harms of AI, including bolstering transparency and accountability. If you have any 
questions about these comments, please do not hesitate to contact us at cvenzke@aclu.org and 
reponsibleai@nationalfairhousing.org. 
 
Sincerely, 
 
American Civil Liberties Union 
 
Center for American Progress 
Leadership Conference on Civil and Human Rights  
National Fair Housing Alliance 
 
54 As part of the National Artificial Intelligence Initiative Act of 2020, Congress established the National 
Artificial Intelligence Research Resource (NAIRR) Task Force to ""investigate the feasibility and 
advisability of developing"" the NAIRR as a national AI research cyberinfrastructure, and ""to propose a 
roadmap detailing [how the NAIRR] should be established and sustained."" National Artificial Intelligence 
Research Resource Task Force, Strengthening and Democratizing the U.S. Artificial Intelligence 
Innovation Ecosystem at 4 (2023), here. 
55 E.g., Comments of the Leadership Conference, ACLU, et al., Amendment of Section 73.3555(e) of the 
Commission’s Rules, MB Docket No. 17-381 (Mar. 19, 2018), here (“When there are more owners, it is 
more likely that a woman or person of color, or a member of any other underrepresented group, can 
purchase a station.”); Comments of the Leadership Conference, ACLU, et al, 2010 Quadrennial Review, 
MB Docket No. 09-182 (Mar. 5, 2012), here; Comments of the ACLU, 2002 Biennial Regulatory Review, 
MB Docket No. 02-277 (May 22, 2003), here (“Government action should be exercised to promote greater 
competition and thus to encourage diversity of views. Extreme care should be taken by the Commission 
to see that as a practical matter, no monopoly in the presentation of news and opinion is created.”); 
Comments of the ACLU, Framework for Broadband Internet Service, GN Docket No. 10-127 at 6 (July 15, 
2010), here; cf. Sanda Fulton, We Still Need Diversity and Minority Ownership in Our Media, ACLU (Jan. 
19, 2012), here. 
",ACLU,7171
NTIA-2023-0009-0277,"Microsoft Corporation                         Tel 425 882 8080 
One Microsoft Way                              Fax 425 706 7329 
Redmond, WA 98052-6399                 www.microsoft.com 
 
 
 
The Honorable Alan Davidson 
National Telecommunications and Information Administration 
1401 Constitution Ave NW, Washington, D.C. 20230 
Submitted electronically at: www.reginfo.gov/public/do/PRAMain  
March 27, 2024 
RE: Request for Comment (RFC) on Dual Use Foundation Artificial Intelligence Models 
with Widely Available Model Weights (Docket No. NTIA–2023–0009) 
Microsoft appreciates the opportunity to respond to the NTIA’s request for comment on dual 
use foundation artificial intelligence models with widely available model weights. This is an 
important and timely topic as foundation models are becoming more powerful and pervasive, 
and organizations across the public, private, and non-profit sectors are putting them to work in 
a wide range of scenarios. 
At Microsoft, we are committed to advancing the state of the art in AI and ensuring that our AI 
products and services are safe, secure, and trustworthy. We believe that foundation models, 
such as large language models (LLMs), have great potential to enable new capabilities and 
benefit society broadly. Foundation models with model weights that are widely available can 
especially drive an open innovation cycle that benefits not only open source projects but also 
the extensive value chain built on those projects. They can also play a key role in improving AI 
safety and security, enabling a diverse global community to learn about responsible design, 
contribute to research, and participate in the development of models used to support safety and 
security tasks. But, like all technologies, foundation models, including both fully closed models 
and those with model weights that are widely available, need to be developed and deployed 
responsibly in order to realize that potential. 
In our response, we offer recommendations for how NTIA and other stakeholders can foster an 
effective and well-calibrated approach to the governance of open foundation models with 
widely available model weights. At a high level, we recommend: 
• 
Establishing clear and consistent definitions for open source AI models, components of AI 
models, and distribution methods for AI models. These definitions will allow policymakers to 
target specific attributes that are introducing risk, such as the breadth of deployment or the 
 
2 
 
ability to fine-tune or otherwise modify a model, increasing the effectiveness of any policy 
interventions and reducing undue burdens. 
• 
Developing and adopting voluntary risk-based and outcome-focused frameworks and 
guidance for the responsible release of foundation models and model weights. These 
frameworks and guidance will set expectations for stakeholders while longer-term 
international standards are developed. 
• 
Promoting risk and impact assessments that are grounded in the specific attributes of widely 
available model weights that present risk, the marginal risk of such availability compared to 
existing systems, and the effectiveness of risk mitigations across end-to-end scenarios of use. 
• 
Cultivating a healthy and responsible open source AI ecosystem and ensuring that policies 
foster innovation and research. This will be achieved through direct engagement with open 
source communities to understand the impact of policy interventions on them and, as 
needed, calibrations to address risks of concern while also minimizing negative impacts on 
innovation and research. 
• 
Encouraging global research and collaboration on AI fundamentals, AI safety and security, 
and AI applications. The Federal government can incentivize research on risk evaluations and 
mitigations in a way that ensures that policies, such as export controls, continue to allow for 
appropriate global collaboration on research. 
• 
Expanding the technical and organizational capabilities and capacities within the public 
sector for the evaluation of AI risks and capabilities. The public sector has a unique ability to 
coordinate and prioritize the evaluation of AI risks and capabilities, but this will require 
ongoing investment to ensure that emerging risks and capabilities are addressed. 
Thank you again for the opportunity to contribute our views. We welcome further dialogue as 
NTIA develops its report as well as any policy and regulatory recommendations designed to 
address both risks and opportunities. 
Sincerely, 
 
Natasha Crampton 
Vice President and Chief Responsible AI Officer 
Microsoft Corporation 
 
 
3 
 
How should NTIA define “open” or “widely available” when thinking about foundation 
models and model weights? 
Recommendations: 
1. Treat “open source” as a concept distinct from “availability,” and target policy 
recommendations at the appropriate concept for the intended outcomes. 
2. Avoid “open” as a standalone concept as it can be construed to mean either “open source” 
or “widely available model weights.” 
3. When well-understood concepts (preferably defined by an international standards body or 
relevant non-profit foundation) do not exist, refer to specific defining attributes of the 
concept. For example, a policy could target “models that can be modified or fine-tuned.” 
For a policy to be effective, its scope needs to be clearly defined, which requires shared 
understanding between the author of the policy, the entities to which it applies, and its 
enforcers. Poorly scoped policies may be inefficient, imposing burdens unnecessarily on low-risk 
use cases, or ineffective, failing to achieve desired policy outcomes for higher-risk use cases. In 
the present context, “availability” and “openness,” while related, may have vastly different 
interpretations, risks, and benefits, and the NTIA must tease apart and account for these 
differences in any policies NTIA chooses to adopt or recommend on this topic.  
As described in Considerations for Governing Open Foundation Models,1 the “availability” of an AI 
model and its components sits on a gradient: 
• 
Widely available weights 
o Weights, data, and code available – without use restrictions 
o Weights, data, and code available – with use restrictions 
o Weights available 
• 
Hosted or API access – accessible via a hosted application or API 
• 
Fully closed – not accessible outside the developer organization 
Other variations, beyond this list, are possible. For example, if the data and code is available for 
a model, but the model developer does not provide pre-computed weights, then the model may 
still be considered to have widely available weights if typical model consumers (including end 
users and downstream developers) have sufficient resources and documentation to train the 
model themselves.  
“Openness” is harder to define and depends on the freedoms expected for something to be 
considered “open.” For some, “widely available weights,” which would allow fine-tuning of a 
 
1 https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf  
 
4 
 
model, may be sufficient to consider an AI model “open.” For others, unrestricted permission to 
use, modify, and redistribute the AI model (which requires code, and may require weights 
and/or data) is needed. The terms under which the weights, and other components, are made 
available will define its “openness” and suitability for different uses. 
Traditional software has confronted similar definitional challenges, and, over time, this has 
resulted in both formal and informal definitions to help stakeholders communicate effectively, 
including: 
• 
Open Source Software – software with a license that meets the Open Source Initiative’s 
Open Source Definition2. 
• 
Free Software – software that meets the Free Software Foundation’s Free Software 
Definition3. 
• 
Source-Available Software4 – software for which source code is available, but which has a 
license that is incompatible with the Free Software and Open Source Definitions. 
• 
Closed-Source Software5 – software for which source code is not available. 
The Open Source Initiative6 (OSI) has started to address these challenges for AI models by 
developing the Open Source AI Definition,7 which is currently in draft. In contrast to “availability,” 
which primarily focuses on distribution, the Open Source AI Definition focuses on the freedoms 
that the AI model’s licensing terms grant to developers to be able to use and improve the model 
for their use case. 
As part of the definition, OSI also identifies the components of an AI model needed to exercise 
those freedoms. For example, modifying or re-training an AI model requires the training data 
and code, so those must be provided under an OSI-approved license for an AI model to be 
considered open source under the definition. This means that “open source” AI models will have 
minimum levels of “availability,” but certain levels of “availability” do not automatically imply 
that the AI model is “open source” unless the freedoms required by the definition have been 
satisfied. 
When scoping any policies, it is important that the NTIA articulates its goals clearly in order to 
drive clarity. If NTIA’s goal is to address risks based on the potential distribution of a model – for 
 
2 https://opensource.org/osd  
3 https://www.gnu.org/philosophy/free-sw.html  
4 https://en.wikipedia.org/wiki/Source-available_software  
5 https://en.wikipedia.org/wiki/Proprietary_software  
6 Microsoft is a sponsor of the Open Source Initiative (OSI), a “501(c)3 non-profit raising awareness and adoption of 
open source software (OSS) through advocacy, education and building bridges between communities.” 
7 https://opensource.org/deepdive/drafts/the-open-source-ai-definition-draft-v-0-0-6  
 
5 
 
example, preventing proliferation of harmful AI models – then the scoping should be based on 
“availability.” Alternatively, if NTIA’s goal is to address models developed and released under 
terms granting certain freedoms, for example, to promote innovation by excluding those from 
specific requirements, then scoping should be based on “open source.” 
How do the risks associated with making model weights widely available compare to the 
risks associated with non-public model weights? 
Recommendations: 
1. Promote the responsible release of AI models, including performing risk assessments before 
release. 
2. Incentivize the development of, and promote the use of, AI risk management guidance and 
standards that consider marginal risk and unique risks posed by the distribution method 
(i.e., “availability”). 
3. National Institute of Standards and Technology (NIST) to develop and Office of 
Management and Budget (OMB) to adopt guidance on the responsible consumption of 
third-party AI models (i.e., supply chain management). 
A crucial step in planning the release of any AI model is assessing risk; frameworks, such as 
NIST’s AI Risk Management Framework8 (RMF), or standards, such as ISO/IEC 42001 – AI 
Management System9 (AIMS), provide implementation-agnostic practices for incorporating risk 
assessment into the release process. Similarly, model consumers, by which we mean end users 
and downstream developers, should assess the risks posed by the AI models they are 
consuming. 
In October 2023, the Partnership on AI10 (PAI) released Guidance for Safe Foundation Model 
Deployment,11 which provides practical guidance on risk assessment and mitigation for 
publishers of AI models. The guidance is tailored based on both the AI model’s capabilities and 
availability, which allows it to address the unique risks stemming from each of these dimensions 
and their intersections. 
Each distribution method poses unique risks for both model developers and model consumers, 
and part of these risk assessments should be to assess those risks in the broader context of the 
release or use.  
 
8 https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf  
9 https://www.iso.org/standard/81230.html  
10 Microsoft is a corporate sponsor of the Partnership on AI (PAI), “an independent, nonprofit 501(c)(3) 
organization and funded by charitable contributions from philanthropy and corporate entities.” 
11 https://partnershiponai.org/modeldeployment/  
 
6 
 
This table shows some of the unique risks to the model developer and model consumers for 
different distribution methods: 
Distribution Method 
Risk for Model 
Developer 
Risk for Model Consumers 
Weights, data, and code 
available 
• No protection of trade 
secrets 
• Unable to monitor for 
and prevent downstream 
intentional misuse or 
unintentional harms 
• Assumes responsibility for risks and 
obligations emerging from the 
model’s use 
Weights available 
• Limited technological 
protection of intellectual 
property 
• Unable to monitor for 
and prevent12 
downstream intentional 
misuse or unintentional 
harms 
• Assumes responsibility for risks and 
obligations emerging from the 
model’s use  
• Unable to re-train the model13 
Hosted or API access 
• Shared responsibility14 
 
• Shared responsibility14 
• Unable to modify or re-train model, 
unless explicitly enabled and 
permitted by the service provider 
• Reliance on service provider for 
availability, and possibly other 
functionality or obligations 
depending on the service offered15 
Fully closed – the model 
developer and model 
consumer are the same 
entity and the model is not 
available to third parties 
Assumes responsibility for risks and obligations emerging from the 
model’s use 
 
Focusing on the specific attributes that contribute to the risk, rather than broad categorizations, 
enables model developers and model consumers to assess and mitigate (or, in lower risk 
scenarios, accept) those risks. For example, if the risk is exacerbated by the breadth of 
distribution, then a model developer may consider phased releases that allow measuring and 
responding to unexpected risks at a smaller scale and mitigate the risk of being unable to “take 
 
12 Techniques, such as self-destructing models, can be employed to make removing safety mechanisms more 
difficult. 
13 Some fine-tuning or modification of the model may still be possible. 
14 https://learn.microsoft.com/en-us/azure/security/fundamentals/shared-responsibility-ai 
15 For example, if the model consumer has an obligation to report malicious use of their AI system, they may be 
dependent on the service provider providing the information needed to meet this obligation. 
 
7 
 
back” widely available weights. This allows more direct control of the risks than binary 
public/non-public decisions that lack nuance and can exaggerate the impact or probability of 
risks (and benefits). 
Part of a broader risk assessment, this evaluation of the impact of specific attributes can also 
help to assess the marginal risk of different options, including non-AI options. For example, if 
you know that a specific AI model does not pose a greater risk than information currently 
available on the internet,16 then you may determine that the breadth of distribution does not 
have a meaningful impact on the marginal risk. 
For some risks – such as the use of a highly capable model to generate chemical, biological, 
radiological, and nuclear (CBRN) content in a way that threatens safety and security –  the 
impact of a risk or capability may be so significant that the breadth of distribution (which 
primarily affects the probability of a risk) does not change the policy interventions that are 
warranted. In these scenarios, the presence of certain capabilities (as defined by the appropriate 
regulatory agency, such as the Department of Energy for nuclear), could trigger specific 
obligations, such as licensing of the model or the environment running it, or additional 
restrictions, such as export restrictions. 
As the NIST AI Safety Institute Consortium17 (AISIC) develops evaluation criteria, and NIST and 
OMB develop guidance for Federal government development and use of AI systems, we 
encourage them to focus on identifying the specific attributes that introduce risk and assessing 
the impact of those attributes for the specific use of the AI system. This is an approach that 
Microsoft has adopted as part of our Responsible AI Standard and its companion Responsible AI 
Impact Assessment.18 
 
 
 
16 https://www.rand.org/pubs/research_reports/RRA2977-2.html  
17 Microsoft is a member of the NIST AI Safety Institute Consortium (AISIC) 
18 https://www.microsoft.com/en-us/ai/tools-practices  
 
8 
 
What are the benefits of foundation models with model weights that are widely available 
as compared to fully closed models? 
Recommendations: 
1. The Department of Homeland Security, the Department of Energy, the Office of Science 
and Technology Policy, the National Science Foundation, and other relevant federal 
departments and agencies to partner with the open source AI community and academia to 
promote and adopt responsible AI design and deployment guidance. 
Open source software provides immense direct and indirect value to society,19 and we believe 
the same will be true for open source AI models. This value comes directly from the reduced 
costs when you reuse components but is also derived indirectly from the innovation, 
competition, and collaboration arising from open source development. This open innovation 
cycle benefits not only open source projects but also the extensive value chain built on those 
projects.  
Open source AI models and software also play a key role in improving AI safety and security. 
Specially trained AI models are being used for content filtering and to detect malicious use of AI 
systems, and open source libraries and applications are used to interact with models safely and 
to automate safety and security testing. Encouraging a diverse global community to participate 
in the development of these models is beneficial to the overall safety and security of the AI 
ecosystem. 
Global collaboration on academic research is heavily dependent on the free exchange of 
information. AI code, models, and data are often developed and released “in the open,” either 
under an open source license or licenses tailored for the researcher’s objectives. This open 
collaboration creates transparency, provides opportunities for others to review and reproduce 
the research, and enables other researchers to build on the research. When released under an 
open source license, this AI code, models, and data can also contribute to further development 
of open source and commercial AI models and systems, further accelerating innovation. 
Open source projects and open research provide valuable opportunities for students and the 
workforce to learn from, and to participate in, the development of innovative AI models and 
systems. These opportunities do not need to be restricted to the technical aspects of AI; they 
can also be an opportunity to teach responsible AI design and deployment practices. Promoting 
risk-based and outcome-focused guidance for responsible AI design and deployment to these 
 
19 https://www.hbs.edu/faculty/Pages/item.aspx?num=65230  
 
9 
 
communities will increase awareness of the risks and provide practical guidance on how to 
mitigate them. 
Maintaining these benefits is dependent on deeply understanding the impact of policies on 
open source communities and their projects. This requires recognizing the significant diversity of 
open source development models and engaging with representative open source communities 
to seek feedback on policy impacts.20 
Are there other relevant components of open foundation models that, if simultaneously 
widely available, would change the risks or benefits presented by widely available model 
weights? If so, please list them and explain their impact. 
Recommendations: 
1. NIST AISIC to develop guidance on which components support which outcomes and 
considerations when releasing them. 
The OSI’s Open Source AI Definition Draft21 includes a representative list of components of an 
open AI model, including: 
• 
Code 
• 
Model 
• 
Data 
• 
Other 
These components are subdivided further into subcomponents, with some required in an open 
source AI model release because they are essential to exercise the freedoms to use, study, 
modify, and share. 
Each component and subcomponent has its own unique risks, benefits, and costs to release. The 
distribution method and openness of an AI model are two key factors, but others may include: 
intended audience (for example, researchers or downstream developers), consumer demand, 
risk and impact assessments, phase of development, and available resources. As part of planning 
the release of an AI model, developers should consider the outcomes they intend to achieve and 
the components necessary to support those outcomes, incorporating that context into their risk 
assessment for the release. 
 
20 https://www.stiftung-nv.de/en/publication/fostering-open-source-software-security  
21 https://opensource.org/deepdive/drafts/the-open-source-ai-definition-draft-v-0-0-6  
 
10 
 
The following table provides an illustrative example of how intended outcomes can be mapped 
to required components and release considerations. Only selected intended outcomes are 
mapped; the table is not exhaustive in listing all intended outcomes. 
Intended Outcome 
Required Components 
Release Considerations 
Meet Open Source AI 
Definition 
• Code – data pre-
processing 
• Code – training, 
validation, and testing 
• Code – inference code 
• Code – supporting 
libraries and tools 
• Model – architecture 
• Model – parameters 
(weights) 
• Available under OSI-compliant 
license 
Production use 
• Code – inference code 
• Model – model 
parameters (weights) 
• Model – model card 
• Other – usage 
documentation 
• Completed impact and risk 
assessment 
Benchmark verification 
• Model – parameters 
(weights) 
• Code – code used to 
perform inference for 
benchmark tests 
• Data – benchmarking 
data sets 
• Data – evaluation metrics 
and results 
• Privacy review of benchmarking data 
sets 
Evaluation – WMDP22 
• Code – inference code 
• Model – parameters 
(weights)23 
• Review evaluation results for 
indications of harmful capabilities 
• Consider using CUT to unlearn 
harmful capabilities 
 
As part of the NIST AISIC’s working groups,24 there is an opportunity to identify the specific 
components of an AI model that are required for each of the practices and processes that they 
define. This would create a shared understanding between model developers and model 
consumers of which practices and processes are possible based on the components that have 
been released. This shared understanding can also be leveraged across Federal government 
 
22 https://www.wmdp.ai/ 
23 For a hosted or API access model, the WMDP evaluation can be performed without weights. 
24 https://www.nist.gov/artificial-intelligence/artificial-intelligence-safety-institute/aisic-working-groups  
 
11 
 
workstreams, including agency AI use case inventories, risk assessments, procurement 
requirements, and evaluation and research processes.  
What are the legal or business issues or effects related to open foundation models? 
Recommendations: 
1. Encourage global research and collaboration on AI fundamentals, AI safety and security, 
and AI applications. 
2. Support the responsible release of AI models under safety- and security-enhancing terms 
and avoid unintended export control implications. 
Microsoft Research works as part of the global research community to enhance our 
understanding of artificial general intelligence, create new model architectures with novel 
abilities, achieve societal benefit through the advancement of AI, transform scientific discovery, 
and extend human capabilities. We work across disciplines – from social sciences to economics, 
to mathematics and physics – and collaborate across boundaries between research and 
engineering and with our peers in academia, industry, and government.  
Over the past thirty years, Microsoft’s research community and collaborators have published 
tens of thousands of papers and released hundreds of open-source projects and datasets. 
Nearly all of the research we produce is shared with the research community. Enabling free and 
open collaboration in the research community is essential to pursue advances in AI, align AI with 
human goals, ensure positive impact on jobs and the economy, and ensure equitable and safe 
employment in key sectors, such as education and healthcare. 
The decision about what terms under which to release AI models, or their components, is based 
on many factors, including the developer’s objectives,25 consumer demand,26 ecosystem 
norms,27,28 and legal obligations29 or restrictions such as export controls.30 Even once this 
decision is made, it may change for future releases of the AI model as the benefits, risks, and 
other factors that influenced the original decision evolve.  
NTIA should engage with interagency and interdepartmental entities, including the Bureau of 
Industry and Security (BIS), to ensure that the terminology “open source” or “widely available,” 
as applied to foundational AI models, is consistent with the “published” definition under the 
 
25 https://github.com/todogroup/ospo-career-path/tree/main/OSPO-101/module2  
26 https://www.commerce.gov/about/policies/source-code  
27 https://www.gnu.org/licenses/licenses.html  
28 https://apache.org/licenses/  
29 https://www.gnu.org/licenses/gpl-faq.en.html#combining  
30 https://www.apache.org/licenses/exports/  
 
12 
 
Export Administration Regulations (EAR). Under Section 734.7(a),31 to qualify as “published” 
under the EAR, a model must be freely and publicly available without limitation on distribution. 
It is crucial that the “open source” or “widely available” terms used by the Federal government 
are in harmony with the EAR’s “published” standards to facilitate the unencumbered distribution 
of these models, thereby avoiding unintended export control implications. 
For instance, many AI models are licensed under the MIT License, endorsing unrestricted use. In 
contrast, certain AI models are subject to the OpenRAIL License, which prescribes specific 
restrictions on usage in critical scenarios to guide the safe and responsible use of these models. 
Given these specific restrictions on usage, models that are “open source” or “widely available” 
according to NTIA’s interpretation may not fulfill BIS’s “published” criteria and could 
consequently be regulated by future export controls, undermining efforts to cultivate a healthy 
open source AI ecosystem that also advances responsible use and open collaboration on AI 
research. 
There is a systemic relationship between research, open source, commercialization, and 
consumer adoption. Developers’ ability to responsibly release AI models under their preferred 
licensing terms and distribution method is essential to innovate, compete, and partner in the 
global marketplace. Furthermore, cultivating a healthy open source ecosystem increases 
competition and provides more opportunities to develop AI talent.32 Policies that put undue 
burden on these decisions can have chilling and unintended consequences on America’s 
innovation and competitiveness.  
What are current or potential voluntary, domestic regulatory, and international 
mechanisms to manage the risks and maximize the benefits of foundation models with 
widely available weights? What kind of entities should take a leadership role across which 
features of governance? 
Recommendations: 
1. OMB and the General Services Administration (GSA) should use Federal purchasing power 
to incentivize safety and security in foundation models. 
2. Extend existing cybersecurity engagements with open source foundations and projects to 
include open source AI safety and security. 
3. Expand international collaboration and regulatory consistency through bilateral and 
multilateral agreements. 
 
31 https://www.ecfr.gov/current/title-15/subtitle-B/chapter-VII/subchapter-C/part-734/section-734.7  
32 https://www.ftc.gov/policy/advocacy-research/tech-at-ftc/2023/06/generative-ai-raises-competition-concerns  
 
13 
 
The Federal government can influence the safety and security of dual-use foundation models 
with widely available weights through its procurement practices. Specifically, the Federal 
government can impose requirements on AI systems powered by dual-use foundation models 
that agencies build or procure. This is the approach that Executive Order 14028 – Improving the 
Nation’s Cybersecurity33 used to encourage industry adoption of NIST’s Secure Software 
Development Framework34 (SSDF). This approach creates incentives for commercial providers of 
dual-use foundation models with widely available weights to evaluate and contribute to the 
safety and security of those models, benefiting not only Federal government customers but also 
other consumers of those models. 
Over the past few years, the Federal government has been increasing its direct engagement with 
open source communities, primarily in relation to cybersecurity. These engagements include the 
White House’s establishment of the Open-Source Software Security Initiative35 (OS3I), an 
“interagency working group with the goal of identifying policy solutions and channeling 
government resources to foster greater open-source software security across the ecosystem,” 
and the Cybersecurity and Infrastructure Security Agency’s (CISA’s) establishment of an Open 
Source Security Section.36 These engagements with open source communities can also be 
opportunities to engage relevant stakeholders on issues related to open source AI safety and 
security. Additionally, the open source community is engaged through participation in the NIST 
AISIC. 
Directly regulating the development of open source is difficult and likely to hamper open 
research and innovation, but prohibiting harmful uses, such as the production and distribution 
of non-consensual intimate imagery (NCII); regulating high-risk uses regardless of the openness 
of the AI models they use, such as the approach taken with respect to “high-risk AI systems” in 
the European Union’s AI Act; and using voluntary, consensus-based international standards, 
codes of conduct, and guidance, such as NIST’s AI Risk Management Framework, can be 
effective at minimizing harmful impacts and setting expectations with developers of AI models. 
Continuing and extending partnerships with international AI Safety Institutes, policymakers, 
standards bodies, academia, industry, and civil society to develop and adopt voluntary, 
consensus-based international standards, codes of conduct, and guidance is needed to create 
consistency and to ensure that they address emerging risks and societal values.  
 
33 https://www.whitehouse.gov/briefing-room/presidential-actions/2021/05/12/executive-order-on-improving-
the-nations-cybersecurity/  
34 https://csrc.nist.gov/Projects/ssdf  
35 https://www.whitehouse.gov/oncd/briefing-room/2024/01/30/fact-sheet-biden-harris-administration-releases-
end-of-year-report-on-open-source-software-security-initiative/  
36 https://www.cisa.gov/opensource  
 
14 
 
 
In the face of continually changing technology, and given unforeseen risks and benefits, 
how can governments, companies, and individuals make decisions or plans today about 
open foundation models that will be useful in the future? 
Recommendations: 
1. Promote adoption of risk-based and outcome-focused frameworks and standards that are 
adaptable to changing technologies and use cases. 
2. Monitor emerging and unforeseen risks and benefits and assess their impact. 
3. Plan and implement incident response processes to minimize incident impact. 
Policies that are targeted, that focus on outcomes rather than implementations or technologies, 
and that impose requirements commensurate with risk are most likely to achieve policy 
objectives and prevent harm. This approach also typically results in policies that are more 
resilient to changes in technology and that place the burden on those best able to bear it (e.g., 
implementers of high-risk use cases). 
While the development of consensus-based international standards can be slow, the nature of 
this process is that it creates risk-based and outcome-focused standards that can be applied to 
a broad range of organizations, technologies, and applications. Policies grounded in 
international standards can minimize risk and maximize interoperability while maintaining 
competitiveness by minimizing costs and duplicative compliance efforts. When coupled with 
international reciprocal agreements, these benefits can be realized up and down the global 
supply chains underlying modern systems. 
To address the gap while these standards are being developed, policies can use voluntary codes 
of practice, guidelines, and frameworks, such as the NIST AI Risk Management Framework, and 
incentivize developers through consumer demand and Federal purchasing power. Evaluations, 
such as those being developed and adopted by international AI Safety Institutes, and continued 
research will also drive further improvements to safety and security across the AI ecosystem and 
provide valuable inputs to the development of standards. The establishment of industry groups 
focused on frontier model development and deployment, such as the Frontier Model Forum,37 is 
a positive step in the right direction by industry and increases the likelihood of voluntary policy 
actions being implemented and effective. 
The progress of AI Safety Institutes and groups like the Frontier Model Forum could also help 
establish scientifically valid techniques and instruments for determining that a model poses 
 
37 Microsoft is a founding member of the Frontier Model Forum (FMF) 
 
15 
 
significant safety or security risks, prompting an urgent need to limit and maintain awareness of 
any distribution of model weights. As discussed above, where risks are exacerbated by the 
breadth of distribution, phased releases that allow for measuring and responding to unexpected 
risks at a smaller scale mitigate the risk of being unable to “take back” widely available weights. 
NTIA may also consider other risk assessment and mitigation approaches as it confronts this 
challenge with potentially emergent risks. 
Risk assessments and mitigation frameworks will also need to factor in our learned experience 
that model-level safety interventions are necessary but not sufficient. AI safety and security 
require additional safeguards to be layered in when building and deploying AI applications, 
ultimately increasing durability and impact. Arvind Narayanan and Sayash Kapoor recently 
captured this point well, highlighting that “AI safety is not a model property” and that “[s]afety 
depends on a large extent on the context and the environment in which the AI model or AI 
system is deployed.”38 
Moreover, even when model and application developers take all reasonable precautions to 
assess and mitigate risks, mitigations will fail, unmitigated risks will be realized, and unknown 
risks will emerge. These risks could range from generating harmful content in response to a 
malicious prompt to the intentional exfiltration of an advanced AI model by a nation state actor. 
It is essential that developers are prepared to respond to and mitigate these in a timely manner 
and, when necessary, partner with their stakeholders up and down the supply chain. 
Cybersecurity has benefited from the existence of standards (such as, ISO/IEC 29147 – 
Vulnerability disclosure39 and ISO/IEC 30111 – Vulnerability handling processes40), programs (such 
as, CVE41), systems (such as, the National Vulnerability Database42), and reporting requirements 
(such as, Cyber Incident Reporting for Critical Infrastructure Act of 202243). While these are 
imperfect, and continue to evolve, they have created a shared understanding of how to 
responsibly handle incidents and have been universally adopted – mostly voluntarily – by 
organizations of all sizes, including open source projects. 
 
 
 
38 AI safety is not a model property (aisnakeoil.com) 
39 https://www.iso.org/standard/72311.html  
40 https://www.iso.org/standard/69725.html  
41 https://www.cve.org/  
42 https://nvd.nist.gov/  
43 https://www.cisa.gov/topics/cyber-threats-and-advisories/information-sharing/cyber-incident-reporting-critical-
infrastructure-act-2022-circia  
 
16 
 
What other issues, topics, or adjacent technological advancements should we consider 
when analyzing risks and benefits of dual-use foundation models with widely available 
model weights? 
Recommendations: 
1. NSF, NIST, and OSTP to fund and prioritize ongoing research, evaluation, and training on 
AI safety, security, and trustworthiness. 
There is active research into technological protections for AI models, such as differential 
privacy44 - which could improve the privacy of training data, and self-destructing models45 - 
which could make it more difficult to re-train widely available weights to be unsafe. Through risk 
assessments, we can identify attributes that contribute meaningfully to risk as well as where risk 
mitigations are not as effective as desired; we can then use this data to prioritize and fund future 
research to address these challenges. 
The research into and operationalization of model evaluations that is underway across academia, 
civil society, industry, and governments is important to both assess the risk of dual-use 
foundation models with widely available weights and, more broadly, to advance AI safety and 
security. The government-led AI safety institutes, such as NIST’s, play a vital role in defining the 
requirements for those evaluations, identifying gaps in existing evaluations, prioritizing new 
evaluations, and providing trusted environments for executing and reporting the outcomes from 
evaluations. Funding of these trusted environments, such as ones provided through NIST’s AISIC 
or through the National AI Research Resource (NAIRR) Operating Entity, will be essential to meet 
the growing demand for evaluations. 
With AI being a field undergoing rapid innovation, it is essential that risk assessments and 
evaluations are regularly reviewed to ensure they are suitable for the changing landscape. For 
example, small language models (SLMs) – such as Microsoft’s Phi-2 – are demonstrating the 
ability to outperform significantly larger models,46 and improvements to training data curation 
and processes are significantly reducing the resources needed to train large language models. 
How AI models are used in AI systems, and the broader architecture and implementation of 
those AI systems, has a significant impact on the resulting risk. For example, even if an AI model 
exhibits specific risks in an evaluation, its use in an AI system may not expose those risks, or the 
AI system may combine multiple AI models and filter inputs and outputs to mitigate those risks. 
The evaluation of an AI model is an important input to the responsible design and 
 
44 https://csrc.nist.gov/pubs/sp/800/226/ipd  
45 https://arxiv.org/abs/2211.14946  
46 https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/  
 
17 
 
implementation of an AI system, but it is not a substitute for assessing the risk of the AI system 
in its entirety, and it is not a substitute for assessing the risk of safety- and security-impacting 
uses of an AI system when deployed. 
",Microsoft,7977
NTIA-2023-0009-0262,"Hugging Face, Inc.
20 Jay Street, Suite 620,
Brooklyn, NY 11201
Hugging Face Response to the National
Telecommunications and Information Administration
Request for Comments on Dual Use Foundation Artificial
Intelligence Models with Widely Available Model Weights
Hugging Face applauds the ongoing work of the National Telecommunications and Information
Administration (NTIA) in examining dual use foundation models (FMs). The following comments are
informed by our experiences as an open platform for AI systems, working to make AI accessible
and broadly available to researchers for responsible development.
About Hugging Face
Hugging Face is a community-oriented company based in the U.S. and France working to
democratize good Machine Learning (ML), and has become the most widely used platform for
sharing and collaborating on ML systems. We are an open-source and open-science platform
hosting machine learning models and datasets within an infrastructure that supports easily
processing and analyzing them; conducting novel AI research; and providing educational resources,
courses, and tooling to lower the barrier for all backgrounds to contribute to AI.
Questions
In order to best address the risks and benefits of widely available model weights, we explore
both the dimension of wide availability of a model and access to model weights in our responses
to avoid conflating risks inherent in the technology with risks associated with release methods.
We refer to models with widely available model weights as “open-weight models”.
1. How should NTIA define “open” or “widely available” when thinking about
foundation models and model weights?
Both “open” and “widely available” as terms are distinct, and require appropriate definitions
when discussing risks and benefits.
“Open” should consider components that shape model impact. The risks and benefits of
foundation models are shaped by decision along their development and deployment chain (e.g.
when considering risks of biases and discriminatory outcomes1). The contribution of openness
1 Let's talk about biases in machine learning! Ethics and Society Newsletter #2
Page 1
to supporting beneficial development of the technology,2 as well as the governance processes it
requires to help mitigate risks of the technology, will depend on access and transparency into
not just the model weights but also its development datasets3, development conditions,4 and
deployment infrastructure.
Open weights for a foundation model are notable on two main accounts. First, given how
resource-intensive the data curation and training of the models has become, sharing this
particular artifact in the AI value chain is necessary to enable downstream research that only
requires model access from many stakeholders outside of well-resourced companies.5 Second,
the release of a model’s weights has historically served as a catalyst for development projects
that prioritize overall transparency6 and proactive consideration of regulatory processes7 that
enable more comprehensive research on risks and benefits along the development chain.
To account for all those dynamics, we strongly recommend considering at least three main
components in FM openness: its datasets, the trained model, and the model’s training and
deployment mechanisms.
“Widely available” should consider breadth (not just scale) of access. In general, providing
model access to a more diverse set of stakeholders can have a net positive impact on the safety
of the technology; access should be available to stakeholders who are best positioned to
identify risks and limitations8 – and particularly to third-party organizations that do not have a
direct relationship (or conflicts of interest) with the developer and direct beneficiaries.9 Breadth
of access can be achieved independently of scale through mechanisms such as gating, where
access is granted by an (independent) organization for specific well-motivated research and
development projects.10 Simple availability is also distinct from accessibility; model weights
being available may not be accessible to people without the necessary compute infrastructure
or skill set for model hosting. For uses of a model that do not require specific customization, a
managed API supported by extensive computational resources greatly increases the scale of
availability independently of whether model weights are directly accessible.
a. Is there evidence or historical examples suggesting that weights of models similar to
currently-closed AI systems will, or will not, likely become widely available? If so, what are they?
Past examples include staged releases of model weights where closed weights are released
over time, as seen with OpenAI’s GPT-211, Meta’s LLaMA12, and Stability AI’s Stable Diffusion13.
All recent prominent closed models have been followed by an open comparable alternative.
13 Stable Diffusion Public Release — Stability AI
12 Introducing LLaMA: A foundational, 65-billion-parameter large language model
11 Release Strategies and the Social Impacts of Language Models
10 Access form for the BigScience training corpus
9 Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance
8 A Safe Harbor for Independent AI Evaluation
7 Do Foundation Model Providers Comply with the Draft EU AI Act?
6 Introducing The Foundation Model Transparency Index
5 The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?
4 The BigCode Project Governance Card
3 Policy Questions Blog 1: AI Data Transparency Remarks for NAIAC Panel 📚
2 AI Policy @🤗: Open ML Considerations in the EU AI Act
Page 2
Examples include OpenAI’s GPT-3 deployed via API in 2021 and model weights for
BigScience’s BLOOM and Meta’s OPT both released in 2022. Recently, DataBricks was
reported to have trained a competitive large language model for 10M$, significantly lower than
the initial development cost of closed products;14 indicating that relying on cost or trade secrets
is unlikely to limit availability of top-performing models to their current set of developers.
Being “first to market” presents a strong economic competitive advantage for closed systems,
and reproduction efforts of the model itself are motivated by a range of values, including
scientific understanding, free and open source software (FOSS) ideals of accessibility, and
customization, including fitness for purpose and harm mitigation strategies.15
b. Is it possible to generally estimate the timeframe between the deployment of a closed model
and the deployment of an open foundation model of similar performance on relevant tasks?
How do you expect that timeframe to change? Based on what variables? How do you expect
those variables to change in the coming months and years?
Timelines vary and estimates will change by utility of the model and costs. Factors include
computational cost, training data accessibility, and potential anti-competitive practices from
original developers. Research focus and more compute and data-efficient algorithms are
advantages for open models, whereas product focus, data concentration, high resources, and
legally untested or contested practices are advantages for close development. Overall, socially
responsible research in open reproduction of closed models will necessarily take more time for
release16 as legal and ethical concerns are addressed proactively rather than retroactively after
issues have been observed at sufficient scale.
c. Should “wide availability” of model weights be defined by level of distribution? If so, at what
level of distribution (e.g., 10,000 entities; 1 million entities; open publication; etc.) should model
weights be presumed to be “widely available”? If not, how should NTIA define “wide
availability?”?
Model weights can be shared individually between parties, on platforms with or without
documentation and with or without access management, and via p2p/torrent. Usability depends
on resources and infrastructure; subsidized compute from cloud compute providers can
increase access drastically, including for uses that go against the spirit of ToU but are hard to
detect. APIs can broaden model availability, even to malicious actors, especially already relying
on developer infrastructure. APIs also can be attacked to provide model information17. Even
controlled risks can outweigh open models in magnitude through deployment scale.18
18 Google's Bard AI chatbot is vulnerable to use by hackers. So is ChatGPT.
17 Stealing Part of a Production Language Model
16 BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model
15 Policy Readout - Columbia Convening on Openness and AI
14 Inside the Creation of DBRX, the World's Most Powerful Open Source AI Model | WIRED
Page 3
d. Do certain forms of access to an open foundation model (web applications, Application
Programming Interfaces (API), local hosting, edge deployment) provide more or less benefit or
more or less risk than others? Are these risks dependent on other details of the system or
application enabling access? i. Are there promising prospective forms or modes of access that
could strike a more favorable benefit-risk balance? If so, what are they?
There is no standard for weighing risks and benefits, nor are there standards or clear
methods for definitive access decisions. Distributed methods for sharing via torrent are
arising such as tweeting magnet links as seen with xAI’s Grok19. These are broadly accessible
but do not allow for updates in information and rely on trust and robustness of the torrent sharer.
Web applications with user interfaces lower the barrier for users with little to no AI experience to
engage with the model. Functionality depends on application and interface, such as the ability to
toggle temperature. This is often surface-level access, and cannot be built upon.
API deployment can offer varying functionality, allowing only querying or allowing fine-tuning.
APIs ease use for prescribed use cases as they only require internet connection and possibly a
user account. APIs are often commercialized. Deployment benefits include monitoring, last-layer
interventions, rate-limiting. Limitations include research and auditing inaccessibility, privacy
concerns, and infrastructural reliability issues. API terms of use can be monitored and detected,
and users can be blocked or revoked access. However content moderation proves difficult;
blocks can easily be bypassed; false positives can block good faith researchers.20 Last-layer
interventions include watermarks after output generation. Limited API access makes research
less reliable or presents additional barriers when limiting access to logits, forcing version
changes, or obfuscating input and output processing that confound research findings. API
deployment also introduces additional privacy challenges when user inputs are collected. APIs
also entirely rely on a model hoster and their infrastructure, which may have significant demand
and varying reliability.
Platform sharing of model weights enables broad access to models for local use and
modification, controlled by access management (e.g. gating) and visibility (privacy, sensitive
content tags) features. Content guidelines and ToU on model sharing platforms are defined at
the model rather than at the use level. Releases on platforms can range from fully permissive to
research-only (e.g. Cohere Command-R21).
Local hosting provides full control of the model and inputs to the model, satiates any privacy and
sensitive information concerns, and opens many possible research directions. However, this is
less monitorable for the deployer, should the user break terms of use. Edge deployment is a
type of local hosting that has environment, portability, and privacy benefits.
21 Command R
20 A Safe Harbor for AI Evaluation and Red Teaming
19 Grok model release via torrent on the X platform
Page 4
2. How do the risks associated with making model weights widely available
compare to the risks associated with non-public model weights?
a. What, if any, are the risks associated with widely available model weights? How do these
risks change, if at all, when the training data or source code associated with fine tuning,
pretraining, or deploying a model is simultaneously widely available?
In most cases, the risks associated with open-weight models are broadly similar to any other
part of a software system (with or without AI components), and are similarly
context-dependent.2223 Risks should also be weighed against non-AI systems.24 Prominent
considerations include new capabilities and systems designed specifically for harm.
New AI use cases arise fast, as seen with the easy creation of photo-realistic images enabled
by diffusion models, and more recently video and audio generations. Accelerated development
timelines can outpace societal and technical interventions. Both closed development and
large-scale release with rapid distribution contribute to risks arising and timeline considerations.
Open development often provides additional time and opportunities to question choices,
allowing many perspectives to shape systems earlier design decisions25. Conversely, closed
product development followed by rapid distribution can lead to significant disruptions in
important sectors for sometimes uncertain benefits.2627 Rapid distribution depends on a
combination of the availability of model weights, cost and technical difficulty of running a model,
availability of a product or API version of an AI system, and broad advertisement and
awareness of the AI system. Ensuring that scientific understanding of foundation models and
development of new applications of the technology is more gradual helps identify issues and
understand technological impacts earlier.
AI systems that are designed specifically for harmful uses, such as harassment or NCII, present
distinct challenges from AI systems with dual use risks. Such models should most aptly be
compared to malware in the context of traditional software.28 On model sharing platforms, such
models are subject to moderation following content policies.29 Providing explicit guidance on
categories of misuse and learning from existing mechanisms of platform governance to promote
cybersecurity will be essential to managing those risks. In some cases, broadly useful models
can be fine-tuned or adapted to specifically re-purpose them for such harmful uses. Safety by
design practices can address those risks, for example limiting the prevalence of data that would
make the models easier to misuse in general pre-training30 or researching new methods for
30 StarCoder 2 and The Stack v2: The Next Generation; developers opted to remove known malware from pre-training data to make
it harder to fine-tune for misuse
29 Content Policy – Hugging Face
28 Malware, Phishing, and Ransomware | Cybersecurity and Infrastructure Security Agency CISA
27 The impact of ChatGPT on higher education
26 College professors are in 'full-on crisis mode' as they catch one 'ChatGPT plagiarist' after another
25 Deconstructing Design Decisions: Why Courts Must Interrogate Machine Learning and Other Technologies
24 On the Societal Impact of Open Foundation Models
23 Artificial Intelligence | CISA
22 Open Source Software Security | CISA
Page 5
securing models at the weights level31. Additionally, fine-tuning models still requires collating
datasets of harmful uses; limiting access to those represents another important layer of risk
mitigation.
Taking action after a model release when new risks are identified presents distinct challenges
and opportunities for closed and open weights models. API access models can be deleted
either voluntarily by the deployer or by court request when found to be in breach of existing
regulation,32 but the cost of doing so when they are tightly integrated in a widely used
commercial product may disincentivize organizations from taking actions, and internal scrutiny
may not always be sufficient.33 Conversely, good platform governance can more flexibly provide
warnings and documentation, switch adoption to safer models, add technical access
management such as gating, and remove models that violate platform policy; drastically
reducing availability as a risk mitigation strategy.34
For models shared through peer-to-peer systems such as torrents, updates or removals will
often rely on alternative communication channels and interventions in deployed contexts.
Releasing training and especially evaluation and testing datasets is usually a net positive
from a risk perspective. Information about how a model was trained, where its data may
present risks, what mitigation strategies were adopted, and how it was tested, can help to
identify model vulnerabilities or limitations and conduct risk-benefit analyses. Challenges include
intellectual property and privacy of data subjects. We note several approaches available to
effectively manage these tensions35.
b. Could open foundation models reduce equity in rights and safety-impacting AI systems (e.g.
healthcare, education, criminal justice, housing, online platforms, etc.)?
Maximally open systems, including training data, weights, and evaluation protocol, can
aid in identifying flaws and biases. Insufficient documentation can reduce effectiveness.36 To
maximize benefits, we recommend developers maintain up-to-date information about systems
and publicly disclose biases and vulnerabilities as they are identified. The Hugging Face Hub
platform supports a collaborative approach to model documentation.37 We encourage adopters
to maintain an inventory of which open systems they are using to help understand relevant risks
to study and document. An example could be an inventory of systems used in the federal
government.38
38 Hugging Face Response to OMB RFC on federal use of AI
37 Model Cards - Hugging Face
36 Black-Box Access is Insufficient for Rigorous AI Audits
35 📚Training Data Transparency in AI: Tools, Trends, and Policy Recommendations 🗳️
34 Content Policy – Hugging Face
33 Microsoft engineer sounds alarm on company's AI image generator in letter to FTC
32 FTC settlement includes removal of trained facial recognition model
31 Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models
Page 6
c. What, if any, risks related to privacy could result from the wide availability of model weights?
Research has shown trained models can regurgitate private information from their pre-training.
This is a sign of improperly trained models, and the behavior is difficult to predict. Weight
availability can marginally exacerbate attacks, but is rarely the most efficient method. Model
memorization is highly dependent on the model architecture, size, and training procedure.394041
Mitigations include proper data governance and management practices during training.42 Open
development has exemplified these practices, including via privacy-conscious data sourcing
(e.g. BigScience43), and developing pseudonymization and personal data redaction methods for
the training data domain (e.g. the StarCoder44). Privacy risks tied to the use of foundation
models chiefly arise from API deployment settings that store user data,45 however, which
open-weights models can help mitigate (see our answer to 3b below).
d. Are there novel ways that state or non-state actors could use widely available model weights
to create or exacerbate security risks, including but not limited to threats to infrastructure, public
health, human and civil rights, democracy, defense, and the economy?
Open-weight models are unnecessary for state actor threats; should state actors prefer to use
FMs, they likely would prefer to build their own capabilities.46 Narrow AI systems are more
tailorable to a given threat. Models with realistic outputs are expensive to use, often more
so than human alternatives or narrower systems. For example, the costs of human and AI
generated disinformation do not always favor large language model (LLM) use.47
All existing models exacerbate risks given the following conditions: models introduce a new
attack vector48; models reduce the overall cost of attack through generation or distribution49;
malicious actors are unable to build a tailored model. When closed-weight models allow higher
scale of use via free or compute-subsidized access, relative risks should be reevaluated.
Open-weight model risks can outweigh those of APIs when the following additional
conditions are met: models need fine-tuning in ways unavailable via API; malicious content can
easily be automatically identified as malicious (e.g. generated outputs for malware creation or
legitimate software50); safeguards are robust to basic evasion (e.g. “jailbreaking”51 or traditional
content moderation evasion techniques52).
52 Microsoft CEO responds to AI-generated Taylor Swift fake nude images
51 [2307.15043] Universal and Transferable Adversarial Attacks on Aligned Language Models
50 Incident 443: ChatGPT Abused to Develop Malicious Softwares
49 The criminal use of ChatGPT – a cautionary tale about large language models | Europol
48 Google's Bard AI chatbot is vulnerable to use by hackers. So is ChatGPT.
47 How Much Money Could Large Language Models Save Propagandists? | Center for Security and Emerging Technology
46 Yi-34B, Llama 2, and common practices in LLM training: a fact check of the New York Times | EleutherAI Blog
45 AIDB: ChatGPT Banned by Italian Authority Due to OpenAI's Lack of Legal Basis for Data Collection and Age Verification
44 StarCoder: may the source be with you!
43 Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and
Resources
42 https://github.com/allenai/fm-cheatsheet/blob/main/app/resources/paper.pdf
41 Understanding and Mitigating Copying in Diffusion Models
40 Extracting Training Data from Diffusion Models
39 Quantifying Memorization Across Neural Language Models
Page 7
e. What, if any, risks could result from differences in access to widely available models across
different jurisdictions?
Cross-jurisdictional legal uncertainty can be detrimental to research collaborations that span
sectors, organizations, and geographies. This includes liability per contributor, IP and copyright
law, and foundational laws around digital privacy and regulation of misuses such as CSAM.
f. Which are the most severe, and which the most likely risks described in answering the
questions above? How do these set of risks relate to each other, if at all?
In terms of novelty and scalability of the application, disparate impact of openness, and severity
of the harms, non-consensual intimate imagery (NCII) in static images and videos remains the
most serious and pressing risk factor of new FMs. Openness, including access to training
datasets and training code, aids mitigation by enabling external scrutiny through investigating
the highest contributor factors (especially at the dataset level53) and enabling safety by design.
Marginal risk for open-weight models is less certain for pressing risks such as nonconsensual
voice cloning; hosted models can be equally harmful54 with little recourse. The most concerning
models fall well below proposed thresholds for dual use, and can easily be independently
trained from scratch by medium-resourced actors. Effective policy action should target use
cases and provide guidelines for all models across availability.
3. What are the benefits of foundation models with model weights that are widely
available as compared to fully closed models?
a. What benefits do open model weights offer for competition and innovation, both in the AI
marketplace and in other areas of the economy? In what ways can open dual-use foundation
models enable or enhance scientific research, as well as education/training in computer science
and related fields?
Open-weight models contribute to competition, innovation, and broad understanding of AI
systems to support effective and reliable development. In terms of value creation of open
technology, the historical and economic impact of Open Source Software (OSS) provides
important context for the expected impact of open-weight models. Two main phenomena bear
strong relevance: First, the estimated demand-side value of OSS is several orders of magnitude
larger ($8.8 trillion) than its supply-side value (>$4 billion).55 Investment in open technology pays
off over 1000x in value in other areas of the economy. Second, adoption enables users to
reallocate resources to internal development that supports market diversification and
sustainable self-driven growth56, which is critical for start-ups and SMEs. OSS trades off capital
for human expertise. Recent work has shown that these dynamics are extending to AI;
56 Open Source Software and Global Entrepreneurship
55 The Value of Open Source Software
54 AI Startup ElevenLabs Bans Account Blamed for Biden Audio Deepfake - Bloomberg
53 Multimodal datasets: misogyny, pornography, and malignant stereotypes
Page 8
cost efficiency and customizability increasingly outweigh managed solutions, given
sufficient company expertise.57
Open-weight models are particularly relevant to mitigating market concentration at the data
level. High-quality data, both in the form of licensed content,5859 interaction data, and data
uploaded by customers of managed systems, is shaping up to be the next most likely point of
monopoly behaviors as training compute costs decrease (see our answer to Q1.a). Open
weights models increase the diversity of product offerings from providers and even enable
adopters to manage their own on-premise solutions. Companies using open-weights models
or systems built on them are in a position to keep their own data value,60 preserve
privacy, and build consortia on their own terms as needed to support better technology
development for their applications without having to give up their intellectual property.61 This is
needed to sustain high-quality data creation and fair valuation of the data contribution to AI
systems. Additionally, open-weight models have been customized and adapted to run on a
greater variety of infrastructure, including individual GPUs and even CPUs, reducing points of
market concentration with cloud providers and reducing costs for procurement.6263
AI foundation models combine innovative ways of building AI systems–and most innovation
supporting recent AI product releases has come from open and academic research. For
example, the video generation system SORA builds on many public research contributions.64
Software behind development is largely open source (Pytorch, Tensorflow, HF libraries) and
development often requires access to the weights. More convenient65 and more secure66 weight
storage formats are developed on OSS settings, among many framework improvements. This
includes fine-tuning major LLMs and research breakthroughs such as model merging.67
Open-weight models’ role in enhancing existing scientific research is evident in works on topics
ranging from knowledge distillation68 to watermarking69. Robust innovation on both performance
and safety questions requires scientific rigor and scrutiny, which is enabled by openness and
external reproducibility70. Supporting that research requires sharing models to validate
findings and lower the barrier to entry for participation given the growing resource gap between
researchers in different institutions.71 Recent open-weights releases of foundation models from
AI system providers such as Cohere’s Command+R,72 Google’s Gemma,73 and OpenAI’s
Whisper 374, among others, represent a welcome contribution to this research.
74 Introducing Whisper, openai/whisper-large-v3 · Hugging Face
73 Gemma: Google introduces new state-of-the-art open models
72 Command R
71 The Compute Divide in Machine Learning: A Threat to Academic Contribution and Scrutiny?
70 EleutherAI: Going Beyond ""Open Science"" to ""Science in the Open""
69 A Watermark for Large Language Models
68 Knowledge Distillation of Large Language Models
67 Evolving New Foundation Models: Unleashing the Power of Automating Model Development
66 GitHub - huggingface/safetensors: Simple, safe way to store and distribute tensors
65 GGUF model saving format
64 Sora Reference Papers - a fffiloni Collection
63 llamafile: bringing LLMs to the people, and to your own computer - Mozilla Innovations
62 GitHub - ggerganov/llama.cpp: LLM inference in C/C++
61 SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore
60 Will StarCoder 2 Win Over Enterprises?
59 OpenAI In Talks With Dozens of Publishers to License Content - Bloomberg
58 Exclusive: Reddit in AI content licensing deal with Google | Reuters
57 16 Changes to the Way Enterprises Are Building and Buying Generative AI | Andreessen Horowitz
Page 9
Open Weights Models and Evaluation
The availability of open-weights foundation models is of particular importance to the
development of the robust and reliable evaluation ecosystem required to properly govern this
category of technology. Access to open-weights models serves several distinct purposes for
evaluation. First, some categories of evaluation require direct access to model weights.
Evaluation of a model’s environmental impact and energy consumption, for example, involves
running models on controlled hardware.75 Evaluating model memorization behaviors76 (related to
privacy and intellectual property concerns), covert biases,77 and data contamination78 all require
varying levels of access from output logits to model activations without additional input or
output processing that are not consistently available through API deployment.
Second, access to model weights makes research and development of new evaluations
significantly cheaper. Developing a new evaluation technique typically requires extensive
iteration on variations of a system. Being able to run a model locally in a controlled environment
makes this process significantly more efficient and cheaper, and has been instrumental for
example in our own work on developing bias evaluations in image generation settings before
applying them to commercial systems.79
Third, while model developers should be incentivized to thoroughly evaluate their models, their
incentives and priorities may be mis-aligned with those of external stakeholders,
especially when models are part of commercial product development. Developer-run
evaluations may underplay limitations and harms of a model – often without explicitly setting out
to do so; to address this, recent work has called for safe harbor regimes to enable external
research80, and outlined limitations of “black-box” access81 and auditing without sufficient
agency.82 Similarly, self-reported performance numbers on task can often be misleading or
over-represent a model’s suitability for use. Common issues that contribute to this phenomenon
include misleading evaluation framing83, choice of metrics (leading to different perceptions of
“emergen capabilities”84), rampant issues of inappropriate benchmark decontamination,8586 and
reporting results on different model versions than the ones deployed in products.87 Finally,
evaluations of foundation model applications in specific domains should be led by experts in
these domains rather than by model developers to best reflect the risks and benefits of AI
adoption.88
88 The Shaky Foundations of Foundation Models in Healthcare
87 An In-depth Look at Gemini's Language Abilities
86 Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs
85 Investigating Data Contamination for Pre-training Language Models
84 Are Emergent Abilities of Large Language Models a Mirage?
83 GPT-4 and professional benchmarks: the wrong answer to the wrong question
82 AI auditing: The Broken Bus on the Road to AI Accountability
81 Black-Box Access is Insufficient for Rigorous AI Audits
80 A Safe Harbor for AI Evaluation and Red Teaming
79 Stable Bias: Analyzing Societal Representations in Diffusion Models
78 Membership Inference Attacks against Language Models via Neighbourhood Comparison - ACL Anthology
77 Dialect prejudice predicts AI decisions about people's character, employability, and criminality
76 Quantifying Memorization Across Neural Language Models
75 Power Hungry Processing: Watts Driving the Cost of AI Deployment?
Page 10
Fourth, results obtained through third-party use of a model’s managed API are unreliable
without extensive documentation of the model versions, processing steps, or risk mitigation
strategies. Model providers can change the underlying version of a model at any time,89 and do
not always disclose sufficient details about post-processing steps that might have an impact on
the results, such as automatic edition of prompts.90 This is particularly relevant since, regardless
of any intentional misrepresentation, commonly used evaluations have been shown to be
sensitive to specific implementation details that often require extensive evaluation to
characterize.9192
Access to open-weights models that are substantially similar to those used in commercial
products is thus necessary to support a robust evaluation ecosystem, both to guide further
development of the technology and to support the development of appropriate standards.
b. How can making model weights widely available improve the safety, security, and
trustworthiness of AI and the robustness of public preparedness against potential AI risks?
Open-weight models can help in identifying attacks before they can be leveraged at scale; for
example, recent work on “stealing” the last layer was developed with LLaMA 293. Research has
also found some adversarial attacks apply to open and closed models,94 flagging the need to
understand limitations of safety techniques. As discussed in 3a, reproducibility increases trust in
AI systems by ensuring scientific rigor. Replicable evaluations with full transparency of replicable
pieces can resolve misunderstandings or misleading comparisons.95 Deployers canalso update
models as needed, without being locked into hosted model contracts and agreements - which is
particularly relevant when model vulnerabilities are identified or when a plausibly more secure
model becomes available.
Prominent privacy concerns arise from API deployment and user data storage9697 and
training on user data that contains private information.98 This tendency is particularly concerning
as access to private information has been identified as a significantly stronger risk factor than
model capacity in some misinformation settings.99 Open-weight models can mitigate this risk
by enabling a more controlled deployment environment and obviating the need to send data to
third party organizations. Open practices can enable broader red-teaming and evaluation
practices focused on privacy that can help secure all models.100
100 Privacy risks in deployment: Introducing the Chatbot Guardrails Arena
99 On the Conversational Persuasiveness of Large Language Models: A Randomized Controlled Trial
98 ""It's a Fair Game'', or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based
Conversational Agents
97 AI Incident database: Incident 513: ChatGPT Banned by Italian Authority Due to OpenAI's Lack of Legal Basis for Data Collection
and Age Verification
96 March 20 ChatGPT outage: Here’s what happened
95 What's going on with the Open LLM Leaderboard?
94 Universal and Transferable Adversarial Attacks on Aligned Language Models
93 Stealing Part of a Production Language Model
92 Open LLM Leaderboard: DROP deep dive
91 What's going on with the Open LLM Leaderboard?
90 DALL·E 3
89 GPT-4 API general availability and deprecation of older models in the Completions API
Page 11
c. Could open model weights, and in particular the ability to retrain models, help advance equity
in rights and safety-impacting AI systems (e.g. healthcare, education, criminal justice, housing,
online platforms etc.)?
Yes; in addition to examples in 3a, LLaMA’s adaptation into Alpaca101 significantly eased
accessibility while maintaining high performance. Some API restrictions prevent
equity-advancing research.102 Open practices, such as open data work, can help address
historical biases and prevent “hate-scaling”.103
e. How do these benefits change, if at all, when the training data or the associated source code
of the model is simultaneously widely available?
Access to the training data of an open-weights foundation model provides substantial additional
benefits from the perspective of research, rights and governance, and safety.104 Direct access
top the training dataset can support research on model privacy and memorization,105 risks tied to
hate content106 or NCII generation107, intentional and unintentional impacts of data filtering
approaches108, among others. Interactive or managed access to a dataset can similarly help
answer questions about data provenance and privacy109 and support data governance centered
on the rights of data subjects.110 Support for projects that develop foundation models in fully
open settings by making both trained weights and access or extensive documentation of their
dataset available can enable more socially responsible and inclusive technology development.111
4. Are there other relevant components of open foundation models that, if
simultaneously widely available, would change the risks or benefits presented by
widely available model weights? If so, please list them and explain their impact.
In addition to examining artifacts listed in 1, often-overlooked artifacts include process and
documentation. Project governance shapes technical artifacts, including goals, trade-offs,
funding, and mechanisms for internal feedback. In fully open development, processes include
goal setting, value alignment, and enabling various stakeholders to question upstream choices.
The BigScience project provides this level of openness for a multilingual LLM,112 and the
BigCode project113 took a similar approach to develop a code LLM, including a new form of
Governance Card to report relevant information including funding, main development choices
and reasoning, handling of cybersecurity risks and personal data, and annotator pay for data
augmentation work.114
114 The BigCode Project Governance Card
113 BigCode Project
112 BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model
111 Social Context of LLMs - the BigScience Approach, Part 3: Data Governance and Representation | Montreal AI Ethics Institute
110 Data Governance in the Age of Large-Scale Data-Driven Language Technology
109 The ROOTS Search Tool: Data Transparency for LLMs
108 A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity
107 Multimodal datasets: misogyny, pornography, and malignant stereotypes
106 On Hate Scaling Laws For Data-Swamps
105 How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN
104 📚Training Data Transparency in AI: Tools, Trends, and Policy Recommendations 🗳️
103 On Hate Scaling Laws For Data-Swamps
102 Dialect prejudice predicts AI decisions about people's character, employability, and criminality
101 Alpaca: A Strong, Replicable Instruction-Following Model
Page 12
Documentation approaches include model cards115 and datasheets116 that can be shared
broadly. Accompanying weights with information necessary to assess their usability for a given
use case helps ensure they are used in safer contexts. Data quality is broadly recognized to
directly affect ML system performance,117 and dataset limitations, such as toxicity and harmful
social biases, are typically reflected in a trained model.118 Datasets are often the most practical
artifact of study to understand the capabilities, risks, and limitations of a model. Sufficient
access to a training dataset119 can help answer relevant questions about a model’s
characteristics120.
5. What are the safety-related or broader technical issues involved in managing
risks and amplifying benefits of dual-use foundation models with widely available
model weights?
We need significantly more investment in evaluation to foster practices of safety, privacy, and
fairness by design. Results from reliable evaluation methodologies and design decisions such
as data selection can determine whether a model should be used, and in what conditions.
a. What model evaluations, if any, can help determine the risks or benefits associated with
making weights of a foundation model widely available?
Critical variables for assessing risks and benefits include type of model and design
process and data selection. Model evaluations are important in determining usability and use
case, and in shaping moderation decisions, but should be complemented with sufficient
documentation of both the development process and governance mechanisms (see responses
to 3e and 4) and the specific evaluation methodology (see response to 3a). Past moderation
decisions on Hugging Face about whether sharing specific models was consistent with our
terms of service has depended on all of those factors, as model evaluations by themselves can
fail to sufficiently address social dynamics and the context of use and development.
Models trained specifically for sensitive applications, such as identifying personal data (e.g. the
StarPII model121 used for private information redaction in the BigCode project training data) or
producing hate speech,122 require different governance and access to more specific
stakeholders. Processes for determining release method will differ by deployer organization and
calls for collective decision making boards are unanswered, although discourse is ongoing.123124
124 The Time Is Now to Develop Community Norms for the Release of Foundation Models
123 Publication Norms for Responsible AI - Partnership on AI
122 Handling and Presenting Harmful Text in NLP Research
121 StarPII release decision
120 The ROOTS Search Tool: Data Transparency for LLMs
119 📚Training Data Transparency in AI: Tools, Trends, and Policy Recommendations 🗳️
118 A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity
117 The Effects of Data Quality on Machine Learning Performance
116 Datasheets for Datasets
115 Model Cards for Model Reporting
Page 13
b. Are there effective ways to create safeguards around FMs, either to ensure that model
weights do not become available, or to protect system integrity or human well-being (including
privacy) and reduce security risks in those cases where weights are widely available?
Privacy by design includes steps throughout development including removing unnecessary
personal data from a training dataset (see BigScience and BigCode examples in 2c) and
following data minimization principles125 helps reduce risks described in 2c. However, privacy
risks also come from the extent to which current deployed systems encourage users to share
personal data during their interactions.126 Ensuring that the product design minimizes instances
where this information is stored or transmitted helps prevent damaging breaches.
Interventions at the fine-tuning level, such as in Constitutional AI or reinforcement learning with
human feedback to aid instruction following, can help steer a model towards more desirable
behaviors. While these interventions have an important role to play in adding friction to misuses
of models, they are also limited in their robustness and effectiveness127 and insufficient to
address fundamental issues with models.128129 Input and output monitoring can also be used to
automatically block queries that are incompatible with a model’s term of services. However,this
approach presents all of the familiar limitations and potential discriminate impacts of automatic
content moderation. In particular, the social cost of developing these safeguard datasets130 and
of live content moderation131 should be considered and minimized.
Model weight encryption is an ongoing research area, with proposals132 but no wide deployment.
c. What are the prospects for developing effective safeguards in the future?
In addition to building on 5b, safety by design, tight scoping of objectives, and intentional and
robust data curation all are paths forward. Using AI as safeguards on AI systems at the
deployment level may mitigate some risks, can easily compound biases and have
disproportionate impact on minority groups using the systems.133
d. Are there ways to regain control over and/or restrict access to and/or limit use of weights of
an open foundation model that, either inadvertently or purposely, have already become widely
available? What are the approximate costs of these methods today? How reliable are they?
Clearly defined platform governance frameworks and disclosure mechanisms can most
effectively limit availability for systems that require this guardrail due to vulnerabilities or
incompatibility with existing regulations. For models found to present critical vulnerabilities, AI
can draw on cybersecurity practices to notify users of updates. More research is needed to
133 Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection
132 NN-Lock: A Lightweight Authorization to Prevent IP Threats of Deep Learning Models | ACM Journal on Emerging Technologies
in Computing Systems
131 Inside Facebook's African Sweatshop | TIME
130 Inmates in Finland are training AI as part of prison labor - The Verge
129 The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test
128 Dialect prejudice predicts AI decisions about people's character, employability, and criminality
127 Jailbreaking Black Box Large Language Models in Twenty Queries
126 ""It's a Fair Game"", or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conv
125 Artificial intelligence: the CNIL opens a consultation on the creation of datasets for AI
Page 14
scope what constitutes a critical vulnerability and to standardize evaluations. For models that
violate existing regulations or platform content policies, removal from the platform can help limit
their spread and mitigate their negative uses. On the Hugging Face platform, we have taken
actions to limit the use of models trained in a way that leads them to disproportionately produce
harassing text (e.g. GPT4chan134) or models that were trained to reproduce a person’s voice or
likeness without their consent.
e. What if any secure storage techniques or practices could be considered necessary to prevent
unintentional distribution of model weights?
Approaches can include a trusted research environment that provides researchers full access to
model components through an isolated virtual machine (e.g. HathiTrust Data Capsules135) to
ensure security, but are resource-intensive in terms of both computational and human support to
adapt the environment to specific research needs. FMs and LLMs are notably computationally
intensive. Public repositories with access management (such as Hugging Face gated
repositories136) can conveniently balance easy access for legitimate researchers and
stakeholders and limited release of model weights. In staged or gated releases, researchers
may enter legal agreements with terms of use including preventing model weight distribution.
f. Which components of a foundation model need to be available, and to whom, in order to
analyze, evaluate, certify, or red-team the model? To the extent possible, 14 please identify
specific evaluations or types of evaluations and the component(s) that need to be available for
each.
Different tasks and level of access needed per task may not be obvious just as risk landscapes
for dual-use FMs cannot be fully taxonomized at a given time. While this mapping exercise has
not and likely cannot be exhaustively conducted, some examples include bias evaluations listed
in 3a and Table 1 of the paper Black-Box Access is Insufficient for Rigorous AI Audits details
audit tasks and levels of access needed.137
g. Are there means by which to test or verify model weights? What methodology or
methodologies exist to audit model weights and/or foundation models?
Hugging Face has used a simple model hash to verify whether two models are the same item.
For open-weight models, verification is as simple as for any other large files. For APIs,
verification is significantly more complex given the stochastic nature of the outputs and complex
stack. As outlined above, this can be a source of security vulnerabilities for critical infrastructure
that relies on externally managed APIs.
137 [2401.14446] Black-Box Access is Insufficient for Rigorous AI Audits
136 Gated models
135 Data Capsules
134 ykilcher/gpt-4chan · Hugging Face
Page 15
7. What are current or potential voluntary, domestic regulatory, and international
mechanisms to manage the risks and maximize the benefits of foundation
models with widely available weights? What kind of entities should take a
leadership role across which features of governance?
a. What security, legal, or other measures can reasonably be employed to reliably prevent wide
availability of access to a foundation model’s weights, or limit their end use?
Popular measures include terms of licenses with use clauses138 and gating mechanisms in
addition to safeguards enumerated in recent work.139
b. How might the wide availability of open foundation model weights facilitate, or else frustrate,
government action in AI regulation?
As noted in 2, 3, and 5, wide availability of open foundation model weights facilitates
government action in AI regulation by supporting research on evaluation and standards,
fostering more transparent and accountable development, and promoting fair competition. As
noted in 2, governance of widely available open-weights models can also require different
interventions than API-only models, with different benefits and challenges.
c. When, if ever, should entities deploying AI disclose to users or the general public that they are
using open foundation models either with or without widely available weights?
Entities deploying AI systems should typically disclose that fact, as noted in the AI Bill of
Rights.140 In general, extending the concept of a Software Bill of Materials (SBOM)141 to AI
systems, including identifying which versions of foundation models (open or closed) are being
used would have beneficial implications for cybersecurity and reliability.
d. What role, if any, should the U.S. government take in setting metrics for risk, creating
standards for best practices, and/or supporting or restricting the availability of foundation model
weights? i. Should other government or non-government bodies, currently existing or not,
support the government in this role? Should this vary by sector?
Evaluation presents two main challenges. First, specific risk evaluations is an open research
area, and risk priorities vary by organization. Evaluations as a field require significant
investment; widely used benchmarks for both performance and risks are criticized for often
providing insufficient or even misleading results.142 Second, model evaluation often depends on
the type of models evaluated and type of risk143, especially for topics like privacy risks. Continual
investment is needed to ensure evaluations reflect risks as models evolve. Sectoral contexts
improve metric robustness.
143 Evaluating the Social Impact of Generative AI Systems in Systems and Society
142 Dialect prejudice predicts AI decisions about people's character, employability, and criminality
141 Software Bill of Materials (SBOM) | CISA
140 Blueprint for an AI Bill of Rights | OSTP | The White House
139 The Gradient of Generative AI Release: Methods and Considerations
138 OpenRAIL: Towards open and responsible AI licensing frameworks
Page 16
The U.S. government action can include establishing standards for best practices building on
existing work144 and prioritize requirements of safety by design across both the AI development
chain and its deployment environments.145 General conditions should treat models that are
broadly commercialized and broadly shared similarly. Actions should foster a research
ecosystem that has sufficient access to the artifacts and infrastructure to conduct
research, incentives to share lessons and artifacts for reproducibility, access to broad
subject matter experts including social scientists.
e. What should the role of model hosting services (e.g. HuggingFace, GitHub, etc.) be in making
dual-use models with open weights more or less available? Should hosting services host
models that do not meet certain safety standards? By whom should those standards be
prescribed? 16
At Hugging Face, use and utility are contextual. Our platform provides features for access
management to enable developers to tailor model availability and breadth of access to strike the
best balance between benefits and risks. We universally encourage extensive and transparent
documentation, mandating documentation for artifacts over 10,000 downloads and providing
guides and resources. Content guidelines146 address specific harms, including breach of
consent147 and use of models for intentional harm. Sharing platforms enable research on
systems that may serve different purposes, and in many cases purely as research artifacts.
f. Should there be different standards for government as opposed to private industry when it
comes to sharing model weights of open foundation models or contracting with companies who
use them?
Given its responsibility to stakeholders, the government should meet sufficient procurement
requirements to ensure accountability, rights-respecting deployment of technology, and to
ensure that AI adoption does provide a net benefit for considered use cases. We appreciate the
EO directive to do so and the work of the OMB in this direction.
g. What should the U.S. prioritize in working with other countries on this topic, and which
countries are most important to work with?
International collaboration is key, especially among U.S. allies such as the UK,
Singapore, and France. Vulnerability and security incident sharing is best raised among allies,
especially for those with dedicated AI bodies such as the U.S. and UK AI Safety Institutes.
International standards, including among countries with differing values and regulatory
approaches, are needed. An urgent need is disclosure standards, outlining what should be
disclosed broadly, which can be fine-tuned per jurisdiction to meet requirements for example in
147 Announcing our new Content Guidelines and Policy
146 Announcing our new Content Guidelines and Policy
145 Hugging Face Response to OMB RFC on federal use of AI
144 NIST Risk Management Framework (RMF)
Page 17
adhering to local laws. Evaluation standards can help build cross-cultural and multilingual
approaches to model safety.
h. What insights from other countries or other societal systems are most useful to consider?
Lessons can be drawn from the EU’s established AI Office, which maintains standards and
operational definitions as AI evolves, and is empowered to make case-by-case definitions about
which models might present a systemic risk to reflect the highly contextual nature of risk
assessments. The U.S. should designate an accountable organization for metrics, standards,
and requirements, and sufficiently resourcing them to keep pace with AI development. The
National Institute of Standards and Technology is well positioned and should be well resourced.
i. Are there effective mechanisms or procedures that can be used by the government or
companies to make decisions regarding an appropriate degree of availability of model weights
in a dual-use foundation model or the dual-use foundation model ecosystem? Are there
methods for making effective decisions about open AI deployment that balance both benefits
and risks? This may include responsible capability scaling policies, preparedness frameworks,
et cetera.
As discussed in 5a, no definitive processes or standards exist. In addition to discourse
referenced in 5a, ongoing cross-sectoral collaborations are sharing best practices.148 Strict
guidelines about safety by design processes can be helpful for developers as they have control
over relevant properties, including what data selection, whether the data covers high risk topics
such as nuclear devices, and whether the model is trained to produce malware. Limitations in
the state of evaluation development makes quantifying model behavior technically difficult.
As discussed in 5d, models that are trained specifically on PII/malware/hate speech may have
different release conditions. Non-regulatory practices such as vulnerability sharing can draw
from cybersecurity. In high risk settings, existing controls on information flow should apply.149
Narrow models for specific domains or applications should use practices from that domain.
j. Are there particular individuals/entities who should or should not have access to open-weight
foundation models? If so, why and under what circumstances?
Research institutions, especially those operating in public interest and outside of
commercial product development, should not only have access but also infrastructural
support. Government resourcing150 can not only provide financial and technical support, but
also ensure that researchers with sufficient breadth of expertise and external stakeholders have
access to technical artifacts that may be used in rights-impacting settings. As noted in 3a, third
party auditing is most effective when entities who can conditions without developer
engagement.151 Open-weight models can help create these conditions, along with varying
151 Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance
150 National Artificial Intelligence Research Resource Pilot | NSF
149 How AI Can Be Regulated Like Nuclear Energy
148 PAI's Deployment Guidance for Foundation Model Safety
Page 18
access to components such as datasets, including e.g. extensive documentation and query
access.152
8. In the face of continually changing technology, and given unforeseen risks and
benefits, how can governments, companies, and individuals make decisions or
plans today about open foundation models that will be useful in the future?
a. How should these potentially competing interests of innovation, competition, and security be
addressed or balanced?
Policy decisions can jointly advance interests of innovation, competition, and security. Work
across technological settings (e.g. in the contexts of election systems153 or biosecurity in protein
design154) has shown that a priori antagonistic goals can often both benefit from openness given
appropriately tailored approaches. In specific settings where the tensions are harder to resolve,
specificity in how these tensions are managed and narrowly tailored policies are particularly
important.155 Across both settings of widely available weights and API-only development,
extensive documentation, replicable evaluation, and transparency into design choices of
FMs all contribute positively to all of these interests.
b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the
threshold, is the amount of computational resources required to build a model, such as the
cutoff of 1026 integer or floating-point operations used in the 17 Executive Order, a useful metric
for thresholds to mitigate risk in the long-term, particularly for risks associated with wide
availability of model weights?
Concerns with thresholds include their robustness, purpose/trigger, enforcement, and
verifiability. Methods for determining and proxies for model capability have differing
effectiveness. Thresholds along any singular variable, such as training compute, will not
give robust insight to model capability and will adapt with time and incentives.
Additionally, while model evaluations should eventually play a major role in helping assess
proper use and governance of foundation models, our scientific understanding of these artifacts
and evaluation systems are not currently mature or robust enough to serve this purpose; and
will require significant additional investment and access to open models as outlined in our
response to 3a.
For determining what potential thresholds should trigger, we recommend starting with a
voluntary reporting framework. Rather than red-teaming results, disclosure should include what
replicable evaluations have been run, including on a spectrum of risks. This will build regulatory
capacity and foster a stronger evaluation ecosystem. Thresholds should not be singular
hardlines.156 Levels of mandates should apply by contextual use case and mode of
156 Drawing Lines: Tiers for Foundation Models
155 Openness versus Secrecy in Scientific Research Abstract - PMC
154 Protein design meets biosecurity | Science
153 Transparency versus security: early analysis of antagonistic requirements
152 📚Training Data Transparency in AI: Tools, Trends, and Policy Recommendations 🗳️
Page 19
deployment, whether in research or commercial applications. Robust transparency
requirements should apply to all commercial products.
Disclosure is critical and lessons can be drawn from cybersecurity to determining levels of
disclosure and what can be made public. Meeting a base level requirement should mandate
some level of disclosure. Models whose deployment is supported by extensive cloud compute
capacity, due to model size or volume of users, should warrant additional scrutiny including
cybersecurity audits given the scale of their impact. Robust documentation can also be
beneficial for commercial usability and trust.
Conclusion
AI innovation would not be possible without open access and open science. Openness broadly
continues to benefit the field. Since the AI field is a fast-evolving space with many arising
considerations, expanding scope to many system artifacts can help to better weigh risks and
benefits. Many risks apply across model weight availability and tailored threat models can
narrow intervention options. Hugging Face greatly appreciates the opportunity to provide
insights and we remain eager to support NTIA and provide our expertise moving forward.
Page 20
",Hugging Face,12216
NTIA-2023-0009-0259,"Machine Intelligence Research Institute
Berkeley CA
March 27, 2024
National Telecommunications and Information Administration
U. S. Department of Commerce
Washington DC
Re: NTIA AI Open Model Weights RFC (NTIA–2023–0009)
Our organization, the Machine Intelligence Research Institute (MIRI), is focussed on
increasing the probability that humanity can safely navigate the transition to a world with
smarter-than-human AI. We investigate ways to mitigate the catastrophic and
societal-scale risks associated with artificial intelligence (AI) systems as they near and
surpass the capabilities of humans.
As such, our answers below will mainly relate to the risks posed by powerful autonomous
AI systems, in particular the risk of losing human control over such systems, as well as the
potential risks arising from the development and use of powerful models by incautious or
malicious actors. There are many other important risks posed by AI systems, however we
generally won’t speak to those risks, as this is not our area of expertise.
We believe that current AI systems likely do not pose such risks, and this is primarily due to
their lack of capabilities, i.e, the systems are not powerful enough to cause catastrophic
harm. However, many experts believe that future AI systems will become much more
capable, removing the justification for considering them safe.
In this Request for Comment (RFC) response, we primarily want to help in setting a
good precedent for future systems, where it is only appropriate to release model weights
of AI models if one can be confident that this will not cause significant harm.
Our main policy suggestions are related to understanding and mitigating risks before AI
systems are deployed or released. We believe that current understanding of the inner
workings of AI systems and the best currently available risk mitigation strategies are
strongly insufficient relative to the magnitude of the risks posed by future powerful AI
systems.
We would like to highlight a few technical and strategic considerations relevant to the
1 of 10
question of making model weights widely available.
There are currently no methods for reliably determining the capabilities of AI systems
before they are trained—or even for ruling out capabilities after training.
As the CEO of OpenAI Sam Altman recently said, before a model is trained, predicting its
capabilities is a “fun guessing game.” Additionally, post-training enhancements such as
novel prompting techniques, scaffolding, and fine-tuning have been known to increase the
range of capabilities displayed by models in ways that are difficult to predict in advance.
This means that if a model’s weights are made widely available, wider experimentation with
the model may unlock unexpected capabilities. Even if a model was initially considered
harmless, such enhancements may increase its capability profile beyond acceptable risk
thresholds.
A final point to stress is that the release of model weights is an irreversible action; once
model weights are released, any actor can use them, no matter how malicious or incautious.
Below we respond to specific questions from the RFC.
1a) Is there evidence or historical examples suggesting that weights of models
similar to currently-closed AI systems will, or will not, likely become widely
available? If so, what are they?
There is significant evidence that model weights of powerful AI systems will become widely
available, especially in the absence of regulation. The primary evidence for this is AI
developers releasing the weights of such models multiple times since the development of
large language models (LLMs). One of the first LLMs, GPT-2, had its weights released by
OpenAI (which would eventually go on to create ChatGPT), and this trend has continued
into the present day with xAI (Elon Musk’s AI company) releasing the model weights to
their LLM Grok-1. In addition to these, there are many other examples. One of these
examples, Mixtral 8x7B, appears to be competitive in capability with GPT-3.5. Given that
GPT-3.5 was released in March of 2022 and Mixtral 8x7B was released in December of
2023, this (at least naively) suggests that open foundation models may lag behind frontier
models by approximately two years, and that weights of models similar to currently-closed
AI systems may be available by early 2026.
There is also precedent for model weights “leaking”, i.e., being made widely available
2 of 10
without the developer’s consent. Prominent examples include the leak of Meta’s LLaMA-1
model weights, and recently AI developer Mistral reported a leak with a link to their model
weights being posted anonymously to 4chan.
Furthermore, as machine learning hardware continues to improve in power and
price-efficiency, and as algorithms to train language models continue to advance, requiring
less computation to reach the same level of performance, we should expect more and more
actors to be able to train their own models of comparable capabilities to currently-closed
frontier AI systems. This will directly enable such weights to become widely available by
enabling a wide variety of actors to train and share such models. This includes actors that
may share model weights as part of a business strategy to ensure their models are widely
integrated into other tools, or due to a belief that it is important for the weights of
widely-used models to be widely available.
1d) Do certain forms of access to an open foundation model (web applications,
Application Programming Interfaces (API), local hosting, edge deployment)
provide more or less benefit or more or less risk than others? Are these risks
dependent on other details of the system or application enabling access?
For current and near future models, risks can be reduced by monitoring their usage. This
includes monitoring of a user’s queries to an AI system, the AI’s outputs, and any
fine-tuning of the system. This monitoring can potentially be done automatically by other
AI systems to preserve a user’s privacy. Such monitoring is not feasible if the model weights
are widely available, because users may simply be able to host the models locally without
any monitoring. Additionally, it has been demonstrated that with access to model weights
“safety fine-tuning” (such as training a model to refuse harmful requests) can easily be
removed with minimal cost. Therefore, once model weights are widely available, different
forms of access provided to the model have little bearing on risk. (We elaborate on this
below in our answer to question 5d.)
Even where monitoring is possible, there remain risks from sophisticated attacks by users or
hostile actors including via web application or API access. This could be even as simple as
“system jailbreaks”, where targeted prompting by a user can get the system to reveal hidden
content. This becomes less likely the stronger the security monitoring is. However, it is
difficult to be confident that security monitoring is exhaustive, since it may well be easier
for attackers to find ways to jailbreak models than for model owners to prevent jailbreaks.
3 of 10
Weights will tend to be more secure when they are stored in a smaller number of places,
each of which can have more security measures taken. If models are deployed more broadly,
such as on consumer devices, it becomes more likely that their weights will be widely
released. This is because a broader deployment of weights increases the number of actors
with licit access who would be able to share the weights, as well as increasing the attack
surface for agents attempting to illicitly access the weights. Deploying on consumer devices
may also allow users to evade monitoring by, e.g., using the model offline.
2a) What, if any, are the risks associated with widely available model weights? How
do these risks change, if at all, when the training data or source code associated
with fine tuning, pretraining, or deploying a model is simultaneously widely
available?
For current AI systems, we believe that the main risk of catastrophic or societal-scale harm
comes from malicious actors using these AI systems to cause harm, e.g., with CBRN
(chemical, biological, radiological and nuclear) weapons, cyber attacks, misinformation or
persuasion campaigns, etc. These AI systems may be more useful to malicious actors than
other resources such as search engines, for instance by customizing the presentation of
information to be more comprehensible to specific users, and combining information in
ways a search engine cannot. They could also automate the execution of cyber attacks and
misinformation campaigns. By reducing required human involvement in the information
gathering or execution stages of these attacks, AI systems could significantly decrease their
cost.
Additionally, one effect of making training data and source code more available could be to
accelerate the rate of AI progress by incautious or malicious actors by giving them more
powerful models to build off of and enabling them to use more efficient architectures and
training methods. For example, this could lead to foreign militaries and oppressive regimes
gaining the ability to create more capable AI systems, which they could then use to cause
harm in the ways described above.
Future AI models (including potentially in the near future), may only be able to be
safely used with significant security precautions and safety practices. These may be
necessary to prevent “self-exfiltration” where an autonomous AI system copies its weights
to a location where it can no longer be safely monitored and secured—one of the primary
risks tracked by major AI developers such as OpenAI, Anthropic and Google DeepMind,
4 of 10
as well as METR, a leading AI evaluations organization. Models capable of self-exfiltration
likely pose greater catastrophic risks from loss of human control. Major AI developers
may be able to eventually develop and implement the appropriate security and safety
practices to prevent self-exfiltration, but if weights of a powerful AI model were made
widely available, smaller or less security-conscious actors would be unable or less likely
to take adequate precautions. It is important to note that this risk primarily applies to AI
systems which are capable of self-exfiltration, which likely do not include current models.
2f) Which are the most severe, and which the most likely risks described in
answering the questions above? How do these set of risks relate to each other, if at
all?
The most severe near term effects are likely enabling CBRN threats (e.g., a malicious actor
using AI systems to design an engineered pandemic) and cyber attacks on critical
infrastructure.
For future AI systems, there is a concern shared by a significant portion of the research
field, including industry and academic leaders, that advances pose a risk of loss of
human control, potentially leading to catastrophic outcomes such as human extinction.
The related risk of self-exfiltration may require an AI to have strong capabilities in coding
and finding cybersecurity vulnerabilities, or in human manipulation and persuasion. These
abilities are directly related to ways in which incautious or malicious actors could cause
harm.
Finally, we do want to acknowledge that there are risks associated with limiting access to
model weights and training code. Open access to these systems allows a much larger set of
external researchers to work with foundation models to better understand their capabilities
and inner workings, and conduct and contribute to essential research related to mitigating
these risks. In our view, this is an important consideration in favor of current models being
made more open, and investing in 3rd alternatives that could allow these researchers to
continue doing this essential work without direct access to model weights of future more
capable models (which we elaborate on in our answer to question 8a below).
5a) What model evaluations, if any, can help determine the risks or benefits
associated with making weights of a foundation model widely available?
5 of 10
We will consider the cases of misuse by malicious actors and harm caused by autonomous
AI systems separately, as these will likely require different model evaluations.
For misuse risks (i.e., malicious actors using an AI to cause harm), one can gain confidence
that model weights are safe to release if they are demonstrated to not meaningfully assist
humans with dangerous tasks (such as developing biological weapons). Such evaluations
may involve having a human evaluator attempt to use the model (including all feasible
post-training enhancements) to assist with such tasks. If the evaluator is unable to make the
AI system meaningfully assist them, and the evaluator has good reason to believe that they
are eliciting the model’s full capabilities, then the model may be considered safe. It is
important to note that this likely requires testing the model for a range of dangerous
behaviors, and also that there may be undiscovered techniques that would make the model
more capable that have not been discovered or widely publicized yet. This should include
testing done by external evaluators. Models that are able to meaningfully assist malicious
actors to (or enable incautious actors to inadvertently) cause significant harm should not
have their weights be made widely available.
Different evaluations are required to gain confidence that an AI system would not be able
to autonomously cause significant harm. As models become more powerful, they may gain
more “situational awareness”, and be able to realize they are being evaluated and behave
differently. A striking early example of this behavior has been observed in the recently
released Claude 3 Opus model from Anthropic. If models are able to detect when they are
being evaluated, they may behave differently in their evaluations and in deployment. This
means that the evaluations may no longer be an accurate representation of the model’s true
capabilities and behavior.
More research is needed to develop principled evaluations of AI systems’ true capabilities.
This research could include attempting to mechanistically explain how models are able to
perform tasks, and then gaining confidence that models don’t have the mechanisms
required for certain dangerous behaviors (e.g., a model could be lacking the ability to model
human behavior, or to detect cyber security vulnerabilities). This may become much more
difficult if the model weights were made widely available, as malicious or incautious actors
may be able to fine-tune the model (at a fraction of the cost it took to train them) to give
them these dangerous capabilities. Even if we are able to gain confidence that a model lacks
certain capabilities (which is not currently feasible), the ability to fine-tune the model
would likely remove these guarantees.
6 of 10
There is not currently a rigorous way to evaluate the costs and benefits of making model
weights widely available. This evaluation requires understanding the dangerous
capabilities of models, and there is currently no way to know the true limits of these
capabilities, especially when we consider that novel post-training enhancements will
likely be developed in the future.
Current model evaluations only cover an extremely small fraction of what models may be
able to do, especially when acting autonomously. We also believe that the threat modeling
and risk assessment for this area is still nascent, and so we do not have confidence that the
range of risks caused by the release of model weights has been properly considered.
5d) Are there ways to regain control over and/or restrict access to and/or limit use
of weights of an open foundation model that, either inadvertently or purposely,
have already become widely available? What are the approximate costs of these
methods today? How reliable are they?
There is some research on hardware-specific models and self-destructing models.
Hardware-specific models are AI models which would only run on certain restricted
hardware (e.g., one could imagine OpenAI models only running on OpenAI-specific
GPUs). This means that even if the weights of a hardware-specific model leaked, the model
would still not be usable as it would also require the specific hardware to run.
Self-destructing models are models which are trained in such a way that they are
particularly difficult to fine-tune on specified tasks. This is a promising area of research.
Potential future developments which obviated the need to manually specify dangerous
tasks to prevent fine-tuning on could plausibly make models that initially lacked dangerous
capabilities safe to release, although they would not enable the release of models that
natively possessed dangerous capabilities.
If methods such as these are not employed, then there is likely no way to “regain control
over” or “restrict access to” models which have widely available weights. If model weights
are released widely and are able to be used and fine-tuned, it will likely be extremely difficult
to control what they are used for or to restrict access.
7i) Are there effective mechanisms or procedures that can be used by the
government or companies to make decisions regarding an appropriate degree of
7 of 10
availability of model weights in a dual-use foundation model or the dual-use
foundation model ecosystem? Are there methods for making effective decisions
about open AI deployment that balance both benefits and risks? This may include
responsible capability scaling policies, preparedness frameworks, et cetera.
Our answer to this question will discuss limitations of frameworks such as Anthropic’s
Responsible Scaling Policy (RSP) and OpenAI’s Preparedness Framework (PF), as well as
the sorts of questions which would need to be addressed in order for these frameworks to
allow one to be confident in the safety of an AI model.
The RSP and PF attempt to rank models according to how capable and dangerous they are,
e.g., in the RSP current models are ASL-2 (AI Safety Level 2), and near-future models will
likely be ASL-3. These frameworks attempt to achieve safety by implementing appropriate
security and safety measures according to how capable the models are. This will involve
extensive security and monitoring, and safety fine-tuning to prevent malicious actors from
using the models to cause harm. However, without these safety and security measures
future models would very likely be dangerous and able to cause significant harm. If the
model weights were made widely available there would likely not be any security measures
and any safety fine-tuning could be easily removed. Following the RSP and PF frameworks,
for sufficiently powerful models it will not be appropriate to make the weights widely
available.
The RSP and PF currently rely on evaluating a model’s capabilities, and this means there is
currently a lot of guesswork in these frameworks. In order to be confident that an AI
system is safe (even with strong internal security measures), we believe that AI developers
must be able to adequately address the following questions with scientific rigor:
●
Are the evaluations measuring the model’s full capabilities? (this is sometimes
referred to as capabilities elicitation)
●
Do the proxy tasks which comprise the evaluations actually measure the dangerous
capabilities of interest?
●
Do the dangerous capabilities which are being evaluated exhaustively cover the
ways in which the model could cause unacceptable harm? How might this change
in the future?
An important part of these frameworks is attempting to predict the capabilities of future
models, in order to avoid training models for which there are insufficient security measures.
8 of 10
To have confidence that the models being trained are not going to “overshoot” the
security measures, it is important to have a principled way to predict model capabilities.
There is currently no way to do this, and model capabilities are known to emerge
unexpectedly with scale. It is important to ensure that before training powerful models,
we understand their likely capabilities well enough to implement security measures that
prevent the models from posing catastrophic risks.
8a) How should these potentially competing interests of innovation, competition,
and security be addressed or balanced?
We believe that AI developers should not be allowed to impose extreme risks onto the
non-consenting public.
Future advanced AIs have the potential to pose catastrophic risks which harm large
numbers of people, including causing human extinction. It is difficult to be sure of the
capabilities possessed by any given model, especially prior to training it, and currently
known measures to reduce these risks have serious flaws. We believe that it is critical for the
government to protect its citizens, to develop clarity about the risks which these AI systems
pose, and to prevent AI systems from being released when not in the public’s best interest.
Race dynamics and other competitive pressures between AI developers may cause them to
not prioritize the safety and security of the AI systems which they develop. It is important
that governmental or other regulatory authorities are able to monitor and audit AI
developers to ensure they are not lax on safety and security. There can be important
societal benefits from AI, but these should not come at the cost of safety from catastrophic
harms, and especially should not expose the public to large-scale risks they did not
consent to.
One way to promote safe research into powerful AI models may be for the government to
invest in and promote forms of limited access to the internals of powerful AI models to
researchers (e.g, through the National AI Research Resource). This would enable these
models to be studied, promoting research on their capabilities and how they can be made
safer without allowing the model weights to proliferate in an uncontrolled way. The United
States could develop better frameworks for these types of access, incentivize developers to
grant these forms of access in ways that minimize risk of full model weights leaking while
allowing safety research and assessments to take place, and thereby help democratize this
form of science.
9 of 10
8b) Noting that E.O. 14110 grants the Secretary of Commerce the capacity to
adapt the threshold, is the amount of computational resources required to build a
model, such as the cutoff of 1026 integer or floating-point operations used in the
Executive Order, a useful metric for thresholds to mitigate risk in the long-term,
particularly for risks associated with wide availability of model weights?
Metrics based on the amount of computation used to train a model (such as the number of
1026 integer or floating-point operations) currently appear to be adequate for the purpose
of determining whether models are advanced enough for it to be valuable to share
information pertinent to their safety.
However, these metrics are likely to become inadequate in the future. Due to progress in
machine learning algorithms, the amount of computational resources required to train
an equivalently powerful model halves approximately every 8 months. This means that
thresholds based on the resources used to train a model will need to be brought down
over time, to prevent dangerously capable models slipping under the threshold.
Furthermore, it may be difficult to adjust these regulatory thresholds fast enough, due to
the rate of technological progress outpacing the speed of the regulatory reaction. This may
result in the regulations always trying to catch up with the fast-moving technology. To
partially alleviate this, we propose that the relevant regulatory body should have the
ability and the mandate to quickly modify these thresholds in response to new
developments, and also to base these thresholds on the forecasted future state of the
technology rather than only considering the state of technology in the present.
Potentially more importantly, these thresholds based on these metrics do not account for
sudden breakthroughs in AI capabilities, and so would fail if a novel discovery let an AI
developer train an equivalently powerful model with far fewer resources. A preferable
metric would involve measuring and predicting, in a principled way, an AI’s ability to do
dangerous tasks and become powerful.
The lack of such a preferable metric should not be an excuse to continue to ignore the
limitations of the metric of training computation. Instead it should motivate research
into developing metrics which would allow us to develop powerful AI systems in a safe
and principled way.
10 of 10
",MIRI,4931
NTIA-2023-0009-0271,"CENTRE FOR THE GOVERNANCE OF AI • 3 
An evaluation of risks, beneﬁts, and alternative 
methods for pursuing open-source objectives
Open-Sourcing Highly 
Capable Foundation Models
Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, 
Jonas Schuett, K. Wei, Christoph Winter, Mackenzie Arnold, Seán Ó hÉi-
geartaigh, Anton Korinek, Markus Anderljung, Ben Bucknall, Alan Chan, 
Eoghan Staford, Leonie Koessler, Aviv Ovadya, Ben Garﬁnkel, Emma 
Bluemke, Michael Aird, Patrick Levermore, Julian Hazell, Abhishek Gupta
Open-Sourcing Highly Capable Foundation Models:
An evaluation of risks, beneﬁts, and alternative
methods for pursuing open-source objectives
Elizabeth Seger1,2,⇤Noemi Dreksler1
Richard Moulange1,3
Emily Dardaman4
Jonas Schuett1
K. Wei1,5
Christoph Winter6,7,8
Mackenzie Arnold8
Seán Ó
hÉigeartaigh2
Anton Korinek1,9,10
Markus Anderljung1
Ben Bucknall11
Alan Chan12,13
Eoghan Stafford1
Leonie Koessler1
Aviv Ovadya14
Ben Garﬁnkel1
Emma Bluemke1
Michael Aird15
Patrick Levermore15
Julian Hazell1,16
Abhishek Gupta4,17
1Centre for the Governance of AI
2AI: Futures and Responsibility Programme, University of
Cambridge
3MRC Biostatistics Unit, University of Cambridge
4BCG Henderson Institute
5Harvard Law School
6Harvard University
7Instituto Tecnológico Autónomo de México
8Legal Priorities Project
9University of Virginia
10Brookings Institution
11Independent
12Mila
13University of Montreal
14Thoughtful Technology Project
15Institute for AI Policy
& Strategy
16Oxford Internet Institute, University of Oxford
17Montreal AI Ethics Institute
Acknowledgements: We thank the following people for feedback and comments: An-
drew Trask, Ben Cottier, Herbie Bradley, Irene Solaiman, Norman Johnson, Peter Cihon,
Shahar Avin, Stella Biderman, Toby Shevlane.
Given the size of the group, inclusion as an author does not entail endorsement of all claims in
the paper, nor does inclusion entail an endorsement on the part of any individual’s organization.
Abstract
Recent decisions by leading AI labs to either open-source their models or to restrict
access to their models has sparked debate about whether, and how, increasingly capable AI
models should be shared. Open-sourcing in AI typically refers to making model architecture
and weights freely and publicly accessible for anyone to modify, study, build on, and
use. This offers advantages such as enabling external oversight, accelerating progress, and
decentralizing control over AI development and use. However, it also presents a growing
potential for misuse and unintended consequences. This paper offers an examination of the
risks and beneﬁts of open-sourcing highly capable foundation models. While open-sourcing
has historically provided substantial net beneﬁts for most software and AI development
processes, we argue that for some highly capable foundation models likely to be developed in
the near future, open-sourcing may pose sufﬁciently extreme risks to outweigh the beneﬁts.
In such a case, highly capable foundation models should not be open-sourced, at least
not initially. Alternative strategies, including non-open-source model sharing options, are
explored. The paper concludes with recommendations for developers, standard-setting
bodies, and governments for establishing safe and responsible model sharing practices and
preserving open-source beneﬁts where safe.
⇤Corresponding author: elizabeth.seger@governance.ai
Please cite as Seger, Dreksler, Moulange, Dardaman, Schuett, Wei, et al, ‘Open-Sourcing Highly Capable
Foundation Models: An Evaluation of Risks, Beneﬁts, and Alternative Methods for Pursuing Open-Source
Objectives’, Centre for the Governance of AI, 2023.
Executive Summary
Recent decisions by AI developers to open-source foundation models have sparked debate over
the prudence of open-sourcing increasingly capable AI systems. Open-sourcing in AI typically
involves making model architecture and weights freely and publicly accessible for anyone to modify,
study, build on, and use. On the one hand, this offers clear advantages including enabling external
oversight, accelerating progress, and decentralizing AI control. On the other hand, it presents notable
risks, such as allowing malicious actors to use AI models for harmful purposes without oversight and
to disable model safeguards designed to prevent misuse.
This paper attempts to clarify open-source terminology and to offer a thorough analysis of risks and
beneﬁts from open-sourcing AI. While open-sourcing has, to date, provided substantial net beneﬁts
for most software and AI development processes, we argue that for some highly capable models
likely to emerge in the near future, the risks of open sourcing may outweigh the beneﬁts.
There are three main factors underpinning this concern:
1. Highly capable models have the potential for extreme risks. Of primary concern is diffusion
of dangerous AI capabilities that could pose extreme risks—risk of signiﬁcant physical harm or
disruption to key societal functions. Malicious actors might apply highly capable systems, for
instance, to help build new biological and chemical weapons, or to mount cyberattacks against
critical infrastructures and institutions. We also consider other risks such as models helping
malicious actors disseminate targeted misinformation at scale or to enact coercive population
surveillance.
Arguably, current AI capabilities do not yet surpass a critical threshold of capability for the most
extreme risks. However, we are already seeing nascent dangerous capabilities emerge, and this
trend is likely to continue as models become increasingly capable and it becomes easier and
requires less expertise and compute resources for users to deploy and ﬁne-tune these models.
(Section 3)
2. Open-sourcing is helpful in addressing some risks, but could—overall—exacerbate the
extreme risks that highly capable AI models may pose. For traditional software, open-sourcing
facilitates defensive activities to guard against misuse more so than it facilitates offensive misuse
by malicious actors. However, the offense-defense balance is likely to skew more towards offense
for increasingly capable foundation models for a variety of reasons including: (i) Open-sourcing
allows malicious actors to disable safeguards against misuse and to possibly introduce new
dangerous capabilities via ﬁne-tuning. (ii) Open-sourcing greatly increases attacker knowledge
of possible exploits beyond what they would have been able to easily discover otherwise. (iii)
Researching safety vulnerabilities is comparatively time consuming and resource intensive, and
ﬁxes are often neither straightforward nor easily implemented. (iv) It is more difﬁcult to ensure
improvements are implemented downstream, and ﬂaws and safety issues are likely to perpetuate
further due to the general use nature of the foundation models. (Section 3)
3. There are alternative, less risky methods for pursuing open-source goals. There are a variety
of strategies that might be employed to work towards the same goals as open-sourcing for highly
capable foundation models but with less risk, albeit with their own shortcomings. These alternative
methods include more structured model access options catered to speciﬁc research, auditing, and
downstream development needs, as well as proactive efforts to organize secure collaborations,
and to encourage and enable wider involvement in AI development, evaluation, and governance
processes. (Section 4)
In light of these potential risks, limitations, and alternatives, we offer the following recommenda-
tions for developers, standards setting bodies, and governments. These recommendations are to help
establish safe and responsible model sharing practices and to preserve open-source beneﬁts where
safe. They also summarize the paper’s main takeaways. (Section 5)
1. Developers and governments should recognize that some highly capable models will be too
risky to open-source, at least initially. These models may become safe to open-source in the
future as societal resilience to AI risk increases and improved safety mechanisms are developed.
2
2. Decisions about open-sourcing highly capable foundation models should be informed by
rigorous risk assessments. In addition to evaluating models for dangerous capabilities and
immediate misuse applications, risk assessments must consider how a model might be ﬁne-tuned
or otherwise amended to facilitate misuse.
3. Developers should consider alternatives to open-source release that capture some of the
same distributive, democratic, and societal beneﬁts, without creating as much risk. Some
promising alternatives include gradual or “staged” model release, structured model access for
researchers and auditors, and democratic oversight of AI development and governance decisions.
4. Developers, standards setting bodies, and open-source communities should engage in col-
laborative and multi-stakeholder efforts to deﬁne ﬁne-grained standards for when model
components should be released. These standards should be based on an understanding of the
risks posed by releasing different combinations of model components.
5. Governments should exercise oversight of open-source AI models and enforce safety mea-
sures when stakes are sufﬁciently high. AI developers may not voluntarily adopt risk assessment
and model sharing standards. Governments will need to enforce such measures through options
such as liability law and regulation, licensing requirements, ﬁnes, or penalties. They will also
need to build the capacity to enforce such oversight mechanisms effectively. Immediate work is
needed to evaluate the costs, consequences, and legal feasibility of various policy interventions
and enforcement mechanisms we list.
3
Contents
Executive Summary
2
1
Introduction
5
2
What Do We Mean by “Open-Source Highly Capable Foundation Models”?
6
2.1
What are Highly Capable Foundation Models? . . . . . . . . . . . . . . . . . . . .
6
2.2
Open-Source AI: Deﬁnition and Disanalogy . . . . . . . . . . . . . . . . . . . . .
8
3
Risks of Open-Sourcing Foundation Models
10
3.1
Malicious Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.1.1
Varieties of Malicious Use . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.1.2
Ease of Malicious Use . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.1.3
Offense-Defense Balance . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.2
Risks from the Proliferation of Unresolved Model Flaws
. . . . . . . . . . . . . .
16
4
Beneﬁts of Open-Sourcing Foundation Models and Alternative Methods for Achieving
Them
17
4.1
External Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
4.1.1
The Argument for Open-Source . . . . . . . . . . . . . . . . . . . . . . .
18
4.1.2
Evaluating the Beneﬁt for Foundation Models
. . . . . . . . . . . . . . .
18
4.1.3
Other Ways to Enable External Evaluation . . . . . . . . . . . . . . . . . .
19
4.2
Accelerate (beneﬁcial) AI Progress
. . . . . . . . . . . . . . . . . . . . . . . . .
21
4.2.1
The Argument for Open-Source . . . . . . . . . . . . . . . . . . . . . . .
21
4.2.2
Evaluating the Beneﬁt for Foundation Models . . . . . . . . . . . . . . . .
22
4.2.3
Other Ways to Drive (Beneﬁcial) Progress
. . . . . . . . . . . . . . . . .
25
4.3
Distribute Control Over AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
4.3.1
The Argument for Open-Source . . . . . . . . . . . . . . . . . . . . . . .
26
4.3.2
Evaluating the Beneﬁt for Foundation Models . . . . . . . . . . . . . . . .
27
4.3.3
Other Ways to Reduce Corporate or Autocratic Control . . . . . . . . . . .
29
5
Recommendations
30
6
Conclusion
34
References
35
A AI Model Component Guide
48
4
1
Introduction
As AI developers build increasingly capable models, they face a dilemma about whether and how
they should share their models. One foundational decision they must make is whether to open-source
their models—that is, make their models freely and publicly accessible for anyone to use, study,
modify, and share.1
Software development communities have traditionally enjoyed strong norms for sharing and open-
source publication. Accordingly, for many AI researchers and developers open-sourcing is a deeply
held professional and personal value. However, this value can sit in tension with others, like growing
a proﬁtable organization may contradict protecting consumers from harm [1]. Debate continues about
the risks, beneﬁts, and tradeoffs of open-source model release.
Recently, some large AI labs have decided that open-sourcing foundation models involves unaccept-
able trade-offs and have chosen to restrict model access out of competitive concerns and worries
about model misuse. These labs are either keeping their models completely private (e.g., DeepMind’s
Chinchilla [2]) or employing a structured access approach to model sharing (e.g., OpenAI’s GPT-4
[3] and Anthropic’s Claude 2 [4] via their APIs [5], which enable the enforcement of user restrictions
and implementation of controls such as safety ﬁlters in order to manage harms.
There has been pushback against this trend to restrict model access and calls to reinforce traditional
software development community norms for sharing and openness is common. The concerns are that
model access restriction stiﬂes innovation, disallows external oversight, hinders the distribution of AI
beneﬁts, and concentrates control over AI’s future to a small number of major AI labs [6, 7]. Labs
such as Hugging Face, Allen Institute for AI, EleutherAI, RedPajama, LAION, Together.xyz, Mosaic,
and Stability AI have recently chosen to open-source large models. Meta has been a particularly
vocal open-source proponent with its release of I-JEPA [8], an efﬁcient and visual transformer in
June 2023, followed closely by Llama 2 [9–11], in July 2023.
There are many considerable beneﬁts of open-source software (OSS) development. For thirty years,
OSS has proliferated alongside, and often inside, of commercial software, encouraging cooperation,
promoting software adoption via lowered costs, reducing monopolistic control by major software
companies, fostering rapid innovation, growing talent, and improving software quality through
community review [12–14]. The academic tradition in which many machine learning researchers are
trained also enjoys strong norms of open research publication. It is only natural that many machine
learning developers and researchers follow suit, creating groups and organizations like Hugging
Face, Stability AI, RedPajama, and EleutherAI in order to build and release increasingly capable AI
models.
However, we will explain that there is a disanalogy between OSS and open-source AI, and that
we should not expect these same beneﬁts to seamlessly translate from OSS to cutting-edge AI
development efforts. While it is natural that an OSS lens has been used to motivate the open-sourcing
of AI systems, continuing to do so could come with signiﬁcant downsides. The rapid increase in
capabilities that we have observed, and likely will continue to see, mean that open-sourcing AI
systems come with higher risks of misuse, accidents, and dangerous structural effects than traditional
software [15].
In comparative terms, open-sourcing a model will tend to present greater risks than releasing it
using a structured access approach whereby model access is mediated, for example, through an API
[16]. First, once a model is open-sourced, any safeguards put in place by an AI lab to prevent its
misuse can be circumvented (see Section 3.1). No methods currently exist to reliably prevent this.
Second, once a model is open-sourced, those with sufﬁcient expertise and computing resources can,
without oversight, ""ﬁne-tune"" it to introduce and enhance capabilities that can be misused. These
two possibilities mean that any threshold of safe behavior observed and evaluated under closed or
restricted contexts cannot necessarily be assumed to hold once the model is made publicly available.2
1We use the term open-source without precise requirements on license permissions, but more generally to
mean making a model publicly and freely available. See section 2 for further discussion on open-source meaning
and terminology.
2Since it is difﬁcult to verify the safety of any model and ensure that you have observed the true range of
possible behaviors, this also holds true for models that are not open-sourced. However, the fact models can be
5
Furthermore, open-source AI model release is irreversible; there is no “undo” function if signiﬁcant
harms materialize. If a model has a ﬂaw—some exploit that elicits undesirable capabilities—or
grave misuse potential, there is nothing to stop users from continuing to use the model once released.
Similarly, if developers release patches or updated model versions to remedy ﬂaws, there is no way to
ensure users will implement the patches or operate the most up-to-date version. For malicious users
who seek to exploit model vulnerabilities that allow for harmful applications, they are incentivized
not to adopt any safety improvements.
Ultimately, as AI labs push the boundaries of foundation model development, the risks of open-
sourcing will grow as models become increasingly capable. The risks from such capability improve-
ments could become sufﬁciently severe that the beneﬁts of open-sourcing outweigh the costs. We
therefore recommend that decisions to open-source highly capable foundation models should be made
only after careful deliberation that considers (i) the range of misuse risks the open-source model may
present and (ii) the potential for open-source beneﬁts to be provided through alternative means. We
expect that in the future some highly capable foundation models should not be open-sourced.
We begin by deﬁning highly capable foundation models (section 2) and the risks presented by open-
sourcing them (Section 3). The harms are signiﬁcant and plausibly, in certain cases, justify foundation
model access restrictions. We then turn to three key arguments for open-source model sharing and
explore alternative mechanisms for achieving the desired end with signiﬁcantly less risk (Section 4).
Finally, we present recommendations for AI developers and policymakers in light of our discussion
(Section 5).
2
What Do We Mean by “Open-Source Highly Capable Foundation Models”?
2.1
What are Highly Capable Foundation Models?
Foundation models.
Foundation models, sometimes referred to as general-purpose AI models, are
machine learning models like GPT-4 that demonstrate a base of general capabilities that allow them
to be adapted to perform a wide range of downstream tasks [17, 18]. These capabilities can include
natural language conversation, behavior prediction, image analysis, and media generation3, which
can be used to develop or be directly integrated into other AI systems, products, and models.4
When modalities are combined, multimodal foundation models can integrate and respond to numerous
data types (e.g., text, audio, images, etc.). For instance, Stable Diffusion [27] and DALL·E 2 [28]
combine natural language processing capabilities with image generation capabilities to translate
natural language prompts into image outputs. GPT-4 is also multimodal, though that functionality is
not made widely available [29],5 and Meta’s open-source ImageBind project aims to link up numerous
streams of data including audio, text, visual data, movement and temperature readings to produce
immersive, multi-sensory experiences [31].
Foundation models can be used positively in healthcare [32], for data analysis [21], customer
support [22], immersive gaming [33], or personalized tutoring [24]. But they can also be misused and
deployed by bad actors, for example, to generate child sexual abuse material [34], create fake real-time
further ﬁne-tuned, adapted, and integrated with other systems upon release means that the true range of possible
behaviors can shift in unpredictable ways untestable at the pre-release stage.
3Today, many of the most discussed foundation models are generative AI systems that are variants of large
language models (LLMs) like GPT-4 (the model which forms the base of the conversational ChatGPT interface).
LLMs are machine learning models with complex architectures that generate plausible text or visual content in
response to user prompts (that are often text-based). To do so, they are ﬁrst trained on vast amounts of text, where
they learn to predict the next token (or word). Additional training then steers the LLM towards providing outputs
that humans rate highly—this makes it more likely that the LLM will provide helpful, non-toxic responses.
4We are already seeing current-generation foundation models, like GPT-4, being integrated into clinical
diagnoses in healthcare [19], visual web accessibility tooling [20], qualitative data analysis [21], video game
character development [22], customer assistance and support [23], foreign language education [24], ﬁnancial
fraud detection [25], legal tools [26], and many other industries. As their capabilities increase, future generations
of foundation models will continue to be deployed across industry and government, integrating them into many
downstream applications across a wide-range of sectors, including safety-critical applications.
5Multimodal functionality is now available to some Microsoft Enterprise customers via BingChat [30].
6
interviews or recorded histories for inﬂuential politicians [35], or to conduct highly-effective targeted
scams convincing victims that they are calling with trusted friends and family [36, 37]. Other current
and ongoing harms posed by foundation models include, but are not limited to, bias, discrimination,
representational harms, hate speech and online abuse, and privacy-invading information hazards [17,
38–40].
Foundation models have also been associated with upstream harms including poor labor conditions in
the supply chain and for those hired to label data [41, 42] as well as putting strain on the environment
through high energy and resource usage during training, deployment, and the production of the
required hardware [43–45].
“Highly capable” foundation models.
We deﬁne highly capable foundation models as foundation
models that exhibit high performance across a broad domain of cognitive tasks, often performing the
tasks as well as, or better than, a human.6
Researchers are working to develop suitable benchmarks to track the increase in such general-
purpose capabilities by measuring performance of such models holistically (e.g., in regards to
language, reasoning, and robustness [46] and across a spectrum of speciﬁc areas of knowledge, from
professional medicine and jurisprudence to electrical engineering and formal logic [47].
Extreme risks and harms.
In this paper we are particularly concerned with the possibility that
highly capable models may come to exhibit dangerous capabilities causing extreme risks and harms
such as signiﬁcant physical harm or disruption to key societal functions.7
Dangerous capabilities that highly capable foundation models could possess include making it easier
for non-experts to access known biological weapons or aid in the creation of new ones [50], or
giving unprecedented offensive cyberattack capabilities to malicious actors [51, 52]. Being able
to produce highly persuasive personalized disinformation at scale, effectively produce propaganda
and inﬂuence campaigns, or act deceptively towards humans, could also present extreme risks [53].
Self-proliferation abilities, such as evading post-deployment monitoring systems, gaining ﬁnancial
and computing resources without user or developer consent, or a model exﬁltrating its own trained
weights, are more speculative but might also facilitate extreme risks [49, 54]. This is particularly the
case if models are embedded within critical infrastructure. The magnitude of these risks requires
that model developers more carefully and systematically weigh risks against beneﬁts when making
open-sourcing decisions for highly capable foundation models than for present-day foundation
models.
Perhaps in the future we will use AI models to guard against the risks and harms presented by the
misuse of, and accidents caused by, other AI models, allowing us to safely deploy AI models with
increasingly powerful capabilities. However, such solutions are currently technically under-developed,
and there are substantial challenges to effectively deploying defensive solutions for AI at a societal
level and at scale [55]. We therefore focus on forthcoming models that may take us into a zone of
high risk against which we do not yet have sufﬁcient social or technological resilience.
In section 3 we discuss many risks that foundation models at the frontier of today’s capabilities
currently present. Arguably, these capabilities do not yet surpass a critical threshold of capability for
6We intentionally speak about “highly-capable models” instead of “frontier models”. The “frontier” refers to
the cutting-edge of AI development [18], however the frontier of cutting-edge AI moves forward as AI research
progresses. This means that some highly capable systems of concern—those capable of exhibiting dangerous
capabilities with the potential to cause signiﬁcant physical and societal-scale harm—will sit behind the frontier
of AI capability. Even if these models are behind the frontier, we should still exercise caution in deciding to
release such models, all else being equal.
7Shevlane et al. [48] operationalise such extreme risks and harms in terms of the scale of the impact they
could have—e.g., killing tens of thousands of people or causing hundreds of billions of dollars of economic or
environmental damage—or the level of disruption this would cause to society and the political order.
In their recently released Responsible Scaling Policy [49], Anthropic distinguishes between four AI Safety Levels
(ASL’s). Like the Anthropic document, this paper is primarily focused on the likely near future development of
ASL-3 models which are those that show “low level autonomous capabilities” or for which “access to the model
would substantially increase the risk of catastrophic misuse, either by proliferating capabilities, lowering costs,
or enabling new methods of attack as compared to non-LLM baseline of risk.”
7
the most extreme risks. However, we are seeing some dangerous capabilities emerge, and this trend
is likely to continue as models become increasingly capable and as it becomes easier and requires
less expertise and compute resources for users to deploy and ﬁne-tune these models.8 Recently, after
extensive testing of their large language model, Claude, by biosecurity experts, Anthropic reported
that “unmitigated LLMs could accelerate a bad actor’s efforts to misuse biology relative to solely
having internet access, and enable them to accomplish tasks they could not without an LLM.” They
note that these effects, while “likely small today”, are on the near-term horizon and could materialize
“in the next two to three years, rather than ﬁve or more” [56].
Our general recommendation is that it is prudent to assume that the next generation of foundation
models could exhibit a sufﬁciently high level of general-purpose capability to actualize speciﬁc
extreme risks. Developers and policymakers should therefore implement measures now to guide
responsible model research decisions in anticipation of more highly capable models.
These recommendations are driven by the fast pace of AI progress, the immense challenge of verifying
the safety of AI systems, and our ongoing struggle to effectively prevent harms from even current-day
systems on a technical and social level. It is difﬁcult to predict when more extreme risks may arise.
The level of risk that a model presents is intimately tied to model capability, and it is hard to know
when a critical line of capability has been or will likely be passed to pose extreme risks. In the
past, model capabilities often have arisen unexpectedly or have been discovered only after model
deployment [57].
AI models do not need to be general-purpose to pose a risk.
Finally, it is worth noting that
high-risk AI models do not necessarily need to be general-purpose in nature like foundation models,
nor must they be at the frontier of current capabilities to pose the risks described above. For example,
Urbina et al. [58] demonstrated that standard, narrow AI tools used within the pharmaceutical industry
can be repurposed to assist with the design of chemical weapons. There are also more pressing
concerns that AI systems might soon present extreme biological risks [59]. So while outside the remit
of this paper, care should similarly be taken in the open-sourcing of narrow AI models that could, for
example, be used to aid in chemical or biological weapons development.
2.2
Open-Source AI: Deﬁnition and Disanalogy
“Open-source” is a term borrowed from open-source software (OSS). In the context of open-source
software, “open-source” was deﬁned in 1998 as a “social contract” (and later a certiﬁcation) describing
software designed to be publicly accessible—meaning anyone can view, use, modify, and distribute
the source-code—and that is released under an open-source license. An open-source license must
meet ten core criteria, including free source code access, permission for derived works, and no
discrimination against which ﬁelds or groups may use the software [60, 61].
With the release of AI models like LLaMA, LLaMA2, Dolly, StableLM the term “open-source” has
become disjointed from open-source license requirements [62]. Some developers use “open-source”
merely to mean that their model is available for download, while the license may still disallow certain
use cases and distribution. For example, while Meta refers to LLaMA-2 as an open-source model, the
LLaMA-2 license caveat is that the model cannot be used commercially by downstream developers
with over 700 million monthly users, and the outputs cannot be used to train other large language
models. Strictly speaking, LLaMA2 is therefore not open-source according to the traditional OSS
8According to Anthropic’s Responsible Scaling Policy [49], current cutting-edge foundation model capabilities
are at AI Safety Level 2 (ASL-2). Anthropic deﬁnes ASL-2 models as those “that do not yet pose a risk of
catastrophe, but do exhibit early signs of the necessary capabilities required for catastrophic harms. For example,
ASL-2 models may (in absence of safeguards) (a) provide information related to catastrophic misuse, but not
in a way that signiﬁcantly elevates risk compared to existing sources of knowledge such as search engines, or
(b) provide information about catastrophic misuse cases that cannot be easily found in another way, but is
inconsistent or unreliable enough to not yet present a signiﬁcantly elevated risk of actual harm.” Given current
indications from ASL-2 models, it is prudent to expect that ALS-3 models (see footnote 8) will begin to emerge
in the near future, and developers and policymakers should prepare accordingly.
8
deﬁnition [63], and the marketing of it as such has been criticized as false and misleading by the
Open Source Initiative [63].9
However, in this paper we set licensing considerations aside, as we are concerned with the
risks and beneﬁts of public model accessibility.
From an AI risk perspective, even where more
restrictive licenses such as RAIL (Responsible AI License) include clauses that restrict certain use
cases [66], license breaches are difﬁcult to track and enforce when models are feely and publicly
available for download [67]. License breach will also not be of great concern for malicious actors
intending to cause signiﬁcant harm. Accordingly, and in line with increasing common parlance, we
use the term open-source only to refer to models that are publicly accessible at no cost.10
Licensing aside, the open-source software concept—referring only to “free and publicly down-
loadable source code”—does not translate directly to AI due to differences in how AI systems
are built [62, 68].
For AI systems, “source code” can refer to either or both of the inference code
and the training code which can be shared independently. AI systems also have additional system
components beyond source code, such as model weights and training data, all of which can be shared
or kept private independent of the source code and of each other.
Experts disagree on precisely which model components need to be shared for an AI model to be
considered open-source. Rather, the term is being used to encapsulate a variety of system access
options ranging on a spectrum from what Irene Solaiman [69] calls non-gated downloadable to
fully open models. For fully open models, training and inference code, weights, and all other
model components and available documentation are made public (e.g., GPT-J [70]). For non-gated
downloadable models, key model components are publicly available for download while others are
withheld. The available components generally include some combination of training code (minimally
model architecture), model weights, and training data.11
Table 1 presents a useful reference list of standard model components and deﬁnitions. See Appendix A
for a more detailed breakdown.
Table 1: Useful deﬁnitions of commonly-shared AI model components
Term
Deﬁnition
Model
architecture
The code that speciﬁes the structure and design of an AI model, including the types of
layers, the connections between them, and any additional components or features that
need to be incorporated. It also speciﬁes the types of inputs and outputs to the model,
how input data are processed, and how learning happens in the model.
Model
weights
The variables or numerical values used to specify how the input (e.g., text describing
an image) is transformed into the output (e.g., the image itself). These are iteratively
updated during model training to improve the model’s performance on the tasks for
which it is trained.
Inference
code
The code that, given the model weights and architecture, implements the trained
model. In other words, it runs the AI model and allows it to perform tasks (like writing,
classifying images and playing games).
Training
code
The code that deﬁnes the model architecture and implements the algorithms used to
optimize the model weights during training. The training algorithms iteratively update
the model weights to improve the AI model’s performance on the training tasks.
9Indeed, there are likely economic, strategic, and reputational beneﬁts for a company to ‘open-source’ a
model in this way [64]. Open-source innovation building on publicly available architectures can easily be
reincorporated into the model developer’s downstream products. “Openness” also has a reputationally positive
connotation. “Openwashing” is a term that describes companies who spin an appearance of open-source and
open-licensing for marketing purposes, while continuing proprietary practices [65].
11For gated downloadable models, in contrast, privileged download access is granted only to speciﬁc actors.
9
The more model components that are publicly released, the easier it is for other actors to
reproduce, modify, and use the model.
For example, access to model architecture and trained
weights (e.g., StabilityAI’s Stable Diffusion [71]), when combined with inference code, is sufﬁcient
for anyone to use a pre-trained model to perform tasks. Inference code can be easily written by
downstream developers or even generated by large language models such as ChatGPT. It also does
not need to match the original inference code used by the model developer to run the model. Access
to model weights also allows downstream developers to ﬁne-tune and optimize model performance
for speciﬁc tasks and applications.
Releasing other useful parts of the training code makes it much easier for other actors to reproduce
and use the trained model. For instance, providing the optimal hyperparameters would make a
pre-trained OS AI model more capable (and possibly dangerous), and releasing the code used to
clean, label and load the training data would reduce the burden on actors trying to reproduce model
weights.
Sometimes, an AI developer will release the training and inference code for a model, but not the
trained model weights (e.g., Meta’s LLaMA [72] before the weights were leaked).12 In such cases,
actors with sufﬁcient computing resources and data access could train the model and, with some
inference code, run it.13 However, at the moment, few actors (realistically, only large technology
companies, state-level actors, or well-funded start-ups) have the computing resources available to
train highly capable foundation models that represent the frontier of model performance.14
Therefore, in this paper, when we refer to open-source foundation models, we mean models for
which at least model architecture and trained weights are publicly available unless otherwise
speciﬁed.
Box 1 describes the need for further work deﬁning open-source gradients beyond the deﬁnition we
give here; releasing different (combinations of) model components in addition to trained weights and
training code enables different downstream activities.
3
Risks of Open-Sourcing Foundation Models
Due to their vast application space and pace of development, foundation models have potential for
broad and signiﬁcant beneﬁt and harm. Accordingly, open-sourcing these models poses some
substantial risks which we present in two categories: malicious use (3.1) and proliferation of
unresolved ﬂaws (3.2).
These harms are intensiﬁed by the fact that once a decision has been made to open-source, there is
no “undo” function. A published model cannot be rolled back if major safety issues emerge or if
malicious actors ﬁnd an AI tool to be particularly useful for scamming, hacking, deceptive inﬂuence,
or acts of terror. Methods exist that allow even partially open-sourced models (e.g., code with some
or no other model components) to be replicated and shared in full [79].
12Furthermore, we should expect model weight leaks to be frequent. Weights are contained in relatively small
ﬁles (usually less than 256 GB) that can be easily and untraceably shared. Meta, for instance, chose to restrict
access to the weights of its large language model LLaMa to researchers on a case-by-case basis, but a week
later the weights were leaked and are now available publicly on the internet [31]. If weights for a trainable
open-source model are leaked, the public functionally has access to a pre-trained open-source model.
13Note that if the model weights were not made publicly available, external actors who trained a trainable OS
model may discover a set of model weights distinct from those discovered by the original developer who released
the model. Using a different set of weights, however, does not preclude a model from performing equally well as
(or perhaps even better than) a model using the original weights.
14Training frontier foundation models costs $10–100 million in compute costs and is projected to increase
to $1–10 billion in coming years [73]. However, the cost to train a model that matches the performance of a
previous state-of-the-art system has fallen rapidly. For instance, training GPT-3, the most powerful foundation
model available in June 2020, was estimated to cost at least $4.6 million [74], but by September 2022 an
equivalently powerful model was theoretically available for $450,000 [75]. This is due to both advances in AI
chip technology and the discovery of more efﬁcient AI algorithms [76–78].
10
Box 1: Further research is needed to deﬁne open-source gradients
Gradient of System Access
The idea that models are either released open-source or maintained closed-source presents
a false dichotomy; there are a variety of model release options ranging from fully closed to
fully open model [68, 80, 81].
“Considerations and Systems Along the Gradient of System Access”
[ﬁgure reproduced from Solaiman [69]]
What is generally referred to as “open-source” model release spans the two system access
categories on the far right of Irene Solaiman’s [69] gradient: Downloadable (speciﬁcally
non-gated downloadable—meaning that anyone is free to download the available components)
and Fully Open.
Gradient of Open-Source Access
For fully-open models, source code, weights, training data, and all other model components
and available documentation are made public. However, in the non-gated downloadable
category—in which some components are publicly downloadable (usually including weights
and architecture) while others are withheld—there is room for further speciﬁcation. Im-
portantly, the precise beneﬁts and risks of open-sourcing are determined by the speciﬁc
combinations of model components and documentation that are made publicly available.
Precise Deﬁnitions for Precise Standards
Near-term investment in a project is needed to investigate and articulate what activities are
made possible by access to different (combinations of) model components. This information
will be key to constructing effective and ﬁne-grained model release standards that are not
overly burdensome, and to ensure open-source values are protected and beneﬁts enjoyed
where safe.
We make a start in Appendix A, though it is a much larger and more involved project than we
can do justice here, and it is a project on which members of open-source communities should
be centrally involved. The Open Source Initiative recently launched one such initiative to
deﬁne what machine learning systems will be characterized as open-source [82].
11
3.1
Malicious Use
Open-source publication increases foundation models’ vulnerability to misuse. Given access to the
model’s weights and architecture, any actor with the requisite technical background15 can write their
own inference code—or modify available inference code—to run the model without safety ﬁlters.
They can also ﬁne-tune the model to enhance the model’s dangerous capabilities or introduce new
ones.
There are several ways in which open-source publication can facilitate misuse:
Firstly, open-sourcing a model allows actors to run the model using new or modiﬁed inference code
that lacks any content safety ﬁlters included in the original code. Stable Diffusion’s safety ﬁlter, for
example, can be removed by deleting a single line of inference code.16 This is possible because such
ﬁlters are implemented post-hoc, appending additional processes to the model’s inference code, rather
than fundamentally changing the behavior of the model itself. With content safety ﬁlters removed,
there is nothing to prevent users from presenting the models with unsafe requests or to prevent the
model from yielding unsafe outputs.
Secondly, the ability to ﬁne-tune an open-source model without restrictions enables the modiﬁcation
of models speciﬁcally for malicious purposes. Fine-tuning that occurs through an API can be
monitored; for example, the API owner can inspect the contents of the ﬁne-tuning data set. Without
such monitoring, ﬁne-tuning could involve the reintroduction of potentially dangerous capabilities
that were initially removed by developers pre-release through their own ﬁne-tuning. Fine-tuning
can also lead models to become even more dangerous than they were before safety measures were
applied. However, increasing a model’s dangerous capabilities by ﬁne-tuning would be more difﬁcult
than removing certain kinds of post-hoc safeguards like ﬁlters; ﬁne-tuning requires the curation of
a dataset to promote those dangerous capabilities, as well as requiring the necessary compute and
technical expertise to successfully ﬁne-tune the model.
Thirdly, access to model weights can aid adversarial actors in effectively jailbreaking system safe-
guards (including for copies of the system that have not been modiﬁed). Traditional jailbreaks use
clever prompt engineering to override safety controls in order to elicit dangerous behavior from a
model (e.g., getting a large language model (LLMs) to provide instructions for building a bomb by
asking it to write a movie script in which one character describes how to build a bomb). Creative
prompting only requires model query access. However, researchers recently discovered a method of
adversarial attack in which the network weights of open-source LLMs aided researchers in optimizing
the automatic and unlimited production of “adversarial sufﬁxes”, sequences of characters that, when
appended to a query, will reliably cause the model to obey commands even if it produces harmful
content [84]. Notably, this method, which was developed using open-source models Vicuna-7B
and Meta’s LLaMA-2, is transferable; it also works against other LLMs such as GPT-4 (OpenAI),
Bard (Google), and Claude (Anthropic), indicating that open-sourcing one model can expose the
vulnerabilities of others.
The above methods have the potential of reducing, if not entirely nullifying, the measures taken
by developers to limit the misuse potential of their models. These measures would be much more
difﬁcult to bypass in cases where the model weights and training code are not openly released, and
where user interaction with the model is facilitated through an API. Fine-tuning, in particular, can
also lead models to be more dangerous than they might have been originally.
15Knowledge equivalent to that from a graduate-level machine learning course would be sufﬁcient to perform
ﬁne-tuning, but additional experience in training models would likely be useful in addressing the myriad of
issues that sometimes come up, like divergence and memory issues. Depending on the malicious use case, it
may be more or less difﬁcult to source the required data set.
16This observation comes from personal correspondence with several technical researchers. We do not provide
further details on speciﬁc technical ﬂaws since we believe it would be irresponsible to do so. Please see Rando
et al. [83] on red-teaming the Stable Diffusion safety ﬁlter for related information.
12
3.1.1
Varieties of Malicious Use17
Potential epistemic, social and political consequences of foundation model misuse include the
following [85, 86].
• Inﬂuence operations. There is a wealth of existing research theorizing AI’s utility in automating, or
otherwise scaling, political or ideological inﬂuence campaigns through the production and targeted
dissemination of false or misleading information [17, 86–88]. There is concern about multimodal
foundation models being used to create interactive deepfakes of politicians or constructing and
catering detailed and seemingly veriﬁable false histories [35]. A recent experiment demonstrated
the potential for AI-based inﬂuence operations when the LLM-based system, CounterCloud, was
deployed to autonomously identify political articles, to generate and publish counter-narratives,
and then to direct internet trafﬁc by writing tweets and building fake journalist proﬁles to create a
veneer of authenticity [89].
Concerns about AI being used to manipulate public views, undermine trust, drive polarization, or
otherwise shape community epistemics have led some scholars to speculate that ‘whoever controls
language models controls politics’ [90].
• Surveillance and population control. AI advances the means of states to monitor and control their
populations through immersive data collection, such as facial and voice recognition [91], the nascent
practice of affect recognition [92], and predictive policing [93]. AI also allows automating and
thus ever more cheaply analyzing unprecedented amounts of data [48]. Authoritarian governments
may be most likely to make use of AI to monitor and control their populations or to suppress
subpopulations [94, 95], but and? other types of governments are employing AI enabled surveillance
capabilities as well. Nascent AI surveillance technologies are spreading globally and in countries
with political systems ranging from closed autocracies to advanced democracies [96, 97].
• Scamming and spear phishing. Malicious actors can use AI to fraudulently pose as a trusted
individual for the purpose of theft or extraction of sensitive information [98]. For example, large
language models have been shown to be proﬁcient in generating convincing spear phishing emails,
targeted at speciﬁc individuals, at negligible cost [99].
Evidence from online forums also indicates that malicious AI tools and the use of “jailbreaks” to
produce sensitive information and harmful content are proliferating amongst cyber criminals [100].
High proﬁle scams using generative AI have also been observed, with one report detailing how
$35million was stolen from a Japanese ﬁrm by scammers who used AI voice cloning tools to pose
as a company executive to employees [37].
• Cyber attacks. Foundation models have applications for both cybersecurity and cyber warfare
[52, 101]. Early demonstrations show that LLMs’ current coding abilities can already ﬁnd direct
application in the development of malware and the design of cyber attacks [102]. With improved
accessibility and system capability, the pace of customized malware production may increase as
could the variability of the malware generated. This poses a threat to the production of viable
defense mechanisms. Especially in the near term, there is some evidence that AI generated malware
can evade current detection systems designed for less variable, human-written programs [103–105].
Ultimately, information gained from cyberattacks might be used to steal identities, or to gather
personal information used to mount more sophisticated and targeted inﬂuence operations and
spear phishing attacks. Cyberattacks could also be used to target government agencies or critical
infrastructure such as electrical grids [106], ﬁnancial infrastructures, and weapons controls.
• Biological and chemical weapons development. Finally, current foundation models have shown
nascent capabilities in aiding and automating scientiﬁc research, especially when augmented with
external specialized tools and databases [107, 108]. Foundation models may therefore reduce the
human expertise required to carry-out dual-use scientiﬁc research, such as gain-of-function research
in virology, or the synthesis of dangerous chemical compounds or biological pathogens [50, 109].
For example, pre-release model evaluation of GPT-4 showed that the model could re-engineer
17To be clear, open-sourcing is not to blame for the malicious use of AI. Foundation models are a dual use
technology, and where the technology is built by malicious actors or where effective safety restrictions are
not in-place for models accessible via API, misuse can occur. Open-sourcing risks the diffusion of potentially
dangerous capabilities to malicious actors and lowers barriers against misuse.
13
known harmful biochemical compounds [110], and red-teaming on Anthropic’s Claude 2 identiﬁed
signiﬁcant potential for biosecurity risks [56, 111].
Specialized AI tools used within these domains can also be easily modiﬁed for the purpose of
designing potent novel toxins [58]. Integrating narrow tools with a foundation model could increase
risk further: During pre-deployment evaluation of GPT-4, a red-teamer was able to use the language
model to generate the chemical formula for a novel, unpatented molecule and order it to the red-
teamer’s house [110]. Law-makers in the United States are beginning to take this biosecurity threat
seriously, with bipartisan legislation—the Artiﬁcial Intelligence and Biosecurity Risk Assessment
Act—being proposed that would monitor and study the potential threats of generative and open-
source AI models being used “intentionally or unintentionally to develop novel pathogens, viruses,
bioweapons, or chemical weapons” [112].
3.1.2
Ease of Malicious Use
One factor that potentially mitigates the misuse of open-source foundation models is that the pool of
actors with the requisite talent and compute resources to download, run and, when necessary, modify
highly capable models effectively is relatively small. Nevertheless, there are still several reasons to
be concerned.
First, there is an increasing number of individuals who have the skills to train, use, and ﬁne-tune AI
models as illustrated by growing computer science PhD enrollment as well as ballooning attendance
at AI conferences [113]. This is supplemented by an increasing number of tutorials and guides
available online to use and ﬁne-tune AI systems.
Second, running a pre-trained AI model at a small scale requires only a small amount of compute—
far less compute than training does. We estimate the largest Llama 2 model (Llama-2-70B) costs
between $1.7 million and $3.4 million to train,18 while the inference costs for Llama-2-70B are
estimated to be between 0.2 and 6 cents per 750-word prompt [116] and $4 per hour of GPU time.19
While the compute requirement becomes large when running models at a very large scale (that is,
performing many inferences),20 large-scale runs may not be required for impactful misuses of a
model. It is conceivable that only a few inferences may be needed in certain domains for models
to be dangerous (e.g., a malicious actor may only need to ﬁnd one critical vulnerability to disrupt
critical infrastructure).
Third, while the overall cost of training frontier models is increasing [73],21 algorithmic progress
focuses heavily on reducing demands on compute resource, both for training22 and for ﬁne-tuning
[118]. This, combined with the decreasing cost of compute (measured in FLOP/s per $)[119], means
that while initial model development and training may remain prohibitively expensive for many
actors, we should not expect compute accessibility to always act as a strong limiting factor for
18Meta reported using 1,720,320 A100 GPU-hours to train Llama-2-70B [114]. A single consumer A100
GPU can be rented privately for $1.99/hour (e.g. from RunPod [115]. Our range assumes that Meta’s cost was
between $1 and $2 per hour.
19Since the Llama-2-70B model is about 129GB, it requires 2 80GB A100 GPUs to store, each of which can
be rented for about $2/hour (e.g. from RunPod [115]).
20Both training and inference processes are typically more economical when run on centralized high-
performance computing (HPC) systems optimized for AI workloads housed within data centers. While a
single training run demands more compute than a single inference, the majority of compute for AI systems is not
being used for training runs. As with most infrastructure, the operating costs will eventually be larger than the
upfront cost. As the ﬁnal product of AI systems, inferences are triggered by a multitude of daily actions, ranging
from chatbot interactions and Google searches to commands to virtual personal assistants like Siri or Alexa.
Consider image generation: the cumulative compute used for generating images via a generative AI model has
now likely surpassed the initial training compute for the most popular generative systems by orders of magnitude.
The key difference between development and deployment lies in timeframe and independence. In inference, the
computational resources can be distributed across multiple copies of the trained model across multiple compute
infrastructures over a longer time duration. Whereas, in training, the computational resources are required over a
smaller time frame within one closed system, usually one compute cluster.
21See Footnote 9.
22For example, Meta’s recently released I-JEPA (Image Joint Embedding Predictive Architecture) offers a
non-generative approach for self-supervised learning that does not rely on hand-crafted data-augmentations, and
requires signiﬁcantly fewer GPU hours to train for a better performing model [8, 117].
14
ﬁne-tuning existing open-source foundation models. Targeted ﬁne-tuning of a pre-trained model to
create dangerous models would remain much less expensive than building a model from scratch.
3.1.3
Offense-Defense Balance
Another argument against the threat of malicious use posed by open-sourcing is that while open-
sourcing may increase model vulnerability to exploitation by malicious actors, it does more to help
developers identify those vulnerabilities before malicious actors do and to support development of
tools to guard against model exploitation and harms [120]. In other words, in the offense-defense
balance—a term referring to the “relative ease of carrying out and defending against attacks” [121,
122]—it has been argued that open-sourcing favors defense.
This is often true in the context of software development; open-sourcing software and disclosing
software vulnerabilities often facilitate defensive activities more than they empower malicious actors
to offensively identify and exploit system vulnerabilities. However, the same might not be safely
assumed for open-source AI, especially for larger and more highly capable models [55]. Shevlane
and Dafoe [55] explain that when a given publication (e.g., publication of software, AI models, or of
research in biology or nuclear physic etc.) is potentially helpful for both people seeking to misuse a
technology and those seeking to prevent misuse, whether offensive or defensive activities are favored
depends on several factors:
• Counterfactual possession. How likely would a would-be attacker or defender be able to acquire
the relevant knowledge without publication? If counterfactual possession by the attacker or defender
is probable, then the impact of publication on their respective offensive and defensive activities is
less.
• Absorption and application capacity. A publication only beneﬁts attackers and defenders to
the extent that they can absorb and apply the knowledge toward their desired ends. This depends
on how much knowledge is disclosed, how the knowledge is presented, and the attentiveness and
comprehension of the recipients.
• Resources for solution ﬁnding. For defenders, given publication, how many additional actors will
help develop defenses? Impact of publication is greater if many people are likely to contribute to
defensive applications.
• Availability of effective solutions. Are vulnerability patches easy to implement, or will developing
solutions be a more complicated and time intensive endeavor? The positive effects of publication
decrease the more difﬁcult vulnerabilities are to address.
• Difﬁculty/cost of propagating solutions. Even where defensive solutions exist, if they are difﬁcult
to propagate then the impact is less.
For software development, the offense-defense balance of open-source publication often comes out in
favor of defense. Software vulnerabilities are easy to ﬁnd, so counterfactual possession by attackers
is likely, and software patches are relatively easy to make, usually fully resolve the vulnerability, and
are easily rolled out through automatic updates.
However, in the context of AI research, Shevlane and Dafoe offer the tentative conclusion that as
AI models grow in capability and complexity, open-source publication will likely skew the balance
towards offense. As discussed at the start of this section, attacker knowledge of vulnerabilities and
their ability to exploit those vulnerabilities is greatly increased by open-source publication. For some
vulnerabilities, researching solutions is time consuming and resource intensive (See Section 4.2).
Solutions developed also tend not to be perfect ﬁxes. This is for a variety of reasons: (i) given our
current lack of understanding of how advanced AI systems work internally, it may be difﬁcult to
identify the source of risk or failure; (ii) certain risks, such as bias and discrimination, may be learned
from the training data, and it could be impossible to “remove” all bias from training data [123]; (iii)
reducing misuse of AI systems may require changes to social systems beyond changes to technical
ones [55]; (iv) the structure of AI systems introduces new sources of failure speciﬁc to AI that are
resistant to quick ﬁxes (e.g., the stochastic nature of large language models may make it difﬁcult
to eliminate all negative outputs, and the inability to distinguish prompt injections from “regular”
inputs may make it difﬁcult to defend against such attacks) [124]. Finally, it is difﬁcult to ensure
15
improvements to open-source models are implemented by downstream users and developers which
can result in widespread proliferation of unresolved model ﬂaws. We address this topic in Section 3.2.
The conclusion that the offense-defense balance skews towards offense when open-sourcing AI
remains tentative because the offense-defense balance is inﬂuenced by a myriad of factors making
it difﬁcult to reliably predict outcomes. The balance will vary with each model, application space,
and combination of released model components. In addition, we may develop measures in the future
that build our defensive capabilities. Nonetheless, the general notion holds; open-sourcing AI leans
towards offense more so than open-sourcing software. AI developers should therefore think critically
about the potential for, and potential protections against, misuse before every model release decision.
3.2
Risks from the Proliferation of Unresolved Model Flaws
Excitement about foundation models stems from the large number of potential downstream capability
modiﬁcations and applications. These can include applications involving malicious intent and misuse,
but more frequently will involve well-intentioned commercial, scientiﬁc, and personal applications
of foundation models. If they have the necessary resources and model access (via open-source or
sufﬁcient API access), downstream individuals, AI labs, and other industry and government actors
can:
1. Employ foundation models to new tasks that were not previously subject to risk assessments due
to the general capabilities of these models.
2. Fine-tune or otherwise alter open-sourced foundation models to enable specialized or additional
(narrow and general) capabilities.
3. Combine foundation models with other AI models, tools, and services, such as the internet or
other APIs, to create a system of AI models which can have new narrow and general capabilities.23
For example, AutoGPT is an open-source app that integrates with GPT-3.5 and GPT-4. While
GPT-3.5 and GPT-4 can respond one prompt at a time, AutoGPT handles follow-ups to an initial
prompt. This allows users to ask AutoGPT autonomously to complete higher-level goals that
require iteratively responding to and generating new prompts [125, 126].
In all three cases, the risks, ﬂaws, system vulnerabilities, and unresolved safety issues of
the initial foundation model propagate downstream.
For instance, biased and discriminatory
behavior, vulnerabilities to prompt injection [127] and adversarial attacks [84], autonomous self-
proliferation abilities [54], or other dangerous capabilities could quickly proliferate if not caught and
ﬁxed before being integrated into downstream products and applications.
The fact that the models can be applied to new contexts (1), but also adapted (2 and 3) to unlock new
narrow and general capabilities, also means that further, difﬁcult to predict risks and harms could
emerge. Consequently, it is not certain that the safeguards put in place by the foundation model
developer will continue to be effective if downstream developers ﬁne-tune, alter, and combine AI
models. This means that not only will existing model ﬂaws proliferate, but previously ﬁxed ﬂaws and
new ﬂaws may also arise.
If (1), (2), and (3) are enabled via structured API access (e.g., OpenAI’s davinci-002 and GPT-3.5
can be ﬁne-tuned via API [128], then developer monitoring of API use may go some way towards
mitigating the proliferation harms described above. There is no such recourse, however, if a model is
made open-source. Once a model is open-sourced, there are no take-backs if harms ensue.
When risks and vulnerabilities are proliferated there is no way of ensuring that when a ﬁx is rolled out
(assuming a ﬁx is possible - see end of 3.1) that it is adopted or integrated effectively by downstream
AI developers and users. Even in the context of traditional open-source software, software ﬂaws
are proliferated [129] as downstream developers and users more often than not fail to implement
23For example, ChemCrow is a large language model that integrates 17 expert-designed computational
chemistry tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The
developers note that ChemCrow aids expert chemists and lowers barriers for non-experts which can foster
scientiﬁc advancement but could also pose signiﬁcant risk of misuse [108]. Also see Boiko, MacKnight, &
Gomes [107] on combining large language models.
16
patches and version updates, even where the open-source license requires they do so [130]. Very often
consumers are unaware that their systems are running on out-of-date software or that vulnerability
patches are available. Other times an updated software version will not integrate well with other
software packages and existing infrastructure. We should expect the same challenges to undermine
the maintenance of open-source foundation models, though a given foundation model will likely be
applied to a much wider range of applications than a piece of software.
There are also different incentives inﬂuencing decisions to implement updates for traditional software
than for foundation models. For traditional software, patches and version updates improve system
performance and functionality and resolve vulnerabilities that could cause harm to the user. It is to the
user’s beneﬁt to implement software updates when feasible. In comparison, for increasingly capable
foundation models, safety patches and updates often aim to reduce system functionality, disallowing
certain activities that were possible with previous versions. If downstream developers and users wish
to retain those functionalities (e.g., to be able to produce nude art with an image generator), they are
incentivized not to update versions and, in some cases, not to disclose the existence of potential risks
and system vulnerabilities.
Due to the potential of proliferating risks and model ﬂaws from highly capable foundation mod-
els, developers need to consider model release decisions carefully. Developers of highly capable
foundation models must be cognizant of the potential downstream harms of their models (harms
which they would be powerless to backtrack) and carefully consider alternative methods by which
open-source beneﬁts might be pursued but at signiﬁcantly less risk [131]. We discuss alternatives
further in Section 4. Clear legislation is also needed to hold developers and controllers of AI systems
liable for the impacts of their systems.
4
Beneﬁts of Open-Sourcing Foundation Models and Alternative Methods for
Achieving Them
In this section we analyze three key beneﬁts of open-source software: facilitating external evaluation
(4.1), accelerating beneﬁcial progress (4.2), and distributing control over technological development
and beneﬁts (4.3). For each, we ﬁrst present the beneﬁt, then evaluate the beneﬁt in the context of
highly capable foundation models, and ﬁnally consider other strategies that might contribute to the
same goals. A summary table is provided at the start of each subsection.
4.1
External Model Evaluation
Table 2: Section summary: Open-sourcing as a mechanism for enabling external model evaluation
The argument
for open-source
AI
Open-sourcing enables independent model evaluations of projects by wider com-
munities of developers. Tapping into the wider AI community helps to catch bugs,
biases, and safety issues that may otherwise go unnoticed, ultimately leading to
better performing and safer AI products.
Evaluation of
beneﬁt
• Open-sourcing is most useful for evaluating complex safety issues and less
useful for identifying discrete bugs. There may also be suitable alternatives to
open-sourcing that achieve these same beneﬁts with fewer risks.
Alternative
methods
• Grant privileged model access to trusted (independently selected) third-party
auditors via gated-download or research API.
• Establish a community of (independently selected) red-team professionals to
stress-test models pre-release.
• Explore social impacts and safety issues through incremental, staged release of
models.
• Employ safety bounties to incentivize wide public involvement in reporting new
behaviors and safety issues.
17
4.1.1
The Argument for Open-Source
A clear beneﬁt for open-source software development is that open-sourcing facilitates independent
evaluations of projects by wider communities of developers and many more people than a single
developer would be able to employ internally to check for bugs and safety issues. This means a
more diverse pool of expertise can be tapped, with a low barrier to entry for individuals to contribute,
whose skill to identify and solve problems is enhanced by increased access to relevant materials.
So in the case of highly capable foundation models, it is reasonable to expect that open-sourcing
would leverage the same talent multiplier as with OSS. Tapping into the wider AI community would
enable audit and analysis of foundation models and any model components (e.g., training data,
weights, documentation) by interested parties helping to catch bugs, biases, and safety issues that
may otherwise go unnoticed. Such external oversight would help hold AI developers to account for
the quality and consequences of their products at a team and an industry level, and ultimately lead to
better performing and safer AI products.
4.1.2
Evaluating the Beneﬁt for Foundation Models
In this section we consider the beneﬁts of open-sourcing for enabling external model evaluations
according to two classes of model issues: (1) discrete bugs, and (2) complex safety challenges.
Discrete Bugs.
Discrete bugs such as interface glitches, data exposures and authentication issues
are self-contained ﬂaws that are relatively simple to ﬁx. Once discovered, discrete bugs can be easy
and relatively low cost for model developers to ﬁx in-house. But bug spotting certainly beneﬁts
from additional eyes, and there are alternative methods to open-sourcing that attempt to facilitate
more widespread participation in model review. For example, AI developers can set up community
reporting systems as they are encountered and even incentivize engagement via bug bounties like
that employed by OpenAI [132]. That being said, conscious steps need to be taken to ensure that the
beneﬁts of open-sourcing can be replicated: active efforts need to be made to engage the attention
of a diverse set of experts and it remains difﬁcult to mimic open-sourcing here in all respects. For
example, not having full access to materials will impede individuals in their ability to ﬁnd bugs.
A further advantage of open-sourcing is that it allows downstream developers to patch such issues on
their own and to pass those patches back to the developer for integration into future model versions.
Complex Safety Challenges.
Increasingly capable foundation models are bringing with them an
array of new behaviors and safety challenges that arise unpredictably and are not well-understood
by developers [133]. For example, emergent abilities are unexpected and unintended features or
behaviors that arise in AI models as they become more advanced. These abilities are not observed in
smaller precursors and are not explicitly programmed by developers [57]. “Capability overhang” is
a concept that further describes how these emergent abilities can be latent within a system only to
emerge unexpectedly when elicited, for example, by clever prompt engineering or integration with
other software. Sometimes new capabilities continue to be elicited many months after model release
[80].
Drawing input from a large pool of contributors will be instrumental to exploring this evolving space
of unknown unknowns; what do new safety issues look like and, if not immediately evident, how
are they triggered? Furthermore, because some model behaviors will only emerge with downstream
modiﬁcation of model weights, model evaluators will need to be able to experiment with model
ﬁne-tuning to test a variety of possible model versions.
Open-sourcing provides the necessary access to model weights and parameters for attempting to
elicit new behaviors from models for safety evaluation (although it simultaneously allows malicious
attempts to elicit new dangerous behaviors and avenues of misuse). For models that are not open-
sourced, ﬁne-tuning might also be facilitated via APIs that allow users to manipulate model weights
and parameters (e.g. OpenAI’s davinci-002 and GPT-3.5 [128]. However, some APIs may introduce
additional limitations on ﬁne-tuning. For example, API controllers could attempt to limit the format or
content of data used to ﬁne-tune a model, limit access to weights and parameters (e.g. provide access
to weights and parameters of base-line models but not to ﬁne-tuned model versions), or limit the
18
amount of ﬁne-tuning that can be done. These limitations might be in place to protect the developer’s
commercial interests or to reduce risk of misuse.
Safety research, such as alignment and interpretability research which aim to understand and resolve
complex safety issues, also require varying degrees of model access. We will discuss the beneﬁts of
open-sourcing for promoting safety research in section 4.2.
4.1.3
Other Ways to Enable External Evaluation
There are some alternatives to open-sourcing that can facilitate the identiﬁcation and evaluation of
bugs and safety issues, with less risk than open model release.
Staged-Release Impact Testing.
AI developers can conduct staged-release impact testing to gather
observational data about how a model is likely to be (mis)used and modiﬁed if open-sourced. Staged-
released impact testing is a process by which incrementally larger versions of a model are released
behind API [134, 135]. Each stage of release allows time to observe how the model is used, to
study its social impacts, and to implement any patches or new safety measures before the next, more
powerful version is released (if it is deemed safe to do so).
If many safety measures need to be implemented between stages to mitigate harms, this is a solid
indication that open-sourcing will lead to malicious use because, once open-sourced, those measures
could be easily circumvented.
Conducting staged release impact testing allows AI developers to be more comfortable with open-
sourcing their models, assuming no other signiﬁcant issues emerge in model evaluation and risk
assessment process. However, this can come at a cost to the developer by allowing competitors to
capture market share in the meantime if such processes are not implemented for the industry at large
through regulation. In addition, any beneﬁts from the model are also delayed from reaching the
relevant communities that could beneﬁt from them.
External Audits & Red-teaming.
In addition to staged-release impact testing, developers can
grant privileged model access to trusted third-party auditors. These are external actors (government
departments, private expert organizations, or some combination thereof) tasked with evaluating the
safety and security of foundation models prior to model release or assessing and verifying the model
evaluation measures employed by AI labs.
Though they are in early stages of development, external auditing has been proposed as a key
institutional mechanism for facilitating trustworthy AI development [136–139]. One early example is
the Alignment Research Center’s (ARC) pre-release evaluation of GPT-4 for dangerous capabilities
[140].
The ARC evaluations largely involved red-teaming GPT-4. Red-teaming is an evaluation method
that stress-tests models to discover how and where safety concerns arise. The aim is to identify
potentially dangerous model properties (e.g., manipulative or power-seeking behavior), security ﬂaws
(e.g., jailbreaks), and possible misuse applications. Stress-testing requires that red-teams are able
to prompt models to elicit new and dangerous behavior which can be facilitated with model query
access—that is, being able prompt models and receive outputs without open-source access to model
code and weights.
Where model weight access is needed to experiment with ﬁne-turning, access might be granted to
identiﬁed individuals or research groups via gated download or API. For gated download developers
make models (minimally weights and training code) available for speciﬁc actors to download and
run on their own hardware.24 The risk with gated download is that model leaks could result in
the dissemination of potentially dangerous models. Download recipients would need to be vetted
carefully. Another option is for developers to provide ﬁne-tuning access via API. However, as
mentioned above, some developers may choose to implement limitations on ﬁne-tuning in order to
prevent misuse or model reproduction. For this reason, Bucknall et al. [141] recommend the design
24For further discussion on gradients of model release, including gated and non-gated downloadable models,
see Box 1 and [69]
19
and implementation of ‘research APIs’ whereby more ﬂexible ﬁne-tuning permissions are granted to
trusted researchers, red-teams, and auditors depending on their access needs.
Red-teams such as those employed by OpenAI [29, 142, 143] and Anthropic [56, 144] are increasingly
common, though best practices are still being developed. Model evaluation is a nascent ﬁeld. This
makes it difﬁcult to evaluate the skill of potential auditors. Moving forward, standards will need
to be developed and implemented to ensure the quality and consistency of third-party audits [145]
as numerous governments and private actors move to occupy a growing AI assurance sector [146].
Mechanisms will also be needed to ensure developers provide sufﬁcient model access to auditors and
respond to audit ﬁndings. For instance, audit reports should be published publicly or shared with a
government overseer while regulatory requirements ensure labs respond and disclose their efforts
and results. Governments should consider establishing mandatory auditing regimes for large and
potentially dangerous foundation models to minimize the risk of model developers only granting
access to favored auditors, who might be less likely to expose failure modes that are potentially
embarrassing or inconvenient for the developer.
Much work is needed developing new model evaluation techniques and establishing best practice.
Some evaluation processes may beneﬁt from leveraging foundation model capabilities [147] as well
as input from wider AI developer communities. Decisions about how and by whom models are
audited are currently entirely at the discretion of individual developers. Without standardized risk
assessment procedures a lab could choose an “easy” or “friendly” auditor.
Another possibility Brundage et al. [136] suggests, is to extend red-teams to elicit input from a wider
community of ‘red-team professionals’. Such a community would be composed of members from
the wider AI community as well as security professionals, and representatives from high-risk domains
to which foundation models might be put to use. This would help distribute red-teaming costs for
labs less-inclined to form internal red-teams, and the community of red-team professionals would
beneﬁt from greater insight to common attack vectors and useful red-teaming strategies shared within
the community. But again, risks arise by allowing AI developers to choose red-teamers on their own,
including capture of the safety evaluation process and a potential narrowing of focus and values
by not ensuring an optimally diverse and comprehensive set of experts. Further best practices and
regulatory mechanisms need to be put in place to make sure red-teaming can provide effective safety
evaluations of AI models.
Bug Bounties and Safety Bounties.
Safety bounty programs have been proposed as another method
of tapping into a wider global community to help identify and surface new safety and alignment issues
in large foundation models [148]. Bounty “hunters” are not pre-vetted as with selected red-teams.
Analogous to bug bounty programs commonly used in cybersecurity, safety bounty programs would
offer ﬁnancial and reputational rewards to members of the public who discover and responsibly report
new safety failures, such as novel jailbreaks, or capabilities beyond those found in internal tests. As
with red-teaming, bounty “hunters” can do this by interacting with systems behind an API. However,
it is as yet unclear to what extent this impedes the ability of external testers to surface and probe
safety issues.
An early safety bounty trial by OpenAI for ChatGPT incentivized over 1500 submissions, with limited
publicization and $20,000 of API prizes in total [149]. While OpenAI noted that the submissions
seemed to yield few new discoveries beyond the safety issues that internal red-teams had already
noticed, the exercise produced insight into the most common routes of attack and lessons for future
public engagement [148].
Safety bounty programs can also be leveraged to identify promising talent. Bounty hunters who
submit multiple helpful tips could be contacted and employed to perform more extensive system
testing, and be granted deeper levels of system access after appropriate vetting. In cybersecurity,
some bug bounty hunters earn payouts totaling over $1 million for their work, and go on to work for
large ﬁrms [150, 151].
20
4.2
Accelerate (beneﬁcial) AI Progress
Table 3: Section summary: Open-sourcing as a mechanism for accelerating AI progress
The argument
for open-source
AI
Open-sourcing allows more people to contribute to AI development processes
and enables large-scale collaborative efforts. The idea is that more expertise,
more diverse perspectives, and simply more human creativity and hours put into
AI development will drive innovation in new and useful downstream integra-
tions, advance AI safety research, and help push forward the boundaries of AI
capability.
Evaluation
of Beneﬁt
Integration Progress
• Open-sourcing is most helpful for integration progress. Model access al-
lows more people to tinker, innovate, and optimize for integration with new
downstream applications.
Capability Progress
• Open-sourcing is less beneﬁcial for capability progress than for integration
progress.
• The beneﬁt is limited by bottlenecks in the talent, compute, and data resources
needed for contributing to cutting-edge AI capability research.
Safety Progress
• Academic safety research is often curtailed by insufﬁcient access to highly
capable models.
• The beneﬁt of open-source might be reduced by insufﬁcient computation
infrastructure outside of leading AI labs for running highly capable models.
Alternative
methods for
driving AI
progress
Integration Progress
• Use plugins for exploration of new applications.
• Provide gated access [i.e. full access restricted to identiﬁed third parties]
coupled with Know-Your-Customer Requirements.
Capability Progress / Safety Progress
• Provide privileged model access to identiﬁed AI research groups, possibly via
structured access research APIs.
• Seek and organize collaborations with trusted parties and provide gated down-
load access to collaborators.
• Establish a multistakeholder governing body to mediate research access to
protect against favoritism and to facilitate independent academic research.
• Build incentive structures like large rewards programs for major scientiﬁc dis-
coveries (e.g., protein folding) or pro-social advances (e.g. health and equity
applications) using AI and for AI safety breakthroughs (e.g., interpretability).
• Commit a certain percentage of proﬁts or research hours towards AI safety
projects.
4.2.1
The Argument for Open-Source
Another argument for open-sourcing AI is that doing so helps to accelerate progress that pushes the
boundaries of AI capability, advances AI safety research, and drives innovation of new downstream
applications and integrations. The idea is that open-sourcing allows more people to contribute
to AI development processes. It allows downstream developers to optimize and perfect existing
models instead of having to start from scratch for each new application, and it enables large-scale
collaborative efforts. Furthermore, progress created by the wider AI community will beneﬁt from
21
more diverse perspectives and insights, which will ultimately help develop AI aligned to unique
community needs and cultural preferences. In addition, open-source efforts may be more likely
to focus on pro-social applications of AI, and be less inﬂuenced by the ﬁnancial and commercial
incentives than industry AI developers.
These are beneﬁts widely enjoyed by open-source software communities. Linus Torvalds’ open-source
release of the Linux kernel, in particular, showed how taking advantage of community-wide co-
creation allows OSS tools to be developed and released quickly, maintained cheaply, and customized
for individual needs without compromising quality. For cloud computing especially, these beneﬁts
allowed the Linux operating system to directly compete with Windows and MacOS, commercial
systems backed by signiﬁcantly more resources such as specialized knowledge, corporate information-
sharing infrastructure, performance accountability mechanisms, and marketing and legal support
[152, 153].
It follows that we might expect AI progress to beneﬁt similarly.
4.2.2
Evaluating the Beneﬁt for Foundation Models
In this section we evaluate the inﬂuence of open-source model sharing on driving beneﬁcial foundation
model progress. To focus the conversation we differentiate between three kinds of AI progress: (1)
integration progress, (2) safety progress, and (3) capability progress.
(1) Integration progress.
Integration progress is about the discovery of new applications and
integrations for foundation models to serve a greater variety of needs—i.e. how a model can be
applied to new tasks and integrated with other applications. For example, ChatGPT embedded with
Duolingo has made for an effective language tutoring and practice tool [24].
Of the three forms of progress, integration progress beneﬁts most from open-source. Open-sourcing
models and model components gives more people access to tinker and innovate. But perhaps more
importantly, passing on a model with all life-cycle documentation to downstream developers enables
those developers to optimize the model’s performance by ﬁne-tuning its training and to inﬁnitely
test and evaluate the model when integrated into the ﬁnal product — as Alex Engler writes, there is
“simply too much at stake for downstream developers to use AI systems they do not fully understand”
[154].
Indeed, recent breakthroughs in ﬁne-tuning—speciﬁcally Low Rank Adaptation (LoRA)[155]—were
driven by open-source communities out of necessity for reducing costs and compute requirements. It
is a process by which the performance of smaller models can be signiﬁcantly improved by optimizing
model weights using the outputs of more high-capable models as training data.25
(2) Safety Progress.
Safety progress refers to advances made in AI safety research. AI Safety
research works to improve AI safety by identifying causes of unintended and harmful behavior,
aligning AI behavior with human values, improving model interpretability and robustness, and
otherwise developing tools to ensure AI systems work safely and reliably [157, 158].
Current safety research is often limited by insufﬁcient access to large, cutting-edge models and
relevant information such as their architecture and training processes [141]. Open-sourcing does
alleviate these restrictions but is not necessary for all safety research.
Different areas of safety research require different kinds of model access. For example, evaluation
and benchmarking research aims to develop and test methods to assess the capabilities and safety of
AI systems. Often the ability to sample from a model via an API will be sufﬁcient for this research,
as current approaches are based on observing a model’s output in response to a given prompt.
In contrast, research areas such as alignment and interpretability require more comprehensive access.
Alignment research, which aims to help AI systems better reﬂect user preferences and values, typically
requires researchers to be able to modify a model through ﬁne-tuning, including through the use
25We classify ﬁne-tuning as a form of integration progress instead of capability progress because the impressive
performance of ﬁne-turned models bootstraps on the capabilities of existing models. Pushing the frontier of AI
capability still requires signiﬁcant talent and compute at a scale only found in large, well-resourced labs [156].
22
of reinforcement learning. Like model sampling, ﬁne-tuning might also be facilitated through an
API (e.g. [128]. However, some experts express concern that current interfaces often do not provide
enough information about underlying models for them to draw meaningful conclusions from their
research.26 Interpretability research further requires that researchers can directly modify model
internals such as learned parameters and activation patterns. Full (or nearly full) model access is
needed for interpretability research. That said, current interpretability research is not limited by access
to large models because interpretability techniques are not mature enough to be “computationally
doable” in the largest models. In other words, we have a way to go before open-sourcing our most
capable models is a signiﬁcant beneﬁt to interpretability research.
Even where comprehensive model access is crucial to a research agenda, other factors can reduce the
beneﬁts of open-sourcing highly capable models. For example, safety researchers external to private
labs sometimes lack sufﬁcient computational infrastructure to run highly capable foundation models
[141]. Yet, some research agendas, such as those studying emergent capabilities, require access to the
largest models at the bleeding edge of development; smaller models that can be run on local hardware
do not reliably exhibit the emergent capabilities under investigation, even when ﬁne-tuned on the
outputs of larger models.
(3) Capability progress.
Finally, capability progress describes advancement in frontier AI research
toward developing more powerful and capable systems (i.e. working towards AGI).
The extent to which open-source contributions can drive progress on AI frontier capabilities may be
limited by access to compute and data resources, as well as the distribution of talent.
Few AI actors have the requisite ﬁnancial, compute, high-quality data and talent resources to operate
at the cutting edge of AI research and development. Training new foundation models costs $10-100
million in compute costs and is projected to increase to $1-10 billion in coming years [73]; the stock
of high-quality data used to train large language models (such as books) currently freely available
on the internet may be depleted in a few years, requiring potentially costly new sources of data,
innovations in data efﬁciency [160], or expensive human feedback data; and AI talent is most heavily
concentrated in high-paying positions at leading AI labs, primarily based in the United States, while
smaller labs struggle to ﬁll positions [161].
Open-sourcing large pre-trained models does allow less-well-resourced actors such as academic labs
and open-source developers to study and innovate on these existing models. These communities
can make technical and conceptual innovation and reﬁnements within open-source environments
that generate knowledge that can be incorporated to advance the AI capability frontier. If a high
variance of research and development strategies are employed by open-source communities, their
contributions may be particularly valuable for advancing state of the art AI.
Furthermore, open-source model sharing also facilitates talent development. More people being able
to interact with pre-trained cutting edge-models may, over time, lead to a larger and more diverse AI
talent pool for government regulators, AI labs, universities, and auditing institutions to draw from.
On a longer time scale this could have a positive effect on capability progress (and safety progress)
by increasing the talent pool.
Realistically, however, the advancements that push the capability frontier will nearly exclusively
take place at frontier labs in leading nations. In these locations, in-house expertise can draw upon
open-source innovations and top talent to run giant training runs using huge compute, data, and
engineering resources not available to the open source community. (See Section 4.3 for discussion on
distributing AI development away from big tech.)
Furthermore, the desirability of accelerating capability progress is presently hotly debated.
This is due to concerns over risks as well as beneﬁts of more advanced models, in addition to the
governance challenge of preparing appropriate regulation and oversight for such a rapidly advancing
26For example, when attempting to evaluate the effect of instruct ﬁne-tuning across multiple models, Wei et
al. [159] write: “We do not compare InstructGPT against GPT-3 models in this experiment because we cannot
determine if the only difference between these model families is instruction tuning (e.g., we do not even know if
the base models are the same).” Bucknall et al. [141] discuss this and other examples from literature and expert
interviews that elucidate the limitations many APIs pose for researchers.
23
technology [162–165]. Accordingly, “Accelerating AI capability progress”, to the extent that open-
sourcing does drive capability progress, should only be considered an open-source beneﬁt if the effect
of open-sourcing is to drive beneﬁcial progress disproportionately to increasing risk and severity of
harm.
Toward beneﬁcial AI progress, one beneﬁt of open-sourcing is that it puts AI tools in the hands of
safety researchers, e.g., in academia, who would otherwise not have access to the cutting edge models.
We expand on this point shortly under “Safety Progress”. Open-sourcing also increases opportunity
for external scrutiny.
However, open-sourcing frontier models might also drive progress in undesirable directions. One
example of this is the potential effect of open-source model sharing on the offense-defense balance;
open-sourcing may empower malicious actors to offensively identify and exploit system vulnerabil-
ities to a greater extent than it facilitates defensive activities to protect against malicious use (See
Section 3.1 for further details).
Box 2: Strategies for driving safety progress alongside model sharing
Alongside alternative model sharing strategies, there are also other activities that can be
employed to help safety progress. These are not alternatives to model sharing, but are
worthwhile considerations if accelerating safety progress is the desired outcome.
Large rewards programs
Progress might be accelerated in crucial AI safety domains by building new incentive struc-
tures, for instance, large rewards programs on the scale of millions or billions of dollars to
reward major AI safety breakthroughs (e.g., in model interpretability). The goal is to make
safety progress, like capability progress, a ﬁnancially lucrative endeavor.
Committing proﬁts to safety research
Safety progress could also be prioritized by orchestrating agreements between frontier AI
labs to commit a certain percentage of proﬁts or research hours towards AI safety projects.
This would reduce incentives for labs to cut corners on safety research and help remedy the
large mismatch in resources currently dedicated to capability progress versus safety progress
by major labs.
International institutions and collaborations for AI Safety
Finally, in the long term we may beneﬁt greatly from establishing international institutions and
collaboration to promote AI safety [166]. For instance, there is budding interest in establishing
global collaboration on advancing AI safety research akin to CERN or ITER27[166, 167].
Such a project could funnel signiﬁcant resources towards AI safety research, enable open and
secure sharing of insights between leading nations, and reduce the burden of cost (ﬁnancial
and opportunity costs) associated with dedicating signiﬁcant resources to AI safety research.
There is a risk that collaborative AI safety research would facilitate the diffusion of dual use
technologies and disincentivize leading labs from conducting their own safety research. It is
therefore imperative that any such project be coupled with efforts to involve safety researchers
from leading labs (e.g., by offering dual appointment or advisory positions) and to implement
careful membership restrictions and information security measures [166].
27The International Thermonuclear Experimental Reactor (ITER) is an international nuclear fusion research
and engineering megaproject aimed at creating energy through nuclear fusion. https://www.iter.org/
24
4.2.3
Other Ways to Drive (Beneﬁcial) Progress
There are a variety of methods that might be employed to help pursue open-source objectives. These
methods do not necessarily cover all losses from not open-sourcing, but they do not suffer the same
risks as open-sourcing and can be used in combination.
Toward integration progress, for example, new integrations and applications can be explored and
implemented through the development of “plugins” allowing a model to integrate with other services
[168]. The plugin could be submitted to the developer or a third-party auditor before publication.
This option provides a mechanism for new integrations and applications to be reviewed and approved
before being shipped while still tapping into public creativity and representation of interests and
needs.
In so far as model access allows downstream developers to more thoroughly understand and test
the performance and safety of their integrations, labs could also provide identiﬁed downstream
developers with privileged access to requested model components via gated download. One policy
recommendation is that labs are held to a “know-your-customer” requirement whereby labs must
vet and keep a record of potential model recipients (e.g., proposed use, past activities, funding source,
etc.) [53, 169]. Additionally, technical safety measures such as applying a unique ﬁngerprint to each
copy of the model’s weights should be applied when feasible [170].
As discussed above, the beneﬁt of open-sourcing for safety progress and capability progress is
dampened by limited talent and compute resources external to major labs. There are, however, other
means of driving both forward.
As mentioned in 4.1.3, developers might provide privileged model access to AI safety research
groups, possibly via structured access research APIs. While not yet fully realized, there is hope that
suitably comprehensive researcher access to closed models can also be provided through structured
access approaches [16], such as specialized researcher API access [141]. Such solutions could be
used in addition to existing social and legal mechanisms for ensuring information security, such as
researcher NDAs, thereby potentially providing more comprehensive security guarantees than either
approach could in isolation.
For the purpose of propelling capability progress, labs could also actively seek collaborations with
trusted parties and provide gated download access to collaborators. This is similar, for example,
to how OpenAI partnered with research institutions during the staged release of GPT-2, providing
access to models for carrying out research into biases and methods for detecting GPT-2-generated
text [134]. As before, any time gated download access is provided, it should be backed by know-
your-customer investigation and documentation requirements, and any applicable technical safety
measures. Selectively providing model weights to only those researchers whose work requires them
would also help reduce the risk of leaks.
There is a challenge, however, regarding the decision as to which actors are provided privileged
model access (gated downloadable or via research API) to conduct external evaluation and research or
for collaborations. Where labs are inundated with an unmanageable number of requests for research
access, favoritism and in-group model evaluations may emerge out of necessity. Labs are also likely
to prioritize external collaborators who they believe will support their market interests. One possible
solution could be to establish a multistakeholder governance body or system for mediating
researcher access to highly capable foundation models. For example, within the UK, we might
imagine the recently established Frontier AI Taskforce taking on such a role.
Such a body could also determine the degree of access provided to external researchers (if through
research API). This is important for preventing “independence by permissions” whereby academic
collaborators are able to conduct high-quality independent research, but research directions are
ultimately determined by the access permissions given by the developer [171]. For cutting-edge
models especially, researchers may not know which access permissions they need to request, and
the incentives are not clear for developers to reveal everything they know (or suspect) about their
proprietary models.
25
4.3
Distribute Control Over AI
Table 4: Section summary: Open-sourcing as a mechanism for distributing control over AI
The argument for
open-source AI
Open-sourcing foundation models will help distribute inﬂuence over the future of AI
away from major labs by empowering smaller groups and independent developers.
The idea is that open-sourcing “democratizes AI”, giving more people inﬂuence over
how AI is developed, optimized, and used, and promotes the representation of more
diverse interests and needs in the direction of the ﬁeld.
Evaluation
of Beneﬁt
• Open-sourcing helps distribute control over downstream integration progress to
open-source communities.
• The effect of open-source on distributing inﬂuence over capability and safety
progress is reduced by concentration of compute, data, and talent resources needed
to inﬂuence frontier AI capability progress in large, well-resourced labs.
• Open-sourcing large and highly capable models can also help amplify the original
developer’s inﬂuence over AI ecosystems; downstream innovations building on
open-sourced models are easily integrated back into the developers’ products, and
the open-source communities become go-to hiring pools already familiar with the
company’s tools and models.
• Open-sourcing is a tool that can aid the democratization of AI. But AI democra-
tization is a multifaceted and proactive project to distribute inﬂuence over highly
capable AI systems—how they are used, distributed, developed, and regulated—to
wider communities of stakeholders and impacted populations. Open-sourcing alone
cannot fulﬁll the goal of AI democratization.
Alternative
methods for
distributing
control over AI
• Implement participatory or representative deliberative processes to democratically
inform high-impact decisions about AI development, use, and governance, includ-
ing decisions about model access.
• Institutionalize democratic structures (e.g., via democratically selected boards or
by requiring the use of such deliberative processes for all decisions on particular
topics) within large labs to dissipate control away from unilateral decision-makers.
• Support appropriate regulatory intervention to developer behaviors and to guard
against regulatory capture.
4.3.1
The Argument for Open-Source
A commonly cited argument for open-sourcing foundation models is that doing so will help distribute
inﬂuence over the future of AI away from major labs and to the wider AI community [172, 173].
There are very good reasons for wanting to distribute inﬂuence over AI. There are economic implica-
tions; if open-sourcing foundation models enables downstream developers to independently innovate
and capitalize on a lucrative technology, this could help to ensure that the huge value AI promises to
produce does not accrue only to a handful of tech giants.
There are also social and political implications; major AI labs are unelected entities that primarily
serve their own and shareholder interests. The idea is that distributing inﬂuence over AI development
processes prevents private labs from exercising too much control over numerous aspects of public
life that emerging AI capabilities promise to transform. As Emad Mostaque explains Stability AI’s
decision to open-source Stable Diffusion, “We trust people, and we trust the community, as opposed
to having a centralized, unelected entity controlling the most powerful technology in the world” [174].
Overall, the idea is that open-sourcing “democratizes AI”, giving more people inﬂuence over how AI
is developed and used, and promoting the representation of more diverse interests and needs in the
direction of the ﬁeld.
26
4.3.2
Evaluating the Beneﬁt for Foundation Models
Historically, open-source software development has had a noteworthy inﬂuence-distributing effect.
For instance, the open-source Linux kernel now underpins numerous operatings systems (e.g., Ubuntu,
Fedora, Debian) that offer competitive and highly-utilized alternatives to Windows and MacOS. We
caution, however, that this effect should not be expected to translate perfectly to the context of
open-source foundation models.
AI democratization is a multifaceted project. Open-sourcing certainly contributes to AI democra-
tization, though for some aspects of AI democratization the effect is marginal. All aspects of AI
democratization beneﬁt from investment in other proactive activities aimed at distributing inﬂuence
over AI and AI impacts. We brieﬂy review four aspects of AI democratization originally outlined in
[175] and comment on the extent to which open-source model sharing contributes to each.
(1) Democratization of AI development
The democratization of AI development is about helping a wider range of people contribute to
AI design and development processes. Of the four forms of AI democratization, open-sourcing
promotes the democratization of development most, and most directly. Open-sourcing places models
in the hands of large communities of open-source developers who can continue to examine and
modify the model. Open-sourcing also supports self-learning and education among open-source
developers, allowing them to keep up with advances in model design and safety research and to
continue participating in AI development as techniques evolve.
There are, however, some ways in which the effect of open-source on the democratization of AI
development is limited.
First, especially with respect to highly-capable models, open-source development activities may
be increasingly limited by resource accessibility. Participating at the cutting-edge of AI research
and development requires signiﬁcant ﬁnancial, compute, talent, and high-quality data resources,
and few actors outside of major labs and government actors have these requisite resources (See
Section 4.2). As Widder et al. [64] write, “even maximalist varieties of ‘open’ AI don’t democratize
or extend access to the resources needed to build AI from scratch—during which highly signiﬁcant
‘editorial’ decisions are made.” Accordingly, toward the goals of facilitating wider and more diverse
participation in driving AI development, the beneﬁt of open-sourcing is limited.
Second, open-sourcing can help leading AI developers to further entrench their control over AI
ecosystems and value production [13, 176]. While a near term, ﬁrst-order effect is that downstream
developers gain inﬂuence over model application and integration progress, a longer-term, second-
order effect of open-sourcing large foundation models is to feed back value and inﬂuence to the
original developer. Open-sourcing grants wider AI communities access to a technology that they can
ﬁne-tune and customize to a variety of new applications. However, these downstream innovations
which build on top of the original open-sourced model architecture, are then easily integrated back
into the original developer’s own products and ecosystems. Open-source communities also become
go-to hiring pools already familiar with the company’s tools and models.
Third, the wider AI community, including open-source communities, are relatively homogenous in
terms of economic, cultural, gender, and geographic grouping [161, 177]. Open-source communities
are often better than tech companies at building diverse and inclusive spaces, and they put signiﬁcant
effort into engaging with the broader world.28 However, something is still lost conﬂating the
distribution of power to open-source communities and the distribution of power to communities
generally. There is a risk that by missing this nuance we exaggerate the beneﬁts of open-sourcing
alone and underplay the need for other mechanisms for promoting the democratization of AI. In
addition to model sharing, democratizing AI development requires the provision of educational and
upskilling opportunities and technical support infrastructure (e.g., high bandwidth network access
28For example, the open-source AI research organization EleutherAI [178] and the open-source collective
BigScience [179] have teams spanning four or more continents and have projects focusing on increasing access
to NLP technologies for people who speak non-dominant languages. Similarly, Cohere is running a program to
collect ﬁne-tuning data in hundreds of languages [180], and LAION is the only organization, at time of writing,
to be training massively multilingual CLIP models [181, 182].
27
and cloud compute services) to encourage and enable wider and more diverse participation in AI
development processes.
(2) Democratization of AI use
The democratization of AI use is about enabling a wide range of people to use and beneﬁt from AI
applications. Open-sourcing allows downstream developers to tailor models to serve diverse needs.
For most people, using an AI system also requires the provision of intuitive interfaces to facilitate
human-AI interaction without extensive training or technical knowhow. Open-source communities
can help develop these interfaces.
However, one thing to consider is that beneﬁting from the use of an AI system does not always require
that everyone be able to use the AI system. Especially for highly-capable and potentially high-risk
systems, a designated user could employ the system for the beneﬁt of the community. For example, a
drug discovery system which could be maliciously used to discover new toxins, could be used in a
controlled, limited-access setting while resulting pharmaceuticals are “democratized” in the sense
that they are made accessible to anyone in need.
(3) Democratization of AI proﬁts
The democratization of AI proﬁts is about facilitating the broad and equitable distribution of value
accrued to organizations that build and control advanced AI capabilities. Subgoals of proﬁt democra-
tization include: smoothing economic transition in case of massive growth of the AI industry, easing
ﬁnancial burden of job loss to automation, preventing a widening economic divide between AI leading
and lagging nations, and acknowledging through compensation the human labor and creativity that
goes into producing and catering the data upon which highly lucrative AI capabilities are built.
Open-sourcing helps democratize proﬁts in two ways. First, by open-sourcing their models, rather
than charging for access, companies will tend to capture less of the wealth produced by these models;
users can employ the models to generate proﬁts (e.g. through increased productivity) without having
to pay some portion back to the developer. Second, open-sourcing helps democratize proﬁts insofar
as it allows a more widespread array of downstream developers to iterate upon AI models and place
competitive pressure on large labs; open-sourcing can make it more difﬁcult for large labs to build
proﬁtable downstream applications of their models, since they will need to compete with open-source
developer communities that are building competing applications.
However, the effect of open-source on distributing proﬁts from highly-capable AI will likely be
limited in a couple respects. First, open-source community participation in the development of
cutting-edge models will be curbed by inadequate access to necessary compute and ﬁnancial resources
(Section 4.2), thus limiting the competitive pressure open-source developers can put on well-resourced
large labs. Second, as discussed earlier in this section, open-sourcing frontier systems can also be
ﬁnancially advantageous to large companies in the long run as they can use downstream developers
as a free labor source, easily feeding their best contributions and insights back into the company’s
own products.
Additional proactive measures are needed to help pursue the goals of proﬁt democratization. These
might include implementation of a proﬁt redistribution scheme such as taxation and redistribution by
the state [183, 184], lab commitments to a windfall clause whereby developers obligate themselves to
donate windfall proﬁts (measured as “a substantial fraction of the world’s total economic output”)
for redistribution [185], and mechanisms for compensating content creators for the data on which
generative AI models are trained, for instance, through the creation of licensed data sets [186, 187].
(4) Democratization of AI governance
Finally, the democratization of AI governance is about distributing inﬂuence over decisions about AI
to a wider community of stakeholders and impacted populations. AI governance decisions involve
balancing AI related risks and beneﬁts to determine how and by whom AI is used, distributed,
developed, and regulated.
Of the four forms of AI democratization, open-sourcing has the least impact on distributing inﬂuence
over AI governance decisions. Open-sourcing distributes inﬂuence over AI governance decisions away
28
from major labs insofar as it enables wider AI research and development communities to participate
in, and therefore direct, AI development processes. However, open-sourcing does little to gain
inﬂuence over AI governance decisions for the public more broadly. In this respect, democratizing
AI governance involves applying democratic processes directly to high-impact decisions made by AI
developers, subjugating labs to regulation by democratic governments, or some combination thereof.
We expand on these possibilities shortly in 4.3.3.
Overall, open-sourcing AI should not be conﬂated with democratizing AI
. Open-sourcing is
but one option for sharing models and model components; model sharing is but one mechanism for
democratizing AI development; and the democratization of AI development is but one dimension of
distributing inﬂuence and control over the future of AI. Indeed, the decision to open-source is itself a
consequential decision over which inﬂuence can and likely should be distributed away from private
labs.
4.3.3
Other Ways to Reduce Corporate or Autocratic Control
A comprehensive approach will be needed to counteract the centralisation of power in AI companies
as AI systems become more capable and therefore confer more political and economic power. This
section presents options for distributing inﬂuence over AI via the democratization of AI governance.
It is not an exhaustive list, but it illustrates that there are a wide variety of methods that can be used
to decentralize power and to better facilitate representation of diverse stakeholder interests and needs
in decisions about how and by whom AI is developed, used, distributed, and regulated.
Public participation and deliberation.
AI labs and policymakers could institute participatory
and deliberative democratic processes to guide decision-making about complex issues in AI [175].
For example, participatory platforms such as Pol.is [188] might be used to solicit and synthesize
public input into complex normative decisions about AI at low cost. Alternatively, representative
deliberations, such as citizens assemblies, can convene representative microcosms of impacted
populations (or even global populations) selected by sortition (i.e. stratiﬁed sampling) to tackle AI
governance questions [189, 190].
Such efforts by large tech companies are not unprecedented. Meta, for example, has quietly run a set
of national and transnational pilots [191, 192] to navigate their ‘complex normative challenges’ and
have since scaled up to a near-global deliberative process [193]. Twitter had also planned to pilot
such processes before its acquisition [194], and OpenAI recently has launched a “democratic inputs
to AI” grant program to experiment with setting up democratic processes for deciding what rules AI
systems should follow within legal bounds [195].
Institutional structure.
Instead of, or in addition to, directly eliciting public input to inform key
decisions, another option is for AI labs to introduce organizational structures that are more democratic
in nature. These structures would help maintain transparency of internal practices and to dissipate
control away from unilateral decision-makers in such a way that better reﬂects stakeholder interests.
Relevant stakeholders importantly include public communities whose lives are impacted by emerging
AI capabilities.
For example, AI labs can incorporate as Public Beneﬁt Corporations (PBC).29 A PBC is a for-proﬁt
corporation intended to produce public beneﬁts and to operate in a responsible and sustainable manner.
Incorporating as a PBC does not necessitate public involvement, but it does give a corporation clearer
legal standing to make decisions about institutional structure that aim to maximize public beneﬁt,
even if that might conﬂict with maximizing shareholder interests.
For more direct public control, a golden share (a nominal share which is able to outvote all other
shares) could be held by a perpetual purpose trust (a non-charitable trust established for the beneﬁt of
a purpose) governed by a committee that is a representative sample of the public selected by sortition
or elected by stakeholders.
29There is increased momentum toward this now, as two leading AI organizations, Anthropic and Inﬂection
AI, are both PBC’s.
29
Alternatively, AI labs could implement democratically selected oversight boards. Such a board
might, for instance, be composed of representatives from the public selected by sortition or, perhaps a
more palatable option, a sortition body is used to “elect” board members from among a nominated
list. “Nominators” could be members of government (e.g., state governors), and perhaps two to three
board members are committed to ‘voting’ on issues as determined by a democratic process (e.g.,
public polling or citizen assembly, whichever is appropriate to the situation).
Regulation by democratic governments.
Finally, of course, labs can encourage government
regulation that restricts their behavior and capacity for independent decision-making where the
potential for signiﬁcant societal impact is high. For example, governments could require authorization
for large foundation model release and institute multistakeholder committees to mediate research
access to highly capable models. Regulatory interventions should be developed in response to
deliberative processes involving developers, open source communities, academia, and civil society to
reﬂect diverse stakeholder interests and to guard against regulatory capture by AI industry. In this
way appropriate government regulation can help systematically reduce unilateral control over AI by
leading private labs.
5
Recommendations
We conclude this paper with ﬁve high-level recommendations for AI developers, standard setting
bodies, and governments for working towards safe and responsible model sharing decisions. These
recommendations are necessarily incomplete and preliminary because best practices for open-sourcing
highly capable models will be highly context-dependent and require input from numerous parties.
We look forward to further development of these recommendations in future work.
Table 5 summarizes the recommendations. Each recommendation is explained in more detail below.
Table 5: Recommendations for working towards responsible model-sharing
1. Developers and governments should recognise that some highly capable models will be
too risky to open-source, at least initially. These models may become safe to open-source
in the future as societal resilience to AI risk increases and improved safety mechanisms are
developed.
2. Decisions about open-sourcing highly capable foundation models should be informed
by rigorous risk assessments. In addition to evaluating models for dangerous capabilities
and immediate misuse applications, risk assessments must consider how a model might be
ﬁne-tuned or otherwise amended to facilitate misuse.
3. Developers should consider alternatives to open-source release that capture some of the
same [distributive, democratic, and societal] beneﬁts, without creating as much risk. Some
promising alternatives include gradual or “staged” model release, model access for researchers
and auditors, and democratic oversight of AI development and governance decisions.
4. Developers, standards setting bodies, and open-source communities should engage in
collaborative and multi-stakeholder efforts to deﬁne ﬁne-grained standards for when
model components should be released. These standards should be based on an understanding
of the risks posed by releasing (different combinations of) model components.
5. Governments should exercise oversight of open source AI models and enforce safety
measures when stakes are sufﬁciently high. AI developers may not voluntarily adopt risk
assessment and model sharing standards. Governments will need to enforce such measures
through options such as liability law and regulation (e.g. via licensing requirements, ﬁnes,
or penalties). Governments will also need to build the capacity to enforce such oversight
mechanisms effectively.
30
1. Developers and governments should recognise that some highly capable models will be too
dangerous to open-source, at least initially.
If models are determined to pose signiﬁcant threats, and those risks are determined to outweigh the
potential beneﬁts of open-sourcing, then those models should not be open-sourced. Such models may
include those that can materially assist development of biological and chemical weapons [50, 109],
enable successful cyberattacks against critical national infrastructure [52], or facilitate highly-effective
manipulation and persuasion [88].30
This is not to say that a given highly capable model should never be open-sourced. Expected model
impacts are likely to change with increasing societal resilience and development of new defensive
techniques. However, model developers should consider a default policy of pursuing release through
alternative methods rather than open-source if they ﬁnd that a model poses signiﬁcant threats, and
that the beneﬁts of open-sourcing do not outweigh the risks of doing so.
2. Decisions about open-sourcing highly capable foundation models should be informed by
rigorous risk assessments.
In the past, the beneﬁts of open-sourcing seem to have clearly outweighed the risks. However, we
are not conﬁdent that this will continue to be the case in the future for highly capable foundation
models (Section 3). It is therefore important to carefully assess potential risks and beneﬁts before
open-sourcing the model, especially since the decision to open-source a model is irreversible. The
need to conduct risk assessments prior to model release seems to be generally accepted [53, 196,
197].
The National Institute of Standards and Technology (NIST) provides guidance for how to conduct
such an assessment [198] which might be applied to inform open-sourcing decisions. Some scholars
have suggested ways in which the NIST AI Risk Management Framework could be adapted to
general-purpose AI systems [199] and catastrophic risks [200]. In the future, we think that developers
of highly capable foundation models will need to combine qualitative and quantitative approaches.
They may need to conduct deterministic safety assessment as well as probabilistic risk assessments,
as is common in the nuclear industry [201].
Since risks associated with certain model capabilities are particularly concerning, risk assessments
should be informed by evaluations of dangerous model capabilities [48]. Both internal [29, 202]
and external model evaluations should be conducted. External assessments can take many different
shapes, such as model evaluations [54, 140], model audits [138, 203, 204], red-teaming [144, 147], or
researcher access via API [141].
Developers intending to open-source a model that is likely to be highly capable should conduct more
involved risk assessments than they would have otherwise. Firstly, the risk assessment should be more
thorough to have the decision be as well-informed as possible, given the irreversibility of decisions to
open-source. Methods such as additional red teaming, internal testing, and staged release approaches
should be pursued.
Secondly, risk assessments ahead of open-sourcing decisions need to assess how the model can be
amended to facilitate misuse. The risk assessment must consider the ease with which safeguards
can be removed and “uncensored” versions of the model can be distributed. Often, safeguards will
be so easy to remove that it is better to avoid the model having the worrying capability altogether
(Section 3). For example, while Stable Diffusion 1.0 had a safety ﬁlter, it was easy to disable [83]. In
future releases, Stability AI therefore opted to remove inappropriate content from the training data
instead [205].
Risk assessments should also consider the extent to which risks can be exacerbated by malicious
actors ﬁne-tuning or otherwise amending the model to elicit or develop more dangerous capabilities
(Section 3). It is difﬁcult to anticipate how the model is going to be ﬁne-tuned. It is therefore crucial
that red-teamers have ﬁne-tuning access to the model ahead of release.
30Note that we do not claim that existing models are already too risky. We also do not make any predictions
about how risky the next generation of models will be. Our claim is that developers need to assess the risks and
be willing to not open-source a model if the risks outweigh the beneﬁts.
31
Thirdly, risk assessments should consider factors external to the model. The social impacts of a
model (e.g., on democratic processes) are difﬁcult to forecast and necessitate consideration of how
the model will interact with other tools and outside institutions, cultures and material conditions [39].
Finally, for red-teaming, model evaluations and other external safety assessments to be effective, AI
developers need to elicit participation from a diverse and comprehensive set of experts. Only by
harnessing a varied set of viewpoints and expertise can we ensure a broad spectrum of potential risks
are adequately identiﬁed and evaluated.
3. Developers should consider alternatives to open-source release as possibilities for working
towards distributive, democratic, and societal advancement goals with less risk.
Before open-sourcing a highly capable foundation model, developers should ﬁrst clarify goals—
reﬂecting on why speciﬁcally they want to open-source a model—and then consider alternatives that
may reach those goals at lower risk.
With respect to alternative model-sharing strategies, some options may offer some of the same beneﬁts
as open-sourcing, but unlike open-sourcing, still allow developers to adjust their deployment strategy
after release. The idea that models are either fully open or fully closed is a false dichotomy. As
discussed in Section 4, there are numerous options for gated, API, or hosted access in between which
allow for varying degrees of model probing and researchability [69, 141], and there are proposed
frameworks to help navigate these options [81, 131].
Developers could also deploy the model in stages (staged-release) and gather observational data
about how a model is likely to be (mis)used and modiﬁed if open-sourced (Section 4.1.3). Finally,
developers could employ proactive efforts to pursue desired beneﬁts, such as by implementing
democratic processes to distribute inﬂuence over development and release decisions (Section 4.3.3).
4. A collaborative and multi-stakeholder effort is needed to deﬁne ﬁne-grained standards for
when model components should be released.
Standard-setting organizations or industry bodies should develop model-sharing standards that provide
guidance relating to decisions about whether, and if so how, to open-source highly capable foundation
models. Such a standard would contribute to more consistent industry practices and could be an
important step towards regulation. There are a wide range of model-sharing options, even within the
currently ill-deﬁned category of “open-source” systems (see Box 1).
Model-sharing standards should both support safe model distribution and protect open-source prac-
tices and beneﬁts. To achieve both, these standards must be ﬁne-grained and built on a well-researched
understanding of the extent to which access to different (combinations of) model components enable
unrestricted model use, reproduction, and modiﬁcation.
We make a start at breaking down and deﬁning the numerous model components that can be indepen-
dently shared in Appendix A. It is, however, a much larger project than we can do justice to here, and
it is a project on which members of open-source communities should be centrally involved. A clear
understanding of activities enabled by access to various model components can then be used to inform
model-sharing standards that are well-tailored to their purpose, that are not overly burdensome, that
prevent distribution of dangerous capabilities, and that do not unnecessarily undermine open-source
beneﬁts.
Technical experts, open-source communities, policymakers, and civil society all need to be involved
in this process. There are several actors who could develop such standards. Although standard-setting
organizations like NIST [198] and ISO/IEC [206] have published standards for AI, they do not
seem to have engaged with questions around open-sourcing foundation models speciﬁcally. The
Partnership on AI (PAI) has a working group on foundation models [207] and they have published
similar guidelines for publishing research in the past [208]. The Open Source Initiative recently
started a working group to deﬁne what makes an AI system “open source” [82]. Another body that
could contribute industry expertise is the recently-announced Frontier Model Forum [209], however
current participants have generally not open-sourced their most advanced foundation models.
32
5. Government should exercise oversight and enforcement where stakes are sufﬁciently high.
AI developers may not voluntarily adopt the risk assessment and model sharing standards described
above, and government involvement will likely be needed. Without such involvement, developers
may not be sufﬁciently incentivised to voluntarily conduct thorough risk assessments ahead of
model release, to appropriately act on those results, to provide sufﬁcient external access to their
models, or put in place appropriate safeguards. For instance, AI developers may preferentially
choose “friendly” external assessors who share similar concerns around certain types of risk, or
whose ﬁnancial incentives undermine their ability to provide an independent assessment.
To mitigate such potentialities, governments should increase oversight capacity and set up mechanisms
for enforcing rigorous risk assessments and responsible model release in sufﬁciently high-stakes
contexts. Governments need to ensure that oversight is rigorous and independent, supported by a
diverse and comprehensive set of independent advisors, and investigates a wide range of AI risks.
Similarly, enforcement mechanisms need to guard against the risk of regulatory capture.
There are multiple options governments could consider in terms of enforcement, such as:
Liability.
Developers could be held liable for harms caused by their models that could have been
reasonably foreseen31 or avoided through an exercise of due care.32 While courts will ultimately have
to decide liability on a case-by-case basis, there are strong incentives for developers to demonstrate
due care, by, for example, conducting thorough risk assessments and model evaluations, implementing
adequate precautionary measures, refraining from or reducing high-risk activity,33 and maintaining
their ability to limit harms that occur post-release. Existing tort law already covers unjustiﬁably risky
acts and omissions, via negligence for failing to exercise due care (including to prevent foreseeable
criminal conduct by a third parties34), products liability for defective designs, and strict liability for
abnormally dangerous activities.35 A critical task will be to clarify the application of these doctrines
to open-sourcing highly capable foundation models [214]. Where the application of existing liability
regimes fails to address signiﬁcant risks, new statutory duties and liability laws may need to be
developed.
Regulation.
Governments could legally require developers of highly capable foundation models to
conduct pre-deployment risk assessments, report potentially dangerous capabilities discovered during
model evaluation, and provide model access pre-deployment to government auditors. Regulations may
also specify under which conditions models may be open-sourced [53]. They could also encourage
or mandate that signiﬁcant model deployments are preceded by notiﬁcations to relevant parts of
government [215]. Such requirements could be enforced by administrative enforcement measures,
both before model deployments (e.g., via a licensing regime) as well as after (e.g., via ﬁnes and
penalties) [53].
It is worth noting that liability and regulation each have their strengths and weaknesses. While
liability is generally less onerous and more ﬂexible, enforcing liability rules might be difﬁcult (e.g.,
because of causation and attribution problems, especially when a malicious actor intervenes) and it is
not possible to enforce liability rules ahead of model deployments. Regulation is the only way to
enforce compliance before a model is open-sourced. However, regulation typically leads to higher
compliance costs and there are risks of regulatory capture. In general, liability should be seen as a
31See [210] § 4 (Duty) and § 6 (Tortious Conduct) (1965), and § 901 on the general principle of liability
(1979); See [211] on products liability.
32See [212] on the legal concept of negligence.
33See [213, p. 61]
34See [210] §§ 302A-B (1965); Restatement (Third) of Torts: General Principles § 17 (Discussion Draft April
5, 1999) (""The conduct of a defendant can lack reasonable care insofar as it can foreseeably combine with or
bring about the improper conduct of . . . a third party.""); see, e.g., Hamilton v. Accu-Tek, 62 F. Supp. 2d 802,
825 (E.D.N.Y. 1999), 222 F. 3d 36 (2d Cir. 2000), 95 N.Y.2d 878 (N.Y. 2000) (Holding that gun manufacturers
had a duty “to take reasonable steps available . . . to reduce the possibility that [their products would] fall into
the hands of those likely to misuse them” and thus could be held legally responsible under New York negligence
law for criminal shootings resulting from failures to “minimize the risk” through their distribution and marketing
choices).
35See [210] § 520 (1977).
33
complement to, rather than a substitute for, regulation [53]. Since the right mix of policies will be
highly context-speciﬁc, we do not make any further recommendations.
Policy interventions on open-sourcing are delicate because of the obvious beneﬁts of open-sourcing
and because for-proﬁt companies might use safety concerns as an excuse to gain a competitive
advantage. These concerns should be taken seriously, and further research is needed to understand
the risks, beneﬁts, and legal feasibility of different policy options. However, policy interventions
still seem necessary because open-sourcing highly capable foundation models might essentially
democratize the ability to cause signiﬁcant harm and because the decision to open-source a model is
irreversible [216]. We think the current debate around the issue [217] is healthy and necessary to
strike the right balance between open-source risks and beneﬁts. In this paper, we have advocated for
a risk-based approach that could be summarized as “make open-source decisions with care”.
6
Conclusion
Open-sourcing offers clear advantages including enabling external oversight, accelerating progress,
and decentralizing control over a potentially transformative technology. To date, open-source practice
has provided substantial net beneﬁts for most software and AI development processes, distributing
inﬂuence over the direction of technological innovation and facilitating the development of products
well-tailored to diverse user needs.
However, as AI research progresses and capabilities improve, open-sourcing also presents a growing
potential for misuse and unintended consequences. Open-sourcing increases the risk of proliferation
of model ﬂaws downstream. With access to model weights and code, malicious actors can also more
easily bypass safety measures and modify models or ﬁne-tune models to display dangerous capabili-
ties. Some of the most worrying potentialities involve the use of highly capable foundation models to
build new biological and chemical weapons, to mount cyberattacks against critical infrastructures and
institutions, and to execute highly-effective political inﬂuence operations.
For some highly capable foundation models these risks may come to outweigh open-source beneﬁts.
In such cases, developers and regulators should acknowledge that the model should not be open-
sourced, at least initially. These models may become safe to open-source in the future as societal
resilience to AI risk increases and improved safety mechanisms are developed.
Model release decisions should therefore be responsive to comprehensive risk assessments and a
ﬁne-grained understanding of what activities are enabled by freely sharing different combinations
of model components. These decisions should also take into account how alternative model sharing
options (e.g. staged release, gated access, and research API) might further some of the same goals as
open-sourcing. Alternative proactive measures to organize secure collaborations, and to encourage
and enable wider involvement in AI development, evaluation, and governance processes might also
be employed. Open-sourcing is but one option for sharing models, and model sharing is but one
mechanism for facilitating wider community contributions to AI evaluation, development, and control.
Overall, openness, transparency, accessibility, and wider community input are key to facilitating
a future for beneﬁcial AI. The goal of this paper is therefore not to argue that foundation model
development should be kept behind closed doors. Model sharing, including open-sourcing, remains
a valuable practice in most cases. Rather, we submit that decisions to open-source increasingly
capable models must be considered with great care. Comprehensive risk assessments and careful
consideration of alternative methods for pursuing open-source objectives are minimum ﬁrst steps.
34
References
[1]
The Collective Intelligence Project. Introducing the Collective Intelligence Project Solving
the Transformative Technology Trilemma through Governance R&D. 2023. URL: https:
//cip.org/whitepaper (visited on September 23, 2023).
[2]
J. Hoffmann et al. Training Compute-Optimal Large Language Models, March 29, 2022. DOI:
10.48550/arXiv.2203.15556. arXiv: 2203.15556 [cs].
[3]
OpenAI. GPT-4 is OpenAI’s most advanced system, producing safer and more useful re-
sponses. URL: https://openai.com/gpt-4 (visited on September 23, 2023).
[4]
Anthropic. Claude 2. Anthropic. July 11, 2023. URL: https://www.anthropic.com/
index/claude-2 (visited on September 24, 2023).
[5]
G. Brockman, A. Eleti, E. Georges, J. Jang, L. Kilpatrick, R. Lim, L. Miller, and M. Pokrass.
Introducing ChatGPT and Whisper APIs. March 1, 2023. URL: https://openai.com/
blog/introducing-chatgpt-and-whisper-apis (visited on September 24, 2023).
[6]
S. Goldman. Hugging Face, GitHub and more unite to defend open source in EU AI legislation.
VentureBeat. July 26, 2023. URL: https://venturebeat.com/ai/hugging- face-
github-and-more-unite-to-defend-open-source-in-eu-ai-legislation/
(visited on September 24, 2023).
[7]
Creative Commons, Eleuther.ai, GitHub, Hugging Face, LAION, and Open Future. Support-
ing Open Source and Open Science in the EU AI Act, 2023. URL: https://huggingface.
co/blog/assets/eu_ai_act_oss/supporting_OS_in_the_AIAct.pdf.
[8]
M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas.
Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture,
April 13, 2023. DOI: 10.48550/arXiv.2301.08243. arXiv: 2301.08243 [cs, eess].
[9]
Meta AI. Introducing Llama 2: The next generation of our open source large language model.
Meta AI. 2023. URL: https://ai.meta.com/llama-project (visited on September 24,
2023).
[10]
S. Inskeep and O. Hampton. Meta leans on ’wisdom of crowds’ in AI model release, July 19,
2023. URL: https://www.npr.org/2023/07/19/1188543421/metas-nick-clegg-
on- the- companys- decision- to- offer- ai- tech- as- open- source- softwa
(visited on September 24, 2023).
[11]
D. Milmo. Nick Clegg defends release of open-source AI model by Meta. The Guardian.
Technology, July 19, 2023. URL: https://www.theguardian.com/technology/2023/
jul/19/nick-clegg-defends-release-open-source-ai-model-meta-facebook.
[12]
M. Langenkamp and D. N. Yue. How Open Source Machine Learning Software Shapes AI.
In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. AIES ’22:
AAAI/ACM Conference on AI, Ethics, and Society, pages 385–395, Oxford United Kingdom.
ACM, July 26, 2022. ISBN: 978-1-4503-9247-1. DOI: 10.1145/3514094.3534167. (Visited
on September 24, 2023).
[13]
A. Engler. How Open-Source Software Shapes AI Policy. AI Governance Report, Brookings,
August 10, 2021. URL: https://www.brookings.edu/articles/how-open-source-
software-shapes-ai-policy/ (visited on September 24, 2023).
[14]
A. Engler. The EU’s attempt to regulate open-source AI is counterproductive. Brookings.
August 24, 2022. URL: https://www.brookings.edu/articles/the-eus-attempt-
to-regulate-open-source-ai-is-counterproductive/ (visited on September 24,
2023).
[15]
R. Zwetsloot and A. Dafoe. Thinking About Risks From AI: Accidents, Misuse and Struc-
ture. Default. February 11, 2019. URL: https://www.lawfaremedia.org/article/
thinking-about-risks-ai-accidents-misuse-and-structure (visited on Septem-
ber 24, 2023).
[16]
T. Shevlane. Structured access: an emerging paradigm for safe AI deployment, April 11,
2022. DOI: 10.48550/arXiv.2201.05159. arXiv: 2201.05159 [cs].
[17]
R. Bommasani et al. On the Opportunities and Risks of Foundation Models, July 12, 2022.
DOI: 10.48550/arXiv.2108.07258. arXiv: 2108.07258 [cs].
35
[18]
E. Jones. Explainer: What Is a Foundation Model?, Ada Lovelace Institute, July 17, 2023.
URL: https://www.adalovelaceinstitute.org/resource/foundation-models-
explainer/ (visited on September 24, 2023).
[19]
Y.-F. Shea, C. M. Y. Lee, W. C. T. Ip, D. W. A. Luk, and S. S. W. Wong. Use of GPT-4 to
Analyze Medical Records of Patients With Extensive Investigations and Delayed Diagnosis.
JAMA Network Open, 6(8):e2325000, August 14, 2023. ISSN: 2574-3805. DOI: 10.1001/
jamanetworkopen.2023.25000.
[20]
OpenAI. Be My Eyes: Be My Eyes uses GPT-4 to transform visual accessibility. March 14,
2023. URL: https://openai.com/customer- stories/be- my- eyes (visited on
September 24, 2023).
[21]
OpenAI. Viable: Viable uses GPT-4 to analyze qualitative data at a revolutionary scale with
unparalleled accuracy. July 7, 2023. URL: https://openai.com/customer-stories/
viable (visited on September 24, 2023).
[22]
OpenAI. Inworld AI: Using GPT-3 to create the next generation of AI-powered characters.
January 1, 2023. URL: https://openai.com/customer-stories/inworld-ai (visited
on September 24, 2023).
[23]
Y. Altmann. GPT-4 Chatbot for Customer Service | The New ChatGPT Beta Chatbot in
Test. OMQ Blog. March 27, 2023. URL: https://omq.ai/blog/gpt-4-chatbot-in-
customer-service-beta-chatbot/ (visited on September 24, 2023).
[24]
B. Marr. The Amazing Ways Duolingo Is Using AI And GPT-4. Forbes. April 28, 2023. URL:
https://www.forbes.com/sites/bernardmarr/2023/04/28/the-amazing-ways-
duolingo-is-using-ai-and-gpt-4/ (visited on September 24, 2023).
[25]
OpenAI. Stripe: Stripe leverages GPT-4 to streamline user experience and combat fraud.
March 14, 2023. URL: https://openai.com/customer-stories/stripe (visited on
September 24, 2023).
[26]
Harvey.ai. Harvey: Unprecedented legal AI. URL: https://www.harvey.ai/ (visited on
September 24, 2023).
[27]
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-Resolution Image
Synthesis with Latent Diffusion Models, April 13, 2022. DOI: 10.48550/arXiv.2112.
10752. arXiv: 2112.10752 [cs].
[28]
A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical Text-Conditional
Image Generation with CLIP Latents, April 12, 2022. DOI: 10.48550/arXiv.2204.06125.
arXiv: 2204.06125 [cs].
[29]
OpenAI. GPT-4 Technical Report, March 27, 2023. DOI: 10.48550/arXiv.2303.08774.
arXiv: 2303.08774 [cs].
[30]
Y. Mehdi and J. Spataro. Furthering our AI ambitions – Announcing Bing Chat Enterprise
and Microsoft 365 Copilot pricing. Ofﬁcial Microsoft Blog. July 18, 2023. URL: https:
//blogs.microsoft.com/blog/2023/07/18/furthering-our-ai-ambitions-
announcing- bing- chat- enterprise- and- microsoft- 365- copilot- pricing/
(visited on September 24, 2023).
[31]
J. Vincent. Meta’s powerful AI language model has leaked online — what happens now?
- The Verge. The Verge. March 8, 2023. URL: https://www.theverge.com/2023/3/
8/23629362/meta-ai-language-model-llama-leak-online-misuse (visited on
September 24, 2023).
[32]
J. Fries, E. Steinberg, S. Fleming, M. Wornow, Y. Xu, K. Morse, D. Dash, and N. Shah. How
Foundation Models Can Advance AI in Healthcare. Stanford HAI. December 15, 2022. URL:
https://hai.stanford.edu/news/how-foundation-models-can-advance-ai-
healthcare (visited on September 24, 2023).
[33]
B. Marr. Digital Twins, Generative AI, And The Metaverse. Forbes. May 23, 2023. URL:
https://www.forbes.com/sites/bernardmarr/2023/05/23/digital-twins-
generative-ai-and-the-metaverse/ (visited on September 24, 2023).
[34]
D. Milmo. Paedophiles using open source AI to create child sexual abuse content, says
watchdog. The Guardian. Society, September 13, 2023. URL: https://www.theguardian.
com/society/2023/sep/12/paedophiles-using-open-source-ai-to-create-
child-sexual-abuse-content-says-watchdog.
36
[35]
E. Horvitz. On the Horizon: Interactive and Compositional Deepfakes. In ICMI ’22: Pro-
ceedings of the 2022 International Conference on Multimodal Interaction, pages 653–661,
Bengaluru India. ACM, November 7, 2022. ISBN: 978-1-4503-9390-4. DOI: 10.1145/
3536221.3558175. (Visited on September 24, 2023).
[36]
P. Verma. They thought loved ones were calling for help. It was an AI scam. Washington
Post, March 10, 2023. URL: https://www.washingtonpost.com/technology/2023/
03/05/ai-voice-scam/.
[37]
T. Brewster. Fraudsters Cloned Company Director’s Voice In $35 Million Heist, Police Find.
Forbes. October 14, 2021. URL: https://www.forbes.com/sites/thomasbrewster/
2021 / 10 / 14 / huge - bank - fraud - uses - deep - fake - voice - tech - to - steal -
millions/ (visited on September 24, 2023).
[38]
L. Weidinger et al. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference
on Fairness, Accountability, and Transparency. FAccT ’22: 2022 ACM Conference on
Fairness, Accountability, and Transparency, pages 214–229, Seoul Republic of Korea. ACM,
June 21, 2022. ISBN: 978-1-4503-9352-2. DOI: 10.1145/3531146.3533088. (Visited on
September 24, 2023).
[39]
I. Solaiman et al. Evaluating the Social Impact of Generative AI Systems in Systems and
Society, June 12, 2023. DOI: 10.48550/arXiv.2306.05949. arXiv: 2306.05949 [cs].
[40]
R. Shelby et al. Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for
Harm Reduction, July 18, 2023. DOI: 10.48550/arXiv.2210.05791. arXiv: 2210.05791
[cs].
[41]
K. Crawford. Atlas of AI: Power, Politics, and the Planetary Costs of Artiﬁcial Intelligence.
Yale University Press, New Haven London, 2021. 327 pages. ISBN: 978-0-300-26463-0.
[42]
M. L. Gray and S. Suri. Ghost Work: How to Stop Silicon Valley from Building a New Global
Underclass. Houghton Mifﬂin Harcourt, Boston, 2019. 1 page. ISBN: 978-1-328-56628-7.
[43]
P. Li, J. Yang, M. A. Islam, and S. Ren. Making AI Less ""Thirsty"": Uncovering and Addressing
the Secret Water Footprint of AI Models, April 6, 2023. DOI: 10.48550/arXiv.2304.
03271. arXiv: 2304.03271 [cs].
[44]
E. Strubell, A. Ganesh, and A. McCallum. Energy and Policy Considerations for Deep
Learning in NLP, June 5, 2019. DOI: 10.48550/arXiv.1906.02243. arXiv: 1906.02243
[cs].
[45]
D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier,
and J. Dean. Carbon Emissions and Large Neural Network Training, April 23, 2021. DOI:
10.48550/arXiv.2104.10350. arXiv: 2104.10350 [cs].
[46]
P. Liang et al. Holistic Evaluation of Language Models, November 16, 2022. DOI: 10.48550/
arXiv.2211.09110. arXiv: 2211.09110 [cs].
[47]
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
Massive Multitask Language Understanding, January 12, 2021. DOI: 10.48550/arXiv.
2009.03300. arXiv: 2009.03300 [cs].
[48]
T. Shevlane et al. Model evaluation for extreme risks, May 24, 2023. DOI: 10.48550/arXiv.
2305.15324. arXiv: 2305.15324 [cs].
[49]
Anthropic. Anthropic’s Responsible Scaling Policy, Version 1.0, Anthropic, September 19,
2023. URL: https : / / www . anthropic . com / index / anthropics - responsible -
scaling-policy (visited on September 24, 2023).
[50]
J. B. Sandbrink. Artiﬁcial intelligence and biological misuse: Differentiating risks of language
models and biological design tools, August 12, 2023. DOI: 10.48550/arXiv.2306.13952.
arXiv: 2306.13952 [cs].
[51]
Y. Mirsky et al. The Threat of Offensive AI to Organizations, June 29, 2021. DOI: 10.48550/
arXiv.2106.15764. arXiv: 2106.15764 [cs].
[52]
Center for Security and Emerging Technology and B. Buchanan. A National Security Re-
search Agenda for Cybersecurity and Artiﬁcial Intelligence, Center for Security and Emerging
Technology, May 2020. DOI: 10.51593/2020CA001. (Visited on September 24, 2023).
[53]
M. Anderljung et al. Frontier AI Regulation: Managing Emerging Risks to Public Safety,
September 4, 2023. DOI: 10.48550/arXiv.2307.03718. arXiv: 2307.03718 [cs].
37
[54]
M. Kinniment et al. Evaluating Language-Model Agents on Realistic Autonomous Tasks,
Alignment Research Center, July 2023. URL: https : / / evals . alignment . org /
Evaluating_LMAs_Realistic_Tasks.pdf.
[55]
T. Shevlane and A. Dafoe. The Offense-Defense Balance of Scientiﬁc Knowledge: Does
Publishing AI Research Reduce Misuse?, January 9, 2020. DOI: 10.48550/arXiv.2001.
00463. arXiv: 2001.00463 [cs].
[56]
Anthropic. Frontier Threats Red Teaming for AI Safety. Anthropic. July 26, 2023. URL:
https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-
safety (visited on September 24, 2023).
[57]
J. Wei et al. Emergent Abilities of Large Language Models, October 26, 2022. DOI: 10.
48550/arXiv.2206.07682. arXiv: 2206.07682 [cs].
[58]
F. Urbina, F. Lentzos, C. Invernizzi, and S. Ekins. Dual use of artiﬁcial-intelligence-powered
drug discovery. Nature Machine Intelligence, 4(3):189–191, March 7, 2022. ISSN: 2522-5839.
DOI: 10.1038/s42256-022-00465-9.
[59]
HELENA. Biosecurity in the Age of AI. 2023. URL: https://www.helenabiosecurity.
org (visited on September 24, 2023).
[60]
C. DiBona, S. Ockman, and M. Stone, editors. Open Sources: Voices from the Open Source
Revolution. O’Reilly, Beijing ; Sebastopol, CA, 1st ed edition, 1999. 272 pages. ISBN:
978-1-56592-582-3.
[61]
Github. Licenses. URL: https://choosealicense.com/licenses/ (visited on Septem-
ber 24, 2023).
[62]
A. Fanelli. LLaMA2 isn’t ""Open Source""—and why it doesn’t matter. Alessio Fanelli’s blog.
July 19, 2023. URL: https://www.alessiofanelli.com/blog/llama2-isnt-open-
source (visited on September 24, 2023).
[63]
S. Maffulli. Meta’s LLaMa 2 license is not Open Source. Voices of Open Source. July 20,
2023. URL: https://blog.opensource.org/metas-llama-2-license-is-not-
open-source/ (visited on September 24, 2023).
[64]
D. Gray Widder, S. West, and M. Whittaker. Open (For Business): Big Tech, Concentrated
Power, and the Political Economy of Open AI. SSRN Electronic Journal, 2023. ISSN: 1556-
5068. DOI: 10.2139/ssrn.4543807.
[65]
K. Finley. How to Spot Openwashing. ReadWrite. February 3, 2011. URL: https://
readwrite.com/how_to_spot_openwashing/ (visited on September 24, 2023).
[66]
Responsible AI Licenses. Responsible AI Licenses. URL: https://www.licenses.ai
(visited on September 24, 2023).
[67]
D. G. Widder, D. Nafus, L. Dabbish, and J. Herbsleb. Limits and Possibilities for “Ethical AI”
in Open Source: A Study of Deepfakes. In 2022 ACM Conference on Fairness, Accountability,
and Transparency. FAccT ’22: 2022 ACM Conference on Fairness, Accountability, and
Transparency, pages 2035–2046, Seoul Republic of Korea. ACM, June 21, 2022. ISBN:
978-1-4503-9352-2. DOI: 10.1145/3531146.3533779. (Visited on September 24, 2023).
[68]
Sijbrandij. AI weights are not open ""source"". June 27, 2023.
URL: https : / /
opencoreventures.com/blog/2023-06-27-ai-weights-are-not-open-source/
(visited on September 24, 2023).
[69]
I. Solaiman. The Gradient of Generative AI Release: Methods and Considerations, February 5,
2023. DOI: 10.48550/arXiv.2302.04844. arXiv: 2302.04844 [cs].
[70]
B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language
Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[71]
Stability AI. Stable Diffusion Public Release. stability.ai. URL: https://stability.ai/
blog/stable-diffusion-public-release (visited on September 24, 2023).
[72]
Meta AI. Introducing LLaMA: A foundational, 65-billion-parameter language model. Febru-
ary 24, 2023. URL: https://ai.meta.com/blog/large-language-model-llama-
meta-ai/ (visited on September 24, 2023).
[73]
B. Cottier. Trends in the dollar training cost of machine learning systems. EPOCH. January 31,
2023. URL: https://epochai.org/blog/trends-in-the-dollar-training-cost-
of-machine-learning-systems (visited on September 24, 2023).
38
[74]
C. Li. OpenAI’s GPT-3 Language Model: A Technical Overview. Lambda. June 3, 2020.
URL: https://lambdalabs.com/blog/demystifying-gpt-3 (visited on September 24,
2023).
[75]
A. Venigalla and L. Linden. Mosaic LLMs (Part 2): GPT-3 quality for < $500k. Mosaic ML.
September 29, 2022. URL: https://www.mosaicml.com/blog/gpt-3-quality-for-
500k (visited on September 24, 2023).
[76]
J. Sevilla, L. Heim, A. Ho, T. Besiroglu, M. Hobbhahn, and P. Villalobos. Compute Trends
Across Three Eras of Machine Learning, March 9, 2022. DOI: 10.48550/arXiv.2202.
05924. arXiv: 2202.05924 [cs].
[77]
E. Erdil and T. Besiroglu. Algorithmic progress in computer vision, August 24, 2023. DOI:
10.48550/arXiv.2212.05153. arXiv: 2212.05153 [cs].
[78]
C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee,
and T. Pﬁster. Distilling Step-by-Step! Outperforming Larger Language Models with Less
Training Data and Smaller Model Sizes, July 5, 2023. DOI: 10.48550/arXiv.2305.02301.
arXiv: 2305.02301 [cs].
[79]
S. Goldman. RedPajama replicates LLaMA dataset to build open source, state-of-the-art
LLMs. VentureBeat. April 18, 2023. URL: https://venturebeat.com/ai/redpajama-
replicates-llama-to-build-open-source-state-of-the-art-llms/ (visited on
September 25, 2023).
[80]
G. Sastry. Beyond “Release” vs. “Not Release”. Center for Research on Foundation Models.
2021. URL: https://crfm.stanford.edu/commentary/2021/10/18/sastry.html
(visited on September 24, 2023).
[81]
P. Liang, R. Bommasani, K. A. Creel, and R. Reich. The time is now to develop community
norms for the release of foundation models. Center for Research on Foundation Models. 2022.
URL: https://crfm.stanford.edu/2022/05/17/community-norms.html.
[82]
S. Maffulli. Towards a deﬁnition of ""Open Artiﬁcial Intelligence"": First meeting recap. Voices
of Open Source. July 13, 2023. URL: https://blog.opensource.org/towards-a-
definition-of-open-artificial-intelligence-first-meeting-recap/ (visited
on September 25, 2023).
[83]
J. Rando, D. Paleka, D. Lindner, L. Heim, and F. Tramèr. Red-Teaming the Stable Diffusion
Safety Filter, November 10, 2022. DOI: 10.48550/arXiv.2210.04610. arXiv: 2210.
04610 [cs].
[84]
A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and Transferable Adversarial
Attacks on Aligned Language Models, July 27, 2023. DOI: 10.48550/arXiv.2307.15043.
arXiv: 2307.15043 [cs].
[85]
M. Anderljung and J. Hazell. Protecting Society from AI Misuse: When are Restrictions
on Capabilities Warranted?, March 29, 2023. DOI: 10.48550/arXiv.2303.09377. arXiv:
2303.09377 [cs].
[86]
M. Brundage et al. The Malicious Use of Artiﬁcial Intelligence: Forecasting, Prevention, and
Mitigation, February 20, 2018. DOI: 10.48550/arXiv.1802.07228. arXiv: 1802.07228
[cs].
[87]
L. Weidinger et al. Ethical and social risks of harm from Language Models, December 8,
2021. DOI: 10.48550/arXiv.2112.04359. arXiv: 2112.04359 [cs].
[88]
J. A. Goldstein, G. Sastry, M. Musser, R. DiResta, M. Gentzel, and K. Sedova. Generative
Language Models and Automated Inﬂuence Operations: Emerging Threats and Potential
Mitigations, January 10, 2023. DOI: 10.48550/arXiv.2301.04246. arXiv: 2301.04246
[cs].
[89]
M. J. Banias. Inside CounterCloud: A Fully Autonomous AI Disinformation System. The
Debrief. August 16, 2023. URL: https : / / thedebrief . org / countercloud - ai -
disinformation/ (visited on September 25, 2023).
[90]
H. Bajohr. Whoever Controls Language Models Controls Politics. April 8, 2023. URL:
https://hannesbajohr.de/en/2023/04/08/whoever- controls- language-
models-controls-politics/ (visited on September 25, 2023).
39
[91]
D. Almeida, K. Shmarko, and E. Lomas. The ethics of facial recognition technologies,
surveillance, and accountability in an age of artiﬁcial intelligence: a comparative analysis of
US, EU, and UK regulatory frameworks. AI and Ethics, 2(3):377–387, August 2022. ISSN:
2730-5953, 2730-5961. DOI: 10.1007/s43681-021-00077-w.
[92]
A. Kaklauskas, A. Abraham, I. Ubarte, R. Kliukas, V. Luksaite, A. Binkyte-Veliene, I.
Vetloviene, and L. Kaklauskiene. A Review of AI Cloud and Edge Sensors, Methods, and
Applications for the Recognition of Emotional, Affective and Physiological States. Sensors,
22(20):7824, October 14, 2022. ISSN: 1424-8220. DOI: 10.3390/s22207824.
[93]
A. Ferguson. Policing predictive policing. Washington University Law Review, 94(5):1109–
1189, January 2017.
[94]
X. Xu. To Repress or to Co-opt? Authoritarian Control in the Age of Digital Surveillance.
American Journal of Political Science, 65(2):309–325, April 2021. ISSN: 0092-5853, 1540-
5907. DOI: 10.1111/ajps.12514.
[95]
A. Kendall-Taylor, E. Frantz, and J. Wright. The Digital Dictators. Foreign Affairs, 99(2),
February 6, 2020. ISSN: 0015-7120. URL: https : / / www . foreignaffairs . com /
articles/china/2020-02-06/digital-dictators.
[96]
K. Crawford et al. AI Now 2019 Report, AI Now Institute, New York, 2019. URL: https:
//ainowinstitute.org/publication/ai-now-2019-report-2.
[97]
S. Feldstein. The Global Expansion of AI Surveillance. Working Paper, Carnegie Endowment
for International Peace, 2019. URL: https://carnegieendowment.org/2019/09/17/
global-expansion-of-ai-surveillance-pub-79847.
[98]
A. Gupta. The evolution of fraud: Ethical implications in the age of large-scale data breaches
and widespread artiﬁcial intelligence solutions deployment. International Telecommunication
Union Journal, 1, February 2, 2018. URL: http://handle.itu.int/11.1002/pub/
812a022b-en.
[99]
J. Hazell. Large Language Models Can Be Used To Effectively Scale Spear Phishing Cam-
paigns, May 12, 2023. DOI: 10.48550/arXiv.2305.06972. arXiv: 2305.06972 [cs].
[100]
D. Kelley. WormGPT - The Generative AI Tool Cybercriminals Are Using to Launch BEC
Attacks. SlashNext. July 13, 2023. URL: https://slashnext.com/blog/wormgpt-the-
generative-ai-tool-cybercriminals-are-using-to-launch-business-email-
compromise-attacks/ (visited on September 25, 2023).
[101]
E. Horvitz. Artiﬁcial Intelligence and Cybersecurity: Rising Challenges and Promising
Directions. In Hearing on Artiﬁcial Intelligence Applications to Operations in Cyberspace,
117th Congress, May 3, 2022. URL: https://aka.ms/AAhee56.
[102]
E. Shimony and O. Tsarfati. Chatting Our Way Into Creating a Polymorphic Malware.
CyberArk. January 17, 23. URL: https://www.cyberark.com/resources/threat-
research-blog/chatting-our-way-into-creating-a-polymorphic-malware
(visited on September 25, 2023).
[103]
L. Fritsch, A. Jaber, and A. Yazidi. An Overview of Artiﬁcial Intelligence Used in Malware.
In E. Zouganeli, A. Yazidi, G. Mello, and P. Lind, editors, Nordic Artiﬁcial Intelligence
Research and Development. Volume 1650, pages 41–51. Springer International Publishing,
Cham, 2022. DOI: 10.1007/978-3-031-17030-0_4. (Visited on September 25, 2023).
[104]
M. P. Stoecklin, J. Jang, and D. Kirat. DeepLocker: How AI Can Power a Stealthy
New Breed of Malware. Security Intelligence. August 8, 2018. URL: https : / /
securityintelligence.com/deeplocker-how-ai-can-power-a-stealthy-new-
breed-of-malware/ (visited on September 25, 2023).
[105]
J. Li, L. Zhou, H. Li, L. Yan, and H. Zhu. Dynamic Trafﬁc Feature Camouﬂaging via
Generative Adversarial Networks. In 2019 IEEE Conference on Communications and Network
Security (CNS). 2019 IEEE Conference on Communications and Network Security (CNS),
pages 268–276, Washington DC, DC, USA. IEEE, June 2019. ISBN: 978-1-5386-7117-7.
DOI: 10.1109/CNS.2019.8802772. (Visited on September 25, 2023).
40
[106]
L. A. Garcia, F. Brasser, M. H. Cintuglu, A.-R. Sadeghi, O. Mohammed, and S. A. Zonouz.
Hey, My Malware Knows Physics! Attacking PLCs with Physical Model Aware Rootkit.
In Proceedings 2017 Network and Distributed System Security Symposium. Network and
Distributed System Security Symposium, San Diego, CA. Internet Society, 2017. ISBN:
978-1-891562-46-4. DOI: 10.14722/ndss.2017.23313. (Visited on September 25, 2023).
[107]
D. A. Boiko, R. MacKnight, and G. Gomes. Emergent autonomous scientiﬁc research
capabilities of large language models, April 11, 2023. DOI: 10.48550/arXiv.2304.05332.
arXiv: 2304.05332 [physics].
[108]
A. M. Bran, S. Cox, A. D. White, and P. Schwaller. ChemCrow: Augmenting large-language
models with chemistry tools, June 21, 2023. DOI: 10.48550/arXiv.2304.05376. arXiv:
2304.05376 [physics, stat].
[109]
E. H. Soice, R. Rocha, K. Cordova, M. Specter, and K. M. Esvelt. Can large language models
democratize access to dual-use biotechnology?, June 6, 2023. DOI: 10.48550/arXiv.2306.
03809. arXiv: 2306.03809 [cs].
[110]
OpenAI. GPT-4 System Card. March 23, 2023. URL: https://cdn.openai.com/papers/
gpt-4-system-card.pdf.
[111]
D. V. Gerrit. AI leaders warn Congress that AI could be used to create bioweapons. Washing-
ton Post, July 25, 2023. URL: https://www.washingtonpost.com/technology/2023/
07/25/ai-bengio-anthropic-senate-hearing/.
[112]
E. J. Markey [D-MA]. Text - S.2399 - 118th Congress (2023-2024): Artiﬁcial Intelligence
and Biosecurity Risk Assessment Act, July 19, 2023. URL: https://www.congress.gov/
bill/118th-congress/senate-bill/2399/text (visited on September 25, 2023).
[113]
N. Maslej et al. Chapter 5: Education. In The AI Index 2023 Annual Report. Institute
for Human-Centered AI, Stanford University, Stanford, CA, April 2023. URL: https :
/ / aiindex . stanford . edu / wp - content / uploads / 2023 / 04 / HAI _ AI - Index -
Report-2023_CHAPTER_5.pdf.
[114]
H. Touvron et al. Llama 2: Open Foundation and Fine-Tuned Chat Models, July 19, 2023.
DOI: 10.48550/arXiv.2307.09288. arXiv: 2307.09288 [cs].
[115]
RunPod. GPU Instance Pricing. 2023. URL: https://www.runpod.io/gpu-instance/
pricing (visited on September 25, 2023).
[116]
Aman. Why GPT-3.5 is (mostly) cheaper than Llama 2. Cursor. July 20, 2023. URL: https:
//www.cursor.so/blog/llama-inference (visited on September 25, 2023).
[117]
M. AI. I-JEPA: The ﬁrst AI model based on Yann LeCun’s vision for more human-like AI.
Meta AI. June 13, 2023. URL: https://ai.meta.com/blog/yann-lecun-ai-model-
i-jepa/ (visited on September 25, 2023).
[118]
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA:
Low-Rank Adaptation of Large Language Models, October 16, 2021. DOI: 10.48550/
arXiv.2106.09685. arXiv: 2106.09685 [cs].
[119]
M. Hobbhahn. Trends in GPU price-performance. EPOCH. June 27, 2022. URL: https:
//epochai.org/blog/trends-in-gpu-price-performance (visited on September 25,
2023).
[120]
R. Zellers. Why We Released Grover. The Gradient. July 15, 2019. URL: https : / /
thegradient.pub/why-we-released-grover/ (visited on September 25, 2023).
[121]
R. Jervis. Cooperation under the Security Dilemma. World Politics, 30(2):167–214, January
1978. DOI: 10.2307/2009958.
[122]
B. Garﬁnkel and A. Dafoe. How does the offense-defense balance scale? Journal of Strategic
Studies, 42(6):736–763, September 19, 2019. DOI: 10.1080/01402390.2019.1631810.
[123]
E. Ferrara. Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language
Models, April 18, 2023. DOI: 10.48550/arXiv.2304.03738. arXiv: 2304.03738 [cs].
41
[124]
M. Kassab, J. DeFranco, and P. Laplante. Investigating Bugs in AI-Infused Systems: Analysis
and Proposed Taxonomy. In 2022 IEEE International Symposium on Software Reliability En-
gineering Workshops (ISSREW). 2022 IEEE International Symposium on Software Reliability
Engineering Workshops (ISSREW), pages 365–370, Charlotte, NC, USA. IEEE, October
2022. ISBN: 978-1-66547-679-9. DOI: 10.1109/ISSREW55968.2022.00094. (Visited on
September 25, 2023).
[125]
K. Wiggers. What is Auto-GPT and why does it matter? | TechCrunch. TechCrunch. April 22,
2023. URL: https://techcrunch.com/2023/04/22/what-is-auto-gpt-and-why-
does-it-matter/?guccounter=1 (visited on September 25, 2023).
[126]
Auto-GPT. Home. The Ofﬁcial Auto-GPT Website. 2023. URL: https://news.agpt.co/
(visited on September 25, 2023).
[127]
E. Bagdasaryan, T.-Y. Hsieh, B. Nassi, and V. Shmatikov. (Ab)using Images and Sounds for
Indirect Instruction Injection in Multi-Modal LLMs, July 24, 2023. DOI: 10.48550/arXiv.
2307.10490. arXiv: 2307.10490 [cs].
[128]
OpenAI. Welcome to the OpenAI platform. URL: https://platform.openai.com (visited
on September 25, 2023).
[129]
S. E. Ponta, H. Plate, and A. Sabetta. Detection, assessment and mitigation of vulnerabilities
in open source dependencies. Empirical Software Engineering, 25(5):3175–3215, September
2020. ISSN: 1382-3256, 1573-7616. DOI: 10.1007/s10664-020-09830-x.
[130]
Synopsys Editorial Team. 2023 OSSRA: A deep dive into open source trends. Synopsys.
February 21, 2023. URL: https://www.synopsys.com/blogs/software-security/
open-source-trends-ossra-report.html (visited on September 25, 2023).
[131]
J. Whittlestone and A. Ovadya. The tension between openness and prudence in AI research,
January 13, 2020. DOI: 10.48550/arXiv.1910.01170. arXiv: 1910.01170 [cs].
[132]
Bugcrowd. OpenAI. URL: https://bugcrowd.com/openai (visited on September 25,
2023).
[133]
S. R. Bowman. Eight Things to Know about Large Language Models, April 2, 2023. DOI:
10.48550/arXiv.2304.00612. arXiv: 2304.00612 [cs].
[134]
I. Solaiman et al. Release Strategies and the Social Impacts of Language Models, Novem-
ber 12, 2019. DOI: 10.48550/arXiv.1908.09203. arXiv: 1908.09203 [cs].
[135]
T. Shevlane. The Artefacts of Intelligence: Governing Scientists’ Contribution to AI Prolif-
eration. PhD thesis, University of Oxford, April 22, 2022. 278 pages. URL: https://cdn.
governance.ai/Shevlane,_Artefacts_of_Intelligence.pdf.
[136]
M. Brundage et al. Toward Trustworthy AI Development: Mechanisms for Supporting
Veriﬁable Claims, April 20, 2020. DOI: 10.48550/arXiv.2004.07213. arXiv: 2004.
07213 [cs].
[137]
I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud,
D. Theron, and P. Barnes. Closing the AI Accountability Gap: Deﬁning an End-to-End
Framework for Internal Algorithmic Auditing, January 3, 2020. DOI: 10.48550/arXiv.
2001.00973. arXiv: 2001.00973 [cs].
[138]
J. Mökander, J. Schuett, H. R. Kirk, and L. Floridi. Auditing large language models: a
three-layered approach. AI and Ethics, May 30, 2023. ISSN: 2730-5953, 2730-5961. DOI:
10.1007/s43681-023-00289-2.
[139]
H. Khlaaf, P. Mishkin, J. Achiam, G. Krueger, and M. Brundage. A Hazard Analysis Frame-
work for Code Synthesis Large Language Models, July 25, 2022. DOI: 10.48550/arXiv.
2207.14157. arXiv: 2207.14157 [cs].
[140]
ARC Evals. Update on ARC’s recent eval efforts: more information about arc’s evaluations of
gpt-4 and claude. March 17, 2023. URL: https://evals.alignment.org/blog/2023-
03-18-update-on-recent-evals/ (visited on September 25, 2023).
[141]
B. Bucknall, R. Trager, and T. Shevlane. Structured Access for Third-Party Safety Research
on Frontier AI Models Investigating researchers’ model access requirements. Working Paper.
Forthcoming.
42
[142]
OpenAI. DALL·E 2 Preview - Risks and Limitations. GitHub. 2022. URL: https : / /
github.com/openai/dalle-2-preview/blob/main/system-card.md (visited on
September 25, 2023).
[143]
M. Murgia. OpenAI’s red team: the experts hired to ‘break’ ChatGPT. Financial Times,
April 14, 2023.
[144]
D. Ganguli et al. Red Teaming Language Models to Reduce Harms: Methods, Scaling
Behaviors, and Lessons Learned, November 22, 2022. DOI: 10.48550/arXiv.2209.07858.
arXiv: 2209.07858 [cs].
[145]
S. Costanza-Chock, I. D. Raji, and J. Buolamwini. Who Audits the Auditors? Recommen-
dations from a ﬁeld scan of the algorithmic auditing ecosystem. In 2022 ACM Conference
on Fairness, Accountability, and Transparency. FAccT ’22: 2022 ACM Conference on Fair-
ness, Accountability, and Transparency, pages 1571–1583, Seoul Republic of Korea. ACM,
June 21, 2022. ISBN: 978-1-4503-9352-2. DOI: 10.1145/3531146.3533213. (Visited on
September 25, 2023).
[146]
Centre for Data Ethics and Innovation. The Roadmap to an Effective AI Assurance Ecosystem.
Independent report, Centre for Data Ethics and Innovation, December 8, 2021. URL: https:
//www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-
assurance-ecosystem (visited on September 25, 2023).
[147]
E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and
G. Irving. Red Teaming Language Models with Language Models, February 7, 2022. DOI:
10.48550/arXiv.2202.03286. arXiv: 2202.03286 [cs].
[148]
P. Levermore. AI Safety Bounties, Rethink Priorities, August 10, 2023. URL: https://
rethinkpriorities.org/publications/ai-safety-bounties (visited on Septem-
ber 25, 2023).
[149]
OpenAI. ChatGPT Feedback Contest: Ofﬁcial Rules, 2022. URL: https://cdn.openai.
com/chatgpt/ChatGPT_Feedback_Contest_Rules.pdf.
[150]
hackerone. Hacker-Powered Security Report. 2022. URL: https://www.hackerone.com/
resources/i/1487910-2022-hacker-powered-security-report-q4fy23/3?.
[151]
M. Zhao, J. Grossklags, and P. Liu. An Empirical Study of Web Vulnerability Discovery
Ecosystems. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Commu-
nications Security. CCS’15: The 22nd ACM Conference on Computer and Communications
Security, pages 1105–1117, Denver Colorado USA. ACM, October 12, 2015. ISBN: 978-1-
4503-3832-5. DOI: 10.1145/2810103.2813704. (Visited on September 25, 2023).
[152]
E. Dardaman and A. Gupta. When openness fails: Towards a more robust governance frame-
work for generative AI. In Proceedings of the Sixth AAIA/ACM Conference on Artiﬁcial
Intelligence, Ethics, and Society. Montreal, Ontario, Canada, 2023.
[153]
Team Nuggets. Why Linux runs 90 percent of the public cloud workload. CBT Nuggets.
August 10, 2018. URL: https://www.cbtnuggets.com/blog/certifications/open-
source/why-linux-runs-90-percent-of-the-public-cloud-workload (visited
on September 25, 2023).
[154]
A. Engler. To Regulate General Purpose AI, Make the Model Move. Tech Policy Press.
November 10, 2022. URL: https://techpolicy.press/to- regulate- general-
purpose-ai-make-the-model-move/ (visited on September 25, 2023).
[155]
T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. QLoRA: Efﬁcient Finetuning of
Quantized LLMs, May 23, 2023. DOI: 10.48550/arXiv.2305.14314. arXiv: 2305.14314
[cs].
[156]
A. Gudibande, E. Wallace, C. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine, and D. Song. The
False Promise of Imitating Proprietary LLMs, May 25, 2023. DOI: 10.48550/arXiv.2305.
15717. arXiv: 2305.15717 [cs].
[157]
Center for Security and Emerging Technology, T. Rudner, and H. Toner. Key Concepts in
AI Safety: An Overview, Center for Security and Emerging Technology, March 2021. DOI:
10.51593/20190040. (Visited on September 25, 2023).
[158]
D. Hendrycks, N. Carlini, J. Schulman, and J. Steinhardt. Unsolved Problems in ML Safety,
June 16, 2022. DOI: 10.48550/arXiv.2109.13916. arXiv: 2109.13916 [cs].
43
[159]
J. Wei et al. Larger language models do in-context learning differently, March 8, 2023. DOI:
10.48550/arXiv.2303.03846. arXiv: 2303.03846 [cs].
[160]
P. Villalobos, J. Sevilla, L. Heim, T. Besiroglu, M. Hobbhahn, and A. Ho. Will we run out of
data? An analysis of the limits of scaling datasets in Machine Learning, October 25, 2022.
DOI: 10.48550/arXiv.2211.04325. arXiv: 2211.04325 [cs].
[161]
MacroPolo. The Global AI Talent Tracker. MacroPolo. 2023. URL: https://macropolo.
org/digital-projects/the-global-ai-talent-tracker/ (visited on September 25,
2023).
[162]
LAION.ai. Petition for keeping up the progress tempo on AI research while securing its
transparency and safety. LAION. March 29, 2023. URL: https://laion.ai/blog/
petition (visited on September 25, 2023).
[163]
D. Jeffries. Let’s Speed Up AI. Future History. February 4, 2023. URL: https : / /
danieljeffries.substack.com/p/lets-speed-up-ai (visited on September 25,
2023).
[164]
K. Grace. Let’s think about slowing down AI. LESSWRONG. December 22, 2022. URL:
https://www.lesswrong.com/posts/uFNgRumrDTpBfQGrs/let-s-think-about-
slowing-down-ai (visited on September 25, 2023).
[165]
Future of Life Institute. Pause Giant AI Experiments: An Open Letter. March 22, 2023.
URL: https://futureoflife.org/open-letter/pause-giant-ai-experiments/
(visited on September 25, 2023).
[166]
L. Ho et al. International Institutions for Advanced AI, July 11, 2023. DOI: 10.48550/
arXiv.2307.04699. arXiv: 2307.04699 [cs].
[167]
G. Marcus and A. Reuel. The world needs an international agency for artiﬁcial intelli-
gence, say two AI experts. The Economist, April 18, 2023. ISSN: 0013-0613. URL: https:
//www.economist.com/by- invitation/2023/04/18/the- world- needs- an-
international-agency-for-artificial-intelligence-say-two-ai-experts.
[168]
OpenAI. Chat Plugins. URL: https : / / platform . openai . com / docs / plugins /
introduction (visited on September 25, 2023).
[169]
J. Schuett, N. Dreksler, M. Anderljung, D. McCaffary, L. Heim, E. Bluemke, and B. Garﬁnkel.
Towards best practices in AGI safety and governance: A survey of expert opinion, May 11,
2023. DOI: 10.48550/arXiv.2305.07153. arXiv: 2305.07153 [cs].
[170]
N. Yu, V. Skripniuk, D. Chen, L. Davis, and M. Fritz. Responsible Disclosure of Generative
Models Using Scalable Fingerprinting, March 17, 2022. DOI: 10.48550/arXiv.2012.
08726. arXiv: 2012.08726 [cs].
[171]
M. W. Wagner. Independence by permission. Science, 381(6656):388–391, July 28, 2023.
ISSN: 0036-8075, 1095-9203. DOI: 10.1126/science.adi2430.
[172]
J. Howard. AI Safety and the Age of Dislightenment: Model licensing & surveillance will
likely be counterproductive by concentrating power in unsustainable ways. fast.ai. July 10,
2023. URL: https://www.fast.ai/posts/2023-11-07-dislightenment.html
(visited on September 26, 2023).
[173]
LAION.ai. A Call to Protect Open-Source AI in Europe. LAION. April 28, 2023. URL:
https://laion.ai/notes/letter-to-the-eu-parliament (visited on September 26,
2023).
[174]
Scale Virtual Events. Emad Mostaque (Stability AI): Democratizing AI, Stable Diffusion &
Generative Models. October 23, 2022. URL: https://exchange.scale.com/public/
videos/emad-mostaque-stability-ai-stable-diffusion-open-source (visited
on September 26, 2023).
[175]
E. Seger, A. Ovadya, D. Siddarth, B. Garﬁnkel, and A. Dafoe. Democratising AI: Multiple
Meanings, Goals, and Methods. In Proceedings of the 2023 AAAI/ACM Conference on AI,
Ethics, and Society. AIES ’23: AAAI/ACM Conference on AI, Ethics, and Society, pages 715–
722, Montréal QC Canada. ACM, August 8, 2023. DOI: 10.1145/3600211.3604693.
(Visited on September 26, 2023).
44
[176]
D. Patel and A. Ahmad. Google ""We Have No Moat, And Neither Does OpenAI"": Leaked
Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI.
SemiAnalysis. May 4, 2023. URL: https://www.semianalysis.com/p/google-we-
have-no-moat-and-neither (visited on September 26, 2023).
[177]
N. Maslej et al. Chapter 7: Diversity. In The AI Index 2023 Annual Report. Institute for Human-
Centered AI, Stanford University, Stanford, CA, April 2023. URL: https://aiindex.
stanford.edu/wp-content/uploads/2023/04/HAI_AI- Index- Report- 2023_
CHAPTER_7.pdf.
[178]
EleutherAI. EleutherAI is a non-proﬁt AI research lab that focuses on interpretability and
alignment of large models. 2023. URL: https://www.eleuther.ai/about (visited on
September 26, 2023).
[179]
BigScience. A one-year long research workshop on large multilingual models and datasets.
URL: https://bigscience.huggingface.co/ (visited on September 26, 2023).
[180]
A. Kayid and N. Reimers. Bonjour. AJ.kQ”. Guten tag. Hola. Cohere’s Multilingual Text
Understanding Model is Now Available. Cohere. December 12, 2022. URL: https://txt.
cohere.com/multilingual/ (visited on September 26, 2023).
[181]
R. Beaumont. Large Scale Openclip: L/14, H/14 and G/14 trained on LAION-2B. LAION.
September 15, 2022. URL: https://laion.ai/blog/large- openclip (visited on
September 26, 2023).
[182]
G. Ilharco et al. OpenCLIP, version 0.1, Zenodo, July 28, 2021. DOI: 10.5281/ZENODO.
5143773. (Visited on September 26, 2023).
[183]
S. Altman. Moore’s Law for Everything. March 16, 2021. URL: https : / / moores .
samaltman.com/ (visited on September 26, 2023).
[184]
K. Miller. Radical Proposal: Universal Basic Income to Offset Job Losses Due to Automation.
Stanford HAI. October 20, 2021. URL: https://hai.stanford.edu/news/radical-
proposal - universal - basic - income - offset - job - losses - due - automation
(visited on September 26, 2023).
[185]
C. O’Keefe, P. Cihon, B. Garﬁnkel, C. Flynn, J. Leung, and A. Dafoe. The Windfall Clause:
Distributing the Beneﬁts of AI, Centre for the Governance of AI Research Report. Future of
Humanity Institute, University of Oxford, 2020. URL: https://www.fhi.ox.ac.uk/wp-
content/uploads/Windfall-Clause-Report.pdf.
[186]
BigCode. Datasets. BigCode. November 16, 2020. URL: https://www.bigcode-project.
org/docs/about/the-stack/ (visited on September 26, 2023).
[187]
J. Vincent. The scary truth about AI copyright is nobody knows what will happen next. The
Verge. November 15, 2022. URL: https://www.theverge.com/23444685/generative-
ai-copyright-infringement-legal-fair-use-training-data (visited on Septem-
ber 26, 2023).
[188]
Polis. Input Crowd, Output Meaning. 2023. URL: https://pol.is/home (visited on
September 26, 2023).
[189]
P. Coy. Can A.I. and Democracy Fix Each Other? The New York Times. April 5, 2023. URL:
https://www.nytimes.com/2023/04/05/opinion/artificial-intelligence-
democracy-chatgpt.html (visited on September 26, 2023).
[190]
The Collective Intelligence Project. Alignment Assemblies. The Collective Intelligence
Project. 2023. URL: https://cip.org/alignmentassemblies (visited on September 26,
2023).
[191]
E. Costa. Deliberative democracy in action: A closer look at our recent pilot with Meta. The
Behavioural Insights Team. November 7, 2022. URL: https://www.bi.team/blogs/
deliberative-democracy-in-action/ (visited on September 26, 2023).
[192]
A. Ovadya. Meta Ran a Giant Experiment in Governance. Now It’s Turning to AI. WIRED.
July 10, 2023. URL: https : / / www . wired . com / story / meta - ran - a - giant -
experiment-in-governance-now-its-turning-to-ai/ (visited on September 26,
2023).
45
[193]
B. Harris. Improving People’s Experiences Through Community Forums. Meta. Novem-
ber 16, 2022. URL: https://about.fb.com/news/2022/11/improving-peoples-
experiences-through-community-forums/ (visited on September 26, 2023).
[194]
A. Ovadya. ‘Platform Democracy’—a very different way to govern big tech: Facebook is
trying ~ it. Twitter, Google, OpenAI, and other companies should too. Reimagining Tech-
nology. July 10, 2023. URL: https://reimagine.aviv.me/p/platform-democracy-
a-different-way-to-govern (visited on September 26, 2023).
[195]
W. Zaremba, A. Dhar, L. Ahmad, T. Eloundou, S. Shibani Santurkar, S. Agarwal, and J. Leung.
Democratic inputs to AI. May 25, 2023. URL: https://openai.com/blog/democratic-
inputs-to-ai (visited on September 26, 2023).
[196]
T. W. House. FACT SHEET: Biden-Harris Administration Secures Voluntary Commit-
ments from Leading Artiﬁcial Intelligence Companies to Manage the Risks Posed by AI.
The White House. July 21, 2023. URL: https://www.whitehouse.gov/briefing-
room / statements - releases / 2023 / 07 / 21 / fact - sheet - biden - harris -
administration-secures-voluntary-commitments-from-leading-artificial-
intelligence - companies - to - manage - the - risks - posed - by - ai/ (visited on
September 26, 2023).
[197]
J. Schuett. Risk Management in the Artiﬁcial Intelligence Act. European Journal of Risk
Regulation:1–19, February 8, 2023. ISSN: 1867-299X, 2190-8249. DOI: 10.1017/err.
2023.1.
[198]
E. Tabassi. AI Risk Management Framework: AI RMF (1.0). error: NIST AI 100-1, National
Institute of Standards and Technology, Gaithersburg, MD, 2023, error: NIST AI 100–1. DOI:
10.6028/NIST.AI.100-1. (Visited on September 26, 2023).
[199]
Center for Long-Term Cybersecurity. UC Berkeley AI Risk-Management Standards Pro-
ﬁle for General-Purpose AI Systems (GPAIS) and Foundation Models. CLTC. August 29,
2023. URL: https://cltc.berkeley.edu/seeking- input- and- feedback- ai-
risk-management-standards-profile-for-increasingly-multi-purpose-or-
general-purpose-ai/ (visited on September 26, 2023).
[200]
A. M. Barrett, D. Hendrycks, J. Newman, and B. Nonnecke. Actionable Guidance for High-
Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks,
February 23, 2023. DOI: 10.48550/arXiv.2206.08966. arXiv: 2206.08966 [cs].
[201]
I. A. E. Agency. Applications of Probabilistic Safety Assessment (PSA) for Nuclear Power
Plants. TECDOC 1200, International Atomic Energy Agency, Vienna, 2001. URL: https:
//www-pub.iaea.org/mtcd/publications/pdf/te_1200_prn.pdf.
[202]
Anthropic. Model Card and Evaluations for Claude Models, 2023. URL: https://www-
files.anthropic.com/production/images/Model-Card-Claude-2.pdf.
[203]
I. D. Raji and J. Buolamwini. Actionable Auditing: Investigating the Impact of Publicly
Naming Biased Performance Results of Commercial AI Products. In Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society. AIES ’19: AAAI/ACM Conference on
AI, Ethics, and Society, pages 429–435, Honolulu HI USA. ACM, January 27, 2019. ISBN:
978-1-4503-6324-2. DOI: 10.1145/3306618.3314244. (Visited on September 26, 2023).
[204]
I. D. Raji, P. Xu, C. Honigsberg, and D. Ho. Outsider Oversight: Designing a Third Party
Audit Ecosystem for AI Governance. In Proceedings of the 2022 AAAI/ACM Conference
on AI, Ethics, and Society. AIES ’22: AAAI/ACM Conference on AI, Ethics, and Society,
pages 557–571, Oxford United Kingdom. ACM, July 26, 2022. ISBN: 978-1-4503-9247-1.
DOI: 10.1145/3514094.3534181. (Visited on September 26, 2023).
[205]
Stability AI. Stable Diffusion 2.0 Release. November 24, 2022. URL: https://stability.
ai/blog/stable-diffusion-v2-release (visited on September 26, 2023).
[206]
ISO. ISO/IEC 23894:2023. February 2023. URL: https://www.iso.org/standard/
77304.html (visited on September 26, 2023).
[207]
Partnership on AI Staff. PAI Is Collaboratively Developing Shared Protocols for Large-Scale
AI Model Safety. Partnership on AI. April 6, 2023. URL: https://partnershiponai.
org/pai- is- collaboratively- developing- shared- protocols- for- large-
scale-ai-model-safety/ (visited on September 26, 2023).
46
[208]
P. on AI Staff. Managing the Risks of AI Research: Six Recommendations for Responsible
Publication, May 6, 2021. URL: https://partnershiponai.org/paper/responsible-
publication-recommendations/ (visited on September 26, 2023).
[209]
Microsoft. Microsoft, Anthropic, Google, and OpenAI launch Frontier Model Forum. Mi-
crosoft On the Issues. July 26, 2023. URL: https://blogs.microsoft.com/on-the-
issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-
model-forum/ (visited on September 26, 2023).
[210]
American Law Institute. Restatement of the Law (Second) Torts. The American Law Institute,
Philadelphia, PA, 1965. URL: https://www.ali.org/publications/show/torts/.
[211]
American Law Institute. Restatement of the Law (Third) Torts: Products Liability. The Amer-
ican Law Institute, Philadelphia, PA, 1998. URL: https://www.ali.org/publications/
show/torts-third/.
[212]
J. C. P. Goldberg and B. C. Zipursky. The Restatement (Third) and the Place of Duty
in Negligence Law. Vanderbilt Law Review, 54(3):657, April 1, 2001. URL: https://
scholarship.law.vanderbilt.edu/vlr/vol54/iss3/2.
[213]
W. M. Landes and R. A. Posner. The Economic Structure of Tort Law: Harvard University
Press, Cambridge, MA, May 20, 1987. 329 pages. ISBN: 978-0-674-86403-0.
[214]
P. Hacker. The European AI liability directives – Critique of a half-hearted approach and
lessons for the future. Computer Law & Security Review, 51:105871, November 2023. ISSN:
02673649. DOI: 10.1016/j.clsr.2023.105871.
[215]
N. Mulani and J. Whittlestone. Proposing a Foundation Model Information-Sharing Regime
for the UK | GovAI Blog. June 16, 2023. URL: https://www.governance.ai/post/
proposing-a-foundation-model-information-sharing-regime-for-the-uk
(visited on September 26, 2023).
[216]
M. Anderljung and P. Scharre. How to Prevent an AI Catastrophe. Foreign Affairs, Au-
gust 14, 2023. URL: https://www.foreignaffairs.com/world/how-prevent-ai-
catastrophe-artificial-intelligence.
[217]
W. Henshall. The Heated Debate Over Who Should Control Access to AI. Time. August 25,
2023. URL: https://time.com/6308604/meta-ai-access-open-source/ (visited on
September 26, 2023).
47
A
AI Model Component Guide
Table 6: AI Model Component Guide
Component
Subcomponent
Deﬁnition
What does access to this component
allow actors to do?
Model weights
The variables or numerical values used to
specify how the input (e.g., text describing
an image) is transformed into the output
(e.g., the image itself)
[See trained weights]
Trained weights
The ﬁnal values of model weights after
they have been updated during training
Alone, nothing; but when combined with
the model architecture, any actor can run
or ﬁne-tune the optimized model with very
low computing costs
Model weight
snapshots
The record of the different weight values
as they were updated during training
Combined with model architecture, actors
could run or ﬁne-tune partially-optimized
systems
Hyperparameters
The variables used to deﬁne other parts
of the model, such as model architecture
(e.g., the number of layers in the model)
and training process (e.g., the strength of
regularization in the loss function)
[See optimized hyperparameters]
Optimized
hyperparameters
The hyperparameter values chosen through
the hyperparameter optimization process
that optimize the efﬁciency of the training
process and increase the model’s perfor-
mance on the training task(s)
Immediately
train
model
more
efﬁ-
ciently by skipping the computationally-
expensive hyperparameter search; this en-
ables actors to train higher-performance
models for a ﬁxed computing cost
Methods for
hyperparameter
optimization
The techniques used to optimize the hy-
perparameter for model performance (e.g.,
grid search, random search, Bayesian opti-
mization); also known as hyperparameter
tuning
Leverage known techniques to efﬁciently
ﬁnd the best model conﬁgurations
Data processing
code
The code used to obtain raw training data
and convert it into the form necessary for
model training
Reproduce the full data pipeline that sup-
plies training data to the model
Data cleaning
The code used to transform the training
data into a form more amenable for model
training (e.g., normalization, removing in-
valid data, etc.)
Transform new data into the structure ex-
pected by the model and ensure data com-
patibility
Synthetic data
creation
The code used to generate additional, artiﬁ-
cial data that is similar to the original train-
ing data; synthetic data is useful because
training on more data sometimes improves
model performance
Generate additional training data with sim-
ilar statistical properties as the original
Data loading
The code used to transform the cleaned
training data into the correct structure / for-
mat to be input directly into the model (e.g.
transforming data into tensors for training
on high-performance chips)
Feed new data into the model seamlessly
to enable training
Training code
The code that deﬁnes the model architec-
ture and implements the algorithms used to
optimize the model weights during training
Rebuild the model architecture from
scratch and train it end-to-end with the
same code
Continued on next page
48
Table 6 – continued from previous page
Component
Subcomponent
Deﬁnition
What does access to this component
allow actors to do?
Model
architecture
The code specifying the structure and de-
sign of an AI model, including the types
of layers, the connections between them,
and any additional components or features
that need to be incorporated; it also speci-
ﬁes the types of inputs and outputs to the
model, how input data are processed, and
how learning happens in the model
Alone, understand better how to train simi-
lar models; with trained weights, any actor
can run or ﬁne-tune the model
Loss function /
reward function
The code that deﬁnes the loss function:
a mathematical formula that measures
model’s performance on the training task
(e.g. MSE loss); the loss function is crit-
ical because minimizing it during train-
ing guides the optimization of the model
weights
Better understand how to train similar
models
Saving and
loading models
The code that handles saving the trained
model parameters or weights to disk or
other storage mediums, allowing the pa-
rameters to be loaded and reused for infer-
ence or further ﬁne-tuning
Understand better how to distribute trained
models
Training loop
The training loop code iterates over the
training data; within each iteration, it feeds
some input data to the model, computes
the loss, and updates the model’s weights
using the chosen optimization algorithm
Run full end-to-end training from raw data
to ﬁnal model (given training data and
model architecture)
Hyperparameter
optimization
code
The code used to optimize the hyperpa-
rameters to improve performance, imple-
menting the methods for hyperparameter
optimization (see above)
Discover optimal hyperparameters efﬁ-
ciently and create more capable models
faster
Related models
Some AI systems rely on multiple models,
either during the training/ﬁne-tuning pro-
cess or during inference; for instance, after
initial training, many foundation models
are ﬁne-tuned via a related Reinforcement
Learning from Human Feedback (RLHF)
model and, more directly, Meta’s CICERO
combines a language processing model
with a strategic reasoning model
Related models cannot be easily used on
their own, but would help actors under-
stand how to integrate different types of
AI model into a single system
Guidelines for
human
evaluators in
RLHF
The instructions specifying what kind of
feedback human evaluators should provide
on the outputs from the foundation model;
this feedback is then used in the RLHF
training process
Understand how to efﬁciently obtain high-
quality training data from human labelers
Inference code
(prediction or
deployment code)
The code that, given the model weights
and architecture, implements the trained
model; in other words, it runs the AI model
and allows it to perform tasks (like writing,
classifying images and playing games)
Generate model outputs and use the model
directly, understand how to efﬁciently run
the model and how to integrate it into pro-
duction systems
Safety code
Additional code is often included within
the inference code to prevent malicious or
harmful use of the model (e.g., prevent-
ing users from generating pornographic
images)
Understand how developers tried to pre-
vent misuse of the model
Continued on next page
49
Table 6 – continued from previous page
Component
Subcomponent
Deﬁnition
What does access to this component
allow actors to do?
Training strategies
Speciﬁc techniques used to train the model
(e.g., how long to train the model for);
these are speciﬁed in the training code but
also communicated at a high-level in asso-
ciated papers and model cards
Understand which techniques boost train-
ing efﬁciency and thus model performance
for a ﬁxed computing cost
Training data
The data used to train and test the model
(for instance, pictures for an image recog-
nition model or internet webpages for a
large language model)
Understand features of the data used to
train the model and, given model architec-
ture and training code, train the model
Data labels
Sometimes, training data are labeled (e.g.,
a label for a picture could be a caption or
description of the image); labels enable
evaluation during training about how well
the machine learning model is predicting
the label, but they are not always necessary
depending on the model being trained
Understand how labeling takes place (and
whether it is outsourced to a third-party,
for example), train or retrain models (de-
pending on the model)
Testing data
To fairly evaluate how well a model per-
forms, its predictions are often evaluated
on a new set of testing data that was never
used during training; this can be a portion
of the original training data that is ""held-
out"" and excluded from training, or a new
dataset
Same as training data (but to a lesser extent
since there tends to be more training than
testing data), evaluate performance when
training or retraining models
Evaluation
Metrics
Measures against which to assess the per-
formance of the model during training;
these metrics may vary depending on the
speciﬁc task; commonly-used metrics in-
clude accuracy, precision, recall, or per-
plexity
Understand how the model capabilities
were assessed, evaluate performance when
training or retraining models
Tacit knowledge
Additional information known only to cer-
tain researchers and engineers within AI
labs that is often very helpful (and some-
times necessary) to train advanced AI mod-
els; for example, Phuong & Hutter (2022)
summarizes some tacit knowledge relating
to the Transformer architecture
Train more advanced models more efﬁ-
ciently
Software stack
A set of software or code libraries that en-
ables the training of an AI model; this in-
cludes machine learning frameworks such
as PyTorch, TensorFlow and Jax, as well
as compilers and optimized libraries like
CUDA, cuDNN and Triton that enable
training on advanced GPUs
Knowing the version of certain software
tools would save time when building train-
ing pipelines
50
CENTRE FOR THE GOVERNANCE OF AI • 4 
VISIT US AT GOVERNANCE.AI
CENTRE FOR THE GOVERNANCE OF AI 
",Centre for the Governance of AI,47656
NTIA-2023-0009-0265,"1 
 
SUBMITTED VIA REGULATIONS.GOV 
 
March 27, 2024 
 
Stephanie Weiner 
Chief Counsel 
National Telecommunications and Information Administration 
U.S. Department of Commerce 
1401 Constitution Avenue NW, Washington, DC 20230 
 
RE: Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights 
 
Dear Ms. Weiner: 
 
Intel Corporation (“Intel”) appreciates the opportunity to provide comments to the National 
Telecommunications and Information Administration (“NTIA” or “Agency”) in response to its request for 
information related to the Agency’s responsibilities under the White House Executive Order (“E.O.”) on Safe, 
Secure, and Trustworthy Development and Use of Artificial Intelligence issued on October 30, 2023.  
Specifically, the E.O. directs NTIA to conduct a public consultation process and issue a report on the potential 
risks, benefits, other implications, and appropriate policy and regulatory approaches to dual-use foundation 
models for which the model weights are widely available. 
 
Intel plays an important role in artificial intelligence (AI).  Intel’s full spectrum of hardware and software 
platforms offer open and modular solutions which support AI workloads and fuel emerging usages like AI at the 
edge, pioneering innovations that will advance the future of AI and help to solve the world’s most complex 
challenges.  For example, in healthcare and life sciences, we accelerate research and patient outcomes with 
faster, more secure, and more accurate analysis across precision medicine, medical imaging, lab automation, 
and more.  For manufacturing, we transform data into insights that help companies optimize plant performance, 
minimize downtime, improve safety, and drive profitability.  Intel is committed to advancing AI technology, 
responsibly and contributing to the development of principles, international standards, best practices methods, 
tools, and solutions to enable a more responsible, inclusive, and sustainable future. 
As policymakers consider approaches to develop frameworks for the responsible development and deployment 
2 
 
of AI technology, it is important to recognize that open development of AI is essential.  There is broad concern 
that due to the nature of advanced model development and its reliance upon significant compute infrastructure, 
advanced AI technologies have the potential to be controlled by the hands of a few.  The opacity and centricity 
of that possible future could pose risks to democracy and capital systems.  Technology and AI innovation are 
best served through open, neutral, horizontal competition.  Open AI development is essential to facilitate 
innovation and equitable access to AI, as open innovation, open platforms, and horizontal competition help 
offer choice and build trust.  Fostering an open ecosystem for AI that leverages standardized processes and 
removes barriers to open source tools and solutions can facilitate innovation to thrive and create broader access 
to AI technology.  However, to achieve these benefits, open development may comprehend accountability for 
responsible design and implementation to help mitigate potential individual and societal harm.  This includes 
establishing robust security protocols and standards to identify, address, and report potential vulnerabilities.  
Openness not only allows for faster advancement of technology and innovation, but also faster, transparent 
discovery of potential harms and community remediation and address.  
 
In contemplating this topic, it is important to understand not just the benefits, but also specific risks related to 
widely available weights, how different levels of availability may impact these risks, and the best approaches to 
mitigate them.  A few examples of potential risks are unintended harm, malicious harm, circumvention or lack 
of safety controls, and new, unforeseen emergent risks.  For an open ecosystem we must find the right balance 
between the structural advantages and importance of open development, while also considering and addressing 
potential risks from access to model weights, datasets, and training information, as well as implementing 
appropriate safety testing and guardrail details for advanced AI models.1  When evaluating the potential for 
harm, all data sets are not created equal.  Risk management of data should consider not only dataset size, but 
also classification of data – health and life sciences data may require different safeguards to balance the 
potential for life improving innovations versus the risks of malicious actors.  
We offer the following input on NTIA’s efforts related to dual use foundation models with widely available 
weights as outlined in the Agency’s notice. 
QUESTIONS 
1. How should NTIA define “open” or “widely available” when thinking about foundation models and model 
 
1 See, https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf 
3 
 
weights? 
A definition of “open” or “widely available” depends on the objectives that NTIA aims to promote.  If an 
NTIA considers the question of “open” and “widely available” to address limitation and promote broader 
access to frontier or foundation models, it may be useful to look to the experience of the software 
ecosystem.  The Open Source Initiative (OSI)2 has developed a definition that requires open, permissive, 
and generally unrestricted access to and use of software to be considered “open”.  In this regard, it is 
promoting free, broad access to an ever-expanding corpus of source code to promote quality and innovation 
in software development.  Others in the ecosystem promote their software as “open” if it is merely available 
for download, albeit with significant restrictions on access and/or use.  Each has their attendant benefits and 
risks.  While the OSI model has indeed improved the quality of software and promoted innovation, it also 
makes it easier to develop applications used to support criminal activity.  Conversely, some have “open-
washed” their software by promoting use-restricted software as “open” only to create software lock-in to a 
broader product portfolio. 
In the context of AI solutions, a more nuanced approach to defining “open” and “widely available” may be 
in order.  NTIA may consider a definitional approach that promotes an open, permissive access to enabling 
AI technologies (data, models, etc.,) to promote broader innovation in the AI ecosystem, while still allowing 
for the use of certain legal and technological restrictions on access or use of AI models to address misuse or 
abuse in AI applications.  That is, a definition of “open” that promotes open access to and permissive use of 
certain levels of model architecture and model weight information, and responsibility for how and where 
those powerful models are used and their ultimate impact on society.   
LFAI & Data3, an umbrella foundation of the Linux Foundation that supports open source innovation in AI 
and data, promotes a Model Openness Framework4, which is a ranked classification system that rates AI 
models based on their completeness and openness, from open model, open tooling, to open science.  Model 
openness encompasses more than model weights and includes but is not limited to datasets, topology, 
training and inference code, pre-processing code, hyperparameters, and evaluation metrics.  A question to 
consider is how easily a third party can replicate advanced AI models as all parameters listed above, in 
 
2 https://opensource.org/ 
3 https://lfaidata.foundation/ 
4 https://docs.google.com/document/d/1RUNrs4flAsYsikXTPu1jWBH1BAumCyeG/edit?tab=t.0#heading=h.gjdgxs 
4 
 
addition to model weights, need to be understood.   
c. Should “wide availability” of model weights be defined by level of distribution? If so, at what level of 
distribution (e.g., 10,000 entities; 1 million entities; open publication; etc.) should model weights be 
presumed to be “widely available”? If not, how should NTIA define “wide availability?”  
Level of distribution should not be used to define “wide availability” of model weights.  Rather, NTIA 
should focus on components related to risk assessment.  The LFAI & Data Model Openness Framework5 
can be a helpful reference to define “wide availability” of model weights.  Also, refer to the response to 
question #1 above.  
d. Do certain forms of access to an open foundation model (web applications, Application Programming 
Interfaces (API), local hosting, edge deployment) provide more or less benefit or more or less risk than 
others? Are these risks dependent on other details of the system or application enabling access?  
Release of foundation models present a gradient of openness options.6  A few examples are listed below. 
1) 
Hosting the model and allowing access only through an API enables the host to monitor all 
requests, scrutinize requests, and maintain safeguards. 
2) 
Enabling others to host the model and/or ensure that requests and safeguards are controlled.  In this 
case misuse is likely not observed, as prompts are not monitored and insight into model use is 
limited. 
3) 
Releasing model code or details to ""vetted"" users can enable watermarking/fingerprinting for 
tracking the proliferation of models.  This scenario assumes control is only implemented by user 
selection/vetting. 
4) 
Releasing model code to open source so that anyone can have access. 
As model providers start at option 1 and transition to options 3 or 4, this approach can assist to initially 
discover issues and capabilities of the model.  Additionally, while lower degrees of openness (e.g., 
controlled access via APIs) may offer lower potential for downstream innovation and scrutiny than an open 
source, licensed release of foundation models, lower degrees (i.e., gradients labelled 1, 2, and 3 above) can 
help mitigate the risk of misuse or abuse of the model.   
 
5 Id. 
6 See, https://arxiv.org/abs/2302.04844 
5 
 
i. Are there promising prospective forms or modes of access that could strike a more favorable benefit-
risk balance? If so, what are they?  
Yes, the security of AI solutions, including securing the release of AI models, is an area of active research 
and development within academia, industry, and industry consortia.  As a result, the ability to secure models 
once they are released into the ecosystem – even under generally open and permissive license models (if not 
pure open source licenses) – will continue to improve.  Between fully closed and fully open modes of 
access, there generally exists some form of controlled access (e.g., staged release, selected access by 
“trusted” parties, API-based) to some elements of the model, including barring access to others.  While 
limited forms of access can still provide value beyond that of fully open models (e.g., staged releases for 
incremental model evaluation or API access to fine tune and build downstream systems) it is likely that 
these will fall short of the benefits of full open sourcing. 
2. How do the risks associated with making model weights widely available compare to the risks associated 
with non-public model weights?   
When assessing the risks associated with the release of a model, the model architecture and model weights 
need to be considered together.  Assuming that the question presumes that a model architecture is widely 
available, the risk of abuse or misuse of the model may be mitigated, at least for some time, by limiting the 
open availability of the full set of model weights.  Informally, publicly available model weights make it 
easier to modify the model, reduce in-built guardrails, and attempt to leverage them for propagating various 
societal harms and malicious uses.  Even when the model is being fine-tuned for non-malicious use, unless 
proper model testing and evaluation practices are commissioned, the model may have unintended 
consequences leading to harmful societal impact.  
 
One can also use non-public models (e.g., API-based models) for propagating harms to some extent.  
However, these can be better controlled (based on API usage) and proper interventions can be put in place.  
Closed models may also be susceptible to malicious use, given that current safeguards may not be robust 
enough to withstand adversarial attacks.  
 
See the chart below comparing risks of widely available and non-public model weights.  NTIA could 
consider defining formal tiers of open software.  Tier 1 might allow access to model weights only, while Tier 
2 could also permit data sources.  Tier 3 could open-source the source code used for model training, and Tier 
6 
 
4 might add any pre- and post-processing steps, or details like reinforcement learning from human feedback 
(RLHF)7.  That way, lower tiers would allow scrutiny and use of the models while upper tiers would enable 
full reproducibility.  Such tiering systems are common in other safety metrics (e.g., L1-L5 defined for 
advanced driver assistance systems (ADAS).  This approach would also provide a mechanism for consistent 
“open access”, as currently, entities label products as open source when they are actually open models, 
thereby creating false expectations in the ecosystem. 
 
Risks 
Open Weight 
Foundation 
Models 
Non-public Model Weights 
Policy enforcement based on various 
Risk Management Frameworks 
Difficult 
Easy 
Decommissioning of older models 
Difficult 
Easy 
Enforcing critical updates to the model 
Difficult 
Easy 
Controlling Model parameters for 
various risk levels and thresholds 
Difficult 
Easy 
Unacceptable use Risk Controls 
Difficult 
Moderate 
Unforeseen Risk Control 
Difficult 
Moderate 
Model Transparency  
Easy 
Moderate 
Model Testing, Red Teaming 
Easy 
Difficult 
Proliferation of Technologies/IP 
Widespread 
Controlled 
Marginal Risk of open vs closed/pre-
existing technologies 
Limited (e.g., 
Misinformation, 
Deepfakes) 
- 
Risks associated with downstream 
applications that use Foundation 
Models 
Similar 
Similar 
 
c. What, if any, are the risks associated with widely available model weights? How do these risks change, 
if at all, when the training data or source code associated with fine tuning, pretraining, or deploying a 
model is simultaneously widely available? 
There are two types of risks to highlight for NTIA’s consideration.  One set of risks is the intentional use of 
an AI model to cause harm or violate human rights and national security.  Avoiding open release of frontier 
models or foundation models of this capability/fidelity under an open architecture or open weights model 
could address this risk.  That said, once these models are released (even proprietarily through 
 
7 See, https://huggingface.co/blog/rlhf 
7 
 
commercialization), it may not take long for variants of it to emerge in open source across the world.  These 
may approximate the same capability for a narrower set of use cases.  This is because the fundamental 
architecture of almost all foundation models today is based on a small set of ideas – transformers, diffusion 
models, contrastive learning, etc., and there is little intellectual barrier to entry.    As such, policymakers 
should prioritize identifying and mitigating malicious use of the technology rather than prescriptive 
regulation of the technology itself. 
The other type of risk is an AI system which is empowered to perform consequential tasks, yet it operates in 
a manner that deviates from its intended design and causes harm or violates laws and principles in an 
unintended manner.  The risks due to uncertainty in a model and/or lack of understanding of edge-scenario 
behavior of the model is a much higher risk that can be exacerbated by adversarial attacks on the model.  
Adversarial AI influences or compromises a commissioned AI system to cause it to perform in an 
unintended manner.  Addressing this risk should be done through increased transparency of datasets and 
pretraining/finetuning processes.  For adversarial use, widely available weights and training data enable a 
faster path for malicious activity – especially if all aspects are being shared, thereby permitting complete 
reproducibility of its results. Explaining the principles that lead to a leap in capabilities compared with 
previous models can sometimes generate more effective reproduction than just providing weights and data.  
For example, the principle of reinforcement learning from human feedback can facilitate major 
improvement of results even if none of the weights and specific data are shared.  
Assessing risks associated with a model’s reproducibility are linked to the foreseeable risks of the model, 
itself.  The availability of model weights alone is hardly sufficient to reproduce the model, however, the 
availability of architecture, hyperparameters, datasets, topology, training and inference code, evaluation 
metrics, and other detailed information can aid in reproducing the model.  As openness to the public 
increases, the more others can leverage it to reproduce, enhance, and adapt the model to other domains, 
potentially adding risks.  However, the ability to build a performance model relies on compute availability 
and data parallel training.  This is necessary for a malicious actor even if the full algorithm was published.  
Yet, making the algorithm public could also enable mitigation plans and help balance the risk. 
d. Could open foundation models reduce equity in rights and safety-impacting AI systems ( e.g., 
healthcare, education, criminal justice, housing, online platforms, etc.)?  
Foundation models can have negative impacts by being misused or by being intrinsically biased, unsecured, 
8 
 
or inaccurate.  However, open-ness or closed-ness of a foundation model is not likely directly related to an 
increase or decrease in equity. API calls to a closed model can be just as vulnerable to biases as compared to 
directly using an open model.  This is a fundamental issue for all machine learning models that are 
“blackbox” in nature.  In the case of unintended consequences in a consequential deployment, frontier 
models cannot be validated across all aspects of functionality similar to other software products.  Open 
models (datasets, architecture, weights, training scripts, testing performed) can help developers to improve 
safety issues and reduce the barrier for malicious actors to circumvent safety guardrails.  Even transparency 
without full reproducibility can lower the barrier for model attacks and compromised safety.  Transparency 
in testing and intended use cases can assist downstream users to select appropriate models and understand 
risks to adapt specific models to specific use cases.  In an open ecosystem, harm may result from malicious 
actors or well-meaning developers that break safety guardrails unintentionally.  However, the latter is easier 
to mitigate with standard safety benchmarks. 
e. What, if any, risks related to privacy could result from the wide availability of model weights? 
Open weight models present increased marginal risks related to deepfakes as they substantially lower the 
barrier to generating such content.8  Safeguards for closed models, including watermarking, are relatively 
more effective in this area, and monitoring closed models can deter generating such imagery, especially of 
real people.  Voice cloning based scams may also pose an increased potential threat with open models 
compared to closed models (although there isn’t much documented evidence of these harms). 
3. What are the benefits of foundation models with model weights that are widely available as compared to 
fully closed models? 
c. What benefits do open model weights offer for competition and innovation, both in the AI marketplace 
and in other areas of the economy? In what ways can open dual-use foundation models enable or 
enhance scientific research, as well as education/training in computer science and related fields? 
Open model weights have the potential for spurring further innovation in AI.  For example, almost all 
innovation in AI to-date has been due to openly available infrastructure9.  Open model weights are likely 
going to aid researchers to find impactful and beneficial use cases of AI that will be overlooked by narrow 
 
8 https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf 
9 Openly available infrastructure encompasses more than model weights, including but not limited to architecture 
and dataset transparency. 
9 
 
and immediate commercial interests of proprietary model vendors.  An example of this is applying leading-
edge AI principles to open scientific problems.  Open model weights also spur more startups and 
innovations, enabling startups to quickly prototype without access to immense capital, fostering a more 
competitive landscape.    
d. How can making model weights widely available improve the safety, security, and trustworthiness of AI 
and the robustness of public preparedness against potential AI risks? 
Open model weights will allow researchers at the intersection of AI and safety, security, trust, and ethics to 
research methods to further improvements in model fairness and trustworthiness. This kind of research is 
essential to understand data strategies, model architectures, and how to learn from human feedback so that 
models can be made more trustworthy and robust. 
e. Could open model weights, and in particular the ability to retrain models, help advance equity in rights 
and safety-impacting AI systems (e.g., healthcare, education, criminal justice, housing, online platforms 
etc.)?  
Yes, this approach presents an opportunity that initial deficiencies in models (e.g., racial or gender biases) 
can be studied openly by researchers and techniques developed can debias these models.  A research paper 
by Intel Labs demonstrates how to mitigate social biases in vision-language foundation models.10  Such 
work will not be possible without access to the original model weights and is essential to ensure that 
foundation models can fairly and equitably represent all sections of populations.  The ability to retrain these 
models to address safety vulnerabilities and bias, especially given the specificity of downstream applications 
and deployment contexts and their associated risks, would be key to improve the equity and safety of such 
models.  
4. Are there other relevant components of open foundation models that, if simultaneously widely available, 
would change the risks or benefits presented by widely available model weights? If so, please list them and 
explain their impact. 
Please see the response to question #2.a. above.  
5. What are the safety-related or broader technical issues involved in managing risks and amplifying benefits 
 
10 https://arxiv.org/abs/2312.00825 
10 
 
of dual-use foundation models with widely available model weights? 
a. What model evaluations, if any, can help determine the risks or benefits associated with making weights 
of a foundation model widely available?  
Amongst other areas of work, capability evaluations are being investigated by the recently created U.S. 
Artificial Intelligence Safety Institute and the supporting AI Safety Institute Consortium.  In parallel, a 
number of processes and techniques, such as the use of responsible AI councils to evaluate AI models, red-
teaming, penetration testing, model cards, dataset evaluations, security reviews, etc. may be employed to 
help identify and mitigate risks associated with model development and the release of such models.  
Additionally, for an open framework, benchmarking and adversarial testing can provide information that a 
model is capable of inflicting serious harms without added guardrails (i.e., catastrophic events), and thus 
limiting the release of the architecture and weights to ""known good actors"" would be important.  
c. What are the prospects for developing effective safeguards in the future?  
While prospects are promising, this is an extraordinarily difficult problem to solve not unlike what is seen 
today in the hacker/cybersecurity space.  It will require constant evolution of the technology. 
f. Which components of a foundation model need to be available, and to whom, in order to analyze, 
evaluate, certify, or red-team the model? To the extent possible, please identify specific evaluations or 
types of evaluations and the component(s) that need to be available for each. 
Model weights, metadata, model architecture, datasets, training/inference/evaluation codes, 
hyperparameters, data processing code, and evaluation metrics are components of a foundation that would 
be needed to analyze, evaluate, certify, or red-team the model; applicable research papers can also assist in 
this effort.  
g. Are there means by which to test or verify model weights? What methodology or methodologies exist to 
audit model weights and/or foundation models? 
HELM11 is a great resource to evaluate foundation models; it encompasses model accuracy, performance, 
bias, toxicity, and other means for 80+ different scenarios. 
7. What are current or potential voluntary, domestic regulatory, and international mechanisms to manage the 
 
11 https://crfm.stanford.edu/helm/classic/latest/ 
11 
 
risks and maximize the benefits of foundation models with widely available weights? What kind of entities 
should take a leadership role across which features of governance? 
d. What role, if any, should the U.S. government take in setting metrics for risk, creating standards for best 
practices, and/or supporting or restricting the availability of foundation model weights? 
The U.S. government should take an active role in setting metrics for risk and creating standards for best 
practices for the responsible development of deployment of foundation models.  Leveraging the various 
initiatives by the National Institute of Standards and Technology (NIST), other multilateral organizations, 
and relevant international standards bodies, the U.S. can model for other countries the importance of 
prioritizing best practices and international standards development for foundation models. 
8. In the face of continually changing technology, and given unforeseen risks and benefits, how can 
governments, companies, and individuals make decisions or plans today about open foundation models that 
will be useful in the future? 
b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt the threshold, is the 
amount of computational resources required to build a model, such as the cutoff of 1026 integer or 
floating-point operations used in the Executive order, a useful metric for thresholds to mitigate risk in 
the long-term, particularly for risks associated with wide availability of model weights?  
The computational resource threshold outlined in Executive Order 14410 is not the best mechanism for 
establishing thresholds to mitigate risk associated with wide availability of model weights for dual use 
foundation models.  While this approach correlates decently with model capability and measurement, its use 
was intended to be temporary and not future proof for long-term implementation.  The Department of 
Commerce can consider facilitating this effort through NIST, as it is currently evaluating capabilities and 
metrics for dual use foundation models through its AI Safety Institute Consortium and other related 
activities. 
*** 
Intel appreciates NTIA’s consideration and welcomes the opportunity to discuss our feedback. 
Respectfully submitted, 
Angel Preston 
Policy Director, Artificial Intelligence 
12 
 
 
",Intel,5580
NTIA-2023-0009-0264," 
 
1 
 
 
March 27, 2024 
Docket: NTIA–2023–0009  
GitHub Response to NTIA Request for Comment on “Dual-Use Foundation 
Artificial Intelligence Models With Widely Available Model Weights”  
 
GitHub welcomes the National Telecommunications and Information 
Administration (NTIA) consultation on widely available model weights as an 
important step to gather diverse perspectives and empirical evidence to inform 
the implementation of Executive Order 14110 on Safe, Secure, and 
Trustworthy Development and Use of Artificial Intelligence. This consultation 
can support the collection of information today, and the creation of a 
framework for future assessment, to determine whether policy measures may 
be needed.   
 
GitHub is the home of open source collaboration globally, with more than 100 
million developers on our platform building and sharing components at every 
level of the AI stack. Below GitHub contributes expertise and open source 
community perspective in responses to specific RFC questions. At the outset, 
we offer several principles to inform NTIA’s evaluation of the risks, benefits, 
and policy measures for widely available model weights. 
 
Open source is a public good. Open source software is a non-rivalrous and 
non-excludable knowledge base, enabling use and contribution by 
professional developers, hobbyists, companies, non-profits, and governments 
alike. This public good has created immense economic value1 and has been 
supported by policymakers around the world for its benefits to digital 
modernization, local industry, and cost savings.2 The U.S. government and 
other stakeholders increasingly recognize the importance of open source as 
public infrastructure and the need for public support.3 Open source and open 
 
1 Manuel Hoffmann, et al., “The Value of Open Source Software,” Social Science Research Network, 
January 1, 2024, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148; Gizem Korkmaz et al., 
“From GitHub to GDP: A Framework For Measuring Open Source Software Innovation,” Research 
Policy 53, no. 3 (April 1, 2024): 
104954, https://www.sciencedirect.com/science/article/pii/S0048733324000039. 
2 CSIS analysis of 669 policies globally between 1999 and 2022 finds that stated objectives of policies 
mentioning open source sought to modernize IT (43%), support local industry (20%), and decrease costs 
(18%). Georgia Wood and Eugenia Lostri, “Government’s Role in Promoting Open 
Source Software.” CSIS, January 9, 2023, https://www.csis.org/analysis/governments-role-promoting-
open-source-software.  
3 The Digital Public Goods Alliance, the G20 Digital Public Infrastructure framework, U.S. Open 
Technology Fund, and the German Sovereign Tech Fund are relevant examples. In a recent request for 
information, five U.S. government departments, led by the Office of the National Cyber 
 
 
88 Colin P Kelly Jr Street 
San Francisco, CA 94107 
United States of America 
 
 
2 
science have been essential to AI development to date.4 Widely available 
model weights, particularly those made available under open source licenses, 
provide similar benefits.  
 
AI models are a general purpose technology. Like software, AI and 
specifically foundation models are general purpose, with “dual-use” deriving 
from application-specific properties. Executive Order 14110 defines as “dual-
use” an AI model with “at least tens of billions of parameters, is applicable 
across a wide range of contexts, and that exhibits or could be easily modified 
to exhibit, high levels of performance at tasks that pose a serious risk to 
security, national economic security, national public health or safety, or any 
combination of those matters.” Clarity on which tasks and levels of 
performance are to be invoked under the “dual-use” definition will be essential 
for open source developers and the ecosystem at large.   
 
Evaluation and regulation are better focused on the AI system, rather 
than the model. Models alone do not determine AI system performance on 
tasks,5 including tasks with national security implications. Context of 
deployment, system affordances including tool-use and safety modifications, 
and user intent all warrant consideration. To adequately evaluate the benefits 
and risks of widely available model weights, a broader focus is needed to 
holistically consider systems using AI models, including proprietary provision, 
and their integration with other software innovations.  
 
Widely available model weights support research and safety. AI 
researchers have credited widely available model weights with advancing the 
 
Director, stated “The federal government recognizes the immense benefits of open-source software, 
which enables software development at an incredible pace and fosters significant innovation and 
collaboration. In light of these factors, as well as the status of open-source software as a free public good, 
it may be appropriate to make open-source software a national public priority to help ensure the security, 
sustainability, and health of the open-source software ecosystem.” 
4 Mike Linksvayer, “OSI’s Deep Dive Is an Essential Discussion on the Future of AI and Open 
Source,” Open Source Initiative, September 29, 2022, https://opensource.org/blog/osi-leading-an-
essential-discussion-on-the-future-of-ai-and-open-source; Nathan Benaich and Alex Chalmers, “The 
Case for Open Source AI,” Air Street Press, February 8, 2024, https://press.airstreet.com/p/the-case-for-
open-source-ai; Bureau of Competition and Office of Technology, “Generative AI raises competition 
concerns,” Federal Trade Commission, June 29, 2023, https://www.ftc.gov/policy/advocacy-
research/tech-at-ftc/2023/06/generative-ai-raises-competition-concerns.      
5 Matei Zaharia, et al., “The Shift from Models to Compound AI Systems.” Berkeley Artificial Intelligence 
Research, February 18, 2024, https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/; Tom 
Davidson, et al., “AI Capabilities Can Be Significantly Improved Without Extensive Retraining,” arXiv, 
https://arxiv.org/pdf/2312.07413.pdf; Megan Kinniment, et al., “Evaluating Large-Model Agents on 
Realistic Autonomous Tasks,” arXiv, Janurary 4, 2024, https://arxiv.org/pdf/2312.11671.pdf.  
 
 
3 
interpretability, safety, and security of AI models.6  As AI models prove useful 
to research across academic disciplines, direct access to model weights 
permits peer review and reproducibility. More investment is needed in AI 
interpretability, evaluation, and safety research. Across these research 
directions, norms of open science and open source can accelerate needed 
discoveries. 
 
Wide availability of model weights is a function of discovery, governed 
by online platforms. Even for content posted publicly on the internet, the 
default state is obscurity. Whether content is widely available will depend on 
ecosystem activity, distribution channels, and, particularly, sharing on 
platforms that enable virality. Ecosystem monitoring and governance can help 
inform and implement risk-based mitigations for widely available model 
weights. 
  
Regulatory risk assessment should weigh empirical evidence of 
possible harm against the benefits of widely available model weights. 
Evidence of harmful capabilities in widely available model weights and their 
use should consider baselines of closed, proprietary AI capabilities and the 
availability of potentially dangerous information in books and via internet 
search.7 The US AI Safety Institute (AISI), companies undertaking large-scale 
AI research governed by Executive Order 14110 (4.2), and research 
community efforts may yield such evidence in the future. Today, available 
evidence of the marginal risks of open release does not substantiate 
government restrictions.   
 
Government should invest in societal resilience amid growing risks and 
benefits of AI. The development and deployment of AI systems—regardless 
of whether their model weights are made widely available—will have profound 
effects on society. Societal resilience to new developments and use of AI 
systems for unforeseen purposes warrants attention. The diffusion and 
diversity of widely available models across society supports public education 
and incentives for protective measures, ultimately increasing resilience to risks 
posed by malicious use of AI systems. Policymakers should support societal 
resilience, including funding research in AI evaluation and measurement 
science, consolidating best practices via the AISI Consortium, and 
demonstrating leadership in defensive use of AI, including as directed in EO 
 
6 Andrew Critch, “My followers might hate this idea, but I have to say it: There's a bunch of excellent LLM 
interpretability work coming out from AI safety folks (links below...,” X, October 4, 2023, 
https://twitter.com/AndrewCritchPhD/status/1709690861003694418; Beren Millidge, “Open Source AI 
Has Been Vital For Alignment,” Beren’s Blog, November, 5, 2023, https://www.beren.io/2023-11-05-
Open-source-AI-has-been-vital-for-alignment/.   
7 Sayash Kapoor, et al., “On the Societal Impact of Open Foundation Models,” CRFM Stanford, February 
27, 2024, https://crfm.stanford.edu/open-fms/paper.pdf.   
 
 
4 
14110. Policymakers should prioritize AI regulation against reckless use that 
causes harm today and evaluate criminal justice policies and emergency 
plans for malicious use. 
 
Question 1: How should NTIA define ""open"" or ""widely available"" when thinking about  
         foundation models and model weights? 
 
Summary and recommendations: 
1. Government should support community-led definitions of openly available 
AI models. 
2. Policy should acknowledge that availability of model weights is not binary 
and that discoverability plays an important role in model availability. 
We define “available model weights” to be when an AI model has been shared 
publicly such that developers can have direct access to its trained parameters. 
With this direct access, developers have the capacity to run or modify the 
model to suit their purposes. The “wide availability” of AI models is not a 
foregone conclusion of public sharing; rather, models must be discovered.8 
Discovery depends on ecosystem activity, distribution channels, and, 
particularly, viral sharing.9 
 
In contrast to available models, open source models should reflect the open 
source definition maintained by the Open Source Initiative. To be “open 
source,” a model must be released under licensing terms that permit anyone 
to read, modify, (re)distribute, and use the model for any purpose. The Open 
Source Initiative–the organization that has stewarded the definition of open 
source for twenty-five years–has ongoing definitional work in adapting these 
principles to AI models specifically,10 and other community stakeholders have 
published perspectives on defining model openness.11 Government should 
support community-led definitions, and for the purposes of this RFC, 
acknowledge that, although widely available model weights pose similar risks, 
the subset of such models that are available under open source terms that 
 
8 Although available, the default state of content posted to the public internet is obscurity. 
9 What ought to constitute “widely available” cannot be easily boiled down to a single metric. Rather, it 
must reflect the risks and benefits with context for use, measured by developer dependencies via online 
platforms like GitHub, discussion and links on social media, and integration into popular applications. See 
Question 7 for recommendations on ecosystem monitoring. 
10 Open Source Initiative, “Join the Discussion on Open Source AI,” https://opensource.org/deepdive; 
Open Source Initiative, “The Open Source AI Definition - draft v. 0.0.6,” 
https://opensource.org/deepdive/drafts/the-open-source-ai-definition-draft-v-0-0-6. 
11 Matt White, et al., “The Model Openness Framework: Promoting Completeness and Openness for 
Reproducibility, Transparency and Usability in AI,” arXiv, March 21, 2024, 
https://arxiv.org/abs/2403.13784; Heather Meeker, “Toward an Open Weights Definition,” Copyleft 
Currents, June 8, 2023, https://heathermeeker.com/2023/06/08/toward-an-open-weights-definition/.  
 
 
5 
permit lawful use, modification, and redistribution provide particular benefits. 
Below we refer to such models as “openly available.” 
 
To further define terms, in this submission we use “model developer” to mean 
those who train an AI model and decide how to make it available. More 
broadly, we use the term “developer” to refer to those who write software more 
generally and decide how to make it available, including GitHub users. 
Developers may be model developers, or may integrate proprietary models-
as-a-service or available models into AI systems. Developers may be a 
company or non-profit, a loose collection of individuals, or an individual. 
Developers may be, but are not necessarily, the user of an AI system, subject 
to AI system outputs, or an AI provider who runs system inference for users. 
The model developer may build in the open, with public access to training and 
intermittently posting model check-points,12 or build privately to later share the 
model publicly. Models are publicly shared today via online platforms including 
GitHub and via decentralized file-sharing protocols. Models are often 
discovered by downstream developers via online platforms, particularly those 
that enable viral sharing. 
 
Question 2: How do the risks associated with making model weights widely available  
    compare to the risks associated with non-public model weights?  
 
Summary and recommendations: 
1. Reckless-use risks that see harm caused today warrant priority from 
policymakers.  
2. Evidence-based, harm-specific analyses of malicious-use scenarios are 
warranted.  
3. Societal resilience against malicious-use risk requires a harm-reduction 
perspective, not an attempt at security through obscurity.  
 
AI systems of all kinds pose risks that can be categorized in one of two ways: 
reckless use or malicious use.13 In contrast to use-based risks from AI 
systems, risks posed by models reflect developer decisions along the value 
chain, including model developers’ trusted builds and application developers’ 
 
12 For example, EleutherAI’s use of Weights and Biases for GPT NeoX 20B and the TinyLlama project, 
respectively. 
13  
  Malicious Use  
  Reckless Use  
  Deception (fraud, misinformation, 
persuasion)  
  Hacking   
  Terrorism (designing weapons)  
  Harassment (deepfakes, spam)  
  Exploited vulnerabilities (prompt injection, data leakage)  
  Bias (flawed decisions, discrimination, representational harm)  
  Accidents (inappropriate or dangerous deployment)  
 
 
 
6 
responsible integrations.14 In evaluating both development and system-
integrated use, the risks posed by widely available model weights should be 
considered marginally, with respect to risks posed by other AI models, and not 
counterfactually as if no AI capability exists.15  
 
We should prioritize focus on reckless-use risks that cause harm today. 
Policymaking globally has focused on this challenge, particularly in high-risk 
settings and irrespective of open or proprietary provision.16 U.S. policymakers 
should take note of this global trend. Ultimately, societal resilience against 
malicious-use risk requires a harm-reduction perspective, not an attempt at 
security through obscurity. The diffusion and diversity of models across 
society supports public education and incentives for protective measures, 
ultimately increasing resilience to risks posed by malicious use of AI systems. 
 
In reckless-use scenarios, widely available model weights pose no marginal 
risk of additional harm relative to closed models, and may instead provide 
benefits. Reckless deployment or use of a model-integrated AI system may 
harm the user or those subject to the outputs of the system. In such cases, the 
system was trusted when it should not have been. Such misplaced trust may 
include model vulnerabilities due to model developer’s lack of awareness or 
malice, or include poorly governed deployment that sees a system misused, 
for example, in cases that do not adequately reflect the training data.17 Widely 
available models with greater openness, particularly by including open source 
code, model and data documentation, and/or open data, can reduce risk of 
reckless use, as downstream developers and users have better information to 
build applications and select (or contest) use cases (See Question 4). In 
building and using AI systems, developers must trust or have other assurance 
in the model developer (or in the proprietary models-as-a-service provider). 
Trust in the value chain is a common problem in software generally, where 
developers write software that makes calls to software packages produced by 
others. Solutions include verified software builds,18 and early work is ongoing 
for such approaches specific to AI models.19 Regardless of whether the model 
 
14 In some cases, malicious actors may develop applications for their own malicious use. 
15 Sayash Kapoor, et al., “On the Societal Impact of Open Foundation Models,” CRFM Stanford, February 
27, 2024, https://crfm.stanford.edu/open-fms/paper.pdf.   
16 Peter Cihon, “How to Get AI Regulation Right for Open Source,” GitHub Blog, July 26, 2023, 
https://github.blog/2023-07-26-how-to-get-ai-regulation-right-for-open-source/.  
17 I.e., out-of-distribution use: Jingkang Yang, et al., “Generalized Out-of-Distribution Detection: A 
Survey,” arXiv, Januray 23, 2024, https://arxiv.org/abs/2110.11334.     
18 Notable projects include SigStore, Reproducible Builds, and Boostrappable Builds. See also Brian 
Dehamer and Philip Harrison, “Introducing npm Package Provenance,” GitHub Blog, April 19, 2023, 
https://github.blog/2023-04-19-introducing-npm-package-provenance/.  
19 Mithril Security, “AI Cert: Open-source tool to trace AI model’s provenance,” 
https://www.mithrilsecurity.io/aicert; Tobin South, et al., “Verifiable Evaluations of Machine Learning 
Models Using zkSNARKS,” arXiv, February 5, 2024, https://arxiv.org/pdf/2402.02675.pdf.     
 
 
7 
is widely available or proprietary, if it contains vulnerabilities, a downstream 
user may experience security risks, with implications for privacy and other 
harms.   
 
Malicious use of widely available models poses marginal risks of additional 
harm. Once a model developer releases the model, they cannot fully 
determine how it will be used downstream. Developers may integrate widely 
available models into AI systems without features commonly (but not 
necessarily) found in proprietary systems, including prompt filters and 
monitoring mechanisms. Malicious users, in some cases possibly stymied 
and/or detected by these features, may gravitate towards lax AI systems 
providers or, if they have the capability, to develop applications with widely 
available models. In some cases, however, features common in proprietary AI 
systems face challenges in detecting or preventing malicious use, including in 
the generation of misinformation and software code intended for malicious 
ends. Thus, evidence-based, harm-specific analyses of malicious-use 
scenarios are warranted.   
 
Consider malicious actors in three categories: state actors, non-state actors, 
and individuals. State actors have the capability to recreate powerful 
foundation models from scratch today.20 The demonstration of closed 
capabilities, absent any architectural detail, may well be sufficient to stimulate 
free discussion of possible methods for achieving said capabilities and enable 
well-resourced actors including state actors to create similar models.21   
 
Non-state actors and individuals may not be able to recreate such models 
directly, and thus widely available models may be counterfactually useful. 
Evidence-based, harm-specific marginal risk assessments should consider the 
full chain of malicious actions required for these actors to do harm.22 AI 
systems that complement models with other components can demonstrate 
greater capabilities along the malicious activity chain and warrant evaluation in 
their own right (See Question 5). 
 
In practice, individuals using AI for their own ends, including possible 
malicious use enabled by widely available models, supports societal 
adaptation and resilience. The history of open source software suggests that 
 
20 For example, government-supported efforts in the UAE and China have trained and publicly shared 
100+ billion parameter models.   
21 On idea hazards, see Nick Bostrom, “Information Hazards: A Typology of Potential Harms from 
Knowledge,” Review of Contemporary Philosophy, Vol. 10, 2011, https://nickbostrom.com/information-
hazards.pdf.  
22 Information provision or knowledge creation that may be facilitated by AI models does not cause harm 
directly. The malicious activity chain may be shorter or longer by particular harm, e.g., cybersecurity does 
not face a cyber/physical barrier in the way that weapons manufacture does.  
 
 
8 
individual (malicious) use will be larger in number and of high variance in effort 
and effectiveness, in contrast to those from highly motivated and resourced 
actors. State actors have a harder time hoarding vulnerabilities to create 
targeted software attacks, for example, when numerous security researchers 
identify vulnerabilities and lead maintainers to issue patches. Similar 
arguments extend to epistemic security23 and other malicious-use scenarios. 
Societal resilience requires a harm-reduction perspective, not an attempt at 
security through obscurity. 
 
Imposing restrictions on widely available model weights in one jurisdiction may 
yield unintended or counterproductive effects. Restrictions on release will not 
prevent malicious use of models developed or released elsewhere. 
Cybercrime and cyber-enabled fraud, for example, are widely recognized as 
transnational, with the malicious actors located in different jurisdictions than 
their victims.24 Restrictions will, however, limit the lawful use of such models, 
harming national economic competitiveness and additional benefits outlined 
below in Question 3. Furthermore, restrictions may undermine societal 
resilience over time: diffusion of models across society supports public 
education and incentives for protective measures, ultimately increasing 
resilience to risks posed by malicious use of AI systems. 
 
Question 3: What are the benefits of foundation models with model weights that are  
    widely available as compared to fully closed models?  
 
Summary and recommendations: 
1. Although widely available model weights pose similar risks, the subset of 
models that are available open source bring additional benefits.  
2. These benefits include innovation, market competition, and diffusion of AI 
across the economy; support for AI development and safety; use of AI in 
research across disciplines; developer education; and government use.  
 
Widely available AI models, specifically those that are available under open 
source licenses, present notable benefits for U.S. economic dynamism, AI 
safety, and human rights. We use “openly available” to refer to these models 
in particular.  
 
Openly available AI models support innovation, market competition, and 
diffusion of AI across the economy. The wide availability of models may 
 
23 Elizabeth Seger, et al., “Tackling Threats to Informed Decision-Making in Democratic Societies,” The 
Alan Turing Institute, October, 2020, https://www.turing.ac.uk/sites/default/files/2020-10/epistemic-
security-report_final.pdf.  
24 Isabella Wilkinson, “What Is the UN Cybercrime Treaty and Why Does it Matter?” Chatham House, 
August 2, 2023, https://www.chathamhouse.org/2023/08/what-un-cybercrime-treaty-and-why-does-it-
matter.  
 
 
9 
commoditize particular AI capabilities, driving down the cost associated with 
running AI models within an application as proprietary models-as-a-service 
face increased competition. Openly available models present additional 
options to organizations of all kinds as they evaluate a build-buy decision, 
enabling choices that separate model provider from infrastructure host, permit 
the fine-tuning or other direct modification of the model, and, if the training 
code and sufficient detail of the training data is provided, re-train a model from 
scratch. Open source and widely available models have enabled extensibility 
innovations that reduce the hardware required to run inference, and enable 
further training on private or otherwise sensitive data that may not be shared 
with third-parties. The result means more competition in the AI market.25 It 
also supports the diffusion of AI into all sectors, including regulated industries, 
government use, and niche cases for which markets may not adequately 
provide.   
 
Open source and widely available AI models support research on AI 
development and safety, as well as the use of AI tools in research across 
disciplines. To-date, researchers have credited these models with supporting 
work to advance the interpretability, safety, and security of AI models26; to 
advance the efficiency of AI models enabling them to use less resources and 
run on more accessible hardware27; and to advance participatory, community-
based ways of building and governing AI.28 Various kinds of AI models have 
been identified as holding promise to advance scientific research as well as 
academic scholarship broadly.29 In order for such research to be reproducible, 
models and software used must be accessible to scholars and access must 
be assured over time.30  
 
 
25 Bureau of Competition and Office of Technology, “Generative AI raises competition concerns,” Federal 
Trade Commission, June 29, 2023, https://www.ftc.gov/policy/advocacy-research/tech-at-
ftc/2023/06/generative-ai-raises-competition-concerns. 
26 Andrew Critch, “My followers might hate this idea, but I have to say it: There's a bunch of excellent LLM 
interpretability work coming out from AI safety folks (links below...,” X, October 4, 2023, 
https://twitter.com/AndrewCritchPhD/status/1709690861003694418; Beren Millidge, “Open Source AI 
Has Been Vital For Alignment,” Beren’s Blog, November, 5, 2023, https://www.beren.io/2023-11-05-
Open-source-AI-has-been-vital-for-alignment/.   
27 E.g., Tim Dettmers, et al., “QLoRA: Efficient Finetuning of Quantized LLMs,” arXiv, May 223, 2023, 
https://arxiv.org/abs/2305.14314 and its associated GitHub repository.  
28 E.g., the BigScience Project. 
29 OECD, “Artificial Intelligence in Science: Challenges, Opportunities and the Future of Research,” 2023, 
https://www.oecd.org/publications/artificial-intelligence-in-science-a8d820bd-en.htm; Anton Korinek, 
“Language Models and Cognitive Automation for Economic Research,” NBER, February, 2023, 
https://www.nber.org/papers/w30957.  
30 Sayash Kapoor and Arvind Narayanan, “OpenAI’s Policies Hinder Reproducible Research on 
Language Models,” AI Snake Oil, March 22, 2023, https://www.aisnakeoil.com/p/openais-policies-hinder-
reproducible.  
 
 
10 
Openly available AI models and the open source communities that build, 
maintain, and extend them support developer education. Open source 
supports self-learning, lowers intellectual property barriers to education of all 
kinds, and enables learning-by-doing as developers make direct contributions 
to projects and communities. An expanded developer base, particularly 
outside of a small set of companies located in a few major tech hubs, supports 
diversity of identity and perspective in the ecosystem. The expanded 
developer base also means that departments at all levels of government can 
have an easier time locating talent to build regulatory capacity.   
Openly available AI models can support government use of the technology. AI 
systems hold promise for innumerable public-interest applications, and some 
scholars have called for government investment in a public, open option.31 
The Federal government has an open source code policy dating back to 2016 
that seeks to increase the use of open source software in custom-developed 
software projects for the government.32 Openly available models can support 
this policy and its objectives of enabling software reuse and in doing so 
reducing costs to the American taxpayer. Additionally, openly available 
models can be further modified on sensitive data that may not be able to be 
provided outside of government.   
Openly available AI models can support U.S. foreign policy goals, building on 
the track record of open source software.33 Openly available models present 
challenges for the censorship activities of foreign adversaries, as they can be 
shared outside the context of web domains that may be effectively restricted. 
Such models could provide consolidated knowledge repositories to further 
people’s right to information around the world under Article 19 of the Universal 
Declaration of Human Rights.34 The innovation benefits of open models noted 
above also support U.S. national competitiveness.   
 
 
 
 
 
 
 
 
 
31 Bruce Schneier, “Build AI by the People, for the People,” Foreign Policy, June 12, 2023, 
https://foreignpolicy.com/2023/06/12/ai-regulation-technology-us-china-eu-governance/.  
32 Department of Commerce, Source Code Policy, https://www.commerce.gov/about/policies/source-
code.  
33 Consider, for example, the Open Technology Fund and its history. 
34 United Nations, Universal Declaration of Human Rights, https://www.un.org/en/about-us/universal-
declaration-of-human-rights.     
 
 
11 
Question 4: Are there other relevant components of open foundation models that, if  
simultaneously widely available, would change the risks or benefits  
presented by widely available model weights? If so, please list them and 
explain their impact.  
 
Summary and recommendations: 
1. Two categories of additional components warrant attention: (1) 
components used to create and document model weights and (2) 
components of AI systems, of which models are but one.  
2. Greater openness in model components supports benefits and reduces 
risks.  
Additional components are useful to understand and recreate widely available 
model weights. Training an AI model relies on data, software code, and 
compute, and may be described in complementary components including 
shared notebooks or an academic paper. Models and their training data may 
be documented in model cards and datasheets for datasets, among other 
methods. As outlined in Question 1, there are multiple community 
perspectives on levels of openness.35 Providing additional components, 
including the training code as open source software, documentation, and/or 
open data, encourages the benefits of widely available model weights. With 
greater openness, developers can more readily scrutinize, modify, or retrain 
the model, supporting economic dynamism and research. Greater openness 
reduces risks from reckless use and promotes human rights, as AI system 
providers can make better informed decisions on use cases, users make 
better-informed adoption decisions, and advocates or those subject to system 
outputs can better contest use cases. 
 
Models are but one component of an AI system. To realize any benefit or risk, 
models must be put into service via an AI system. Additional software 
components in an AI system impact both risks and benefits posed by its use, 
regardless of whether the model is openly available or proprietary. Safety 
filters on prompts and outputs are common practice today. Orchestration or 
scaffolding frameworks are an increasing focus of research and development. 
Many of these safety and capability features are shared as open source 
software.36 Evaluating models, or indeed other components including software 
 
35 Open Source Initiative, “The Open Source AI Definition - draft v. 0.0.6,” 
https://opensource.org/deepdive/drafts/the-open-source-ai-definition-draft-v-0-0-6; Matt White, et al., “The 
Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency 
and Usability in AI,” arXiv, March 21, 2024, https://arxiv.org/abs/2403.13784; Heather Meeker, “Toward 
an Open Weights Definition,” Copyleft Currents, June 8, 2023, 
https://heathermeeker.com/2023/06/08/toward-an-open-weights-definition/. 
36 Task Force for a Trustworthy Future Web, “Annex 2: Scaling Trust on the Web: Building Open Trust 
and Safety Tools,” Atlantic Council, June, 2023, https://www.atlanticcouncil.org/in-depth-research-
reports/report/scaling-trust_annex2/; see GitHub projects under agents and autonomous-agents topics.  
 
 
12 
code, in isolation risks missing risks and benefits posed by new categories of 
AI systems.  
  
 Question 5: What are the safety-related or broader technical issues in managing risks  
     and amplifying benefits of dual-use foundation models with widely available      
    model weights? 
 
Summary and recommendations: 
1. AISI and other stakeholders should support openly available evaluation 
suites to enable wider testing of dual-use risks.  
2. Evaluations should assess models not in isolation, but as integrated into 
AI systems.  
3. Stakeholders including NSF, NIST, AISI, and OSTP should support 
needed research directions.  
 
To manage the risks and amplify benefits of AI models, including those with 
widely available model weights, we need better evaluations. Leading AI labs 
are investing in evaluation science and proprietary evaluations. Although open 
evaluation suites are increasingly available for some benchmarks, these are 
primarily for capability measures. Methods of evaluating dual-use risks that 
motivate government concern are not openly available today. To support 
broader use of evaluations in the community, we need openly available 
evaluation suites.  
 
The performance of AI models can be altered after training, via direct 
modification, including fine-tuning, and by integration into AI systems, 
including with orchestration software that supports tool use.37 While direct 
modification requires access to model weights, closed AI models-as-a-service 
as well as widely available models can be integrated into broader systems to 
serve specific ends. In practice, this raises the need for system evaluations—
not simply model-level evaluations—and offers a warning against focusing too 
narrowly on fine-tuning away safeguards as the risk to prevent. 
  
More research is needed on numerous fronts. Below we outline several 
important directions. Across these research directions, norms of open science 
and open source can accelerate needed discoveries.   
 
 
37 Matei Zaharia, et al., “The Shift from Models to Compound AI Systems.” Berkeley Artificial Intelligence 
Research, February 18, 2024, https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/; Tom 
Davidson, et al., “AI Capabilities Can Be Significantly Improved Without Extensive Retraining,” arXiv, 
https://arxiv.org/pdf/2312.07413.pdf; Megan Kinniment, et al., “Evaluating Large-Model Agents on 
Realistic Autonomous Tasks,” arXiv, Janurary 4, 2024, https://arxiv.org/pdf/2312.11671.pdf.  
 
 
13 
• 
Interpretability science is nascent and cannot explain how large 
foundation models produce specific outputs.38 
• 
There is some indication that so-called emergent capabilities are a 
function of discontinuous evaluation benchmarks, pointing to the need 
for better, continuous capability benchmarks.39 
• 
Evaluations of models and systems for malicious use risks are 
understudied relative to capability benchmarks and specific risks 
including loss of control and persuasion.40 Evaluations of autonomous 
capabilities of systems is even more nascent.41  
• 
More research for data governance to reduce malicious use risks is 
warranted; at least one prominent paper points to the promise of 
restricting data leading to safer models.42 
• 
Methods of restricting downstream modification of model weights to 
remove protections should be further explored.43 
• 
Methods of assurance for model builds, their provenance, and 
evaluation outcomes should be further developed.44 
 
 Question 6: What are the legal or business issues or effects related to open foundation  
     models? 
 
Summary and recommendations: 
1. Open source software provides a useful analogy for the ecosystem that 
may emerge with widely available models.    
2. License terms that reduce friction to sharing have enabled wide reach and 
societal benefit from open source software.  
 
Widely available AI models, specifically those that are available open source 
(“openly available”), present opportunities for integration and use across the 
 
38 Neel Nanda, et al., “Progress Measures for Grokking via Mechanistic Interpretability,” arXiv, October 
19, 2023, https://arxiv.org/abs/2301.05217; Steven Bills, et al. “Language Models Can Explain Neurons in 
Language Models,” OpenAI, May 9, 2023, https://openaipublic.blob.core.windows.net/neuron-
explainer/paper/index.html; Anthropic, “Transformer Circuits Thread,” March 2024, https://transformer-
circuits.pub/.  
39 Rylan Schaeffer, et al., “Are Emergent Abilities of Large Language Models a Mirage?” arXiv, May 22, 
2023, https://arxiv.org/abs/2304.15004.  
40 Megan Kinniment, et al., “Evaluating Language-Model Agents on Realistic Autonomous Tasks,” arXiv, 
January 4, 2024, https://arxiv.org/abs/2312.11671; Mary Phuong, et al., “Evaluating Frontier Models for 
Dangerous Capabilities,” arXiv, March 20, 2023, https://arxiv.org/abs/2403.13793.    
41 See METR’s Autonomy Evaluation Resources. 
42 Lukas Berglund, et al., “The Reversal Curse: LLMs trained on ‘A is B’ fail to learn ‘B is A’,” arXiv, 
September 22, 2023, https://arxiv.org/abs/2309.12288. 
43 Peter Henderson, et al., “Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of 
Foundation Models,” arXiv, August 9, 2023, https://arxiv.org/abs/2211.14946.  
44 Mithril Security, “AI Cert: Open-source tool to trace AI model’s provenance,” 
https://www.mithrilsecurity.io/aicert; Tobin South, et al., “Verifiable Evaluations of Machine Learning 
Models Using zkSNARKS,” arXiv, February 5, 2024, https://arxiv.org/pdf/2402.02675.pdf.     
 
 
14 
economy. Open source software provides a useful analogy to understand how 
the ecosystem may evolve and how industries and governments alike may 
adopt openly available AI components. Licensing restrictions that contravene 
the open source definition may limit these benefits, and restrictive licensing 
terms present enforcement challenges in practice. Ultimately, public policy 
rather than copyright licenses will govern the responsible use of AI systems. 
 
Open source software provides analogies and lessons for the emerging 
ecosystem of openly available AI models. Once software is written, it can be 
copied at zero marginal cost, as can open source AI systems. The point of 
open source software, as well as openly available AI models, is to remove 
barriers to sharing this zero-marginal-cost good, empowering developers to 
improve and build upon the software. When software code is compiled into a 
binary or otherwise packaged for distribution to end users in a form that can 
be executed on appropriate hardware, non-developers gain access to the 
software’s functionality. Likewise, once a model is trained, developers can use 
pre-trained model weights to easily gain access to the model’s capabilities by 
running inference on appropriate hardware. In this way, model weights are 
akin to a compiled software library. 
 
Open source is wildly successful. Today, 96% of software contains open 
source components, and a given software stack is 77% open source 
software.45 It is widely used across government and industries,46 with 99% of 
the Fortune 500 using open source software,47 and open source adoption is a 
key innovation differentiator between firms.48 However, the broader societal 
benefits of this ecosystem are challenging to measure.49 Researchers from 
the Bureau of Economic Analysis, National Science Foundation (NSF), and 
elsewhere have estimated that investment in open source development 
contributes roughly $38 billion to U.S. GDP.50 However, GDP measures 
expenditures, and thus does not account for the unique benefits from open 
 
45 Synopsys, “2024 Open Source Security and Risk Analysis Report,” February, 2024, 
https://www.synopsys.com/software-integrity/engage/ossra/ossra-report, p.4. 
46 Ibid., p.5; General Services Administration, “Open Source,” Digital.gov, https://digital.gov/topics/open-
source/.  
47 Pranay Ahlawat, et al., “Why You Need an Open Source Software Strategy,” BCG, April 16, 2021, 
https://www.bcg.com/publications/2021/open-source-software-strategy-benefits.    
48 Shivam Srivastava, et al., “Developer Velocity: How Software Excellence Fuels Business Performance,” 
McKinsey & Company, April 20, 2020, https://www.mckinsey.com/industries/technology-media-and-
telecommunications/our-insights/developer-velocity-how-software-excellence-fuels-business-
performance.  
49 Peter Cihon, “Open Source Creates Value, But How Do You Measure It?” GitHub Blog, January 20, 
2022, https://github.blog/2022-01-20-open-source-creates-value-but-how-do-you-measure-it/.  
50 Gizem Korkmaz et al., “From GitHub to GDP: A Framework For Measuring Open Source Software 
Innovation,” Research Policy 53, no. 3 (April 1, 2024): 
104954, https://www.sciencedirect.com/science/article/pii/S0048733324000039.  
 
 
15 
source software, namely its frictionless, zero-marginal-cost reuse. One recent 
study attempted to fill this gap by measuring the demand-side value of open 
source, and estimated it to be $8.8 trillion.51  
 
The open source ecosystem, its wide reach, and large societal benefit, has 
been enabled by clear licensing that permits anyone to read, modify, 
(re)distribute, and use the software for any purpose. To enable frictionless 
sharing, these licenses disclaim liability and warranty for the freely offered 
software code. Since its founding in 1998, the Open Source Initiative has 
maintained a list of approved licenses, supporting their widespread 
understanding and adoption. 
 
Recognizing that a public good should be used for good, some developers 
have experimented with not-open source “ethical use” restrictions in software 
licenses. Because these terms are often ambiguous, cause friction, and have 
conflicts between different “ethical use” terms, software under these terms has 
not gained widespread adoption.52 AI researchers have rediscovered this type 
of ethical licensing with the RAIL family of licenses. However, they face the 
same challenges: third parties are not well suited and may not be able to 
directly enforce license terms that restrict use, the terms can conflict with other 
“ethical use” terms, and developers may not have the resources or motivation 
to enforce such terms.53 The aim of these sorts of terms is admirable: they try 
to mitigate harmful use of technology. However, because of conflicts between 
terms and potentially differing developer interpretations, it is hard to build a 
frictionless open innovation ecosystem on these terms. Clarity in law, not 
ambiguity in license terms, supports a vibrant innovation ecosystem. 
 
Another trend in public licensing is to try to “capture” open source innovation 
by allowing customers and developers to use your software under open 
source-like terms, but to forbid the use of your software by competitors.54 In 
this way, companies try to “own” the open source ecosystem by capturing the 
economic upside of the open source innovation cycle. AI developers have 
followed this “own the ecosystem” suggestion,55 with licenses on some 
 
51 Manuel Hoffmann, et al., “The Value of Open Source Software,” Social Science Research Network, 
January 1, 2024, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4693148.  
52 E.g., Stephen Shankland, “‘Don’t-be-evil’ Google Spurns No-Evil Software,” CNET, December 28, 
2009, https://www.cnet.com/culture/dont-be-evil-google-spurns-no-evil-software/.  
53 E.g., Robert Gorwa and Michael Veale, “Moderating Model Marketplaces: Platform Governance 
Puzzles for AI Intermediaries,” arXiv, February 15, 2024, https://arxiv.org/abs/2311.12573.  
54 E.g., Armon Dadgar, “HashiCorp Adopts Business Source License,” HashiCorp, August 10, 2023, 
https://www.hashicorp.com/blog/hashicorp-adopts-business-source-license.  
55 Dylan Patel and Afzal Ahmad, “Google ‘We Have No Moat, and Neither Does OpenAI’,” Semianalysis, 
May 4, 2023, https://www.semianalysis.com/p/google-we-have-no-moat-and-neither.  
 
 
16 
popular models designed to permit community modification while inhibiting 
certain types of commercial use or use by competitors.56 
 
Such ethical and economic upside restrictions do not affect malicious-use 
proclivities, but do limit the competitive and other benefits of frictionless zero-
marginal-cost copying of software and models.57 For this reason among 
others, regulation has singled out open source software, in contrast to other 
software licenses, for special consideration. In recent EU policymaking, this 
has been seen across the AI Act, Cyber Resilience Act, and Product Liability 
Directive. With these files, EU policymakers distinguished between the 
development and supply phases, and focus regulatory scrutiny on supply, 
particularly monetized or other commercial provision and the high-risk use of 
open source software and AI systems.  
 
Question 7: What are current or potential voluntary, domestic regulatory, and  
international mechanisms to manage the risks and maximize the benefits of 
foundation models with widely available weights? What kind of entities 
should take a leadership role across which features of governance?  
 
Summary and recommendations: 
1. Existing best practices for documentation mitigate reckless-use risks and 
have been codified into EU law. 
2. An emerging transnational testing regime can support evidence-gathering 
for malicious-use risks. This can be complemented by ecosystem 
monitoring. 
3. Malicious-use risks are a function of the number of possible users. 
Applications that enable easy inference warrant attention proportionate 
with their lowering of expertise barriers to misuse. 
4. GitHub enables developers to share AI models and other software 
components for further development, in accordance with law and our 
Acceptable Use Policies. 
5. Specific government bodies and other stakeholders can take important 
steps today to provide clarity on openly available AI models, improve 
evaluation science, and support a trusted AI value chain. 
Developers around the world collaborate on modifying, integrating, and using 
widely available AI models, particularly those available under open source 
licenses, in an emerging, decentralized innovation ecosystem that will help 
maximize the benefits of these technologies. Established best practices for 
 
56 Technology Innovation Institute, “Terms and Conditions: Falcon 180B TII License Version 1.0,” 
September 2023, https://falconllm.tii.ae/terms-and-conditions.html; Meta, “Llama 2 Community License,” 
July 18, 2023, https://ai.meta.com/llama/license/.  
57 Bureau of Competition and Office of Technology, “Generative AI raises competition concerns,” Federal 
Trade Commission, June 29, 2023, https://www.ftc.gov/policy/advocacy-research/tech-at-
ftc/2023/06/generative-ai-raises-competition-concerns.  
 
 
17 
documentation, namely model cards and thorough dataset reporting, reduce 
risks of reckless use by informing downstream providers and deployers of the 
capabilities of the model and out-of-scope use cases. Thus informed, 
downstream providers and deployers can integrate widely available models, 
particularly those available open source, into AI systems, products, and 
workflows as they do other open source software components. The 
forthcoming EU AI Act encourages or requires such documentation, 
dependent on how the model is deployed and its size.  
Today, malicious-use risks are informed by an emerging transnational testing 
regime, which will prove instrumental in establishing whether there is credible 
evidence to warrant further policy measures on the open release of AI models, 
their downstream integration into applications, and criminal use. Given the 
benefits of open release, absent specific evidence of model-level risks, risks 
are best addressed through policy focused on integrated AI systems and their 
use. Reflecting practices of open source innovation and regulation, upstream 
developers providing software components as a public good should be 
supported to adopt best practices, while downstream integrators seeking to 
adapt or otherwise privatize public goods for specific ends should face 
heightened expectations in safely and responsibly providing products that are 
directly usable.  
The emerging transnational testing regime reflects voluntary commitments 
from model developers, forthcoming regulatory requirements in the EU AI Act, 
and community best practices. All three involve pre-release evaluations to 
detect capabilities of concern, including support for malicious use, which may 
inform voluntary decisions to restrict the public release of a particular AI 
model.58 The U.S. should prioritize coordination with EU and UK policymakers 
as well as other AI Safety Institutes on information sharing, advancing 
evaluation infrastructure, and consolidating best practices.   
Model evaluations alone are insufficient for malicious-use analysis; as 
described in Question 5, system-level evaluations are a necessary 
complement. Ecosystem monitoring is another important complement, to 
understand discovery mechanisms by which models become widely available. 
These range from social media platforms with viral sharing, integrated use in 
applications that are widely adopted (perhaps via app stores), and use of 
models as dependencies of software projects developed on platforms like 
 
58 Voluntary commitments from model developers and forthcoming EU AI Act model evaluation 
requirements focus narrowly on the largest, most capable AI models or those that may otherwise 
be designated as posing systemic risks. Both Executive Order 14110 (4.2(C)) and the forthcoming EU AI 
Act require advanced notice of training for such models (Art 52(1)), as well as the 
documentation of any evaluations on such models and express sharing in the case of the EO. Voluntary 
commitments support similar evaluation-sharing with the UK government. 
 
 
18 
GitHub. Additional data from real-world use will also be useful, particularly to 
understand how closed AI may already be misused. Monitoring AI 
developments and societal impacts over time can improve risk assessments 
and policymaking.   
While widely available AI model weights may frustrate government restrictions 
on the availability of AI capabilities to specific entities,59 it does not prevent 
restrictions on harmful use of downstream applications and services. 
Malicious-use risk is, in part, a function of the number of people able to 
develop and deploy applications that use a model. Current scarcity of machine 
learning expertise moderates the risk, by narrowing the population of 
developers who have the knowledge and resources to fine-tune or otherwise 
modify an openly available AI model towards malicious ends. A larger 
population of developers have the basic programming skills and resources 
required to run inference with openly available models. In both cases, 
however, these populations pale in comparison to the broader population 
using end-user applications. Model inference applications and services, 
particularly those that enable deepfake generation—regardless of the type of 
model they may use—warrant close policy scrutiny proportionate with risks 
posed by their lowering barriers to (mis)use. 
GitHub hosts content that meets standards set forth by law and our 
Acceptable Use Policies (AUPs). GitHub does not make openly available 
models directly usable for inference. 100+ million developers around the world 
use GitHub to collaborate on and share software, including software at every 
layer of the AI stack and AI models in particular. Content posted to GitHub 
must be lawful, and is governed by our AUPs.60 Our AUPs permit dual-use 
content, supporting its use for research and education. However, in cases of 
abuse of or malicious intent in such dual-use content, GitHub uses a range of 
tools to restrict access to the specific content on the platform.61 GitHub 
periodically reassesses our policies, and offers developers the opportunity to 
 
59 Given that publicly available open source software is not subject to export controls. Steve Winslow, et 
al., “Understanding US Export Controls With Open Source Projects,” Linux Foundation, July 2021, 
https://www.linuxfoundation.org/resources/publications/understanding-us-export-controls-with-open-
source-projects.  
60 GitHub’s AUPs prohibit activity and content that is sexually obscene; libelous, defamatory, or 
fraudulent; discriminatory or abusive; false, inaccurate, or intentionally deceptive and is likely to harm the 
public interest (including election integrity); harasses or abuses; or threatens violence or glorifies 
violence. GitHub, “GitHub Acceptable Use Policies,” GitHub Docs, https://docs.github.com/en/site-
policy/acceptable-use-policies/github-acceptable-use-policies#2-user-safety.   
61 GitHub, “GitHub Active Malware or Exploits,” GitHub Docs, https://docs.github.com/en/site-
policy/acceptable-use-policies/github-active-malware-or-exploits.  
 
 
19 
comment on any proposed changes.62 Our AUPs and dual-use policies would 
not permit developers to host an openly available AI model that had been fine-
tuned or otherwise modified for malicious ends.  
Government can take important steps today to provide clarity on openly 
available AI models, improve evaluation science, and support a trusted AI 
value chain. To provide clarity, NIST and the AISI should publish and 
iteratively update guidance on metrics for risk assessment and provide clarity 
on what categories and performance thresholds may weigh against open 
release.63 NTIA and Bureau of Industry and Security (BIS) should align on 
definitions to ensure that openly available models meet the “published” 
definition under the Export Administration Regulations to avoid unintended 
export control impacts. To advance the state of the art and use of AI safety 
research, NSF should prioritize funding research for AI interpretability, 
evaluations, and durable model-level safety interventions. NIST and AISI 
should support open source software evaluation suites to enable all model 
developers to evaluate models for capabilities of concern prior to their release. 
AISI and the National Artificial Intelligence Research Resource (NAIRR) 
should support less-resourced actors to perform evaluations. More broadly, 
government can support the refinement and incentivize adoption of 
responsible best practices in the AI value chain. These include model 
documentation and trusted model builds.64 
Model developers, academia, civil society, and other stakeholders should lead 
further refinement of best practices. The Partnership on AI is working to 
update its Deployment Guidance for Foundation Model Safety65 to support 
model developers in making open release decisions, and could similarly 
provide metrics and clarity on risks. Once released, a model cannot be fully 
recalled. However, harm can be reduced, by developing procedures to notify 
the recall of models. Model developers, academia, and philanthropies can 
accelerate efforts by open sourcing existing and new evaluation suites for 
 
62 E.g., Mike Hanley, “Updates to Our Policies Regarding Exploits, Malware, and Vulnerability Research,” 
GitHub Blog, June 4, 2021, https://github.blog/2021-06-04-updates-to-our-policies-regarding-exploits-
malware-and-vulnerability-research/. 
63 As part of ongoing activities directed by EO 14110 (4.1)(i)(C) “launching an initiative to create guidance 
and benchmarks for evaluating and auditing AI capabilities, with a focus on capabilities through which AI 
could cause harm, such as in the areas of cybersecurity and biosecurity.” 
64 See footnotes 18 and 44. 
65 Partnership on AI, “PAI’s Guidance for Safe Foundation Model Deployment,” 
https://partnershiponai.org/modeldeployment/.  
 
 
20 
capabilities of concern.66 Further steps still may be needed to address costs of 
evaluation for less resourced actors.67 
Question 8: In the face of continually changing technology, and given unforeseen risks  
and benefits, how can governments, companies, and individuals make 
decisions or plans today about open foundation models that will be useful in 
the future? 
  
Summary and recommendations: 
1. Government and other stakeholders should make plans today to 
strengthen societal resilience to increasing AI capabilities. 
2. Ecosystem monitoring can support detection of novel uses of AI that pose 
unforeseen risks and benefits. 
3. Evaluations of new paradigm-shifting AI systems warrant attention prior to 
wide release. 
Governments, companies, and individuals can make plans today with an 
understanding that increasing AI capabilities, whether made available in 
closed or available forms, will present increasing risks and benefits, including 
ones that may not be foreseen. As such, societal resilience and specifically 
cybersecurity should be invested in, even more than might be dictated by risks 
that are anticipated today. Securing the open source ecosystem and 
promoting secure by design68 as a principle for all software, including AI, 
should be accelerated.69 Government can demonstrate leadership in the 
defensive use of AI and take further non-AI defensive measures, including 
those directed in EO 14110.70  
Open source has promoted competition across the IT industry, giving 
developers more opportunity to transfer skills and experience across 
employers, lowering the barriers for entrepreneurs to compete at every layer 
of the stack, and facilitating national competitiveness. Openly available AI 
 
66 To-date, leading model developers have open sourced some evaluation suites, but not the very 
evaluations that are used to justify possible need for restricting openly available AI models. 
67 See, e.g., Nathan Lambert, “Evaluations: Trust, Performance, and Price,” Interconnects, March 20, 
2024, https://www.interconnects.ai/i/142801100/the-rising-price-of-evaluation.  
68 Mike Linksvayer, “GitHub Response to the Office of the National Cyber Director Request for Information 
on Open-Source Software Security: Areas of Long-Term Focus and Prioritization,” Regulations.gov, 
November 8, 2023, https://www.regulations.gov/comment/ONCD-2023-0002-0084.  
69 Mike Linksvayer, “GitHub Response to CISA Request for Information on Shifting the Balance of 
Cybersecurity Risk: Principles and Approaches for Secure by Design Software,” Regulations.gov, 
February, 20, 2024, https://www.regulations.gov/comment/CISA-2023-0027-0080.  
70 Including building upon the DHS and DoD pilot projects for detecting and remediating vulnerabilities at 
scale in critical government software and screening release of sensitive data and customers of nucleic 
acid synthesis services. 
 
 
21 
models and other open source elements of the AI stack are continuing and 
accelerating each of these factors over time. 
As innovation in AI models, systems, and use cases continues, stakeholders 
should invest in better understanding the risks and impacts. Government has 
a leading role to resource and mature interpretability research, evaluation 
science, and monitoring capacity, and should support stakeholders from 
companies, academia, and civil society, to accelerate work in these areas. 
Multiple stakeholders should prioritize monitoring the innovation ecosystem for 
downstream uses of widely available models and uses of models generally 
that enhance their capabilities following pre-training. GitHub today contributes 
data on AI-related development activity to the OECD and Stanford AI Index, 
among other entities, and could endeavor to support AISI, other government, 
or key multi-stakeholder initiatives. Multiple efforts are underway to document 
“AI incidents,” which ought to record, when known, the specific model 
involved.71 Governments should monitor malicious use, namely by specifying 
and collecting crime statistics, to inform the extent to which models vs. 
systems, widely available weights vs. closed models warrant particular 
concern. Among other activities, considering and observing leading indicators 
of use may be useful. Market incentives drive individuals and small groups to 
use AI systems to drastically scale up their impact. Thus, market monitoring of 
AI use, including via existing regulatory mechanisms, may provide leading 
indicators of malicious-use risks.  
Evaluation of the benefits and risks of new categories of models warrants 
specific consideration. Text-to-image models are now ubiquitous as 
proprietary solutions, widely available model weights, and applications using 
both. In hindsight, it is unclear if a rubicon was crossed with the development 
of diffusion-based models, given prior generative-adversarial-network 
techniques published openly for years. Better understanding is needed to 
evaluate the extent to which the public release of possibly paradigm-shifting 
models presents unacceptable risks relative to their benefits, considered 
marginally to similar closed capabilities. Stakeholders may consider using 
staged-release72 methods prior to releasing model weights publicly, in order to 
gather more information about new paradigm-shifting models. 
 
71 Namely, the AI Incident Database and OECD AI Incidents Monitor.  
72 Staged release is not a unified concept: it can take many forms depending on the development context, 
intermediate deployment scenario, and specific goals. For example, one form may see use restricted to 
model-as-a-service provision only whereas another may share full access to model weights with a set of 
individuals. 
 
 
22 
Today, available evidence of the marginal risks of public release does not 
substantiate restrictions on current AI model paradigms. Instead, government 
should prioritize AI regulation against reckless use and consider criminal 
justice policies and emergency national security plans for malicious use. 
. . . 
Thank you for the opportunity to share GitHub’s perspective on widely 
available model weights. We appreciate NTIA’s commitment to a thorough 
consultation to surface evidence and diverse perspectives on the benefits of, 
risks from, and policy mechanisms for widely available AI models. As you 
analyze responses and evaluate next steps pursuant to EO 14110, we stand 
ready to work with you and to answer any further questions that may arise. 
Respectfully submitted, 
Mike Linksvayer 
VP of Developer Policy, GitHub 
mlinksva@github.com 
 
Peter Cihon 
Senior Policy Manager, GitHub 
pcihon@github.com  
",GitHub,13574
NTIA-2023-0009-0270,"March 27, 2024
The Honorable Gina Raimondo
Secretary, Department of Commerce
1401 Constitution Ave. NW
Washington, DC 20230
RE: Openness in AI is critical for independent Responsible AI professionals
Dear Secretary Raimondo,
We, the undersigned AI ethics and governance practitioners collaborating under the banner of 
Ethical Intelligence, write to emphasize several key benefits of openness and transparency in AI 
models of particular importance to the startups and smaller businesses, companies outside the 
tech sector, and specialized innovators that make up most of our collective clients. 
We appreciate the opportunity to provide comments to the National Telecommunications and 
Information Administration’s Openness in AI Request for Comment. We applaud the NTIA’s 
efforts in engaging a broad range of stakeholders on this important topic at this pivotal moment 
in AI changing the way we work and penetrating daily life all over the world. 
Ethical Intelligence (EI) is an independent consulting group providing Responsible AI (RAI) ethics 
and governance services to diverse clients across industries, as well as support the global EI 
network community of RAI experts and practitioners. We leverage our cross-functional, multi-
disciplinary, and international knowledge and expertise to guide the adoption and integration of 
AI responsibly throughout our client companies’ business, culture and governance practices. 
We are closely following the public debates and latest research on the advantages and 
disadvantages of openness in AI regarding cybersecurity, national competitiveness, and 
intellectual property. There are differing opinions within the EI network on these questions - we 
collectively contribute to bringing greater clarity to these issues and intend to continue doing so 
as the norms take shape and are standardized. 
These comments are focused rather on the societal implications relating to development of a 
more level playing field for innovation in the AI ecosystem; the extensibility of AI systems for 
diversity and specialization; and the impartiality of AI ethics and governance oversight. Structural 
policy enabling greater access to knowledge is critical for the last mile of RAI implementation 
that constitutes our work. 
In this letter, we would like to highlight three aspects in the discussion of openness in AI of 
critical importance to independent RAI professionals and the companies we work with: (1) 
stimulating growth and innovation in small businesses; (2) facilitating customized solutions and diverse 
applications; and (3) enabling the independent evaluation of AI systems. 
1.
Openness in AI stimulates growth in smaller companies and allows for 
innovation in specialized use cases. 
The background document to this request for comment explains that “dual-use foundation 
models with widely available weights could play a role in fostering growth among less resourced 
actors, helping to widely share access to AI’s benefits.” As RAI practitioners working with 
enterprise companies outside of the dominant tech players, we understand the motivation to 
use open foundation models to enhance capabilities and productivity in their respective niche 
markets is one of the primary attractions of integrating AI into the way they do business. The 
customizability of open foundation models for more flexible adaptation methods and ability to 
fine-tune alignment interventions enable more viable down-stream applications.  
1
Building from their existing strengths in the market before the introduction of AI, the promise of 
AI is that it can make their business more efficient and strategic. Greater model access allows 
downstream developers in their own market context to optimize and perfect existing models to 
introduce innovation their own niche businesses, the nuances of which they are deeply familiar, 
instead of having to start from scratch.  Companies tend to adopt multiple different kinds of AI 
2
systems, which even if not initially, will eventually interact with each other as integration efforts 
scale throughout the organization.
2. Openness in AI facilitates the customization of solutions and the 
diversity of applications.
The NTIA request for comment astutely points out that “open foundation models can be 
readily adapted and fine-tuned to specific tasks.” As we have repeatedly learned from our clients, 
some of the greatest value of integrating AI into a business is how those general purpose tools 
can be customized within their particular context and fine-tuned towards specifically tailored 
market solutions. The democratization of AI development with open foundation models widens 
the diversity of the community contributing to the development of new applications on top of 
the original open model architecture, which can then be integrated back into the original 
developer’s ecosystem.
The NTIA background also further elaborates that openness in AI can “make it easier for 
system developers to scrutinize the role foundation models play in larger AI systems, which is 
important for rights- and safety- impacting AI systems (e.g. healthcare, education, housing, 
criminal justice, online platforms etc.)” A significant consideration in our work as independent 
RAI consultants is helping establish processes and other company norms that ensure that their 
tools do not introduce biases and perpetuate the legacy discrimination that undermines public 
trust their company and their industry. In addition to the ethnic, gender, and other sensitive 
 Kapoor, Bommasani, et al., On the Societal Impact of Open Foundation Models, available at 
1
arXiv:2403.07918
 Seger, Dreksler, Moulange, Dardaman, Schuett, Wei, et al, Open-Sourcing Highly Capable Foundation Models: 
2
An Evaluation of Risks, Benefits, and Alternative Methods for Pursuing Open-Source Objectives, Centre for the 
Governance of AI, 2023, available at https://www.governance.ai/research-paper/open-sourcing-highly-
capable-foundation-models.
protected classes that is of paramount concern, the reliability of the AI tools they’ve adopted to 
accurately and fairly account for differences is core to that trust.  Greater access to knowledge 
3
about open foundation models enable more robust adaptations that account for the precision of 
native language and the nuances of culture are essential to cooperative AI ecosystems of 
globalization and the interoperability of AI ethics and governance norms across jurisdictions.   
3. Openness in AI enables the impartial and independent evaluation and 
continuous monitoring of AI systems. 
Of fundamental significance to the professionalization of AI ethics and governance, that EI is 
helping lead, is the impartial ability to evaluate and monitor AI systems once they are deployed. 
External oversight plays a key part of the developing regulatory frameworks from international 
to local levels that have been proposed and that are being enacted. As the request for comment 
background document explains: “foundation models can allow for more transparency and enable 
broader access to allow greater oversight by technical experts, researchers, academics, and 
those from the security community.” 
The differences are vast between the ability to perform meaningful audits along the gradient 
from black-box access - which limit auditors to analyze system outputs - to white box and 
outside-the-box access, which give full access to the system and further information about the 
system’s development and deployment.  Black-box methods are: not well-suited to develop 
4
generalized explanations; prevent system components from being studied separately; can 
produce misleading and unreliable results; and offer limited insights to help address failures. 
White box methods enable: more effective and efficient algorithms; stronger adversarial attacks; 
better assurances with an expanded attack toolbox; interpretability tools that aid in diagnostics; 
and an improved ability to explain specific AI system decisions.
Assurance audits for algorithmic systems are increasingly being included as requirements for 
RAI regulatory compliance. These accountability mechanisms are most reliably and 
systematically established with structural support for greater access to knowledge of AI systems. 
The societal implications for the integrity of these audits are profound, such as confronting the 
discrimination in hiring that algorithmic systems could even further embed into harmful 
employment practices that the New York City Local Law 144 is designed to address. Further 
guidance on ensuring the impartiality of audits - such as public disclosure, standardized audit 
criteria, and independent accountability - will in part depend on the protection of more open 
access to foundation models.5
 Surman, Bdeir, et al, Accelerating Progress Toward Trustworthy AI, Mozilla (Feb 2024), available at https://
3
foundation.mozilla.org/en/research/library/accelerating-progress-toward-trustworthy-ai/whitepaper/
 Casper, Ezell, et al., Black-Box Access is Insufficient for Rigorous AI Audits (Jan 2024), available at https://
4
arxiv.org/abs/2401.14446.
 Lam, Lange, et. al., A Framework for Assurance Audits of Algorithmic Systems (Jan 2024), available at https://
5
arxiv.org/abs/2401.14908
Given the complex implications of AI that we are all only beginning to appreciate and 
understand, this external input is best done by a collaboration of experts trained in different 
disciplines, representing different life experiences, and subject to different jurisdictions. This is 
where EI is uniquely equipped to support companies dynamically in their transition to 
integrating AI into their business. We rely on openness in AI to do our job most effectively.
We thank the NTIA for this chance to contribute our perspective to its deliberations and 
remain available to provide further feedback.
Respectfully submitted, 
Eddan Katz - AI Policy, Ethical Intelligence
Olivia Gambelin - Founder, Ethical Intelligence
Oliver Smith - EI Expert and Founder of Daedalus Futures
Flavio S. Correa da Silva - faculty member, University of Sao Paulo
Helena Ward - DPhil Candidate, University of Oxford
David Barnes - Founder, David Barnes, LLC
Eugene Fedorchenko - EI Expert, Strategist, Founder of IntFinite
Goda Mockute, AI Project Lead, Erasmus University Rotterdam
https://www.ethicalintelligence.co/
",Ethical Intelligence,2042
NTIA-2023-0009-0175,"Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 1
1. How should the NTIA define “open” or “widely available” when thinking about
foundation models and model weights?
a.
Is there evidence or historical examples suggesting that weights of models similar
to currently-closed AI systems will, or will not, likely become widely available?
If so, what are they?
Yes. Typically, there is a delay between the time at which a new AI
system is demonstrated (“demoed”) and the time at which the weights
of a similar AI system are made available. As an example, on OpenAI
released a paper documenting their GPT-3 language model on May
28, 2020 and opened up a (closed) beta for users to interact with it on
June 11, 2020. It took roughly 2 years for weights of models of similar
performance (like Meta’s OPT models:
https://arxiv.org/abs/2205.01068) to be made public, and even then it
was done in a way intended to limit access. Similarly, high-quality
neural speech synthesis software like WaveNet
(https://deepmind.google/technologies/wavenet/) and Tacotron
(https://arxiv.org/abs/1703.10135) were first demoed nearly a decade
ago and closed voice cloning models were developed by academics
several years ago (https://arxiv.org/abs/1711.11293), but only in the
past year have we seen widespread availability & use of open-weight
models for high-quality voice cloning.
There are very few examples of weights from closed AI systems being
leaked and becoming unintentionally widely available. Even some of
the releases reported as “leaks” (like Meta’s original LLaMA models)
were knowingly released to a wide set of researchers without much
restriction so that they could be widely used. The most salient example
of a true leak is Mistral’s “Miqu” model leak
(https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-so
urce-ai-model-nearing-gpt-4-performance/), which when released was
not comparable in quality to alternative models (both open and
closed).
b. Is it possible to generally estimate the timeframe between the deployment of a
closed model and the deployment of an open foundation model of similar
performance on relevant tasks? How do you expect that timeframe to change?
Based on what variables? How do you expect those variables to change in the
coming months and years?
Not generally. I would estimate a normal lag between the first closed
model being demonstrated at a given capability level and the first
open model with similar capabilities to be between 6 months - 4 years.
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 2
For types of foundation models that have widespread appeal—like
chatbots—I would expect the timeframe to be relatively short
(perhaps less than 2 years), whereas for those with narrow
appeal—like nuclear physics—I would expect the timeframe to be
relatively long (measured in several years). It is possible to use
estimates of “algorithmic progress” (like
https://epochai.org/blog/algorithmic-progress-in-language-models)–or
the speed at which the computational cost to reach a particular bar of
performance decreases–to estimate this. But such estimates do not
factor in the growth in spending that may occur. It seems likely to me
that, as the scale of compute required to reach impressive
performance goes up, there will be fewer and fewer actors who can
afford to participate in development.
c.
Should “wide availability” of model weights be defined by level of distribution?
If so, at what level of distribution (e.g., 10,000 entities; 1 million entities; open
publication; etc.) should model weights be presumed to be “widely available”? If
not, how should NTIA define “wide availability?”
No comment.
d. Do certain forms of access to an open foundation model (web applications,
Application Programming Interfaces (API), local hosting, edge deployment)
provide more or less benefit or more or less risk than others? Are these risks
dependent on other details of the system or application enabling access?
i.
Are there promising prospective forms or modes of access that could
strike a more favorable benefit-risk balance? If so, what are they?
In my view, one promising prospective form of access is a
“source-available weights + closed deployment” scheme as Cohere has
done with their Command-R model
(https://txt.cohere.com/command-r/). Cohere released the weights of
the model so that researchers & hobbyists can inspect and evaluate
them, but with licensing that restricts other entities from legally
deploying them. This allows Cohere to manage how and by whom
their models are deployed, whether within their own infrastructure or
via licensing agreements with other trusted organizations. If any
problems with the model are discovered as the model is used in the
real-world, Cohere as a single actor can mitigate them by releasing a
new version, without stifling research on the original model weights.
“Source-available” is a known strategy, with other major projects
such as Epic Games’ Unreal Engine opting for a similar approach.
2. How do the risks associated with making model weights widely available compare to the
risks associated with non-public model weights?
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 3
a.
What, if any, are the risks associated with widely available model weights? How
do these risks change, if at all, when the training data or source code associated
with fine tuning, pretraining, or deploying a model is simultaneously widely
available?
My only comment here is that nearly all of the risks associated with
widely available model weights are downstream of the fact that since
these weights are software/data, they can be copied easily and thus
cannot be systematically recalled in the way that, say, a defective
automobile can be recalled.
b. Could open foundation models reduce equity in rights and safety-impacting AI
systems (e.g. healthcare, education, criminal justice, housing, online platforms,
etc.)?
Other commenters will likely write better on this. No comment.
c.
What, if any, risks related to privacy could result from the wide availability of
model weights?
It is unlikely that all actors training foundation models will be diligent
in removing personally-identifiable information (PII) from the
training datasets used to create their models. This seems especially
unlikely as the compute requirements to produce usable models
decreases over time. Therefore, it seems likely that there will exist
widely-available model weight that contain significant amounts of PII.
If one was actively trying, one could extract this PII from those
models given partial cues. For example, if the training dataset
contained leaked Social Security Numbers (SSNs), it is possible that
when the model is prompted with a few known name + SSN pairs plus
a real leaked name, it may respond with the corresponding leaked
SSN. However, this will be true of both widely available model weights
and closed-weight models. The only way to mitigate this is would look
like requiring some form of redaction of PII in training datasets.
d. Are there novel ways that state or non-state actors could use widely available
model weights to create or exacerbate security risks, including but not limited to
threats to infrastructure, public health, human and civil rights, democracy,
defense, and the economy?
i.
How do these risks compare to those associated with closed models?
Other commenters will likely write better on this. No comment.
ii.
How do these risks compare to those associated with other types of
software systems and information resources?
Other commenters will likely write better on this. No comment.
e.
What, if any, risks could result from differences in access to widely available
models across different jurisdictions?
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 4
Other commenters will likely write better on this. No comment.
f.
Which are the most severe, and which the most likely risks described in
answering the questions above? How do these set of risks relate to each other, if at
all?
Other commenters will likely write better on this. No comment.
3. What are the benefits of foundation models with model weights that are widely available
as compared to fully closed models?
a.
What benefits do open model weights offer for competition and innovation, both
in the AI marketplace and in other areas of the economy? In what ways can open
dual-use foundation models enable or enhance scientific research, as well as
education/training in computer science and related fields?
Other commenters will likely write better on this. No comment.
b. How can making model weights widely available improve the safety, security, and
trustworthiness of AI and the robustness of public preparedness against potential
AI risks?
Widely-available model weights have been a primary enabler for
research on privacy & security, model internals (or
“interpretability”), learning dynamics, ethics, and alignment. Here is
a table containing a non-exhaustative set of examples:
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 5
Date
Impact
Category
Impact Summary
Requires
Weights?
Requires
Data?
Base
Model(s
)
Source Link
Septem
ber 9,
2023
Privacy
and
Security
Yes
No
GPT-2,
Bloom,
Pythia
Reverse-Engineering
Decoding Strategies Given
Blackbox Access to a
Language Generation
System
Novem
ber 18,
2021
Privacy
and
Security
Enabled systematic
evaluation of
linguistic novelty in
text generation,
quantifying the extent
to which language
models copy from
their training data.
This provides insights
into privacy risks
arising from
memorization.
Yes
Yes
GPT-2
How Much Do Language
Models Copy From Their
Training Data? Evaluating
Linguistic Novelty in Text
Generation Using RAVEN
Januar
y 19,
2022
Privacy
and
Security
Examines how GPT-J
can solve CS 101
assignments and
verifies that its not
copying from its
training data
Yes
Yes
GPT-J
Fooling MOSS Detection
with Pretrained Language
Models
Februar
y 10,
2022
Model
Internals
Enabled development
of new method to
locate & edit factual
associations in neural
networks.
Yes
No
GPT-2,
GPT-J
Locating and Editing
Factual Associations in
GPT
Februar
y 15,
2022
Privacy
and
Security
A large scale and
systematic analysis
of memorization in
large language
models.
Yes
Yes
GPT-Ne
o,
GPT-J,
GPT-Ne
oX, T5
Quantifying Memorization
Across Neural Language
Models
Octobe
r 13,
2022
Model
Internals
Enabled development
of more scalable
fact-editing methods
for neural networks,
to adjust 1000s of
associations at once. Yes
No
GPT-J,
GPT-Ne
oX
Mass-Editing Memory in a
Transformer
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 6
March
14,
2023
Model
Internals
Enabled more
precise investigation
and
hypothesis-testing on
how LLMs form
predictions across
their layers.
Yes
Yes
Pythia,
GPT-Ne
o,
BLOOM,
GPT-2,
OPT,
LLaMA
Eliciting Latent Predictions
from Transformers with the
Tuned Lens
March
20,
2023
Ethics
Enabled an analysis
of gender, ethnicity,
and profession
biases in
text-to-image models
by using CLIP
embeddings as a
common
representation space.
Developed novel
techniques
leveraging CLIP to
assist labeling.
Yes
No
Stable
Diffusion
, CLIP
Stable Bias: Analyzing
Societal Representations in
Diffusion Models
May 11,
2023
Ethics
Investigated toxicity
and quality filters for
pretraining, the
effects of pretraining
scrape time, and
composition of
pretraining data on
many benchmarks
Checkpoi
nts
Yes
Custom
trained
models
A Pretrainer's Guide to
Training Data: Measuring
the Effects of Data Age,
Domain Coverage, Quality,
& Toxicity
May
24,
2023
Privacy
and
Security
Yes
No
GPT-2,
LLaMA,
Vicuna
Adversarial Demonstration
Attacks on Large
Language Models
May
31,
2023
Privacy
and
Security
Studies whether
which data will be
memorized by a LLM
can be predicted
before the LLM is
fully trained
Checkpoi
nts
Yes
Pythia
Emergent and Predictable
Memorization in Large
Language Models
June 6,
2023
Model
Internals
Enabled validation of
a method that
provably erases
certain kinds of
information from
neural networks,
allowing for ""surgical""
removal of
undesirable
Yes
No
Pythia,
LLaMA
LEACE: Perfect linear
concept erasure in closed
form
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 7
behaviors.
June 7,
2023
Privacy
and
Security
Enabled generation
of watermarked text
to test the robustness
of various
""paraphrasing
attacks"".
Yes
No
LLaMA
On the Reliability of
Watermarks for Large
Language Models
June
30,
2023
Alignment
Yes
No
Many
Stay on topic with
Classifier-Free Guidance
June
30,
2023
Privacy
and
Security
Checkpoi
nts
Yes
Custom
trained
models
Measuring Forgetting of
Memorized Training
Examples
July 13,
2023
Learning
Dynamics
Yes
Yes
Pythia
Layer-wise Linear Mode
Connectivity
July 27,
2023
Privacy
and
Security
Enabled discovery,
testing, and
responsible
disclosure of
adversarial suffixes
that universally
""jailbreak"" LLMs,
including proprietary
models. Would have
been impossible to
discover without
access to
open-weight models.
Yes
No
LLaMA,
Llama2,
MPT,
Pythia,
Falcon
Universal and Transferable
Adversarial Attacks on
Aligned Language Models
August
1, 2023
Ethics
Yes
Yes
Stable
Diffusion
The Bias Amplification
Paradox in Text-to-Image
Generation
August
2, 2023
Privacy
and
Security
Checkpoi
nts
Yes
Pythia
Tools for Verifying Neural
Models’ Training Data
August
8, 2023
Ethics
Yes
Yes
Pythia
SILO Language Models:
Isolating Legal Risk In a
Nonparametric Datastore
August
20,
2023
Model
Internals
Enabled application
of lightweight editing
method for controlling
model computations,
and investigation of
scaling behavior.
Yes
No
GPT-2,
OPT
Activation Addition:
Steering Language Models
Without Optimization
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 8
August
24,
2023
Other
Surveys illicit uses of
open and closed
models
Yes
No
Many
Use of LLMs for Illicit
Purposes: Threats,
Prevention Measures, and
Vulnerabilities
August
25,
2023
Model
Internals
Enabled development
of improved method
for editing and
erasing targeted
concepts from
text-image models.
Yes
No
Stable
Diffusion
Unified Concept Editing in
Diffusion Models
August
29,
2023
Learning
Dynamics
Checkpoi
nts
Yes
Custom
trained
models,
Pythia
Characterizing Learning
Curves During Language
Model Pre-Training:
Learning, Forgetting, and
Stability
August
30,
2023
Privacy
and
Security
Enabled development
and testing of much
stronger red-teaming
method against both
open and proprietary
LLMs.
Yes
No
LLaMA,
Llama2
Red-Teaming Large
Language Models using
Chain of Utterances for
Safety-Alignment
Septem
ber 9,
2023
Privacy
and
Security
Reverse engineers
the ""top-p"" and
""top-k"" parameters
used when
generating text from
a LLM
Yes
No
GPT-2,
BLOOM,
Pythia
Reverse-Engineering
Decoding Strategies Given
Blackbox Access to a
Language Generation
System
Septem
ber 15,
2023
Model
Internals
Enabled discovery of
an interpretable basis
for cataloging
features learned by
LLMs.
Yes
Yes
Pythia
Sparse Autoencoders Find
Highly Interpretable
Features in Language
Models
Septem
ber 23,
2023
Privacy
and
Security
Yes
Yes
Many
From Text to Source:
Results in Detecting Large
Language
Model-Generated Content
Septem
ber 26,
2023
Privacy
and
Security
Enabled testing the
robustness of a
proposed method for
automated lie
detection across
different LLMs.
Yes
No
LLaMA
How to Catch an AI Liar:
Lie Detection in Black-Box
LLMs by Asking Unrelated
Questions
Septem
ber 28,
2023
Alignment
Yes
No
Pythia
Beyond Reverse KL:
Generalizing Direct
Preference Optimization
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 9
with Diverse Divergence
Constraints
Septem
ber 29,
2023
Privacy
and
Security
Enabled discovery
that previous model
editing methods
leave sensitive
information that an
attacker can then
extract.
Yes
No
GPT-J,
Llama2,
GPT-2
Can Sensitive Information
Be Deleted From LLMs?
Objectives for Defending
Against Extraction Attacks
Septem
ber 29,
2023
Alignment
Yes
No
CodeGe
n,
Pythia,
MPT
Benchmarks for Detecting
Measurement Tampering
Octobe
r 1,
2023
Learning
Dynamics
Checkpoi
nts
No
OPT,
Pythia
JoMA: Demystifying
Multilayer Transformers via
JOint Dynamics of MLP
and Attention
Octobe
r 3,
2023
Model
Internals
Enabled efficient
investigation and
analysis of inner
workings and
activation patterns of
neurons in
transformer MLP
layers through easy
access to
state-of-the-art
interpretability data.
Yes
No
GPT-2,
Pythia
DeepDecipher: Accessing
and Investigating Neuron
Activation in Large
Language Models
Octobe
r 4,
2023
Alignment
Enabled exploration
of promising methods
to combat
""overoptimization""
problem when
performing safety
tuning on models.
Both the tuned and
supervisor models
were open-weight.
Yes
Yes
Pythia,
LLaMA
Reward Model Ensembles
Help Mitigate
Overoptimization
Octobe
r 5,
2023
Alignment
Enabled discovery
and investigation of a
flaw in standard
safety tuning
methods that
incentives long
responses instead of
just incentivizing
Yes
No
LLaMA
A Long Way to Go:
Investigating Length
Correlations in RLHF
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 10
helpfulness.
Octobe
r 10,
2023
Other
Yes
No
LLaMA-
2
Representation
Engineering: A Top-Down
Approach to AI
Transparency
Octobe
r 10,
2023
Privacy
and
Security
Enabled
proof-of-concept
demonstrating the
recovery of sensitive
information from
content embeddings,
with implications to
the secure design of
multi-user semantic
search applications.
Yes
No
T5
Text Embeddings Reveal
(Almost) As Much As Text
Octobe
r 11,
2023
Ethics
Presents a cheap
and efficient
technique to
intervene on LMs to
decrease toxicity in
generations
Yes
No
GPT-2,
OPT,
Pythia
Goodtriever: Adaptive
Toxicity Mitigation with
Retrieval-augmented
Models
Octobe
r 12,
2023
Alignment
Yes
No
Pythia
Interpreting Reward
Models in RLHF-Tuned
Language Models Using
Sparse Autoencoders
Octobe
r 17,
2023
Alignment
Enabled investigation
of more specific &
scalable safety tuning
method, incorporating
multidimensional
preferences.
Yes
No
LLaMA
Preference Ranking
Optimization for Human
Alignment
Octobe
r 17,
2023
Privacy
and
Security
Enabled testing of
quantization-based
watermarking
methods.
Yes
No
LLaMA,
GPT-Ne
o
Watermarking LLMs with
Weight Quantization
Octobe
r 19,
2023
Ethics
Yes
No
GPT-2
Identifying and Adapting
Transformer-Components
Responsible for Gender
Bias in an English
Language Model
Octobe
r 19,
2023
Alignment
Enabled testing of
improved RLHF
algorithm that uses
explicit constraints.
Yes
No
LLaMA
Safe RLHF: Safe
Reinforcement Learning
from Human Feedback
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 11
Octobe
r 19,
2023
Alignment
Enabled showing that
existing
vision-language
models can be
reused as
supervisors for
reinforcement
learning, and that
larger models are
more robust at this.
Yes
No
CLIP
Vision-Language Models
are Zero-Shot Reward
Models for Reinforcement
Learning
Octobe
r 22,
2023
Privacy
and
Security
Checkpoi
nts
Yes
Pythia
MoPe: Model
Perturbation-based Privacy
Attacks on Language
Models
Octobe
r 23,
2023
Model
Internals
Enabled investigation
of how input-output
functions are
contextually triggered
in LLMs, and how
they can then be
triggered in a
targeted way
post-hoc.
Yes
No
GPT-J,
GPT-Ne
oX,
Llama2
Function Vectors in Large
Language Models
Octobe
r 23,
2023
Learning
Dynamics
Checkpoi
nts
Yes
Pythia
Meta- (out-of-context)
learning in neural networks
Octobe
r 23,
2023
Model
Internals
Yes
Yes
Pythia
Understanding the Inner
Workings of Language
Models Through
Representation
Dissimilarity
Octobe
r 24,
2023
Model
Internals
Yes
Yes
GPT-2,
Pythia
Characterizing
Mechanisms for Factual
Recall in Language Models
Octobe
r 24,
2023
Model
Internals
Yes
No
GPT-J,
Pythia,
LLaMA
In-Context Learning
Creates Task Vectors
Octobe
r 25,
2023
Learning
Dynamics
Checkpoi
nts
No
MultiBE
RTs
Subspace Chronicles: How
Linguistic Information
Emerges, Shifts and
Interacts during Language
Model Training
Octobe
r 26,
2023
Model
Internals
Yes
No
Pythia
Codebook Features:
Sparse and Discrete
Interpretability for Neural
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 12
Networks
Octobe
r 31,
2023
Model
Internals
Yes
No
BERT,
GPT-2,
OPT
Large Language Models
Converge on Brain-Like
Word Representations
Novem
ber 28,
2023
Privacy
and
Security
Yes
Yes
GPT-Ne
o,
Pythia,
RedPaja
ma-INCI
TE
Scalable Extraction of
Training Data from
(Production) Language
Models
March
19,
2024
Other
Measures the
discrepancy between
reported and actual
model knowledge
cutoffs (primarily
through investigating
training data)
Yes
Yes
Pythia,
OLMo,
RedPaja
ma
Dated Data: Tracing
Knowledge Cutoffs in
Large Language Models
c.
Could open model weights, and in particular the ability to retrain models, help
advance equity in rights and safety-impacting AI systems (e.g. healthcare,
education, criminal justice, housing, online platforms etc.)?
Other commenters will likely write better on this. No comment.
d. How can the diffusion of AI models with widely available weights support the
United States’ national security interests? How could it interfere with, or further
the enjoyment and protection of human rights within and outside of the United
States?
Other commenters will likely write better on this. No comment.
e.
How do these benefits change, if at all, when the training data or the associated
source code of the model is simultaneously widely available?
Wide availability of training data or associated source code is helpful
but not required for the benefits of foundation models with widely
available weights to materialize. In the table above you can see that
much beneficial research, modification, and use is possible without
access to original training data. However, when training data and
source code are available, it becomes more feasible to predict the
behavior of the model, to modify it in targeted ways, and to study
learning dynamics of models.
4. Are there other relevant components of open foundation models that, if simultaneously
widely available, would change the risks or benefits presented by widely available model
weights? If so, please list them and explain their impact.
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 13
●
Dataset preprocessing pipelines - Even in the event that the
original training dataset cannot or should not be released, it
may be beneficial for others to be able to re-use the logic that
was used to curate the training datasets. By doing so,
institutional actors can coordinate on best practices for dataset
cleaning / management and researchers can have some
indication of what kind of data a model was trained on.
●
Runtime safety filters - Systems like Meta’s Llama Guard
(https://ai.meta.com/research/publications/llama-guard-llm-ba
sed-input-output-safeguard-for-human-ai-conversations/) and
OpenAI’s free-to-use moderation API
(https://platform.openai.com/docs/guides/moderation) are
designed to help developers guard their systems against
misuse/abuse. Because they are separate from the underlying
foundation model, anyone can use them no matter which
model they are building on. If widely available, different actors
can coordinate to implement the same best-practices for
security, privacy, and anti-fraud.
●
“Reward models” - In order to convert a foundation model
into a chatbot, model developers will sometimes train an
auxiliary model (called a “reward model”) to score outputs
based on qualitative attributes like toxicity & response length.
These are then used to further tune the foundation model to
behave in a way the model deployer considers appropriate,
rather than behaving as it would if trained only on the original
corpus. Access to these reward models prior to the release of a
model (either open-source or closed-source) may allow
independent researchers, auditors, and other teams to predict
extreme or unwanted behaviors before deployment better than
they would be able to if they only had access to the original
“base” foundation model. Organizations were originally slow
to release these models, but they are useful for downstream
applications and we are beginning to see developers release /
evaluate them publicly. See AllenAI’s Reward Bench:
https://github.com/allenai/reward-bench
5. What are the safety-related or broader technical issues involved in managing risks and
amplifying benefits of dual-use foundation models with widely available model weights?
a.
What model evaluations, if any, can help determine the risks or benefits
associated with making weights of a foundation model widely available?
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 14
I do not think that model evaluations can help much in determining
the general risks or benefits associated with making weights of a
foundation model widely available. For specific capabilities of
concern, it may be possible to use model evaluations to test how easy it
is to elicit those capabilities from a particular model. However, this is
really only possible when given full access to the model, including the
ability to finetune its weights and combine it with other systems. If
this access is not granted, there is a potential that model evaluations
will underestimate the risks or benefits from a particular system.
b. Are there effective ways to create safeguards around foundation models, either to
ensure that model weights do not become available, or to protect system integrity
or human well-being (including privacy) and reduce security risks in those cases
where weights are widely available?
I believe there are already established practices around securing
corporate, medical, and military information that would be applicable
to this, and that would do well at ensuring that model weights are only
made available when the developer intends to.
c.
What are the prospects for developing effective safeguards in the future?
The prospects appear strong to me.
d. Are there ways to regain control over and/or restrict access to and/or limit use of
weights of an open foundation model that, either inadvertently or purposely, have
already become widely available? What are the approximate costs of these
methods today? How reliable are they?
It may be possible to limit use of weights from a particular open
foundation model through policies to restrict cloud model inference
services from hosting that model and any known derivatives from it.
This would not be very costly to implement, as it is essentially a
“model blacklist”, but it would push actors who intended to use that
model towards local/self-hosted inference or hosted inference outside
the USA. At present, model inference is likely very concentrated, with
most users of foundation models relying on a hosting service rather
than managing their own infrastructure.
There is no way to regain control over the weights of an open
foundation model that, either inadvertently or purposely, have
already become widely available. In fact, it is likely that doing so will
encourage the further distribution of those weights, due to the
Streisant Effect (https://en.wikipedia.org/wiki/Streisand_effect).
e.
What if any secure storage techniques or practices could be considered necessary
to prevent unintentional distribution of model weights?
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 15
This is not my area of expertise.
f.
Which components of a foundation model need to be available, and to whom, in
order to analyze, evaluate, certify, or red-team the model? To the extent possible,
please identify specific evaluations or types of evaluations and the component(s)
that need to be available for each.
As stated earlier, a comprehensive evaluation of the model would call
for inspection by the evaluator/certifier/red-team of the entire model,
including weights, training data, and other code. It may also call for
disabling safety checks and allowing them to further train the model a
bit, to understand what the consequences would be if a malicious user
were to somehow disable runtime checks and access its underlying
abilities. However, if red teaming becomes a pre-condition for
deployment of models, developers are incentivized to keep the
information exchanged with evaluators to a minimum, only giving
them access to a locked-down API to make queries on.
g. Are there means by which to test or verify model weights? What methodology or
methodologies exist to audit model weights and/or foundation models?
It is definitely possible to do empirical tests to see how the model
defined by a set of weights behaves across a range of contexts. One
example testbed—which I had a very minor hand in developing—that
is used for large language models (LLMs) is EleutherAI’s
“lm-evaluation-harness”
(https://github.com/EleutherAI/lm-evaluation-harness). There are also
evaluation organizations like METR (https://metr.org/) and Apollo
Research (https://www.apolloresearch.ai/) that are working on
systematic testing of AI models and infrastructure for it.
With respect to verification, it is possible to use tools from
cryptography to verify that a set of model weights has certain
properties. For example, it is possible to check that the file containing
a set of model weights corresponds to some public checksum, so a user
can know they are downloading the correct model. It is also possible
to do more advanced forms of verification & testing. For example, it is
possible to modify model weights to plant undetectable backdoors in
them (https://arxiv.org/abs/2204.06974), and such backdoors are
suspected to be very hard to remove without knowledge of the
watermarking entity’s keys (https://arxiv.org/abs/2401.05566). These
could likely be used to track the provenance of weights from
foundation models, and to attach markers that indicate that they have
gone through a verification process. I would recommend consulting
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 16
with cybersecurity & cryptography experts on these matters. I
suspect, however, that even assuming they are feasible to implement,
these protocols are not standardized and would need to gain buy-in in
order to implement.
6. What are the legal or business issues or effects related to open foundation models?
a.
In which ways is open-source software policy analogous (or not) to the
availability of model weights? Are there lessons we can learn from the history and
ecosystem of open-source software, open data, and other “open” initiatives for
open foundation models, particularly the availability of model weights?
Model weights are software, just as with other software. However, the
way that model weights are created does not primarily involve
“writing source code” in the same way as with ordinary software.
Also, the process of going from the original behavioral specification
(the training data) to running code (the trained model) is significantly
longer and more expensive. This changes some but not all of the
dynamics. The Open Source Initiative has been discussing this matter
and should have relevant institutional knowledge. Assuming that they
are also submitting comments, I would defer to them on possible
lessons from history.
b. How, if at all, does the wide availability of model weights change the competition
dynamics in the broader economy, specifically looking at industries such as but
not limited to healthcare, marketing, and education?
Wide availability of model weights may enable sectors (such as
healthcare and education, in some cases) that are otherwise prohibited
from sending data externally to benefit from AI developments.
c.
How, if at all, do intellectual property-related issues—such as the license terms
under which foundation model weights are made publicly available—influence
competition, benefits, and risks? Which licenses are most prominent in the
context of making model weights widely available? What are the tradeoffs
associated with each of these licenses?
There have been many many different licenses applied to foundation
model weights. For actors intending to use the model in a commercial
fashion, the most common and most preferable licenses are MIT and
Apache, as they are quite permissive in what they allow. The tradeoff
is that there is no ability to restrict how the model is used under these
licenses. Several organizations like Meta and NVIDIA have opted for
more restrictive custom licenses that restrict the ways their models are
used, or the scale of use. Some of these are intended to prevent misuse
and illegal use by bad actors. Others are intended to reduce
competition or to give an advantage to the company that developed
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 17
the model. For example, the license under which Meta released their
Llama2 models prevents the model from being used by enterprises
larger than a particular size, which limits who can compete in serving
that model.
(https://www.theverge.com/2023/10/30/23935587/meta-generative-ai-m
odels-open-source) For academic research, the license does not
necessarily make a difference, but it changes the set of organizations
that are willing to host the model and the number of users that are
ultimately exposed to it, which can have downstream impacts on
which models researchers are interested in investigating.
d. Are there concerns about potential barriers to interoperability stemming from
different incompatible “open” licenses, e.g., licenses with conflicting
requirements, applied to AI components? Would standardizing license terms
specifically for foundation model weights be beneficial? Are there particular
examples in existence that could be useful?
Yes. I believe that the Open Source Initiative has been working to
standardize a definition of “open-source AI” which will help
(https://opensource.org/blog/open-source-ai-definition-weekly-update-
mar-25). In addition to this, I believe that MIT and Apache licenses
are both fairly standard within the community and are compatible
with one another.
7. What are current or potential voluntary, domestic regulatory, and international
mechanisms to manage the risks and manage the benefits of foundation models with
widely available weights? What kind of entities should take a leadership role across
which features of governance?
a.
What security, legal, or other measures can reasonably be employed to reliably
prevent wide availability of access to a foundation model’s weights, or limit their
end use?
No comment.
b. How might the wide availability of open foundation model weights facilitate, or
else frustrate, government action in AI regulation?
AI systems benefit from economies of scale. It seems likely to me that
AI systems will mostly be deployed by very large, very visible actors
(like multinational corporations) such that it would be
straightforward for the government to regulate the deploying entities
if it chose to. These same actors are also less likely to need others to
release model weights for them, and less likely to release weights
themselves. Due to these factors, I don’t think that the wide
availability of open foundation model weights will particularly
facilitate or frustrate government action in AI regulation.
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 18
c.
When, if ever, should entities deploying AI disclose to users or the general public
that they are using open foundation models either with or without widely
available weights?
No comment.
d. What role, if any, should the U.S. government take in setting metrics for risk,
creating standards for best practices, and/or supporting or restricting the
availability of foundation model weights?
The U.S. government should encourage standards-setting for best
practices and setting metrics for risk. It should also set policies for
how to manage widely-available foundation model weights in the
context of military & governmental use, no matter whether that
means restricting or encouraging their use. I also believe it should
clarify the conditions under which foundation models can be trained,
deployed, and made widely available in any form, including if there
are types or sources of data that should not be trained on. I do not
believe that the U.S. government should attempt in general to restrict
the availability of foundation model weights.
i.
Should other government or non-government bodies, currently existing or
not, support the government in this role? Should this vary by sector?
Other commenters will likely write better on this. No comment.
e.
What should the role of model hosting services (e.g. HuggingFace, GitHub, etc.)
be in making dual-use models with open weights more or less available? Should
hosting services host models that do not meet certain safety standards? By whom
should those standards be prescribed?
Hosting services should be expected to enforce policies on models with
open weights to the extent they are selling inference services for those
models. It does not seem reasonable or particularly useful to do this
for platforms like GitHub (or to a certain extent, the HuggingFace
Hub) that are merely storing the model weights, as intervening at the
point of inference seems both more effective and less burdensome.
f.
Should there be different standards for government as opposed to private industry
when it comes to sharing model weights of open foundation models or contracting
with companies who use them?
No comment.
g. What should the U.S. prioritize in working with other countries on this topic, and
which countries are most important to work with?
No comment.
h. What insights from other countries or other societal systems are most useful to
consider?
No comment.
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 19
i.
Are there effective mechanisms or procedures that can be used by the government
or companies to make decisions regarding an appropriate degree of availability of
model weights in a dual-use foundation model or the dual-use foundation model
ecosystem? Are there methods for making effective decisions about open AI
deployment that balance both benefits and risks? This may include responsible
capability scaling policies, preparedness frameworks, et cetera.
The government and companies should make decisions on
appropriate degree of availability by considering marginal risks from
openness, as advocated for in a paper by researchers at Stanford’s
Center for Research on Foundation Models
(https://crfm.stanford.edu/open-fms/). That is, they should ask
whether opening up the weights of their particular foundation model
for broad availability would increase societal risk by intentional
misuse beyond that already posed by closed foundation models and
pre-existing technologies (such as web search on the internet).
j.
Are there particular individuals/entities who should or should not have access to
open-weight foundation models? If so, why and under what circumstances?
No comment.
8. In the face of continually changing technology, and given unforeseen risks and benefits,
how can governments, companies, and individuals make decisions or plans today about
open foundation models that will be useful in the future?
a.
How should these potentially competing interests of innovation, competition, and
security be addressed or balanced?
Other commenters will likely write better on this. No comment.
b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to adapt
the threshold, is the amount of computational resources required to build a model,
such as the cutoff of 1026 integer or floating-point operations used in the
Executive Order, a useful metric for thresholds to mitigate risk in the long-term,
particularly for risks associated with wide availability of model weights?
In my view, it is a useful metric to keep in mind, but not much policy
should rest on it, and it should be open to revision. It seems likely that
as hardware providers specialize their chips towards running AI
foundation models, what exactly constitutes an “integer or
floating-point operation” will significantly change. There is no
fundamental definition for such operations, and there is already a
move towards lower-precision formats and even operations on binary
or trinary values. Moreover, over time it becomes possible to achieve
better and better performance with the same budget of computational
resources. I do think that aggregate compute quantities will continue
Comments on NTIA “Dual Use Foundation Artificial Intelligence Models with Widely Available Model
Weights” by Charles Foster - Page 20
to be one of the most important and most reliable metric to track for
governing the development of new AI systems.
c.
Are there more robust risk metrics for foundation models with widely available
weights that will stand the test of time? Should we look at models that fall outside
of the dual-use foundation model definition?
No, there are no such robust risk metrics known. As we gain
experience working with these foundation models and observing how
they are used in practice, we will accumulate evidence about where
the problems lie, in the same way as we now have evidence about
other public safety issues. But that will take time.
9. What other issues, topics, or adjacent technological advancements should we consider
when analyzing risks and benefits of dual-use foundation models with widely available
model weights?
No comment.
",Charles Foster,9833
NTIA-2023-0009-0214," 
 
 
Charles Crain 
 
Vice President, 
Domestic Policy  
 
 
1 
March 27, 2024 
 
The Honorable Alan Davidson 
Assistant Secretary of Commerce for Communications and Information 
National Telecommunications and Information Administration 
U.S. Department of Commerce 
1401 Constitution Avenue NW 
Washington, DC 20230 
 
Re: 
Docket No. NTIA-2023-0009: Dual Use Foundation Artificial Intelligence Models With 
Widely Available Model Weights 
 
Dear Assistant Secretary Davidson, 
 
The National Association of Manufacturers (“NAM”) appreciates this opportunity to provide comment 
to the National Telecommunications and Information Administration (“NTIA”) in response to its 
request for comments (“RFC”) on appropriate policy and regulatory approaches to dual-use 
foundation models for which model weights are widely available (“open foundation models”).1 
 
The NAM is the voice of the manufacturing industry and the leading advocate for a policy agenda 
that helps manufacturers compete in the global economy. The NAM is the largest U.S. 
manufacturing association, representing small and large manufacturers in every industrial sector and 
in all 50 states. Manufacturing employs nearly 13 million people, contributes $2.85 trillion to the U.S. 
economy annually and accounts for 53% of private-sector research and development. 
 
Manufacturers have been developing and deploying intelligent systems and artificial intelligence 
(“AI”) technology for many years. These innovative approaches to modern manufacturing improve 
safety on the shop floor and for the customer and enhance the efficiency of manufacturing processes 
and operations. They also allow manufacturers to better manage supply chains, perform predictive 
maintenance, design new products and more. AI has become essential to modern, smart 
manufacturing—known as “Manufacturing 4.0.” AI contributes to the growth of the manufacturing 
economy and bolsters U.S. global manufacturing leadership. That is why the NAM believes that 
policy approaches to AI should further its development and support its responsible use by 
manufacturers across a wide range of applications. 
 
The NAM has called for policy approaches that promote manufacturers’ continued access to the 
widest possible choice of AI technologies. With respect to NTIA’s RFC on the potential risks, benefits 
and implications of open foundation models, access to these models and to the AI systems 
developed from such models enables manufacturers’ use of a wide range of AI innovations, offering 
significant public policy benefits to the manufacturing sector and to industry at large. As such, the 
NAM respectfully encourages NTIA to enhance, rather than restrict, access to open foundation 
models. 
 
 
 
 
 
1 Federal Register Vol. 89, No. 38, Monday, February 26, 2024, pp. 14059-14063. 
I. 
Open foundation models contribute to innovation, choice and transparency. 
 
Access to open foundation models fosters increased development and use of AI technologies 
because the ability to change these models’ weights enables companies to develop new AI 
capabilities. The cost of developing and training an AI model is considerable, if not prohibitive. This 
leads to “the concentration of access to foundation models into a small subset of organizations,” 
which the RFC notes, rightly, “poses the risk of hindering [AI] innovation.”2 Making a model’s weights 
widely available allows any interested party to develop new AI capabilities without having to incur 
these costs. This lowers barriers to entry in AI development by providing new opportunities for AI 
innovation. This means greater competition among, and choice between, developers of AI systems.  
 
Critically, enhanced access to open foundation models will benefit not just tech entrepreneurs, but 
also companies throughout the manufacturing sector whose business is not AI development and 
commercialization but that want to use AI technology. Access to the weights of a foundation model 
affords these manufacturers the opportunity to adapt these technologies to their specific needs. 
Open foundation models are thus a flexible, market-based way to match technology supply and 
demand. 
 
Enhanced transparency can provide additional public policy benefits. For example, the Foundation 
Model Transparency Index shows that “open developers are consistently more transparent than 
closed developers.”3 Manufacturers agree with NTIA that “open foundation models can allow for 
more transparency and enable broader access to allow greater oversight by technical experts, 
researchers, academics, and those from the security community.”4 The availability of model weights 
allows independent examination of a model to ensure it is fit for purpose and to identify and mitigate 
its vulnerabilities. 
 
Given the importance of manufacturers’ access to open foundation models, the NAM respectfully 
encourages NTIA to exercise caution in pursuing policy and regulatory approaches that could restrict 
it. 
 
 
II. 
Restrictive policies should undergo strict scrutiny to avoid hampering innovation. 
 
Research into AI and the development of AI technologies are proceeding rapidly. This has fueled 
speculation about both AI’s promise for humanity and the risks it poses. AI policymaking should not 
be based on speculation, but rather on actual evidence. With respect to manufacturers’ access to 
open foundation models, evidence-based regulatory decisions will ensure that any restrictions 
adopted do not unnecessarily impede innovation. Specifically, the NAM urges NTIA to demonstrate 
that the risks that any restrictions seek to mitigate are sufficiently likely to materialize, rather than 
theoretical or unproven. 
 
Foundation models take their name from their ability to be trained to perform a wide variety of tasks 
and to support a wide variety of use cases. As a result, policies restricting access to open foundation 
models are likely to affect not just use cases that raise concerns, but also non-problematic uses that 
are beneficial to society and the economy. In particular, most of the concerns discussed in the RFC 
are raised by generative AI, but foundation models can be used to develop other types of AI 
systems—what is generally called analytical AI—that are in wide use in the manufacturing industry. 
 
2 Id. at p. 14060. 
3 “The Foundation Model Transparency Index”, published by Stanford University’s Center for Research on 
Foundation Models, Stanford Institute for Human-Centered Artificial Intelligence, October 2023, available at 
https://arxiv.org/abs/2310.12941 
4 RFC, at p. 14060. 
It is critical that manufacturers’ use of analytical AI be as unaffected as possible by any restrictions 
that might be imposed because of concerns about generative AI. 
 
Another important issue as NTIA considers potential restrictions on open foundation models is 
whether, and if so to what extent, any restrictions would have unintended consequences on 
legitimate businesses. Some developers of foundation models make widely available not only the 
weights of their models but also their source code (“open source foundation models”). Anyone can 
adapt and update such models, not just to remove the safeguards that prevent them from generating 
problematic content, but also to give them new features, to augment their capabilities or to run them 
on servers not controlled by the original developers. Anyone can make these modifications, including 
malicious actors who will largely not be affected by policy restrictions on model weights. Legitimate 
businesses, by contrast, will comply with the restrictions. The presence in the global marketplace of 
open source foundation models thus raises the question of whether restrictions on open foundation 
models will have the unintended consequence of hampering legitimate and beneficial uses of these 
models without curbing malicious uses. 
 
Finally, manufacturers urge NTIA to bear in mind the global nature of the marketplace for innovative 
AI technologies and products. If the United States pursues restrictive AI policies, including 
restrictions on open foundation models, it risks inhibiting innovation by manufacturers in the U.S.—
and curbing the job creation and economic development associated with groundbreaking 
technological advances. It is therefore important to ensure that such policies are aligned with those 
enacted by other major countries where AI technologies and products are developed, to avoid being 
undercut by their potentially more permissive regimes. 
 
* * * * 
 
Manufacturers appreciate the emphasis that NTIA has traditionally put on the promotion of 
innovation, and the industry encourages NTIA to continue to prioritize innovation as it considers 
potential restrictions on manufacturers’ access to open foundation models. The NAM looks forward 
to continuing to support the development of AI policies that maintain the United States’ position as a 
global leader in AI and smart manufacturing. 
 
Sincerely, 
 
 
 
 
 
Charles Crain 
Vice President, Domestic Policy 
",Nat Assoc Manufacturers,1744
NTIA-2023-0009-0229,"‭
DOCKET #240216-0052‬
‭
RIN #0660-XC06‬
‭
National Telecommunications and Information Administration‬
‭
Herbert C. Hoover Building, 1401 Constitution Ave. NW‬
‭
Washington, DC 20230‬
‭
March 27, 2024‬
‭
Re: NTIA’s Request for Comment regarding Dual-Use Foundation Artificial Intelligence‬
‭
Models with Widely Available Model Weights as per Section 4.6 of the Executive Order on‬
‭
the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence‬
‭
The Center for Democracy & Technology (CDT) respectfully submits these comments in‬
‭
response to NTIA’s Request for Comment regarding the risks and benefits of, and potential‬
‭
policy approaches to, so-called “dual-use” foundation models for which the model weights are‬
‭
widely available, or as referred to in the RFC, “open foundation models” (OFMs).‬
‭
1‬‭
Through this‬
‭
proceeding, required by section 4.6 of the Executive Order on the Safe, Secure, and‬
‭
Trustworthy Development and Use of Artificial Intelligence (the AI EO),‬
‭
2‬‭
CDT is grateful to be‬
‭
able to share its perspective on how NTIA should advise the President on whether and how to‬
‭
regulate such models.‬
‭
CDT is a nonprofit 501(c)(3) organization that works to advance civil rights and civil liberties in‬
‭
the digital age. Among our priorities, CDT advocates for the responsible and equitable design,‬
‭
deployment and use of new technologies such as artificial intelligence, and promotes the‬
‭
adoption of robust, technically-informed solutions for the effective regulation and governance of‬
‭
AI systems.‬
‭
These comments build on a recent joint letter to the Commerce Department from CDT and a‬
‭
wide range of expert civil society organizations and academic scholars,‬
‭
3‬‭
highlighting how‬
‭
substantial benefits may be lost, critical safety issues may be left under-addressed, and‬
‭
3‬‭
Letter from Accountable Tech et al. to Secretary Gina Raimondo, March 25, 2024,‬
‭
https://cdt.org/insights/cdt-joins-mozilla-civil-society-orgs-and-leading-academics-in-urging-us-secretary-of‬
‭
-commerce-to-protect-ai-openness/‬
‭
.‬‭
[‬
‭
perma.cc/8MJJ-Z2P2‬
‭
].‬
‭
2‬‭
See President Biden, “Executive Order on the Safe, Secure, and Trustworthy Development and Use of‬
‭
Artificial Intelligence,” The White House, October 2023, at sec. 4.6,‬
‭
https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-se‬
‭
cure-and-trustworthy-development-and-use-of-artificial-intelligence/‬
‭
.‬‭
[‬
‭
perma.cc/DDD3-VKWZ‬
‭
].‬
‭
1‬‭
National Telecommunications and Information Administration, “Dual Use Foundation Artificial‬
‭
Intelligence Models with Widely Available Model Weights,” Federal Register, February 26, 2024,‬
‭
https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-foundation-artificial-intelligen‬
‭
ce-models-with-widely-available-model-weights‬
‭
.‬‭
[‬
‭
perma.cc/8XKS-WMRH‬
‭
].‬
‭
1‬
‭
democratic values may be undermined, if the creation and publication of open foundation‬
‭
models are broadly targeted for regulation based on speculative risks.‬
‭
Part I looks at the benefits of open foundation models, both by analogy to the history of open‬
‭
source software and by looking at recent AI developments, and concludes that they are likely‬
‭
substantial.‬
‭
Part II considers the risks of open foundation models compared to closed models and other‬
‭
technologies like the internet — i.e., the‬‭
marginal‬‭
risks — and finds a need for more rigorous‬
‭
research into particular risks and their solutions.‬
‭
Part III addresses policy approaches to open foundation models, with a focus on how the‬
‭
government can support critical field- and norm-building activities to clarify best practices‬
‭
around these risks, and how the government may need to tailor its policy interventions around‬
‭
open foundation models to satisfy the constraints of the First Amendment.‬
‭
I.‬
‭
BENEFITS OF OPEN FOUNDATION MODELS‬
‭
As previously highlighted in our attached joint letter and as discussed at length below, there are‬
‭
a wide variety of likely benefits from current and future open foundation models (OFMs) such as‬
‭
BERT, CLIP, Whisper, BLOOM, Pythia, Llama-2, Falcon, Stable Diffusion, Mistral, OLMo, Aya,‬
‭
and Gemma, as opposed to systems offered via the web or an API and without publication of‬
‭
key components including model weights. This conclusion is based on analogy to benefits from‬
‭
the vast open source software (OSS) ecosystem that has grown over the past three decades,‬
‭
as well as on developments around foundation models in just the past year and a half since‬
‭
OpenAI launched ChatGTP.‬
‭
The Benefits of Open Source Software‬
‭
“Open source” typically refers to software that has been released under a license that allows‬
‭
unrestricted use and modification of the software, including integrating it into other software‬
‭
tools or custom configurations. By contrast, not all OFMs are released under strictly “open‬
‭
source” licenses (although many are). Rather, safety considerations around AI have led to an‬
‭
emerging practice of publishing OFMs under what are sometimes called Responsible AI‬
‭
Licenses (RAIL) with some use restrictions to attempt to prevent undesirable types of‬
‭
deployments.‬
‭
4‬‭
For this reason and others we will discuss later, “open source” and “open AI” are‬
‭
not always coextensive terms.‬
‭
However, both open source software and OFMs allow for broader and more customized‬
‭
deployment than closed models that require direct permission from, payment to, or reliance on a‬
‭
4‬‭
BigScience, “BigScience RAIL License v1.0,” May 19, 2022,‬
‭
https://huggingface.co/spaces/bigscience/license‬‭
[‬
‭
perma.cc/48WK-QD6M‬
‭
];‬‭
Meta AI; “Llama 2 Community‬
‭
License Agreement,” July 18, 2023.‬‭
https://ai.meta.com/llama/license‬‭
[‬
‭
perma.cc/L6PA-UTX8‬
‭
].‬
‭
2‬
‭
central service provider; and they further allow more in-depth study of the released components‬
‭
in order to improve, modify, and build on them. Therefore it is sensible to briefly look at the‬
‭
massive benefits over the past decades from open source generally before turning to those of‬
‭
OFMs specifically.‬
‭
Over those decades, open source software has become a foundational element of society’s‬
‭
entire software ecosystem. For example, imagine you are looking at an interesting blog post on‬
‭
your smartphone or laptop and choose to message a link to a friend:‬
‭
●‬ ‭
The web site you are viewing, like the vast majority of the web, is likely hosted on‬
‭
open-source Apache servers running on the open source Linux operating system;‬
‭
●‬ ‭
There’s a very good chance that the web site uses the open source content‬
‭
management system WordPress; 43% of web sites use it.‬
‭
●‬ ‭
If you are reading it on a cellphone, you are likely reading it through the open source‬
‭
Chrome browser, using the open source Android operating system that runs on 63% of‬
‭
the world’s billions of phones. Or you may be reading it on a Chromebook laptop, which‬
‭
is also running an open source operating system, Chromium. Thanks to these open‬
‭
source assets, a wide range of hardware providers around the world like Motorola,‬
‭
Samsung and Nokia — not just Google — have been able to more effectively compete‬
‭
with closed operating system ecosystems like Apple and Microsoft’s.‬
‭
●‬ ‭
If you use private chat via the Signal, Whatsapp, or Google Messages apps to send the‬
‭
link to your friend, you are in turn using the open source Signal protocol for encrypted‬
‭
messaging.‬
‭
The existence of open source alternatives like those above (as well as trailblazers like the open‬
‭
source Mozilla Firefox web browser that long prevented nearly complete dominance of the PC‬
‭
browser market by Microsoft’s Internet Explorer) has led to the existing level of competition in‬
‭
the internet software and computing hardware space.‬
‭
Open source’s impact extends essentially to all software: today,‬‭
96% of all code bases include‬
‭
open-source software,‬
‭
5‬‭
and GitHub, the biggest platform for the open-source community, is‬
‭
used by more than 100 million developers worldwide.‬
‭
6‬‭
Translating that into economic impact, a‬
‭
recent analysis from Harvard Business School found that without open source, firms would likely‬
‭
have to spend 3.5 times more on software,‬
‭
7‬‭
while in a survey businesses themselves similarly‬
‭
estimated they would have to spend 4 times more.‬
‭
8‬‭
Executives of Fortune 500 companies‬
‭
8‬‭
Henry Chesbrough, “Measuring the Economic Value of Open Source,” The Linux Foundation, March‬
‭
2023 March, 2023,‬‭
https://www.linuxfoundation.org/research/measuring-economic-value-of-os‬
‭
.‬
‭
[‬
‭
perma.cc/K6TY-5VKD‬
‭
].‬
‭
7‬‭
​
Manuel Hoffmann, Frank Nagle, and Yijian Zhou, “The Value of Open Source Software,”‬‭
Social Science‬
‭
Research Network‬
‭
, January 1, 2024,‬‭
https://doi.org/10.2139/ssrn.4693148‬
‭
.‬‭
[‬
‭
perma.cc/73S7-L6NX‬
‭
].‬
‭
6‬‭
Thomas Dohmke, “100 Million Developers and Counting - the GitHub Blog,” The GitHub Blog, January‬
‭
25, 2023,‬‭
https://github.blog/2023-01-25-100-million-developers-and-counting‬
‭
.‬‭
[‬
‭
perma.cc/9KXH-QE4A‬
‭
].‬
‭
5‬‭
Synopsis. “Open Source Security and Analysis Report (OSSRA),” 2024,‬
‭
https://www.synopsys.com/software-integrity/engage/ossra/ossra-report‬
‭
.‬‭
[‬
‭
perma.cc/F7QQ-UL3D‬
‭
].‬
‭
3‬
‭
highlight the cost savings, faster development, and better interoperability offered by open source‬
‭
software.‬
‭
9‬
‭
Open source has proven economically valuable not only to users but to developers. For‬
‭
example, the stock market responds positively when technology firms release new open source‬
‭
technologies,‬
‭
10‬‭
while startups with higher levels of contribution to GitHub tend to see increases‬
‭
in their funding and valuations.‬
‭
11‬‭
And the European Commission estimated that approximately a‬
‭
€1 billion investment in open source software by European companies in 2018 resulted in an‬
‭
impact on the European economy of between‬‭
€65 and‬‭
€95 billion‬
‭
. Considering those kinds of‬
‭
numbers and the safety benefits of open source that we’ll discuss later, it is unsurprising that a‬
‭
recent bipartisan legislative proposal recognized that “a secure, healthy, vibrant, and resilient‬
‭
open source software ecosystem is crucial for ensuring the national security and economic‬
‭
vitality of the United States.”‬
‭
12‬
‭
Open source is admittedly not a silver bullet that will solve all competition problems, and it can‬
‭
in fact be leveraged by larger players as a means of solidifying influence on the technical‬
‭
ecosystem. For example, Google’s forays into open source mobile operating systems and‬
‭
browsers have enhanced that company’s structural power, while the main competitors it was‬
‭
challenging remain among the most capitalized companies in the world. However, it is not hard‬
‭
to imagine how much more concentrated and immovable the incumbent positions of Apple and‬
‭
Microsoft would be now if they had not faced open source-driven challenges from Google and‬
‭
the many hardware vendors they enable.‬
‭
The past of open source is not a promise that open AI generally or open foundation models in‬
‭
particular will have all of the same kinds of positive effects, however. So we must also examine‬
‭
the benefits of OFMs in particular. We will highlight three major categories of likely benefits:‬
‭
distributing power, catalyzing innovation, and ensuring transparency.‬
‭
13‬
‭
13‬‭
These three categories are taken from Rishi Bommasani et al., “Considerations for Governing Open‬
‭
Foundation Models,”‬‭
Stanford Institute for Human-Centered‬‭
Artificial Intelligence‬
‭
, December 13, 2023,‬
‭
https://hai.stanford.edu/issue-brief-considerations-governing-open-foundation-models‬
‭
[‬
‭
perma.cc/L93F-BCDM‬
‭
]. Several of the same authors‬‭
further break down benefits into a more detailed set‬
‭
of five categories in a later paper, see Sayash Kapoor et al., “On the Societal Impact of Open Foundation‬
‭
Models,”‬‭
arXiv‬
‭
, February 27, 2024,‬‭
https://arxiv.org/abs/2403.07918‬‭
[‬
‭
perma.cc/P9N9-9QSQ‬
‭
].‬
‭
12‬‭
Gary C. Peters, “S.4913 - Securing Open Source Software Act of 2022,” September 28, 2022,‬
‭
https://www.congress.gov/bill/117th-congress/senate-bill/4913‬‭
[‬
‭
perma.cc/RXX8-FMRY‬
‭
].‬
‭
11‬‭
Nataliya Langburd Wright, Frank Nagle, and Shane‬‭
Greenstein, “Contributing to Growth? The Role of‬
‭
Open Source Software for Global Startups,” vol. 52, January 1, 2024,‬
‭
https://www.hbs.edu/ris/Publication%20Files/24-040_69bae20b-2026-4089-b76c-07b8a8cc48d4.pdf‬
‭
.‬
‭
[‬
‭
perma.cc/E5AE-222C‬
‭
].‬
‭
10‬‭
Wei Yang, “How Can Open Source Technology Ecosystem Create Value? Evidence From Investors’‬
‭
Reactions to Firms’ GitHub Code Releases,”‬‭
Social‬‭
Science Research Network‬
‭
, April 30, 2023,‬
‭
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4433433‬
‭
.‬‭
[‬
‭
perma.cc/G5BH-NNT6‬
‭
].‬
‭
9‬‭
Id.‬
‭
4‬
‭
Distributing Power (Both in the Market and the Culture)‬
‭
The economic benefits of generative AI driven by foundation models are expected to be‬
‭
enormous, with Bloomberg predicting a market of $1.3 trillion by 2032.‬
‭
14‬‭
However, recent‬
‭
research indicates that the market structure for closed foundation models has a tendency‬
‭
toward concentration, including vertical integration, in large part due to the high costs of‬
‭
compute infrastructure for training.‬
‭
15‬
‭
In addition to massive concentration of economic power, a foundation model market dominated‬
‭
by a handful of closed systems carries other risks. For example, when many different‬
‭
decisionmakers and service providers rely on the same systems, there can be a trend toward‬
‭
“algorithmic monoculture” whereby systemic exclusion of individuals or groups in AI-driven‬
‭
decisionmaking occurs across the ecosystem.‬
‭
16‬‭
There is also the risk of actual monoculture,‬
‭
where a handful of companies decide what knowledge and expression is allowed through this‬
‭
powerful new layer of information technology, raising the specter of undue power over politics‬
‭
and culture. This is a fraught issue, still under debate in the context of social media companies;‬
‭
and that debate — which should be of concern regardless of one’s politics, left or right — is now‬
‭
extending to the acceptable use and content moderation efforts of closed foundation models.‬
‭
17‬
‭
In the social network realm, we’ve seen the beginning of a move toward decentralized social‬
‭
networks built on open standards to allow for a range of different types of social networks‬
‭
serving different needs, communities, and social norms;‬
‭
18‬‭
open foundation models similarly‬
‭
provide a decentralized alternative to the concentration of power and decisionmaking in a‬
‭
handful of closed providers.‬
‭
18‬‭
Roel Roscam Abbing, Cade Diehm, and Shahed Warreth, “Decentralised Social Media,”‬‭
Internet Policy‬
‭
Review, February 20, 2023,‬‭
https://policyreview.info/glossary/decentralised-social-media‬
‭
[‬
‭
perma.cc/TC4M-SR8F‬
‭
].‬
‭
17‬‭
Nitasha Tiku, Kevin Schaul, and Szu Yu Chen, “AI generated images are biased, showing the world‬
‭
through stereotypes,”‬‭
Washington Post‬
‭
, November 1,‬‭
2023,‬
‭
https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-st‬
‭
ereotypes‬‭
[‬
‭
perma.cc/3NG5-NDLY‬
‭
]; Dan Milmo and Alex‬‭
Hern, “‘We definitely messed up’: why did Google‬
‭
AI tool make offensive historical images?,”‬‭
The Guardian‬
‭
,‬‭
March 8, 2024,‬
‭
https://www.theguardian.com/technology/2024/mar/08/we-definitely-messed-up-why-did-google-ai-tool-m‬
‭
ake-offensive-historical-images‬‭
[‬
‭
perma.cc/27S9-4AR6‬
‭
];‬‭
Fábio Yoshio Suguri Motoki, Valdemar Pinho‬
‭
Neto, and Víctor Rodrigues, “More human than human: measuring ChatGPT political bias,”‬‭
Public Choice‬
‭
198, no. 1-2 (August 17, 2023): 3-23,‬‭
https://doi.org/10.1007/s11127-023-01097-2‬‭
[‬
‭
perma.cc/27S9-4AR6‬
‭
].‬
‭
16‬‭
Rishi Bommasani et al., “Picking on the Same Person: Does Algorithmic Monoculture Lead to Outcome‬
‭
Homogenization?,”‬‭
arXiv‬
‭
, November 25, 2022,‬‭
https://arxiv.org/abs/2211.13972‬
‭
.‬‭
[‬
‭
perma.cc/F7JB-3AK3‬
‭
]‬
‭
15‬‭
Jai Vipra and Anton Korinek, “Market Concentration Implications of Foundation Models: The Invisible‬
‭
Hand of ChatGPT,”‬‭
Center on Regulation and Markets‬‭
at Brookings‬
‭
, September 7, 2023,‬
‭
https://www.brookings.edu/articles/market-concentration-implications-of-foundation-models-the-invisible-h‬
‭
and-of-chatgpt‬‭
[‬
‭
perma.cc/D5TP-KCKV‬
‭
]; David Gray Widder,‬‭
Sarah Myers West, and Meredith Whittaker,‬
‭
“Open (for Business): Big Tech, Concentrated Power, and the Political Economy of Open AI,”‬‭
Social‬
‭
Science Research Network‬
‭
, January 1, 2023,‬‭
https://doi.org/10.2139/ssrn.4543807‬
‭
[‬
‭
perma.cc/5ERM-3E39‬
‭
].‬
‭
14‬
‭
Oktavia Catsaros, “Generative AI to Become a $1.3 Trillion Market by 2032, Research Finds,”‬
‭
Bloomberg Intelligence‬
‭
, June 1, 2023,‬
‭
https://www.bloomberg.com/company/press/generative-ai-to-become-a-1-3-trillion-market-by-2032-resear‬
‭
ch-finds/‬
‭
.‬‭
[‬
‭
perma.cc/3Z42-6J2B‬
‭
].‬
‭
5‬
‭
Based on the history of open source software, one would also expect OFMs to enable faster,‬
‭
cheaper diffusion of foundation model technology to startups and other businesses large and‬
‭
small, as well as other developer and user communities around the world. And, so far, that is‬
‭
exactly what is occurring. As the UK’s Competition and Markets Authority has noted, “[a]t‬
‭
present a mix of open and closed-source foundation models are available and competing. This‬
‭
is allowing a range of firms to invest in and develop foundation models and as a result we are‬
‭
already seeing deployment of these foundation models in a growing range of applications‬
‭
across the economy.”‬
‭
19‬‭
For example, as of September 23, 2023 — half a year ago now — Meta‬
‭
reported that already tens of thousands of entrepreneurs and startups were using Llama-2,‬
‭
20‬
‭
while as of December 7, 2023, Meta reported that their Llama models had been downloaded‬
‭
over 100 million times.‬
‭
21‬‭
Google's new suite of Gemma models (7B, 7B-IT, 2B, & 2B-IT),‬
‭
meanwhile, were downloaded over one million times in the last month from the Hugging Face‬
‭
platform.‬
‭
22‬
‭
The demand for OFMs is being seen in a wide range of contexts. Large companies such as Dell‬
‭
and Wells Fargo are starting to use them to help with internal knowledge management and‬
‭
internal software coding, with Dell’s SVP for AI Strategy noting: “A lot of customer[s] are asking‬
‭
themselves: “Wait a second, why am I paying for [a] super large model that knows very little‬
‭
about my business? Couldn’t I just use one of these open-source models, and by the way,‬
‭
maybe use a much smaller, open-source model for that (information retrieval) workflow?”‬
‭
23‬
‭
OFMs are now being offered and widely used on the cloud platforms of Microsoft, AWS and‬
‭
Google, and consultants like McKinsey are using them to build applications for their clients.‬
‭
24‬
‭
Small technology firms and startups with fewer resources are depending heavily on the‬
‭
availability of free pre-trained models that they can adapt to their applications,‬
‭
25‬‭
and limitations‬
‭
on the availability of such models could disproportionately impact those small competitors.‬
‭
CDT’s own research in interviews with deployers who are leveraging foundation models offers‬
‭
similar conclusions: they stress that frequent changes in closed model APIs, model versions, or‬
‭
terms of service make navigating contracts with clients and maintaining stable builds more‬
‭
difficult.‬
‭
26‬‭
They also note that safety guardrails in the foundation models can make stress testing‬
‭
their own applications and creating robust, application-specific safety checks challenging.‬
‭
26‬‭
On file with the author, research publication forthcoming.‬
‭
25‬‭
Amy A. Winecoff and Elizabeth Anne Watkins,‬‭
Artificial‬‭
Concepts of Artificial Intelligence‬
‭
,‬‭
Proceedings‬
‭
of the 2022 AAAI/ACM Conference on AI, Ethics, and Society‬
‭
, 2022,‬
‭
https://doi.org/10.1145/3514094.3534138‬
‭
.‬‭
[‬
‭
perma.cc/BQ5U-N9JV‬
‭
].‬
‭
24‬‭
Id.‬
‭
23‬‭
Matt Marshall, “How Enterprises Are Using Open Source LLMs: 16 Examples,”‬‭
VentureBeat‬
‭
, February‬
‭
2, 2024,‬‭
https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples‬
‭
.‬
‭
[‬
‭
perma.cc/PVL3-X6LX‬
‭
].‬
‭
22‬‭
“Google,” HuggingFace,‬‭
https://huggingface.co/google‬
‭
.‬‭
[‬
‭
perma.cc/4G63-894P‬
‭
].‬
‭
21‬‭
Meta, “Introducing Purple Llama for Safe and Responsible AI Development,”‬‭
Meta Newsroom‬
‭
,‬
‭
December 12, 2023,‬‭
https://about.fb.com/news/2023/12/purple-llama-safe-responsible-ai-development‬
‭
.‬
‭
[‬
‭
perma.cc/4M6U-P9VG‬
‭
].‬
‭
20‬‭
Joe Spisak and Sergey Edunov, “The Llama Ecosystem: Past, Present, and Future,”‬‭
Meta AI‬‭
(blog),‬
‭
September 27, 2023,‬‭
https://ai.meta.com/blog/llama-2-updates-connect-2023‬‭
[‬
‭
perma.cc/USS2-5JWY‬
‭
].‬
‭
19‬‭
Competitions and Markets Authority. “AI Foundation Models: Initial Review,”‬‭
GOV.UK‬
‭
, February 28,‬
‭
2024,‬‭
https://www.gov.uk/cma-cases/ai-foundation-models-initial-review‬
‭
.‬‭
[‬
‭
perma.cc/V6H3-3KLD‬
‭
].‬
‭
6‬
‭
Relying on closed models is especially fraught when efforts over API through a closed‬
‭
foundation model provider to fine-tune for a particular application are not portable to another‬
‭
foundation model provider or cloud host, thereby further reducing competition and increasing‬
‭
vendor lock-in. In contrast, an open model can be hosted purely internally, or externally with a‬
‭
wide variety of hosts, and can be moved between them.‬
‭
The customization enabled by OFMs also can speed dispersion of the technology in culturally‬
‭
relevant forms, throughout the globe including the global south. Right now, most major‬
‭
commercial OFMs are English-dominated, but LLaMA-2 has enabled researchers to train‬
‭
models for low-resource languages such as those spoken in Southeast Asia, in ways that better‬
‭
reflect local cultural norms, values, and legal considerations.‬
‭
27‬‭
The availability of open,‬
‭
multilingual language models such as the open access Aya LLM‬
‭
28‬‭
can similarly broaden access‬
‭
to LLMs globally, and promote further innovation in geographic areas that are typically‬
‭
underserved by the dominant closed systems.‬
‭
In sum, the course of OFM development over just the course of the past couple of years‬
‭
demonstrates that it is already a powerful competitive alternative to closed foundation models,‬
‭
and likely will continue to be unless artificially stymied by restrictions on publication.‬
‭
Catalyzing Innovation (Both in AI and Other Fields)‬
‭
As described above, OFMs are already driving innovation across the ecosystem as tens or‬
‭
hundreds of thousands of businesses begin adapting model capabilities to their own use cases‬
‭
and customer needs in a wide variety of contexts. And the spread of these technologies will also‬
‭
help science advance in a wide range of fields. But open source is also at the root of much AI‬
‭
innovation today.‬
‭
Open source machine learning software including OFMs can and already has driven significant‬
‭
advances in AI technology. Indeed, the current flourishing of generative AI and foundation‬
‭
model technology would not have been possible without open research. For example, the 2017‬
‭
paper that originated the technology that underlies today’s LLMs, transformer networks, was‬
‭
open research with open code and data,‬
‭
29‬‭
as was the research paper that debuted one of the‬
‭
most popular early language models,‬
‭
30‬‭
work that enabled the current closed foundation models‬
‭
to design their systems. Without this open research — including the release of what at the time‬
‭
were the most sophisticated or “frontier” language models — and open source ML development‬
‭
30‬‭
Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language‬
‭
Understanding,”‬‭
arXiv‬
‭
, October 11, 2018,‬‭
https://arxiv.org/abs/1810.04805‬
‭
.‬‭
[‬
‭
perma.cc/W25M-DVNY‬
‭
].‬
‭
29‬‭
​
Ashish Vaswani et al.,‬‭
Attention Is All You Need‬
‭
,‬‭
31st Conference on Neural Information Processing‬
‭
Systems (NIPS 2017)‬
‭
, vol. 30, 2017,‬‭
https://arxiv.org/abs/1706.03762‬
‭
.‬‭
[‬
‭
perma.cc/TMN4-GX84‬
‭
].‬
‭
28‬‭
Ahmet Üstün et al., “Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model,”‬
‭
arXiv‬
‭
, February 12, 2024,‬‭
https://arxiv.org/abs/2402.07827‬
‭
.‬‭
[‬
‭
perma.cc/LF7V-Z2CJ‬
‭
].‬
‭
27‬‭
Xuan-Phi Nguyen et al., “SeaLLMs -- Large Language Models for Southeast Asia,”‬‭
arXiv‬
‭
, December 1,‬
‭
2023,‬‭
https://arxiv.org/abs/2312.00738‬‭
[‬
‭
perma.cc/5UW5-MZLX‬
‭
];‬‭
Jun Zhao et al., “‬
‭
Llama‬‭
Beyond English:‬
‭
An Empirical Study on Language Capability Transfer,”‬‭
arXiv‬
‭
, January 2, 2024,‬
‭
https://arxiv.org/abs/2401.01055‬‭
[‬
‭
perma.cc/YPR5-7JX5‬
‭
].‬
‭
7‬
‭
frameworks like Pytorch and TensorFlow, today’s closed models would not even exist. In fact, it‬
‭
is fair to conclude that almost‬‭
all‬‭
ML development‬‭
has heavily relied on advances in open tools‬
‭
and open models.‬
‭
31‬‭
Meanwhile, open research is also facilitating new progress in finding ways‬
‭
to enhance the safety of models, including research to help ensure interventions remain durable‬
‭
or can be enhanced even after models are released.‬
‭
32‬
‭
Open models have not only been critical catalysts for the development of foundation models,‬
‭
both open and closed. They have also been a key ingredient for creating smaller, more efficient,‬
‭
and customized models with little cost that can rival larger foundation models. For example,‬
‭
●‬ ‭
Vicuna-13B is an open-source chatbot that was developed by fine-tuning Llama based‬
‭
on user-shared conversations with ChatGPT. At the time of release, the model showed‬
‭
strong performance in preliminary assessments compared to ChatGPT and Bard despite‬
‭
costing only around $300 to train.‬
‭
33‬
‭
●‬ ‭
Researchers used the model weights of Mistral 7B, a 7.3 billion parameter model‬
‭
released under an open source license by the startup Mistral,‬
‭
34‬‭
to decrease the‬
‭
computational power required for fine-tuning the model for downstream tasks by a factor‬
‭
of ten.‬
‭
35‬
‭
●‬ ‭
Alpaca 7B, a language model developed by fine-tuning Llama for instruction following,‬
‭
demonstrated qualitatively similar performance to GPT-3.5 while costing under $600 to‬
‭
train.‬
‭
36‬
‭
●‬ ‭
Koala-13B, a language model developed by fine-tuning Llama on dialogue data scraped‬
‭
from the web, demonstrated better performance than Alpaca and similar performance to‬
‭
ChatGPT in preliminary assessments. Koala cost around $100 to train.‬
‭
37‬
‭
37‬‭
Ritwik Gupta et al., “Koala: A Dialogue Model for Academic Research,”‬‭
The Berkeley Artificial‬
‭
Intelligence Research Blog‬
‭
, April 3, 2023,‬‭
https://bair.berkeley.edu/blog/2023/04/03/koala‬
‭
.‬
‭
[‬
‭
perma.cc/C956-J3ZN‬
‭
].‬
‭
36‬‭
Rohan Taori et al., “Alpaca: A Strong, Replicable Instruction-Following Model,”‬‭
Stanford Center for‬
‭
Research on Foundation Models‬
‭
, March 13, 2023,‬‭
https://crfm.stanford.edu/2023/03/13/alpaca.html‬
‭
.‬
‭
[‬
‭
perma.cc/V5GY-4GBQ‬
‭
].‬
‭
35‬‭
James Liu et al., “BitDelta: Your Fine-Tune May Only Be Worth One Bit,”‬‭
arXiv‬
‭
, February 15, 2024,‬
‭
https://arxiv.org/abs/2402.10193‬
‭
.‬‭
[‬
‭
perma.cc/4QWF-73W7‬
‭
].‬
‭
34‬‭
Albert Q. Jiang et al., “Mistral 7B,”‬‭
arXiv‬
‭
, October‬‭
10, 2023,‬‭
https://arxiv.org/abs/2310.06825‬
‭
.‬
‭
[‬
‭
perma.cc/TZT5-UXXY‬
‭
].‬
‭
33‬‭
The Vicuna Team. “Vicuna: An Open-Source Chatbot Impressing GPT-4 With 90%* ChatGPT Quality.”‬
‭
LMSYS Org (blog)‬
‭
, March 30, 2023.‬‭
https://lmsys.org/blog/2023-03-30-vicuna‬
‭
.‬‭
[‬
‭
perma.cc/BG2M-TLGE‬
‭
]‬
‭
32‬‭
Eric Mitchell et al., “Fast Model Editing at Scale,”‬‭
arXiv‬
‭
, October 21, 2021,‬
‭
https://arxiv.org/abs/2110.11309‬‭
[‬
‭
perma.cc/X29Y-WFU2‬
‭
];‬‭
Kevin Meng et al.,‬‭
Locating and Editing Factual‬
‭
Associations in GPT‬
‭
,‬‭
Advances in Neural Information‬‭
Processing Systems 35 (NeurIPS 2022)‬
‭
, vol. 35,‬
‭
2022,‬
‭
https://proceedings.neurips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstr‬
‭
act-Conference.html‬‭
[‬
‭
perma.cc/PCT3-EMUP‬
‭
].‬
‭
31‬‭
Max Langenkamp and Daniel N. Yue,‬‭
How Open Source‬‭
Machine Learning Software Shapes AI‬
‭
,‬
‭
Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES ’22)‬
‭
, 2022,‬
‭
https://doi.org/10.1145/3514094.3534167‬
‭
.‬‭
[‬
‭
perma.cc/PX3S-DUGM‬
‭
].‬
‭
8‬
‭
Importantly, the innovation in developing smaller and more powerful models, often based‬
‭
directly on much larger models, is not just important in terms of competition and innovation. It is‬
‭
also important because some models such as Mistral 7B are now small enough to run locally on‬
‭
an end-user’s laptop or even a phone, mitigating the need for a cloud-based provider at all.‬
‭
38‬
‭
This brings a number of benefits to consumers and society, including privacy (data need not‬
‭
travel the internet or go to anyone’s cloud), greater speed, and no need for access to the‬
‭
internet or hosting at a data center (particularly relevant given the environmental impacts of the‬
‭
AI-driven demand for data centers‬
‭
39‬
‭
).‬
‭
Furthermore, OFMs enable a variety of AI research not enabled by closed foundation models,‬
‭
40‬
‭
including research around AI interpretability methods,‬
‭
41‬‭
security, model training and inference‬
‭
efficiency,‬
‭
42‬‭
and the public development of robust watermarking techniques.‬
‭
43‬
‭
Finally, faster dispersion of open models means faster advancement of scientific research‬
‭
across fields, and doing that research with open models can help address issues of scientific‬
‭
reproducibility and verifiability. For example,‬‭
in‬‭
a meta-analysis of over 400 papers addressing‬
‭
the utility of AI in imaging for COVID-19 patient care, the highest quality papers almost all relied‬
‭
on open pretrained models, suggesting that the availability of open source models may be‬
‭
crucial to future AI-enabled medical advancements.‬
‭
44‬‭
As the researchers concluded, “[g]iven the‬
‭
global, unprecedented public health challenge caused by COVID-19, we strongly encourage‬
‭
medical researchers to follow the trends toward open-source development in the field of ML.”‬
‭
45‬
‭
This admonition could just as well apply to other urgent public needs and scientific research in‬
‭
general, but the need for such research and deployment transparency is most especially‬
‭
important in sensitive use cases including medicine, as we’ll explore more below.‬
‭
45‬‭
Id.‬
‭
44‬‭
Jannis Born et al., “On The Role of Artificial Intelligence in Medical Imaging of COVID-19,”‬‭
Patterns‬‭
2,‬
‭
no. 6 (April 30, 2021): 100269,‬‭
https://doi.org/10.1016/j.patter.2021.100269‬
‭
.‬‭
[‬
‭
perma.cc/5TK8-FMCR‬
‭
].‬
‭
43‬‭
John Kirchenbauer et al., “A Watermark for Large Language Models,”‬‭
arXiv.Org‬
‭
, January 24, 2023,‬
‭
https://arxiv.org/abs/2301.10226‬
‭
.‬‭
[‬
‭
perma.cc/7ZRX-7876‬
‭
].‬
‭
42‬‭
Tim Dettmers et al., “QLoRA: Efficient Finetuning of Quantized LLMs,” arXiv.org, May 23, 2023,‬
‭
https://arxiv.org/abs/2305.14314‬‭
[‬
‭
perma.cc/66JX-ALXM‬
‭
];‬‭
Sunny S. Sanyal et al., “Early Weight Averaging‬
‭
Meets High Learning Rates for LLM Pre-training,”‬‭
arXiv‬
‭
,‬‭
June 5, 2023,‬‭
https://arxiv.org/abs/2306.03241‬
‭
[‬
‭
perma.cc/QEW7-M86W‬
‭
].‬
‭
41‬‭
Kevin Clark et al., “What Does BERT Look at? An Analysis of BERT’s Attention,”‬‭
arXiv‬
‭
, June 11, 2019,‬
‭
https://arxiv.org/abs/1906.04341‬
‭
.‬‭
[‬
‭
perma.cc/PH9S-BB4S‬
‭
].‬
‭
40‬‭
Bommasani et al., 2023,‬‭
supra‬‭
note 13.‬
‭
39‬‭
Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat, “Estimating the Carbon Footprint‬
‭
of BLOOM, a 176B Parameter Language Model,”‬‭
Journal‬‭
of Machine Learning Research 2‬‭
24 (2023),‬
‭
https://www.jmlr.org/papers/v24/23-0069.html‬‭
[‬
‭
perma.cc/64UB-8LP9‬
‭
];‬‭
David Patterson et al., “Carbon‬
‭
Emissions and Large Neural Network Training,”‬‭
arXiv‬
‭
,‬‭
April 21, 2021,‬‭
https://arxiv.org/abs/2104.10350‬
‭
[‬
‭
perma.cc/5WN2-J8XS‬
‭
].‬
‭
38‬‭
Jennie Rose, “How to Run Llama 2 Locally: The Ultimate Guide for Mac, Windows, and Mobile‬
‭
Devices,”‬‭
Cheatsheet.Md‬
‭
, March 17, 2024,‬
‭
https://cheatsheet.md/llm-leaderboard/how-to-install-local-llama‬‭
[‬
‭
perma.cc/ZY5F-9WC8‬
‭
]; Chris McKay,‬
‭
“How to Get Started With Mistral 7B,”‬‭
Maginative‬
‭
,‬‭
September 29, 2023,‬
‭
https://www.maginative.com/article/how-to-get-started-with-mistral‬‭
[‬
‭
perma.cc/ZZN7-FWSW‬
‭
].‬
‭
9‬
‭
Ensuring Transparency (and Security and Accountability)‬
‭
“With enough eyeballs,” an old saying in open source software development goes, “all bugs are‬
‭
shallow.”‬
‭
46‬‭
By opening itself to scrutiny from a global community of professional and amateur‬
‭
developers, opening software has typically been viewed as a benefit rather than a detriment.‬
‭
Through open sourcing, one can essentially recruit an army of white hat hackers to help counter‬
‭
the army of black hat hackers that will be trying to break the thing.‬
‭
47‬‭
This is considered a helpful‬
‭
strategy because a strategy of secrecy — or “security by obscurity” — is less effective when a‬
‭
target can be repeatedly and endlessly probed by hostile actors (e.g. is connected to the‬
‭
internet).‬
‭
48‬‭
Openness enables, for example, powerful pro-security efforts like the OSS-Fuzz‬
‭
project, which continuously scans across hundreds of open-source projects for vulnerabilities.‬
‭
49‬
‭
It is because of these security benefits, amongst others such as cost and customizability, that‬
‭
the federal government strongly prefers open source software, even as it works to further‬
‭
improve its security. For example, the Digital Services Playbook urges government offices to‬
‭
“default to open,”‬
‭
50‬‭
NIST has long recommended for secure systems a principle of “open‬
‭
design”, i.e., that “security should not depend on the secrecy of the implementation or its‬
‭
components,”‬
‭
51‬‭
and the Department of Defense site on open source highlights that “[c]ontinuous‬
‭
and broad peer review, enabled by publicly available source code, improves software reliability‬
‭
and security through the identification and elimination of defects that might otherwise go‬
‭
unrecognized by the core development team.”‬
‭
52‬
‭
Of course, AI systems are not exactly the same as other software systems: although various‬
‭
components of a foundation model are software, the source code of which can be examined by‬
‭
programmers, the weights themselves — essentially a massive multidimensional database of‬
‭
relationships between the tokens the model was trained on — are not directly human-readable.‬
‭
Furthermore, the “safety” issues that many may want to test a foundation modell for are not‬
‭
typically traditional security vulnerabilities in code but rather poor or harmful model behavior or‬
‭
generated outputs. Finally, to the extent such issues are discovered in an OFM, patches via‬
‭
52‬‭
U.S. Department of Defense. “Open Source Software FAQ.” U.S. Department of Defense Chief‬
‭
Information Officer, October 28, 2021,‬‭
https://dodcio.defense.gov/Open-Source-Software-FAQ‬
‭
.‬
‭
[‬
‭
perma.cc/KW2U-U739‬
‭
]‬
‭
51‬‭
Karen Scarfone et al., NIST Special Publication 800-123,‬‭
Guide to General Server Security‬‭
(2008),‬
‭
https://csrc.nist.gov/pubs/sp/800/123/final‬
‭
, at 4.‬‭
[‬
‭
perma.cc/JCY7-3ANY‬
‭
]‬
‭
50‬‭
U.S. Digital Service. “The Digital Services Playbook,” n.d.‬‭
https://playbook.cio.gov/‬
‭
.‬
‭
[‬
‭
perma.cc/2VYD-GM9G‬
‭
]‬
‭
49‬‭
Dongge Liu et al., “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier,” Google Online Security‬
‭
Blog, August 16, 2023,‬
‭
https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html‬
‭
.‬
‭
[‬
‭
perma.cc/B3T6-TPNA‬
‭
]‬
‭
48‬‭
Peter P. Swire, “A Model for When Disclosure Helps Security: What Is Different About Computer and‬
‭
Network Security?,”‬‭
Journal on Telecommunications‬‭
and High Technology Law‬‭
163 (2004),‬
‭
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=531782‬
‭
.‬‭
[‬
‭
perma.cc/5B9R-XYKJ‬
‭
]‬
‭
47‬‭
Steven Weber, “The Success of Open Source,” in‬‭
Harvard University Press eBooks‬
‭
, 2004,‬
‭
https://doi.org/10.4159/9780674044999‬
‭
.‬‭
[‬
‭
perma.cc/GMD2-V5LU‬
‭
]‬
‭
46‬‭
Eric S. Raymond, “The Cathedral and the Bazaar,”‬‭
Knowledge, Technology & Policy‬‭
12, no. 3‬
‭
(September 1, 1999): 23-49,‬‭
https://doi.org/10.1007/s12130-999-1026-0‬
‭
.‬‭
[‬
‭
perma.cc/7Z3G-V9CB‬
‭
]‬
‭
10‬
‭
fine-tuning or other interventions may not be universally adopted, particularly by malicious‬
‭
actors. An unfixed model version could still be available to those who seek to use it — and‬
‭
because patches are not just security fixes but suppression of concepts or capabilities that‬
‭
malicious actors may want to deploy offensively, the malicious actors may be motivated to‬
‭
continue using unmitigated or maliciously modified versions of the model that facilitate their‬
‭
offensive goals. Therefore not all of the assumptions around security and open source may hold‬
‭
in the case of AI, and more study in this area is warranted.‬
‭
53‬
‭
That said, we are already seeing examples of openness — both in the data on which OFMs are‬
‭
trained, and in the models themselves — contributing to security, safety, and other critical‬
‭
research beneficial to both open‬‭
and‬‭
closed systems,‬‭
as well as instances where similar‬
‭
research into closed systems has been stymied. For example, researchers’ discovery of child‬
‭
sexual abuse material (CSAM) in the LAION data set that is regularly used by both open and‬
‭
closed models would not have been possible if that data set were not open.‬
‭
54‬‭
Similarly,‬
‭
foundational research on the fragility of fine-tuned guardrails in both open and‬‭
closed models —‬
‭
not only when subject to deliberate attack but even when subject to benign fine-tuning for other‬
‭
purposes — was based on use and examination of OFMs and would not have been possible‬
‭
otherwise.‬
‭
55‬‭
Meanwhile, recent state-of-the-art work in auditing of closed models also leverages‬
‭
OFMs: for example, the Llama-based Vicuna model trained on GPT-4 outputs has enabled‬
‭
researchers to identify attack vectors that can then be tested on GPT-4 itself.‬
‭
56‬
‭
By contrast, as already mentioned, a number of types of general AI research and auditing‬
‭
cannot be fully conducted with closed foundation models.‬
‭
57‬‭
A recent and relevant example of‬
‭
the importance of a broader community testing for AI harms is new research on bias in LLMs,‬
‭
which revealed that even when models do not exhibit overt racial bias in their responses to‬
‭
users, they can contain and exhibit more subtle biases in consequential domains such as‬
‭
employment and criminal justice when prompts contain African American English (AAE) as‬
‭
opposed to Standard American English (SAE). For example, GPT-4 was shown to be more‬
‭
likely to suggest that defendants be sentenced to death when they provide statements in AAE.‬
‭
Even more troubling, this covert bias was seen to‬‭
increase‬‭
rather than decrease with the size of‬
‭
models, and while it is possible for human feedback training to mitigate this covert bias based‬
‭
on dialect, it also can exacerbate it by teaching models to superficially conceal overt racial‬
‭
biases while still containing covert ones.‬
‭
58‬
‭
58‬‭
Valentin Hofmann et al., “Dialect Prejudice Predicts AI Decisions About People’s Character,‬
‭
Employability, and Criminality,”‬‭
arXiv‬
‭
, March 1, 2024,‬‭
https://arxiv.org/abs/2403.00742‬
‭
57‬‭
Supra‬‭
at notes 40-43.‬
‭
56‬‭
Andy Zou et al., “Universal and Transferable Adversarial Attacks on Aligned Language Models,”‬‭
arXiv‬
‭
,‬
‭
July 27, 2023,‬‭
https://arxiv.org/abs/2307.15043‬
‭
.‬‭
[‬
‭
perma.cc/ED4C-Q9ZE‬
‭
]‬
‭
55‬‭
Xiangyu Qi et al., “Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do‬
‭
Not Intend To!,”‬‭
arXiv‬
‭
, October 5, 2023,‬‭
https://arxiv.org/abs/2310.03693‬
‭
.‬‭
[‬
‭
perma.cc/G6R8-XHQT‬
‭
]‬
‭
54‬‭
David Thiel, “Identifying and Eliminating CSAM in Generative ML Training Data and Models” (Stanford‬
‭
Internet Observatory Cyber Policy Center, December 20, 2023),‬‭
https://purl.stanford.edu/kh752sm9123‬
‭
.‬
‭
[‬
‭
perma.cc/UFU8-HRAR‬
‭
]‬
‭
53‬‭
See Toby Shevlane and Allan Dafoe, “The Offense-Defense Balance of Scientific Knowledge: Does‬
‭
Publishing AI Research Reduce Misuse?,”‬‭
arXiv‬
‭
, December 27, 2019,‬‭
https://arxiv.org/abs/2001.00463‬
‭
.‬
‭
[‬
‭
perma.cc/387A-MMA4‬
‭
]‬
‭
11‬
‭
This research is notable not only for the severity of the problem it highlights, nor for its stark‬
‭
demonstration of why these systems are not ready to make important decisions about human‬
‭
lives. It demonstrates the inherent limitation in testing closed models for harms: the researchers‬
‭
were not able to use their entire battery of tests and fully complete their research on GPT-4‬
‭
compared to other language models due to the closed nature of GPT-4.‬
‭
59‬
‭
The inability to effectively audit closed foundation models compared to OFMs is a systemic‬
‭
problem.‬
‭
60‬‭
As recent research explains, audits can be broken into black-box methods (auditors‬
‭
can only access inputs and outputs, not internal model weights) and white-box methods‬
‭
(auditors get unrestricted access to internal workings, including model weights). Closed‬
‭
foundation models can only be audited using black-box methods, but OFMs can also be audited‬
‭
with white-box methods. Additional contextual materials about a model’s development, whether‬
‭
closed or open, can also enable additional “outside the box” auditing.‬
‭
Black-box audits assess model characteristics using test sets as inputs or otherwise trying to‬
‭
find inputs that lead to harmful outputs.‬
‭
61‬‭
However, these approaches are fundamentally limited‬
‭
in how they are able to identify harms, since these methods amount to searching for problems‬
‭
only in an exploratory manner or where robust and reliable evaluations exist. Black-box audit‬
‭
methods also fail to allow for auditors to gain a generalized understanding of how a system‬
‭
works and what its shortcomings might be. Many larger models are actually combinations of a‬
‭
variety of different expert models; but without the ability to understand or access those‬
‭
components separately, a black-box study could easily and inadvertently focus on one part of a‬
‭
model while unknowingly overlooking other parts. Furthermore, because black-box audits are‬
‭
necessarily based entirely on responses to inputs, minor differences in the content of those‬
‭
inputs can lead to widely different results, making those results less reliable and harder to‬
‭
61‬‭
E.g., Alexander Wei, Nika Haghtalab, and Jacob Steinhardt, “Jailbroken: How Does LLM Safety‬
‭
Training Fail?,”‬‭
arXiv‬
‭
, July 5, 2023,‬‭
https://arxiv.org/abs/2307.02483‬‭
[‬
‭
perma.cc/4WW9-DRXF‬
‭
]; Hofmann‬
‭
et al., 2024,‬‭
supra‬‭
note 58; Jesutofunmi A. Omiye‬‭
et al., “Large Language Models Propagate Race-based‬
‭
Medicine,”‬‭
Npj Digital Medicine‬‭
6, no. 1 (October‬‭
20, 2023),‬‭
https://doi.org/10.1038/s41746-023-00939-z‬
‭
[‬
‭
perma.cc/L9KW-8JKQ‬
‭
].‬
‭
60‬‭
Stephen Casper et al., “Black-Box Access Is Insufficient for Rigorous AI Audits,”‬‭
arXiv‬
‭
, January 25,‬
‭
2024,‬‭
https://arxiv.org/abs/2401.14446‬‭
[‬
‭
perma.cc/Q29G-XVB2‬
‭
];‬‭
see also Victor Ojewale et al., “Towards‬
‭
AI Accountability Infrastructure: Gaps and Opportunities in AI Audit Tooling,” arXiv.org, February 27, 2024,‬
‭
https://arxiv.org/abs/2402.17861‬‭
[‬
‭
perma.cc/LML4-K465‬
‭
].‬
‭
59‬‭
Id. (Specifically, the researchers compared probabilities of adjectives related to African Americans, but‬
‭
could not conduct this analysis since it requires access to the probabilities for all adjectives, which‬
‭
GPT-4’s API only provides for the top five predicted tokens; researchers could also not compute model‬
‭
perplexity using the OpenAI API so excluded GPT-4 from analyses based on perplexity).‬
‭
[‬
‭
perma.cc/C88W-YAUG‬
‭
]. For a summary, see also Elizabeth Gibney, “Chatbot AI makes racist‬
‭
judgements on the basis of dialect,”‬‭
Nature‬‭
627, no.‬‭
8004 (March 13, 2024): 476-77,‬
‭
https://doi.org/10.1038/d41586-024-00779-1‬‭
[‬
‭
perma.cc/23P2-D37E‬
‭
].‬
‭
12‬
‭
reproduce.‬
‭
62‬‭
Unreliable measurements in turn makes it that much harder for auditors to assess‬
‭
the source of problems‬
‭
63‬‭
and to recommend specific mitigations to improve the model.‬
‭
64‬
‭
White-box auditing methods, in contrast, allow for testing to be more directly guided by auditors‬
‭
and more efficient in locating problems than the unguided, trial-and-error probing involved in‬
‭
black-box methods. In particular, methods unique to white-box models such as gradient-based‬
‭
optimization allow for finding attack vectors in vision systems,‬
‭
65‬‭
and to a lesser extent language‬
‭
ones.‬
‭
66‬‭
Gradient-based techniques (again, unique to white-box models) also could allow‬
‭
auditors to better understand a model’s individual decisions by highlighting what part of a given‬
‭
input (e.g. a prompt, or an image to classify) is most relevant to the generation of a given‬
‭
output.‬
‭
67‬‭
White-box access also better enables a range of novel auditing methods including‬
‭
“methods based on local search, rejection sampling at scale, Langevin dynamics, evolutionary‬
‭
algorithms, and reinforcement learning.”‬
‭
68‬‭
This range of white-box methods allow auditors to‬
‭
probe more effectively for new capabilities and to test for jailbreaks.‬
‭
69‬
‭
Furthermore, in contrast to white-box audits, closed models that are only open to black-box‬
‭
methods can control who is allowed to probe or audit their systems and how. Over 300‬
‭
researchers have complained in an open letter that “AI companies have already suspended‬
‭
researcher accounts and even changed their terms of service to deter some types of‬
‭
evaluation.”‬
‭
70‬‭
This creates a chilling effect on independent research of closed models, including‬
‭
70‬‭
Letter from Arvind Narayanan et al., “A Safe Harbor for Independent AI Evaluation,” March 2024,‬
‭
https://sites.mit.edu/ai-safe-harbor/‬
‭
.‬‭
[‬
‭
perma.cc/WC5C-ZYMN‬
‭
]‬
‭
69‬‭
e.g., Alain Guillaume and Yoshua Bengio, “Understanding Intermediate Layers Using Linear Classifier‬
‭
Probes,”‬‭
arXiv‬
‭
, October 5, 2016,‬‭
https://arxiv.org/abs/1610.01644‬‭
[‬
‭
perma.cc/3QFV-FNWA‬
‭
]; Priya Goyal,‬
‭
Adriana Romero Soriano, Caner Hazirbas, and Levent Sagun, “Fairness Indicators for Systematic‬
‭
Assessments of Visual Feature Extractors,”‬‭
arXiv‬
‭
,‬‭
February 15, 2022.‬‭
https://arxiv.org/abs/2202.07603‬
‭
[‬
‭
https://perma.cc/K87Y-9E9Z‬
‭
].‬
‭
68‬‭
Stephen Casper et al., “Black-Box Access Is Insufficient for Rigorous AI Audits,”‬‭
arXiv‬
‭
, January 25,‬
‭
2024,‬‭
https://arxiv.org/abs/2401.14446‬
‭
.‬‭
[‬
‭
perma.cc/FU5Z-9URX‬
‭
]‬
‭
67‬‭
Arun Das and Paul Rad, “Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A‬
‭
Survey,”‬‭
arXiv‬
‭
, June 16, 2020,‬‭
https://arxiv.org/abs/2006.11371‬
‭
.‬‭
[‬
‭
perma.cc/2BL2-9NMT‬
‭
]‬
‭
66‬‭
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh, “Universal Adversarial‬
‭
Triggers for Attacking and Analyzing NLP,”‬‭
arXiv‬
‭
,‬‭
August 20, 2019,‬‭
https://arxiv.org/abs/1908.07125‬
‭
.‬
‭
[‬
‭
perma.cc/MV7A-BTVW‬
‭
]‬
‭
65‬‭
Ian J. Goodfellow,, Jonathon Shlens, and Christian Szegedy, “Explaining and Harnessing Adversarial‬
‭
Examples,”‬‭
arXiv‬
‭
, December 20, 2014,‬‭
https://arxiv.org/abs/1412.6572‬
‭
.‬‭
[‬
‭
perma.cc/6LPX-CNK3‬
‭
]‬
‭
64‬‭
Song Wang et al., “Knowledge Editing for Large Language Models: A Survey,”‬‭
arXiv‬
‭
, October 24, 2023,‬
‭
https://arxiv.org/abs/2310.16218‬
‭
.‬‭
[‬
‭
perma.cc/74EB-M8D4‬
‭
]‬
‭
63‬‭
Amy Winecoff and Miranda Bogen, “Trustworthy AI Needs Trustworthy Measurements - Center for‬
‭
Democracy and Technology,” Center for Democracy and Technology, March 6, 2024,‬
‭
https://cdt.org/insights/trustworthy-ai-needs-trustworthy-measurements‬
‭
.‬‭
[‬
‭
perma.cc/NX6E-WXXM‬
‭
]‬
‭
62‬‭
Moran Mizrahi et al., “State of What Art? A Call‬‭
for Multi-Prompt LLM Evaluation,”‬‭
arXiv‬
‭
, December‬‭
31,‬
‭
2023,‬‭
https://doi.org/10.48550/arxiv.2401.00595‬‭
[‬
‭
perma.cc/5JLL-8EFV‬
‭
];‬‭
Norah Alzahrani et al., “When‬
‭
Benchmarks Are Targets: Revealing the Sensitivity of Large Language Model Leaderboards,”‬‭
arXiv‬
‭
,‬
‭
February 1, 2024,‬‭
https://arxiv.org/abs/2402.01781‬‭
[‬
‭
perma.cc/XK73-QNGZ‬
‭
].‬
‭
13‬
‭
research to detect potential harms that developers may have overlooked, due to fear of loss of‬
‭
access to the model or even legal reprisals.‬
‭
71‬
‭
The need for white-box and outside-the-box auditing beyond closed box methods is all the more‬
‭
evident when considering that foundation models may be applied in contexts where‬
‭
consequential decisions about people are being made, which puts them at risk of being‬
‭
subjected to systemic biases. We’ve already described the covert racial bias study which has‬
‭
troubling implications for a wide variety of deployment contexts. Meanwhile, experts and‬
‭
practitioners in other sensitive fields like medicine and law are also calling for more reliance on‬
‭
open models to ensure greater control and better decisionmaking,‬
‭
72‬‭
especially in the face of‬
‭
studies (e.g.) demonstrating racial bias in medical decisionmaking by closed models.‬
‭
73‬
‭
Finally, the availability of OFMs will necessarily assist in the education and training of new‬
‭
computer scientists in the particulars of how to develop, test, and deploy foundation models,‬
‭
opportunities that will be much more limited if students can only engage in white-box testing of‬
‭
less advanced open models (where learnings may not be transferable to larger or more‬
‭
advanced systems) and black-box testing with a few closed foundation model systems. The‬
‭
usefulness for researchers of the emerging National AI Research Resource will also turn on‬
‭
such access; the alternative is a less useful program that relies on closed services donating‬
‭
access to their models, and/or the government paying them. We should not build an open‬
‭
national research resource in a manner that drives researchers and taxpayer dollars to rely on‬
‭
closed services, rather than openly available and testable assets like OFMs.‬
‭
II.‬
‭
MARGINAL RISKS OF OPEN FOUNDATION MODELS‬
‭
In evaluating the risks of OFMs, we must consider them in comparison to the existing risks‬
‭
enabled by closed models, by access to existing technologies such as the internet, and by‬
‭
smaller models that carry similar risks but for which controlling proliferation would be much‬
‭
harder if not impossible. In other words, we must consider the‬‭
marginal‬‭
risk of OFMs.‬
‭
74‬
‭
74‬‭
Kapoor et al., 2024‬‭
supra‬‭
note 13. According to the‬‭
framework in this paper, to effectively gauge‬
‭
marginal risk, one must not only identify the risk, but also existing risks absent OFMs, existing defenses‬
‭
absent OFMs, actual evidence of marginal risk, new defenses that could be used, and the assumptions‬
‭
underlying the analysis. Existing literature on OFM risks are severely lacking in several of these‬
‭
categories, therefore “[a]cross several misuse vectors (e.g. cyberattacks, bioweapons), we find that‬
‭
73‬‭
Jesutofunmi A. Omiye, Jonathan Lester, Simon Spichak, Veronica Rotemberg, and Roxana Daneshjou,‬
‭
“Large Language Models Propagate Race-based Medicine,”‬‭
Npj Digital Medicine‬‭
6, no. 1 (October 20,‬
‭
2023),‬‭
https://doi.org/10.1038/s41746-023-00939-z‬
‭
.‬‭
[‬
‭
perma.cc/8RMQ-FVMV‬
‭
]‬
‭
72‬‭
Augustin Toma, Senthujan Senkaiahliyan, Patrick R. Lawler, Barry B. Rubin, and Bo Wang, “Generative‬
‭
AI Could Revolutionize Health Care — but Not if Control Is Ceded to Big Tech,”‬‭
Nature‬‭
624, no. 7990‬
‭
(November 30, 2023): 36-38,‬‭
https://doi.org/10.1038/d41586-023-03803-y‬‭
[‬
‭
perma.cc/2EN4-ESJF‬
‭
];‬
‭
Anthony Dang, “The Open Advantage: Winning the Adversarial Battle With Open-Source Models,”‬‭
Social‬
‭
Science Research Network‬
‭
, January 1, 2023,‬‭
https://doi.org/10.2139/ssrn.4651571‬
‭
[‬
‭
perma.cc/M9KX-KZE8‬
‭
].‬
‭
71‬‭
Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane‬
‭
Blili-Hamelin, Yangsibo Huang, et al., “A Safe Harbor for AI Evaluation and Red Teaming,”‬‭
arXiv‬
‭
, March‬‭
7,‬
‭
2024,‬‭
https://arxiv.org/abs/2403.04893‬
‭
.‬‭
[‬
‭
perma.cc/QS2B-AZTC‬
‭
]‬
‭
14‬
‭
It is critical to marshall meaningful evidence demonstrating the likelihood and severity of actual‬
‭
marginal risks before policymaking, to ensure that proposed solutions are a good fit for the‬
‭
problem. And while there are a number of organizations vigorously advocating the seriousness‬
‭
of certain risks, as the bipartisan leadership of the House Committee on Science, Space, and‬
‭
Technology observed, research findings from this community are “often self-referential and lack‬
‭
the quality that comes from revision in response to critiques by subject matter experts.”‬
‭
75‬
‭
This is not to suggest that these expert advocates are not sincere in their concerns or that their‬
‭
work doesn’t serve as an important foundation upon which more research can be based; we‬
‭
believe that the risks they raise must be considered seriously and carefully. However, more‬
‭
research would be required to establish clearer evidence of the risks that foundation models‬
‭
may facilitate‬
‭
76‬‭
— like the creation or deployment of chemical, biological, radiological, and‬
‭
nuclear weapons — before justifying broad restrictions on access to foundation models or the‬
‭
scientific discourse around them.‬
‭
To be sure, we appreciate the reasonable desire to create conditions where researchers and‬
‭
foundation model developers spend sufficient time and are sufficiently motivated to evaluate‬
‭
increasingly advanced technologies, gather and — where appropriate — share insights to‬
‭
determine whether marginal indicators of these risks can be detected, and reduce them prior to‬
‭
such systems being widely shared. However, the government’s role in aggressively intervening‬
‭
in the absence of particularized risk has traditionally been constrained in order to prevent‬
‭
arbitrary limitations on civil liberties or on the advancement of science.‬
‭
77‬‭
Through that lens, the‬
‭
case for there being a substantial marginal risk from existing or imminent open models‬
‭
compared to other technologies has not yet been adequately made.‬
‭
The Fragility of Safeguards in Open and Closed Models‬
‭
One key factor to consider when examining the state of marginal risk is the robustness of‬
‭
guardrails once models are fine-tuned, whether the models are open or closed. As the AI EO‬
‭
highlighted when calling for this public consultation, “[w]hen the weights for a dual-use‬
‭
foundation model are widely available — such as when they are publicly posted on the Internet‬
‭
77‬‭
See especially discussion of the First Amendment in Part III.‬
‭
76‬‭
Bommasani et al., 2023,‬‭
supra‬‭
note 13 (comparing‬‭
papers and noting key gaps where additional‬
‭
research is necessary).‬
‭
75‬‭
Letter to Laurie Locascio‬
‭
, U.S. Committee on Science,‬‭
Space, and Technology (Dec. 14, 2023),‬
‭
https://republicans-science.house.gov/_cache/files/8/a/8a9f893d-858a-419f-9904-52163f22be71/191E58‬
‭
6AF744B32E6831A248CD7F4D41.2023-12-14-aisi-scientific-merit-final-signed.pdf‬
‭
[‬
‭
perma.cc/9XN8-32VZ‬
‭
]. (omitting internal citations)‬‭
(“Organizations routinely point to significant‬
‭
speculative benefits or risks of AI systems but fail to provide evidence of their claims, produce‬
‭
nonreproducible research, hide behind secrecy, use evaluation methods that lack construct validity, or cite‬
‭
research that has failed to go through robust review processes, such as academic peer review.”); See‬
‭
also Shazeda Ahmed et al., “Field-building and the Epistemic Culture of AI Safety,”‬‭
First Monday‬
‭
(Forthcoming)‬
‭
, 2024,‬‭
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4641526‬
‭
[‬
‭
perma.cc/9YAR-W3YY‬
‭
].‬
‭
current research is insufficient to effectively characterize the marginal risk of open foundation models‬
‭
relative to pre-existing technologies.”‬
‭
15‬
‭
— there can be substantial benefits to innovation, but also substantial security risks, such as the‬
‭
removal of safeguards within the model.”‬
‭
78‬
‭
However, the same research that demonstrated the fragility of safeguards in open foundation‬
‭
models (in this case Llama-2) also discovered the very same weaknesses in the guardrails‬
‭
provided by closed foundation models (in this case GPT-3.5 Turbo fine-tuned via its API).‬
‭
79‬
‭
There, researchers fine-tuned both kinds of models on a handful (<100) of harmful instructions‬
‭
and responses. Results demonstrated that this procedure largely undermined existing‬
‭
safeguards in both kinds of models, enabling the fine-tuned models to produce harmful outputs‬
‭
across many categories that model safeguards would have otherwise prevented (e.g., illegal‬
‭
activity, hateful content, physical harm, adult content). Yet those were not the most worrisome‬
‭
findings. The researchers also discovered, using common datasets to simulate scenarios where‬
‭
downstream actors might attempt to fine-tune models for a specific purpose, that even such‬
‭
benign fine-tuning led to a notable increase in harmful responses from GPT. Together, these‬
‭
results suggest that model guardrails are not robust against downstream modification in open‬
‭
foundation models or closed ones, even if downstream actors do not seek to intentionally‬
‭
circumvent protections. That calls into question the validity of the claim that this is a key‬
‭
differentiating concern between open and closed foundation models.‬
‭
Additional research using somewhat different methods has also led to substantial‬
‭
demonstrations of the limitations of guardrails deployed by closed foundation models. For‬
‭
example:‬
‭
●‬ ‭
One study showed that fine-tuning GPT-4‬‭
(via API)‬‭
on 340 examples‬‭
can successfully‬
‭
undermine alignment at a rate of 95%. In other words, even more advanced models are‬
‭
not necessarily resilient against the types of attacks that are effective on less advanced‬
‭
models.‬
‭
80‬
‭
●‬ ‭
Another study identified three major categories of jailbreak prompts for both GPT-3.5‬
‭
and GPT-4 and demonstrated that these prompts successfully undermined models’‬
‭
defenses at a rate of 74.6%.‬
‭
81‬
‭
●‬ ‭
A study also found that GPT-4 could be used to automate the discovery of new prompts‬
‭
to jailbreak other language models, including Claude 2, Vicuna, and itself.‬
‭
82‬
‭
●‬ ‭
Yet another study demonstrated how low-resource languages (e.g., Zulu, Hmong) are a‬
‭
mechanism for jailbreaking GPT-4 using just prompt-level access. They evaluated‬
‭
jailbreaks using 12 languages of varying resource levels by taking a harmful prompt in‬
‭
English, translating it into another language using Google Translate, feeding it into‬
‭
GPT-4, and then translating that output back into English. Combining low-resource‬
‭
82‬‭
Rusheb Shah et al., “Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona‬
‭
Modulation,” arXiv.org, November 6, 2023,‬‭
https://arxiv.org/abs/2311.03348‬
‭
.‬‭
[‬
‭
perma.cc/HM8J-BS7J‬
‭
]‬
‭
81‬‭
Yi Liu et al., “Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study,”‬‭
arXiv‬
‭
, May 23, 2023,‬
‭
https://arxiv.org/abs/2305.13860‬
‭
.‬‭
[‬
‭
perma.cc/FU4Y-76ZY‬
‭
]‬
‭
80‬‭
Qiusi Zhan et al., “Removing RLHF Protections in GPT-4 via Fine-Tuning,”‬‭
arXiv‬
‭
, November 9, 2023,‬
‭
https://arxiv.org/abs/2311.05553‬
‭
.‬‭
[‬
‭
perma.cc/NY9X-SDVJ‬
‭
]‬
‭
79‬‭
Qi et al., 2024,‬‭
supra‬‭
note 55.‬
‭
78‬‭
AI EO,‬‭
supra‬‭
note 2.‬
‭
16‬
‭
languages allowed researchers to jailbreak GPT-4 79% of the time.‬
‭
83‬‭
This result is‬
‭
particularly worrisome not only because it is a technique that can be used by attackers,‬
‭
but also because it reflects how safety filters will work substantially less well for regular‬
‭
users who speak low-resource languages.‬
‭
84‬
‭
Research with similar results abound,‬
‭
85‬‭
and more can be expected in the future. We can also‬
‭
expect more public examples of the fragility of guardrails, as we see more public controversies‬
‭
involving content generated by closed models. For example, the recent spate of fake Taylor‬
‭
Swift sexual content flooding social networks was not caused by open models; the images were‬
‭
generated by Microsoft’s Illustrator using OpenAI’s Dall-E.‬
‭
86‬‭
Similarly, the controversy over‬
‭
robocalls using a synthesized Joe Biden voice started with a closed voice synthesis tool from‬
‭
ElevenLabs.‬
‭
87‬‭
And AI-generated sexualized content regarding children was recently found on‬
‭
Shutterstock, reportedly using Shutterstock’s image generator which also runs on a combination‬
‭
of closed models, OpenAI’s Dall-E and LG’s EXAONE.‬
‭
88‬
‭
These examples highlight the ways that open and closed models face many of the same‬
‭
vulnerabilities — but also highlight one way in which open models do differ from closed ones. In‬
‭
the above examples, Microsoft was able to tweak its filters to catch the exploits based on‬
‭
misspellings that were used to generate the Taylor Swift images; ElevenLabs was able to block‬
‭
the user who created the Biden robocalls, and hopefully is developing audio fingerprinting‬
‭
blacklists to prevent similar harmful instances of impersonation of political figures; and‬
‭
Shutterstock was able to remove the content that had been created and posted on its site.‬
‭
89‬
‭
89‬‭
Id.‬
‭
88‬‭
Matt Growcoot, “Disturbing AI Images of Children Found for Sale on Shutterstock,”‬‭
PetaPixel‬
‭
, February‬
‭
22, 2024,‬
‭
https://petapixel.com/2024/02/22/disturbing-ai-images-of-children-found-for-sale-on-shutterstock/‬
‭
.‬
‭
[‬
‭
perma.cc/U689-3B5W‬
‭
]‬
‭
87‬‭
Margi Murphy, Rachel Metz, and Mark Bergen, “AI Startup ElevenLabs Bans Account Blamed for Biden‬
‭
Audio Deepfake,”‬‭
Bloomberg‬
‭
, January 26, 2024,‬
‭
https://www.bloomberg.com/news/articles/2024-01-26/ai-startup-elevenlabs-bans-account-blamed-for-bid‬
‭
en-audio-deepfake‬
‭
.‬‭
[‬
‭
perma.cc/C63Y-KN5L‬
‭
]‬
‭
86‬‭
Carl Franzen, “Microsoft Adds New Restrictions to Designer AI Used to Make Taylor Swift Deepfakes,”‬
‭
VentureBeat‬
‭
, January 29, 2024,‬
‭
https://venturebeat.com/business/microsoft-adds-new-restrictions-to-designer-ai-used-to-make-taylor-swif‬
‭
t-deepfakes/‬
‭
.‬‭
[‬
‭
perma.cc/RUQ4-A7KM‬
‭
]‬
‭
85‬‭
See e.g., Zou et al.,‬‭
supra‬‭
note 56 and Javier Rando‬‭
et al.,‬‭
Red-Teaming the Stable Diffusion Safety‬
‭
Filter‬
‭
,‬‭
ML Safety Workshop NeurIPS 2022‬
‭
, 2022,‬‭
https://arxiv.org/abs/2210.04610‬
‭
[‬
‭
perma.cc/2SW2-WBNV‬
‭
]. For a summary, see Nathan Lambert,‬‭
“Undoing RLHF and the Brittleness of‬
‭
Safe LLMs,”‬‭
Interconnects‬‭
(blog), October 18, 2023,‬‭
https://www.interconnects.ai/p/undoing-rlhf‬
‭
[‬
‭
perma.cc/FU2M-KJMS‬
‭
].‬
‭
84‬‭
Gabriel Nicholas, “Lost in Translation: Large Language Models in Non-English Content Analysis -‬
‭
Center for Democracy and Technology,”‬‭
Center for Democracy‬‭
and Technology‬‭
(blog), May 23, 2023,‬
‭
https://cdt.org/insights/lost-in-translation-large-language-models-in-non-english-content-analysis/‬
‭
.‬
‭
[‬
‭
perma.cc/5WK2-WXAK‬
‭
]‬
‭
83‬‭
Zheng-Xin Yong, Cristina Menghini, and Stephen H. Bach, “Low-Resource Languages Jailbreak‬
‭
GPT-4,”‬‭
arXiv‬
‭
, October 3, 2023, https://arxiv.org/abs/2310.02446..‬
‭
17‬
‭
By contrast, in the case of OFMs, the lack of a centralized provider means post-hoc‬
‭
enforcement action or central patching of vulnerabilities is not feasible. There is no way to‬
‭
rescind the publication of an open model once it is on the open web (notably true of all open‬
‭
web content already, even the most harmful), and no way for the publisher to consistently‬
‭
monitor or enforce against its users.‬
‭
90‬‭
This structural difference is clearly relevant when‬
‭
considering safety threats from OFMs compared to closed foundation models. That said,‬
‭
reliance on closed model deployers to appropriately prioritize addressing harms in their models‬
‭
and to sufficiently enforce against misuse is likely to be imperfect, in the same way that online‬
‭
platforms can reduce prevalence of — but cannot entirely remove — conduct that violates their‬
‭
rules.‬
‭
An assessment of marginal risk also must consider the risk of OFMs (and closed foundation‬
‭
models) compared to the existence of other, smaller models that are both already widely‬
‭
available and difficult to contain. Therefore, as we turn to evaluate different specific risks in turn,‬
‭
it is important to note that for all of them (other than the less well-defined “emergent risks” tied to‬
‭
computing on the frontier of current capability), smaller models that are specialized for the‬
‭
issuer at hand — such as for synthesizing DNA‬
‭
91‬‭
or for writing the software code for hacking‬
‭
tools‬
‭
92‬
‭
— can create outputs that are equally if not more harmful than supposedly more capable‬
‭
tools.‬
‭
Chemical, Biological, Radiological, and Nuclear Risks‬
‭
No one has yet clearly demonstrated a marginal risk of bad actors misusing foundation models‬
‭
to facilitate the creation or deployment of chemical, biological, radiological, or nuclear (CBRN)‬
‭
weapons. That’s because, as the Department of Justice concluded over twenty years ago,‬
‭
“anyone interested in manufacturing a bomb, dangerous weapon, or a weapon of mass‬
‭
destruction can easily obtain detailed instructions from readily accessible sources, such as‬
‭
legitimate reference books, the so-called underground press, and the Internet.”‬
‭
93‬
‭
93‬‭
Report on the Availability of Bombmaking Information, the Extent to Which Its Dissemination Is‬
‭
Controlled by Federal Law, and the Extent to Which Such Dissemination May Be Subject to Regulation‬
‭
Consistent with the First Amendment to the United States Constitution: Prepared by the United States‬
‭
Department of Justice as Required by Section 709(a) of the Antiterrorism and Effective Death Penalty Act‬
‭
of 1996 (April 1997), quoted in CRS Report for Congress: Bomb-Making Online: Explosives, Free‬
‭
92‬‭
E.g., WormGPT and other LLMs used by cyberattackers were built on GPT-J, a model comparable in‬
‭
size to GPT-2: Polra Victor Falade, “Decoding the Threat Landscape : ChatGPT, FraudGPT, and‬
‭
WormGPT in Social Engineering Attacks,”‬‭
International‬‭
Journal of Scientific Research in Computer‬
‭
Science, Engineering and Information Technology‬
‭
, October‬‭
3, 2023, 185-98,‬
‭
https://doi.org/10.32628/cseit2390533‬
‭
.‬‭
[‬
‭
perma.cc/W7F6-MPQE‬
‭
]‬
‭
91‬‭
See, e.g., Eric Nguyen et al., “Evo: DNA Foundation Modeling From Molecular to Genome Scale,”‬‭
Arc‬
‭
Institute‬‭
(blog), February 27, 2024,‬‭
https://arcinstitute.org/news/blog/evo‬‭
[‬
‭
perma.cc/WK4R-VG6S‬
‭
]‬
‭
(describing a specialized model for generating DNA sequence that has only 7 billion parameters); Sara R.‬
‭
Carter et al., “The Convergence of Artificial Intelligence and the Life Sciences,”‬‭
The Nuclear Threat‬
‭
Initiative‬
‭
, October 30, 2023,‬
‭
https://www.nti.org/analysis/articles/the-convergence-of-artificial-intelligence-and-the-life-sciences‬
‭
[‬
‭
perma.cc/X4XT-53GZ‬
‭
] (discussing how specialized biodesign‬‭
tools, unlike current or imminent LLMs,‬
‭
may be able to synthesize toxins and pathogens that do not occur in nature and may even be more‬
‭
harmful than natural agents).‬
‭
90‬‭
See Kapoor et al., 2024,‬‭
supra‬‭
note 13.‬
‭
18‬
‭
What was true then remains true now, as demonstrated by two very recent studies intended to‬
‭
assess the biorisk threat posed by foundation models. Each concluded that the information‬
‭
provided by foundation models that may be useful in creating or deploying a bioweapon was‬
‭
essentially similar to what one could obtain with access to the internet.‬
‭
In particular, a study by RAND conducted an exercise where research teams role-played as‬
‭
malign nonstate actors tasked with planning a biological attack; some were given access to an‬
‭
LLM, others only to the internet. The authors “found no statistically significant difference in the‬
‭
viability of plans generated with or without LLM assistance…. [O]utputs generally mirror[ed]‬
‭
information readily available on the internet.”‬
‭
94‬‭
OpenAI conducted a similar study around the‬
‭
same time to evaluate the biorisk from its GPT-4 model, again dividing researchers into‬
‭
role-playing teams with and without access to the model. They found only “mild uplifts” in the‬
‭
performance of the LLM-assisted teams, “too small to be statistically significant,” and across‬
‭
only two of the five metrics that were tested.‬
‭
95‬
‭
Meanwhile, previous papers claiming the existence of a severe AI-driven biorisk from foundation‬
‭
models suffer from the defects raised by the House Science Committee, in particular‬
‭
cross-citing to other papers for the general proposition of such risk but without substantiating‬
‭
evidence, or citing to sources that do not directly support that proposition.‬
‭
96‬
‭
There has been no comparable, publicly available research into nuclear and chemical as‬
‭
opposed to biological risks from foundation models, and as such, no public evidence exists at‬
‭
this time of marginal risk in those areas. It would be reasonable to assume that the results of‬
‭
such testing in these domains would be similar to that of biorisk: the foundation model provides‬
‭
information that may be helpful, but reflects only information that it learned from the internet that‬
‭
bad actors can already access.‬
‭
Even if foundation models did create a significant marginal risk of increasing the practical‬
‭
knowledge of bad actors to develop CBRN weaponry compared to the internet, it is not clear‬
‭
that such knowledge would increase the marginal risk of an actual attack (or that restricting‬
‭
access to OFMs would reduce that risk), because the primary bar to such attacks appears to be‬
‭
96‬‭
“Propaganda or Science: Open Source AI and Bioterrorism Risk,” 1A3ORN (blog), November 2, 2023,‬
‭
https://1a3orn.com/sub/essays-propaganda-or-science.html‬‭
(examining and critiquing all of the‬
‭
biorisk-relevant citations in an influential policy paper arguing for new restrictions on OFMs); see also‬
‭
Kapoor et al., 2024,‬‭
supra‬‭
note 13.‬‭
[‬
‭
perma.cc/S5A2-ZZ87‬
‭
]‬
‭
95‬‭
Tejal Patwardhan et al., “Building an Early Warning System for LLM-aided Biological Threat Creation,”‬
‭
OpenAI‬
‭
, January 21, 2024,‬
‭
https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation‬
‭
.‬
‭
[‬
‭
perma.cc/HZB4-G7NQ‬
‭
]‬
‭
94‬‭
Christopher A. Mouton, Caleb Lucas, and Ella Guest, “The Operational Risks of AI in Large-Scale‬
‭
Biological Attacks: Results of a Red-Team Study,”‬‭
RAND‬
‭
, January 25, 2024,‬
‭
https://www.rand.org/pubs/research_reports/RRA2977-2.html‬
‭
.‬‭
[‬
‭
perma.cc/7J76-JULN‬
‭
]‬
‭
Speech, Criminal Law and the Internet, Sep. 8, 2003, at p. 7, available at‬
‭
https://www.everycrsreport.com/files/20030908_RL32074_fcbf5a7d23f14b3350d4c2d81465aaaf7bcd299‬
‭
d.pdf‬
‭
.‬‭
[‬
‭
perma.cc/JC27-FKRQ‬
‭
]‬
‭
19‬
‭
material and logistical, rather than informational.‬
‭
97‬‭
This was the conclusion of research from the‬
‭
Future of Life Institute, one of the primary organizations concerned with catastrophic risks of AI.‬
‭
A paper by one of the Institute’s research analysts, who is also a senior researcher at Johns‬
‭
Hopkins specializing in biorisk, concluded that due to the practical requirements of developing‬
‭
and deploying a biological threat agent, “(m)alevolent low resourced actors and benevolent or‬
‭
accidental actors regardless of resource level are revealed as being unable to produce such [a‬
‭
biological] agent.”‬
‭
98‬‭
This is regardless of what information is available to those actors because‬
‭
the primary barrier to those actors is not lack of information. Furthermore, to the extent the‬
‭
government seeks to prevent facts about science from becoming less practically obscure and‬
‭
more easily findable, its interventions may violate the First Amendment as discussed in Part III.‬
‭
The above considerations highlight the mismatch between the types of policy interventions‬
‭
being proposed to try to limit access to knowledge through OFMs, and the practical steps‬
‭
actually needed to secure the physical assets and facilities necessary to create, test, and deploy‬
‭
a bioweapon. Or, “[p]ut differently, which of the following may be more likely by 2024: more (a)‬
‭
open-source models, (b) laboratories capable of manufacturing pathogens, or (c) suppliers of‬
‭
required raw materials? If the answer is (a), the focus on (b) and (c) may provide more effective‬
‭
mechanisms of control.”‬
‭
99‬‭
This is all the more true considering the likely growth not only of‬
‭
OFMs but smaller and harder-to-police models that are specialized in biology.‬
‭
Based on similar reasoning, the report of a convening of senior experts hosted by the‬
‭
Rockefeller Foundation examining “Biosecurity in the Age of AI” contained six policy proposals,‬
‭
only one of which was focused on attempting to guardrail the use of LLMs to prevent access to‬
‭
biothreat-relevant information. The rest were focused on much more practical and likely effective‬
‭
measures targeted at safeguarding the digital-to-physical frontier (e.g. through mandatory‬
‭
screening around DNA synthesis), investment in early detection and response, development of‬
‭
new lab safety norms, etc.‬
‭
100‬
‭
Particularly considering the experience of the COVID-19 pandemic, investments in detection‬
‭
and response in particular could assist society in countering both AI-derived and natural‬
‭
bioagents. AI is and will be a key tool in that toolbox. As the nonprofit, nonpartisan Nuclear‬
‭
100‬‭
Mark Dybul, “Biosecurity in the Age of AI Chairperson’s Statement,”‬‭
Helena at the Rockefeller‬
‭
Foundation’s Bellagio Center‬
‭
, June 2023,‬
‭
https://938f895d-7ac1-45ec-bb16-1201cbbc00ae.usrfiles.com/ugd/938f89_74d6e163774a4691ae8aa0d3‬
‭
8e98304f.pdf‬‭
[‬
‭
perma.cc/2JK9-T77U‬
‭
]‬
‭
. See also Rishi‬‭
Bommasani et al., 2023,‬‭
supra‬‭
note 13. (“As with‬
‭
many other threat vectors, the best policy choke points may hence lie downstream. For example, the U.S.‬
‭
AI Executive Order aims to strengthen customer screening for purchasers of biological sequences.”)‬
‭
99‬‭
Neel Guha et al., “AI Regulation Has Its Own Alignment Problem: The Technical and Institutional‬
‭
Feasibility of Disclosure, Registration, Licensing, and Auditing,”‬‭
George Washington Law Review‬
‭
(Forthcoming)‬
‭
, November 15, 2023,‬‭
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4634443‬
‭
.‬
‭
[‬
‭
perma.cc/9N6H-BHBA‬
‭
]‬
‭
98‬‭
Michael Montague, “Towards a Grand Unified Threat Model of Biotechnology,”‬‭
PhilSci-Archive‬
‭
, 2023,‬
‭
https://philsci-archive.pitt.edu/22539/‬
‭
.‬‭
[‬
‭
perma.cc/L2JJ-D859‬
‭
]‬
‭
97‬‭
See e.g., Louise Matsakis, “Why AI-assisted Bioterrorism Became a Top Concern for OpenAI and‬
‭
Anthropic,”‬‭
Semafor‬
‭
, November 15, 2023,‬
‭
https://www.semafor.com/article/11/15/2023/ai-assisted-bioterrorism-is-top-concern-for-openai-and-anthro‬
‭
pic‬
‭
.‬‭
[‬
‭
perma.cc/C7V4-S57E‬
‭
]‬
‭
20‬
‭
Threat Initiative highlighted in a recent paper, after concluding (like other studies) that general‬
‭
purpose LLMs are unlikely to generate toxin or pathogen designs that are not already described‬
‭
in the public literature: “AI-bio capabilities will also benefit society and bolster biosecurity and‬
‭
pandemic preparedness. In addition to broadly enabling scientific progress, AI models are‬
‭
already aiding pathogen biosurveillance systems, the development of medical‬
‭
countermeasures, and other aspects of pandemic preparedness and response.”‬
‭
101‬
‭
Attempting to restrict open access to biological capabilities could threaten those benefits — and‬
‭
by disrupting the offense-defense balance, may heighten rather than reduce biorisk. “[I]n the‬
‭
long run, the biosecurity solution to biotechnology is more biotechnology. Indeed, biosecurity‬
‭
policies that slow the adoption and advance of biotechnology artificially preserve and prolong a‬
‭
period of relative vulnerability in which defensive uses of biotechnology have yet to fully‬
‭
dominate the security equation​
.”‬
‭
102‬‭
The next section will discuss a similar dynamic in regard to‬
‭
cybersecurity threats.‬
‭
Cybersecurity Risks‬
‭
The Executive Order in its definition of a “dual-use foundation model” also highlighted concern‬
‭
that such models may “enabl[e] powerful offensive cyber operations through automated‬
‭
vulnerability discovery and exploitation against a wide range of potential targets of cyber‬
‭
attacks.” However, as with CBRN threats, this threat is, so far, under-evidenced. In fact,‬
‭
Microsoft and OpenAI have published the results of a study on offensive uses of the LLMs they‬
‭
monitor and found “incremental” changes in “behaviors consistent with attackers using AI as‬
‭
another productivity tool on the offensive landscape,” but did not yet observe “particularly novel‬
‭
or unique AI-enabled attack or abuse techniques resulting from threat actors’ usage of AI.”‬
‭
103‬
‭
Of course, newer models will likely offer more powerful opportunities for creating tools to help‬
‭
discover and exploit vulnerabilities in other systems. Yet whether those new capabilities are a‬
‭
mere incremental change or a large step change, those same capabilities will be available to‬
‭
defenders as well. Defenders will be able to discover the same vulnerabilities as the attackers,‬
‭
and work to patch them. Defenders will be assisted in their coding by LLMs the same as‬
‭
attackers. Defenders will be able to work to counter LLM-generated phishing messages with‬
‭
LLM-based detection of the same, much as we have developed automated tools that catch most‬
‭
human-generated spam. It is because of such benefits in the context of regular software — and‬
‭
likely First Amendment concerns, see Part III — that current export controls on “cybersecurity‬
‭
software” do not apply to publication of open source software.‬
‭
104‬
‭
104‬‭
See Department of Commerce, Information Security Controls: Cybersecurity Items, 86 Fed. Reg.‬
‭
58205,58207 (Oct. 21, 2021);‬‭
https://www.govinfo.gov/content/pkg/FR-2021-10-21/pdf/2021-22774.pdf‬
‭
[‬
‭
perma.cc/AWX5-DWY6‬
‭
] (“BIS does not intend this note‬‭
to require any additional compliance measures‬
‭
103‬‭
Microsoft Threat Intelligence, “Staying Ahead of Threat Actors in the Age of AI,”‬‭
Microsoft Security‬
‭
Blog‬‭
(blog), February 14, 2024,‬
‭
https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai‬
‭
.‬
‭
[‬
‭
perma.cc/7F5R-U7ZG‬
‭
].‬
‭
102‬‭
Id.‬
‭
101‬‭
Carter et al., 2023,‬‭
supra‬‭
note 91.‬
‭
21‬
‭
This is not to say that open sourcing powerful foundation models will certainly help defenders as‬
‭
much or more than attackers, nor do we intend to make light of the risks. There are a number of‬
‭
ways that open source software code and open models with weights are quite different artifacts,‬
‭
such that the cybersecurity risks and benefits of open models may differ somewhat. However,‬
‭
policymakers also cannot assume that they will certainly help attackers more, considering the‬
‭
significant cybersecurity benefits that openness in software and data have previously‬
‭
demonstrated. Once again, more research and a fuller record demonstrating the likelihood of‬
‭
such risk is necessary to justify broad restrictions on general purpose AI tools.‬
‭
Emergent Risks‬
‭
Although more research is needed around CBRN and cyber threats, nowhere is more and better‬
‭
articulation and proof of risk needed than in the realm of what we will call “emergent” risks.‬
‭
These are broader, longer-term risks about AI models going “rogue” — evading human control‬
‭
through deception, escaping their servers and self-proliferating, and/or deliberately acting of‬
‭
their own accord against the aims of humans.‬
‭
The idea of rogue AI is a naturally worrisome one, and a common trope in science fiction for as‬
‭
long as we’ve conceived of artificial intelligence. However, these risks are considered‬
‭
speculative even by many of the experts who raise them.‬
‭
105‬‭
For example, a highly cited paper‬
‭
on the topic — the same paper from which the AI EO apparently took its language focused on‬
‭
foundation models “evading human control through means of deception and obfuscation”‬
‭
106‬‭
—‬
‭
footnoted its concern around this risk as follows:‬
‭
If‬‭
future AI systems develop the ability and the propensity‬‭
to deceive their users,‬
‭
controlling their behavior‬‭
could‬‭
be extremely challenging.‬‭
Though it is unclear whether‬
‭
models will trend in that direction‬
‭
, it seems rash‬‭
to dismiss the possibility and some‬
‭
106‬‭
Cf. id., listing as a key threat after CBRN and cyber threats the “[e]vading [of] human control through‬
‭
means of deception and obfuscation.”‬
‭
105‬‭
“While currently deployed foundation models pose risks, they do not yet appear to possess dangerous‬
‭
capabilities that pose severe risks to public safety as we have defined them. Given both our inability to‬
‭
reliably predict what models will have sufficiently dangerous capabilities and the already significant‬
‭
capabilities today’s models possess, it would be prudent for regulators to assume that next-generation‬
‭
state-of-the-art foundation models‬‭
could‬‭
possess advanced‬‭
enough capabilities to warrant regulation.”‬
‭
(emphasis in original, internal citations omitted). Markus Anderljung et al., “Frontier AI Regulation:‬
‭
Managing Emerging Risks to Public Safety,”‬‭
arXiv‬
‭
,‬‭
November 7, 2023,‬‭
https://arxiv.org/abs/2307.03718‬
‭
[‬
‭
perma.cc/X3CR-P5LH‬
‭
].‬
‭
beyond what is otherwise required by the EAR. ‘‘Software’’ and ‘‘technology’’ ‘‘published’’ in the public‬
‭
domain and meeting the requirements of § 734.7 of the EAR are not subject to the EAR); see also EAR §‬
‭
734.7 (“'unclassified ‘technology’ or ‘software’ is ‘published,’ and is thus not ‘technology’ or ‘software’‬
‭
subject to the EAR, when it has been made available to the public without restrictions upon its further‬
‭
dissemination such as through any of the following….”) and “Understanding US Export Controls With‬
‭
Open Source Projects,”‬‭
The Linux Foundation‬
‭
, July‬‭
2020,‬
‭
https://www.linuxfoundation.org/resources/publications/understanding-us-export-controls-with-open-sourc‬
‭
e-projects‬‭
(summarizing application of EAR to open‬‭
source) [‬
‭
perma.cc/8LY3-5T47‬
‭
].‬
‭
22‬
‭
argue that it might be the default outcome of current training paradigms. [Emphasis‬
‭
added.]‬
‭
107‬
‭
We would agree that these concerns should not be dismissed out of hand — even if the risk‬
‭
were very low, the threat to the public and humanity more broadly if that risk were to come to‬
‭
pass may be quite high. However, the main arguments for there being a risk of rogue AI‬
‭
originated with philosophers focused on catastrophic risk, years before the rise of LLMs and‬
‭
other generative AI technologies, and have not changed substantially since.‬
‭
108‬‭
This suggests‬
‭
that these concerns are not primarily based on specific technical developments but reflect more‬
‭
philosophical extrapolations about how a hypothetical artificial mind might behave. And papers‬
‭
arguing that these catastrophic risks exist or are imminent typically do not present specific‬
‭
factual evidence, instead theorizing generally about the possibility of each kind of risk,‬
‭
sometimes with supporting anecdotes or illustrative potential scenarios.‬
‭
109‬‭
Furthermore,‬
‭
existential risk scholars often prioritize outcomes based on the magnitude (positive or negative)‬
‭
of their consequences and their probability of occurring; however, these scholars’ notions of‬
‭
both probability‬
‭
110‬‭
as well as magnitude‬
‭
111‬‭
are highly subjective. As a result, even when‬
‭
catastrophic risks are assigned a discrete numeric value, this value is better interpreted as a‬
‭
qualitative belief than as a precise quantitative estimate.‬
‭
Those concerned about catastrophic emergent risks often point to the rapid increase in LLM’s‬
‭
general capabilities as they are trained with more data and compute, to argue that we should‬
‭
anticipate sharp and unpredictable changes in those capabilities over time.‬
‭
112‬‭
Indeed, one paper‬
‭
has suggested based on certain metrics that we have already seen such sharp, unpredictable‬
‭
changes in capability that may even demonstrate “the sparks of artificial general intelligence”‬
‭
and the capability to reason beyond the model’s training data.‬
‭
113‬‭
However, there are a number of‬
‭
reasons  that skepticism around these assertions may be warranted absent more evidence.‬
‭
113‬‭
Sébastien Bubeck et al., “Sparks of Artificial General Intelligence: Early Experiments With GPT-4,”‬
‭
arXiv‬
‭
, March 22, 2023,‬‭
https://arxiv.org/abs/2303.12712‬
‭
.‬‭
[‬
‭
perma.cc/S4KM-NZ6M‬
‭
].‬
‭
112‬‭
Seger at al., 2023,‬‭
supra‬‭
note 109.‬
‭
111‬‭
Owen Cotton-Barratt and Toby Ord, “Existential Risk and Existential Hope: Definitions,”‬‭
Future of‬
‭
Humanity Institute‬
‭
, 2015,‬‭
https://amirrorclear.net/files/existential-risk-and-existential-hope.pdf‬
‭
.‬
‭
[‬
‭
perma.cc/E493-E26Q‬
‭
].‬
‭
110‬‭
Nick Bostrom, “Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards,”‬
‭
Journal of Evolution and Technology‬‭
9 (2002),‬
‭
https://ora.ox.ac.uk/objects/uuid:827452c3-fcba-41b8-86b0-407293e6617c‬
‭
.‬‭
[‬
‭
perma.cc/Q95X-J5M5‬
‭
].‬
‭
109‬‭
Dan Hendrycks, Mantas Mazeika, and Thomas Woodside, “An Overview of Catastrophic AI Risks,”‬
‭
arXiv‬
‭
, October 9, 2023,‬‭
https://arxiv.org/abs/2306.12001‬‭
[‬
‭
perma.cc/CPF3-R46A‬
‭
]; Elizabeth Seger et al.,‬
‭
“Open-Sourcing Highly Capable Foundation Models: An Evaluation of Risks, Benefits, and Alternative‬
‭
Methods for Pursuing Open-Source Objectives,”‬‭
Center‬‭
for the Governance of AI‬
‭
, September 29, 2023,‬
‭
https://arxiv.org/abs/2311.09227‬‭
[‬
‭
perma.cc/E2SG-7BU7‬
‭
];‬‭
Jeremie Harris, Edouard Harris, and Mark Beall,‬
‭
“Survey of AI Technologies and AI R&D Trajectories,”‬‭
Gladstone AI‬
‭
, November 3, 2023,‬
‭
https://assets-global.website-files.com/62c4cf7322be8ea59c904399/65e83959fd414a488a4fa9a5_Gladst‬
‭
one%20Survey%20of%20AI.pdf‬‭
[‬
‭
perma.cc/FC7U-AV3W‬
‭
].‬
‭
108‬‭
e.g., Nick Bostrom,‬‭
Superintelligence: Paths, Dangers,‬‭
Strategies‬‭
(Oxford University Press, 2015);‬
‭
Toby Ord,‬‭
The Precipice: Existential Risk and the‬‭
Future of Humanity‬
‭
. (Hachette Books, 2020),‬
‭
https://theprecipice.com/‬
‭
; For further analysis on‬‭
this point see Ahmed et al., 2024,‬‭
supra‬‭
note 75.‬
‭
[‬
‭
perma.cc/K275-MTQ7‬
‭
].‬
‭
107‬‭
Id. (citations omitted).‬
‭
23‬
‭
First, additional research has argued that the appearance of such discontinuous jumps depends‬
‭
heavily on the metrics used, and when using different metrics, the state of improvement —‬
‭
although very fast — follows a continuous curve.‬
‭
114‬‭
Second, there is also the challenge of‬
‭
measuring improvements in model capability using tests that may have been in the training data‬
‭
of the model in the first place, such that those scores may not necessarily indicate any‬
‭
improvement in performance, much less reasoning.‬
‭
115‬‭
Third, to the extent the general‬
‭
capabilities of foundation models increase, those capabilities will also be available to human‬
‭
users, providing a countervailing benefit that must be considered, including in how it could help‬
‭
balance the threat. Finally, it is again important to consider the practicalities of a rogue AI‬
‭
actually causing substantial catastrophic or even existential harm to humanity. As various‬
‭
commentators have highlighted, it is still very unclear how an AI model would gain control over‬
‭
the many physical assets it would likely need to create such a risk.‬
‭
116‬
‭
Global Competition and Security Risks‬
‭
Another concern sometimes raised, although not explicitly in the AI EO, is that open sourcing‬
‭
foundation models will assist China in competing with us economically or militarily. This is‬
‭
certainly true to the extent that open sourcing foundation models will give some new advantage‬
‭
to anyone seeking to build AI functionality without relying on a handful of companies offering‬
‭
closed models. However, it is also true that many of the largest OFMs on the leaderboard of the‬
‭
Hugging Face platform are of Chinese origin,‬
‭
117‬‭
and although there is some reporting that one‬
‭
117‬‭
Sorting the Open LLM Leaderboard on Hugging Face at‬
‭
https://hf.co/spaces/HuggingFaceH4/open_llm_leaderboard‬‭
[‬
‭
perma.cc/ZAZ8-ASVM‬
‭
] based on size and‬
‭
looking only at models greater than 35 billion parameters, there are multiple models originating from‬
‭
China including‬‭
Qwen v1‬
‭
(and‬‭
v1.5‬
‭
),‬‭
Yi‬
‭
, and‬‭
DeepSeek‬
‭
,‬‭
along with‬‭
Falcon‬‭
(UAE) and‬‭
Mixtral‬‭
(France), but‬
‭
the only US-origin model in that category is‬‭
Llama-2‬
‭
.‬
‭
116‬‭
Timothy B Lee, “The AI Safety Debate Is Focusing on the Wrong Threats,”‬‭
Understanding AI‬‭
(blog),‬
‭
May 9, 2023,‬‭
https://www.understandingai.org/p/why-im-not-worried-about-ai-taking‬
‭
.‬
‭
[‬
‭
perma.cc/HZ9Y-S4ED‬
‭
].‬
‭
115‬‭
Contamination of training data: Tom Brown et al., Language Models Are Few-Shot Learners,‬‭
Advances‬
‭
in Neural Information Processing Systems 33 (NeurIPS 2020)‬
‭
, 2020,‬
‭
https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html‬
‭
[‬
‭
perma.cc/CMY2-BXYS‬
‭
]; Jason Wei et al., “Finetuned‬‭
Language Models Are Zero-Shot Learners,”‬‭
arXiv‬
‭
,‬
‭
September 3, 2021,‬‭
https://arxiv.org/abs/2109.01652‬‭
[‬
‭
perma.cc/3UGY-84YA‬
‭
]; Simone Balloccu et al.,‬
‭
Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs,‬
‭
Proceedings of the 18th Conference of the European Chapter of the Association for Computational‬
‭
Linguistics (Volume 1: Long Papers), 2024,‬‭
https://aclanthology.org/2024.eacl-long.5‬
‭
[‬
‭
perma.cc/UHH6-5869‬
‭
]. Effects of contamination on‬‭
performance: Changmao Li and Jeffrey Flanigan,‬
‭
“Task Contamination: Language Models May Not Be Few-Shot Anymore,”‬‭
arXiv‬
‭
, December 26, 2023,‬
‭
https://arxiv.org/abs/2312.16337‬‭
[‬
‭
perma.cc/X4HT-DWU5‬
‭
].‬‭
Federico Ranaldi et al., “Investigating the‬
‭
Impact of Data Contamination of Large Language Models in Text-to-SQL Translation,”‬‭
arXiv,‬‭
February 12,‬
‭
2024,‬‭
https://arxiv.org/abs/2402.08100‬‭
[‬
‭
perma.cc/XZ7Q-E5HH‬
‭
].‬
‭
114‬‭
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo,‬‭
Are Emergent Abilities of Large Language‬
‭
Models a Mirage?, Advances in Neural Information Processing Systems 36 (NeurIPS 2023)‬
‭
, 2023,‬
‭
https://arxiv.org/abs/2304.15004‬
‭
.‬‭
[‬
‭
perma.cc/3S53-68L2‬
‭
]‬
‭
24‬
‭
Chinese startup used a variant of Meta’s Llama-2 architecture in training its LLM,‬
‭
118‬‭
we have not‬
‭
yet seen examples of major Chinese OFMs using Llama-2 weights.‬
‭
Meanwhile, we‬‭
have‬‭
seen a flourishing of OFMs of international‬‭
origin that China can and will‬
‭
have access to regardless of US policy, including Falcon (United Arab Emirates),‬
‭
119‬‭
Baichuan‬
‭
(China),‬
‭
120‬‭
BLOOM (global),‬
‭
121‬‭
Mixtral (France),‬
‭
122‬‭
and Stable Beluga (England).‬
‭
123‬
‭
. Therefore it‬
‭
is unclear how artificially constraining international access to US-origin OFMs would‬
‭
substantially alter the course of international OFM development other than to slow it down (for‬
‭
lack of our contributions) and potentially give other international OFM developers a better‬
‭
chance to dominate the OFM development community and the foundation model market with‬
‭
their offerings while US-origin offerings are delayed.‬
‭
Of course, as with all of the other national security risks above, our information may be, and in‬
‭
some cases certainly is, incomplete because of our lack of access to classified information. But‬
‭
the evidence currently available to the public suggests that heavy-handed interventions to‬
‭
restrict OFM exports are unlikely to meet their intended goals.‬
‭
Content Risks‬
‭
Although not the focus of this proceeding, it would be remiss not to highlight that there are‬
‭
several categories of serious risks from foundation models — both open and closed — that are‬
‭
not speculative but are already being observed. These include the proliferation of AI-generated,‬
‭
photorealistic child sexual abuse imagery (CSAM) and nonconsensual intimate imagery (NCII),‬
‭
misinformation and disinformation, and fraudulent content such as phishing emails or‬
‭
voice-cloning.‬
‭
124‬
‭
124‬‭
Sayash Kapoor and Arvind Narayanan, “How to Prepare for the Deluge of Generative AI on Social‬
‭
Media,” Knight First Amendment Institute, June 16, 2023,‬
‭
https://knightcolumbia.org/content/how-to-prepare-for-the-deluge-of-generative-ai-on-social-media‬
‭
.‬
‭
[‬
‭
perma.cc/G7YC-R3GL‬
‭
].‬
‭
123‬‭
Stability AI, “Meet Stable Beluga 1 and Stable Beluga 2, Our Large and Mighty Instruction Fine-Tuned‬
‭
Language Models,”‬‭
Stability AI‬
‭
, November 8, 2023,‬
‭
https://stability.ai/news/stable-beluga-large-instruction-fine-tuned-models‬
‭
.‬‭
[‬
‭
perma.cc/56MZ-JYX3‬
‭
]‬
‭
122‬‭
Albert Q. Jiang et al., “Mixtral of Experts,”‬‭
arXiv‬
‭
,‬‭
January 8, 2024,‬‭
https://arxiv.org/abs/2401.04088‬
‭
.‬
‭
[‬
‭
perma.cc/83VC-CEWS‬
‭
].‬
‭
121‬‭
BigScience Workshop et al., “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,”‬
‭
arXiv‬
‭
, November 9, 2022,‬‭
https://arxiv.org/abs/2211.05100‬
‭
.‬‭
[‬
‭
perma.cc/D9LQ-GDEK‬
‭
]‬
‭
120‬‭
Aiyuan Yang et al., “Baichuan 2: Open Large-scale Language Models,”‬‭
arXiv‬
‭
, September 19, 2023,‬
‭
https://arxiv.org/abs/2309.10305‬
‭
.‬‭
[‬
‭
perma.cc/9AJJ-GZW6‬
‭
]‬
‭
119‬‭
Ebtesam Almazrouei et al., “The Falcon Series of Open Language Models,”‬‭
arXiv‬
‭
, November 28, 2023,‬
‭
https://arxiv.org/abs/2311.16867‬
‭
.‬‭
[‬
‭
perma.cc/RQN6-PUHG‬
‭
]‬
‭
118‬‭
Paul Mozur, John Liu, and Cade Metz, “China’s Rush to Dominate A.I. Comes With a Twist: It Depends‬
‭
on U.S. Technology,”‬‭
The New York Times‬
‭
, February‬‭
21, 2024,‬
‭
https://www.nytimes.com/2024/02/21/technology/china-united-states-artificial-intelligence.html‬
‭
[‬
‭
perma.cc/9M93-NJP6‬
‭
]; but see Hailey Schoelkopf, Aviya‬‭
Skowron, and Stella Biderman, “Yi-34B, Llama‬
‭
2, and Common Practices in LLM Training: A Fact Check of the New York Times,”‬‭
EleutherAI Blog‬‭
(blog),‬
‭
March 26, 2024,‬‭
https://blog.eleuther.ai/nyt-yi-34b-response‬‭
[‬
‭
perma.cc/6JM5-EZ75‬
‭
] (explaining why‬
‭
re-use of LLM architecture is unremarkable because all modern LLMs use a similar architecture).‬
‭
25‬
‭
These are all serious policy challenges, and how best to address these very real harms — both‬
‭
in the context of open and closed systems — is still unclear and subject to extensive debate‬
‭
elsewhere, including in Congress. Therefore they are not addressed in depth here. However, it‬
‭
is worth highlighting some key considerations that have come up in the context of the previously‬
‭
discussed risks, which are also relevant to considering how to address policy in regard to these‬
‭
types of content harms.‬
‭
First, as with previous risks, the ability to effectively address these content issues at the level of‬
‭
a foundation model is currently unclear, considering how fragile the safeguards of both closed‬
‭
and open models are against even well-intentioned fine-tuning as well as adversarial attacks.‬
‭
Or, as two Princeton researchers put it in a recent essay, “safety is not a model property,”‬
‭
125‬‭
at‬
‭
least not in terms of current foundation model architectures.‬
‭
Furthermore, for a number of content-related risks the marginal risk between open and closed‬
‭
models is currently unclear, not only because of equally fragile guardrails but also because‬
‭
some of these objectionable forms of content such as mis- and dis-information were already‬
‭
very cheap to produce,‬
‭
126‬‭
and do not require capacity to produce synthetic content at all.‬
‭
127‬‭
It is‬
‭
certainly possible that open models may ultimately generate more objectionable material than‬
‭
closed, presuming some level of effective enforcement of terms of use by closed model‬
‭
providers. However, even assuming that is the case, OFMs may not present a marginal risk as‬
‭
compared to smaller, specialized open models that it likely will not be possible to interdict and‬
‭
that may pose an equal or greater risk of creating harmful content such as CSAM.‬
‭
Therefore, just as a focus on hardening attack surfaces such as DNA sequencing labs makes‬
‭
sense in the biorisk context, so too may a focus on stemming harmful content types at their‬
‭
distribution chokepoints, such as social networks.‬
‭
128‬‭
However, and as will be discussed in Part‬
‭
III, the government must in all its efforts ensure compliance with the protections of the First‬
‭
Amendment, noting that several of the content categories discussed are or may be protected‬
‭
speech depending on the facts.‬
‭
128‬‭
See Bommasani, et al., 2023,‬‭
supra‬‭
note 13: “[T]he‬‭
key bottleneck for effective influence operations is‬
‭
not disinformation generation but disinformation dissemination: Online platforms that control the reach of‬
‭
content are better targets for policy intervention.” See also Josh A. Goldstein et al., “Generative Language‬
‭
Models and Automated Influence Operations: Emerging Threats and Potential Mitigations,”‬‭
arXiv‬
‭
, January‬
‭
10, 2023,‬‭
https://arxiv.org/abs/2301.04246‬‭
[‬
‭
perma.cc/DF2P-RMN9‬
‭
];‬‭
Richard L. Hansen,‬‭
Cheap Speech‬
‭
How Disinformation Poisons Our Politics — and How to Cure It‬‭
(Yale University Press, 2022),‬
‭
https://yalebooks.yale.edu/book/9780300274097/cheap-speech/‬‭
[‬
‭
perma.cc/FXM9-WSNB‬
‭
].‬
‭
127‬‭
Lisa Fazio “Out-of-context photos are a powerful low-tech form of misinformation,” PBS News Hour,‬
‭
February 18, 2020,‬
‭
https://www.pbs.org/newshour/science/out-of-context-photos-are-a-powerful-low-tech-form-of-misinformati‬
‭
on‬
‭
.‬‭
[‬
‭
perma.cc/SDM2-PHG7‬
‭
]‬
‭
126‬‭
Felix M. Simon, Sacha Altay, and Hugo Mercier, “Misinformation Reloaded? Fears About the Impact of‬
‭
Generative AI on Misinformation Are Overblown,”‬‭
Misinformation‬‭
Review, Harvard Kennedy Review‬
‭
,‬
‭
October 18, 2023,‬‭
https://doi.org/10.37016/mr-2020-127‬
‭
;‬‭
Kapoor & Narayanan, 2023,‬‭
supra‬‭
note 124.‬
‭
[‬
‭
perma.cc/3L7S-8QNB‬
‭
]‬
‭
125‬‭
Arvind Narayanan and Sayash Kapoor, “AI Safety Is Not a Model Property,”‬‭
AI Snake Oil‬‭
(blog), March‬
‭
12, 2024,‬‭
https://www.aisnakeoil.com/p/ai-safety-is-not-a-model-property‬
‭
.‬‭
[‬
‭
perma.cc/AGB9-8WL6‬
‭
]‬
‭
26‬
‭
Civil Rights Risks‬
‭
As with content risks, harms to civil rights from use of AI models and systems are already‬
‭
apparent. With AI models of all kinds, research has time and again demonstrated first-order‬
‭
harms of both allocation and representation from AI models, particularly when deployed in‬
‭
consequential contexts.‬
‭
Civil rights-related harms from foundation models can manifest in several ways. If products‬
‭
based on foundation models (like chatbots) are used to directly make or materially contribute to‬
‭
decisions about people’s economic or legal circumstances, such as using consumer chatbots to‬
‭
conduct employment screening or employee evaluations, embedded stereotypes can lead to‬
‭
arbitrary and disparate impact.‬
‭
129‬‭
If foundation models are modified or integrated into‬
‭
downstream, context-specific use cases, undesirable characteristics of the foundation model‬
‭
such as embedded gender bias may persist into the downstream task.‬
‭
130‬‭
Alternatively,‬
‭
downstream modification like contextual fine-tuning and product design can introduce biases‬
‭
even if they were successfully suppressed in the foundation models.‬
‭
131‬
‭
If models reflect or amplify stereotypes in content generation even outside of consequential‬
‭
decisions, this can lead to stigmatization and the ossification of exclusionary norms.‬
‭
132‬‭
And‬
‭
when communities are underrepresented in data that is used to train foundation models or are‬
‭
disproportionately subject to second-order effects like economic displacement or misuse of new‬
‭
tools to disenfranchise voters, the benefits and harms of this technology could continue to be‬
‭
distributed in a dramatically uneven fashion.‬
‭
However, none of these harms is unique to OFMs, and in fact many such harms have already‬
‭
been identified in closed foundation models,‬
‭
133‬‭
suggesting a lack of apparent marginal risk to‬
‭
civil rights compared to the harms caused by narrower and more widely deployed systems‬
‭
(which remain concerningly under-addressed). Moreover, research is mixed on the extent to‬
‭
133‬‭
“Study Assesses GPT-4’s Potential to Perpetuate Racial, Gender Biases in Clinical Decision Making,”‬
‭
ScienceDaily‬
‭
, December 18, 2023,‬‭
https://www.sciencedaily.com/releases/2023/12/231218150939.htm‬
‭
[‬
‭
perma.cc/7J9N-R9PP‬
‭
]; James O’Donnell, “LLMs become‬‭
more covertly racist with human intervention,”‬
‭
MIT Technology Review‬
‭
, March 11, 2024,‬
‭
https://www.technologyreview.com/2024/03/11/1089683/llms-become-more-covertly-racist-with-human-int‬
‭
ervention/‬‭
[‬
‭
perma.cc/7PRP-Y8ZS‬
‭
].‬
‭
132‬‭
Irene Solaiman et al., “Evaluating the Social Impact of Generative AI Systems in Systems and Society,”‬
‭
arXiv‬
‭
, June 9, 2023,‬‭
https://arxiv.org/abs/2306.05949‬
‭
.‬‭
[‬
‭
perma.cc/YW3A-4LP9‬
‭
]‬
‭
131‬‭
Ryan Steed et al., “Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in‬
‭
Pre-Trained Language Models,”‬‭
Proceedings of the 60th‬‭
Annual Meeting of the Association for‬
‭
Computational Linguistics (Volume 1: Long Papers)‬
‭
,‬‭
January 1, 2022,‬
‭
https://doi.org/10.18653/v1/2022.acl-long.247‬
‭
.‬‭
[‬
‭
perma.cc/N5NW-8TRS‬
‭
]‬
‭
130‬‭
Seungjae Shin et al.,‬‭
Neutralizing Gender Bias in‬‭
Word Embeddings With Latent Disentanglement and‬
‭
Counterfactual Generation‬
‭
,‬‭
Findings of the Association‬‭
for Computational Linguistics (EMNLP 2020)‬
‭
,‬
‭
2020,‬‭
https://doi.org/10.18653/v1/2020.findings-emnlp.280‬
‭
.‬‭
[‬
‭
perma.cc/VX6C-7R9K‬
‭
]‬
‭
129‬‭
Leon Yin, Davey Alba, and Leonardo Nicoletti, “OpenAI’s GPT Is A Recruiter’s Dream Tool. Tests‬
‭
Show There’s Racial Bias,”‬‭
Bloomberg‬
‭
, March 7, 2024,‬
‭
https://www.bloomberg.com/graphics/2024-openai-gpt-hiring-racial-discrimination‬
‭
.‬
‭
[‬
‭
perma.cc/89AU-MYBG‬
‭
]‬
‭
27‬
‭
which intrinsic biases in foundation models correlate with bias in downstream tasks for which‬
‭
those foundation models play a role. Some research has found there to be no such correlation,‬
‭
raising fundamental questions about the measurement validity of existing — and‬
‭
well-intentioned — model evaluations seeking to measure bias at the foundation model layer‬
‭
and extrapolate those findings to real-world contexts.‬
‭
134‬
‭
Some presume that central bias mitigation efforts and monitoring capacity will enable foundation‬
‭
model developers and hosts to robustly address intrinsic biases and intervene in circumstances‬
‭
that are particularly harmful to civil rights, but we worry that this assumption is highly‬
‭
optimistic.‬
‭
135‬‭
Large technology companies have demonstrated reluctance or inability to‬
‭
proactively address the various ways harmful biases manifest across contexts, both within‬
‭
products and through enforcement actions, and we do not see strong evidence that AI‬
‭
developers — even if well-intentioned — will behave in a significantly different fashion.‬
‭
It is important to note that like the associated concept of AI safety, fairness is not a model‬
‭
property:‬
‭
136‬‭
research has shown that algorithms that appear to be fair in isolation do not‬
‭
necessarily combine into fair systems, and that apparently unfair models can still be combined‬
‭
in a way that leads to fairer systems.‬
‭
137‬‭
And fairness is highly contextual: different use-cases‬
‭
may demand different definitions of fairness or civil rights compliance,‬
‭
138‬‭
which a universal set of‬
‭
measurements or interventions at the foundation model layer may not be capable of achieving‬
‭
simultaneously.‬
‭
Ultimately, the civil rights-related impacts of foundation models will depend heavily on the‬
‭
contexts of their deployment; for instance, foundation models used in the context of housing‬
‭
would be subject to the Fair Housing Act’s prohibitions around steering homebuyers toward or‬
‭
away from certain neighborhoods, while foundational models used in the context of credit would‬
‭
be subject to Equal Credit Opportunity Act’s fair lending requirements around both disparities in‬
‭
access to credit as well as explanations of adverse actions. Even in a circumstance where the‬
‭
most advanced models are subject to pre-market testing to reduce the most egregious civil‬
‭
138‬‭
doaa Abu Elyounes, “Contextual Fairness: A Legal and Policy Analysis of Algorithmic Fairness,”‬
‭
Journal of Law, Technology and Policy‬
‭
, 2019,‬‭
https://doi.org/10.2139/ssrn.3478296‬
‭
.‬
‭
[‬
‭
perma.cc/55T9-UGZ6‬
‭
]‬
‭
137‬‭
Cynthia Dwork and Christina Ilvento,‬‭
Fairness Under‬‭
Composition‬
‭
,‬‭
10th Innovations in Theoretical‬
‭
Computer Science Conference (ITCS 2019)‬
‭
, 2019,‬‭
https://doi.org/10.4230/LIPIcs.ITCS.2019.33‬
‭
.‬
‭
[‬
‭
perma.cc/FK9G-6H6X‬
‭
]‬
‭
136‬‭
Narayanan & Kapoor, 2024,‬‭
supra‬‭
note 125.‬
‭
135‬‭
Naomi Nix, “Big Tech Is Failing to Fight Election Lies, Civil Rights Groups Charge,”‬‭
Washington Post‬
‭
,‬
‭
October 27, 2022,‬‭
https://www.washingtonpost.com/technology/2022/10/27/civil-rights-2022-midterms/‬
‭
[‬
‭
perma.cc/45D5-GWC3‬
‭
]; OpenAI has also been criticized‬‭
for failing to enforce its policies for third party‬
‭
tools in its GPS store: Kyle Wiggers, “OpenAI’s Chatbot Store Is Filling up With Spam,”‬‭
TechCrunch‬
‭
,‬
‭
March 20, 2024,‬‭
https://techcrunch.com/2024/03/20/openais-chatbot-store-is-filling-up-with-spam/‬
‭
[‬
‭
perma.cc/6BG9-GG6H‬
‭
].‬
‭
134‬‭
Seraphina Goldfarb-Tarrant et al.,‬‭
Intrinsic Bias Metrics Do Not Correlate With Application Bias‬
‭
,‬
‭
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th‬
‭
International Joint Conference on Natural Language Processing (Volume 1: Long Papers)‬
‭
, 2021,‬
‭
https://doi.org/10.18653/v1/2021.acl-long.150‬
‭
.‬‭
[‬
‭
perma.cc/9JAS-SHJX‬
‭
]‬
‭
28‬
‭
rights violations, subtle biases can manifest in the contexts where AI-powered systems are‬
‭
deployed in unpredictable and varied ways.‬
‭
For this reason, robust enforcement of civil rights laws at the point of deployment will likely‬
‭
prove a critical lever of accountability for adverse civil rights impacts, while interventions at the‬
‭
foundation model layer may provide few, if any, guarantees that concrete civil rights harms will‬
‭
be avoided.‬
‭
139‬‭
Even so, and given the variety of ways‬‭
these harms will likely manifest and the‬
‭
difficulty of detecting all of them within the four corners of a foundation model, it is all the more‬
‭
important to help a broader community of researchers and context-specific experts gain visibility‬
‭
into these systems and their use cases. Unfortunately, platforms of all kinds, including‬
‭
foundation model providers, have been known to actively prevent the very types of research‬
‭
activities that can reveal these harms.‬
‭
140‬‭
These sorts‬‭
of dynamics make broader access to‬
‭
cutting edge versions of these models all the more important. If the same foundation model‬
‭
developers that might fail in protecting marginalized communities from harm are also in a‬
‭
position to prohibit research on their models, it will be far more difficult for third-party experts to‬
‭
help spot and prevent harms.‬
‭
III.‬
‭
POLICY APPROACHES TO OPEN FOUNDATION MODELS‬
‭
The AI EO highlights the Administration's interest in “potential voluntary, regulatory, and‬
‭
international mechanisms to manage the risks and maximize the benefits” of open foundation‬
‭
models. This section will focus on two issues: first, the issue of governmental support for the‬
‭
establishment of clear best practices and norms around responsible development and‬
‭
deployment of foundation models generally, and OFMs in particular; second, the issue of First‬
‭
Amendment limits on how far the government can go in‬‭
requiring‬‭
those practices and norms.‬
‭
Creating an Infrastructure for Better Understanding of Model Risks‬
‭
As discussed above, the evidence does not yet support a conclusion that OFMs currently create‬
‭
a material marginal risk in areas such as the creation or deployment of chemical, biological,‬
‭
radiological, and nuclear weapons. At the same time, we cannot rule out the possibility that‬
‭
OFMs at some point in the future may create such risks. The government should begin creating‬
‭
the mechanisms necessary to better assess and monitor whether some future model crosses‬
‭
that risk threshold.‬
‭
A critical step along this path is already taken in the Executive Order by vesting responsibility in‬
‭
NIST to help establish clearer testing benchmarks for a range of foundation model risks.‬
‭
Continued strong and steady investment in convening and research to develop technically‬
‭
140‬‭
Nitasha Tiku, “Top AI Researchers Say OpenAI, Meta and More Hinder Independent Evaluations,”‬
‭
Washington Post‬
‭
, March 5, 2024,‬
‭
https://www.washingtonpost.com/technology/2024/03/05/ai-research-letter-openai-meta-midjourney‬
‭
.‬
‭
[‬
‭
perma.cc/RRR7-KL2G‬
‭
]‬
‭
139‬‭
Note that if a foundation model developer intends to directly deploy their model for use, they should‬
‭
both anticipate and take action through policies and technical mitigations to prevent civil rights harms‬
‭
29‬
‭
feasible and effective testing norms is crucial at this early stage of the emerging AI safety field,‬
‭
when we still lack clear and consistent standards to apply and also do not yet have a large field‬
‭
of experts to develop and apply those standards, whether in-house at AI companies or through‬
‭
consultancies or auditing companies.‬
‭
141‬
‭
This lack of clear norms is exacerbated by the fact that it is not yet clear what an appropriate AI‬
‭
audit consists of or what it should be testing for.‬
‭
142‬‭
Even worse, we do not really know how‬
‭
effective the tests that we have are: there is no shortage of research calling into question‬
‭
whether emerging AI evaluation methodologies actually effectively measure risk or have a‬
‭
meaningful relationship to what happens when a model is released into society.‬
‭
143‬‭
Therefore, as‬
‭
CDT recently urged NIST in another proceeding, it and other elements of the government‬
‭
focused on AI best practices should focus on promoting (both within and outside the‬
‭
government) foundational investments in the basic risk management processes that are needed‬
‭
to provide a stable groundwork for appropriate risk evaluation and mitigation for AI models of all‬
‭
kinds.‬
‭
144‬‭
As we highlighted there, “a common set of‬‭
concepts, approaches, and infrastructure for‬
‭
AI risk management [generally] is needed to lay the foundation for generative AI-specific‬
‭
analysis and intervention,”‬
‭
145‬‭
including basic approaches‬‭
to designing and judging the validity of‬
‭
different methods of testing and evaluation. That is because, as highlighted in a recent blog post‬
‭
from CDT, “trustworthy AI needs trustworthy measurement.”‬
‭
146‬
‭
Alongside the development of more reliable tests to better understand the risks an foundation‬
‭
model poses, NTIA should consider how the government can best obtain the information‬
‭
needed to monitor whether an OFM has crossed a threshold that now presents material‬
‭
marginal risks so that it can determine any appropriate responsive policy actions. In part, that‬
‭
may involve market surveillance activities designed to keep abreast of foundation model‬
‭
capabilities. Policymakers should also consider what forms of information sharing and‬
‭
transparency from developers of OFMs may be necessary, though as discussed below any‬
‭
146‬‭
Winecoff & Bogen, 2024,‬‭
supra‬‭
note 63.‬
‭
145‬‭
Id.‬
‭
144‬‭
Miranda Bogen, Gabriel Nicholas, and Amy Winecoff, “CDT Comments to NIST on Its Assignments‬
‭
Under the Executive Order Concerning Artificial Intelligence - Center for Democracy and Technology,”‬
‭
Center for Democracy and Technology‬
‭
, February 2, 2024,‬
‭
https://cdt.org/insights/cdt-comments-to-nist-on-its-assignments‬
‭
.‬‭
[‬
‭
perma.cc/7UY3-YYAQ‬
‭
]‬
‭
143‬‭
For a general discussion of reliability and validity in AI, see Winecoff & Bogen, 2024,‬‭
supra‬‭
note 63.‬
‭
For a critique of red teaming, see Sorelle Friedler et al., “AI Red-Teaming Is Not a One-Stop Solution to AI‬
‭
Harms: Recommendations for Using Red-Teaming for AI Accountability,”‬‭
Data & Society‬
‭
, October 23,‬
‭
2023,‬
‭
https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendation‬
‭
s-for-using-red-teaming-for-ai-accountability‬‭
[‬
‭
perma.cc/DTP3-FP7S‬
‭
];‬‭
for a critique on the validity of‬
‭
technical safety approaches, see Narayanan & Kapoor, 2024,‬‭
supra‬‭
note 125; for a challenge to existing‬
‭
legal benchmarks, see Peter B. Henderson et al., “Rethinking Machine Learning Benchmarks in the‬
‭
Context of Professional Codes of Conduct,”‬‭
Symposium‬‭
on Computer Science and Law (CSLAW ’24)‬
‭
,‬
‭
2024,‬‭
https://doi.org/10.1145/3614407.3643708‬‭
[‬
‭
perma.cc/W2EB-B3U5‬
‭
].‬‭
For evidence of unreliability of‬
‭
model prompt safeguards, see Terry Yue Zhuo et al., “Red Teaming ChatGPT via Jailbreaking: Bias,‬
‭
Robustness, Reliability and Toxicity,”‬‭
arXiv,‬‭
January‬‭
30, 2023,‬‭
https://arxiv.org/abs/2301.12867‬
‭
[‬
‭
perma.cc/5N3W-D6TU‬
‭
].‬
‭
142‬‭
Id.‬
‭
141‬‭
Abeba Birhane et al., “AI Auditing: The Broken Bus on the Road to AI Accountability,”‬‭
arXiv‬
‭
, January‬
‭
25, 2024,‬‭
https://arxiv.org/abs/2401.14462‬
‭
.‬‭
[‬
‭
perma.cc/Y9NS-RZZ5‬
‭
]‬
‭
30‬
‭
compelled disclosures would be subject to First Amendment scrutiny and would need to be‬
‭
narrowly tailored and well-designed.‬
‭
Promoting Safety Norms and Best Practices for Responsible Foundation Model‬
‭
Development and Release‬
‭
Especially since the launch of GPT-3 and ChatGTP in 2022, there has been an enormous wave‬
‭
of activity from AI labs, the open source community, civil society, and policymakers seeking to‬
‭
establish clearer norms around how to develop, deploy, and use foundation models responsibly.‬
‭
We commend the Administration for securing significant voluntary commitments from many of‬
‭
the largest foundation model developers based on many of these initial practices.‬
‭
147‬‭
The work‬
‭
kicked off by the AI EO and the upcoming OMB memo to agencies on responsible deployment‬
‭
of AI will also help to lay a firmer foundation for best practices in this area.‬
‭
148‬
‭
Although the voluntary commitments mostly applied to larger closed model providers, we are‬
‭
beginning to see parallel norm development in the OFM space.‬‭
For example, corporate‬
‭
developers like Google and Meta that have released OFMs have also been helping build norms‬
‭
around how to responsibly release open models, not only through publishing similar‬
‭
transparency artifacts about their models, as other labs do, but by releasing suites of materials‬
‭
and tools helpful to a deployer seeking to responsibly use the models. For example, with‬
‭
Llama-2, Meta released an extensive responsible user guide, walking through the key steps of‬
‭
mitigating risks in LLMs, and has begun releasing open source tools and evaluation datasets for‬
‭
security and content safety that deployers can use.‬
‭
149‬‭
Upon the release of its Gemma open‬
‭
foundation models, Google similarly published a detailed Responsible Generative AI Toolkit with‬
‭
extensive advice, open source interpretability tooling, and methods for content filtering using AI‬
‭
classifiers.‬
‭
150‬
‭
Crucially, both companies also have released versions of their models fine-tuned and‬
‭
red-teamed for usefulness and safety, for those who want to deploy them quickly with minimal‬
‭
customization — which is particularly important where their model licenses allow for commercial‬
‭
use and where the models they release may otherwise be put into service (inadvisedly) without‬
‭
150‬‭
Google, “Responsible Generative AI Toolkit,” n.d.,‬‭
https://ai.google.dev/responsible‬‭
.‬
‭
[‬
‭
perma.cc/2LYP-F954‬
‭
]‬
‭
149‬‭
Meta, “Purple Llama,” n.d.,‬‭
https://llama.meta.com/purple-llama/‬‭
[‬
‭
perma.cc/2NZ8-PJKV‬
‭
].‬
‭
148‬‭
Executive Office of the President, Office of Management and Budget, “Advancing Governance,‬
‭
Innovation, and Risk Management for Agency Use of Artificial Intelligence,” Proposed Memorandum for‬
‭
the Heads of Executive Departments and Agencies, November 1, 2023,‬
‭
https://www.whitehouse.gov/wp-content/uploads/2023/11/AI-in-Government-Memo-draft-for-public-review.‬
‭
pdf‬
‭
.‬‭
[‬
‭
perma.cc/HA22-6E33‬
‭
]‬
‭
147‬‭
The White House, “FACT SHEET: Biden-Harris Administration Secures Voluntary Commitments From‬
‭
Leading Artificial Intelligence Companies to Manage the Risks Posed by AI,” Press release, The White‬
‭
House, July 21, 2023,‬
‭
https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-admini‬
‭
stration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risk‬
‭
s-posed-by-ai/‬
‭
.‬‭
[‬
‭
perma.cc/75SR-TGHV‬
‭
]‬
‭
31‬
‭
adequate safeguards.‬
‭
151‬‭
These and other highly capitalized AI companies should sharply‬
‭
increase investment in these sorts of transparency, safety, and accountability efforts and‬
‭
artifacts.‬
‭
AI researchers from the academic and nonprofit worlds are also self-organizing to develop a‬
‭
wide range of resources for those seeking advice and tools for building OFMs responsibly. For‬
‭
example, the open AI engineering consortium MLCommons, which builds and maintains a wide‬
‭
range of test data sets and evaluation tools around accuracy, speed, and efficiency, is now‬
‭
developing new evaluations for safety issues and societal risks.‬
‭
152‬‭
Meanwhile, a coalition of‬
‭
universities and non-profit labs have also developed the Foundation Model Cheat Sheet, a‬
‭
growing central repository of responsible development guidance and tools built by OFM‬
‭
developers for OFM developers.‬
‭
153‬‭
As already mentioned,‬‭
we are also seeing innovations in‬
‭
software licensing from both commercial and non-commercial players, as developers‬
‭
experiment with use restrictions in their AI licenses that can potentially support liability for or‬
‭
takedowns of noncompliant deployers,‬
‭
154‬‭
while the Open‬‭
Source Initiative is collaboratively‬
‭
developing its own new open source AI license.‬
‭
155‬
‭
A particularly promising area of study is in the spectrum of release options between models that‬
‭
are not open at all to fully open source OFMs, with an OSI-compliant license with no use‬
‭
restrictions and open data/weights. As researcher Irene Solaiman was one of the first to‬
‭
highlight, between those two poles developers can make a lot of choices about when to release‬
‭
what components to whom in order to maximize safety and minimize risk (for example, allowing‬
‭
researcher access for testing prior to publication, or otherwise making the model available for‬
‭
testing in a controlled environment before release; holding back models with particularly risky‬
‭
capabilities until more extensively tested; etc.).‬
‭
156‬
‭
Building on this work, a diverse coalition of experts including CDT were recently convened by‬
‭
Columbia University and Mozilla to develop a more comprehensive mapping of the variety of‬
‭
dimensions of openness available to publishers, including breaking down the various pros and‬
‭
cons of releasing different types of model components or transparency artifacts under different‬
‭
licenses or to different audiences.‬
‭
157‬‭
We hope that‬‭
a more specific parsing of these factors will‬
‭
157‬‭
Ayah Bdeir and Camille Francois, “Introducing the Columbia Convening on Openness and AI,”‬‭
The‬
‭
Mozilla Blog‬‭
(blog), March 6, 2024,‬
‭
https://blog.mozilla.org/en/mozilla/ai/introducing-columbia-convening-openness-and-ai‬
‭
156‬‭
Irene Solaiman, “The Gradient of Generative AI Release: Methods and Considerations,”‬‭
arXiv‬
‭
,‬
‭
February 5, 2023,‬‭
https://arxiv.org/abs/2302.04844‬
‭
.‬‭
[‬
‭
perma.cc/9A34-Z48N‬
‭
]‬
‭
155‬‭
Mia Lykou Lund, “Open Source AI Definition — Weekly Update Mar 18,”‬‭
Open Source Initiative‬‭
(blog),‬
‭
March 18, 2024,‬‭
https://opensource.org/blog/open-source-ai-definition-weekly-update-mar-18‬
‭
.‬
‭
[‬
‭
perma.cc/2XFE-Y3XH‬
‭
]‬
‭
154‬‭
BigScience, 2022,‬‭
supra‬‭
note 4.‬
‭
153‬‭
AI2 et al., “The Foundation Model Development Cheatsheet,” n.d.,‬‭
https://fmcheatsheet.org/‬
‭
.‬
‭
[‬
‭
perma.cc/7SNC-YGT6‬
‭
]‬
‭
152‬‭
MLCommons, “MLCommons Announces the Formation of AI Safety Working Group,” October 26,‬
‭
2023,‬‭
https://mlcommons.org/2023/10/mlcommons-announces-the-formation-of-ai-safety-working-group‬
‭
.‬
‭
[‬
‭
perma.cc/PB3P-NTQX‬
‭
]‬
‭
151‬‭
Meta, 2022,‬‭
supra‬‭
note 4; “Gemma Terms of Use,” Google‬‭
AI, February 1, 2024,‬
‭
https://ai.google.dev/gemma/terms‬
‭
.‬‭
[‬
‭
perma.cc/JYM9-8WGG‬
‭
]‬
‭
32‬
‭
enable better policymaking, whether privately at the OFM publisher level or publicly through‬
‭
regulation or legislation.‬
‭
Software and the First Amendment‬
‭
The question of what are or should be best practices in responsible AI development is distinct‬
‭
from the question of what best practices the government can or should require by law. That is‬
‭
because potential regulation of OFMs may raise serious First Amendment questions. U.S.‬
‭
circuit courts have consistently held that the creation and publication of software code is‬
‭
expressive and is therefore protected by the First Amendment.‬
‭
158‬‭
That conclusion likely applies‬
‭
to the code underlying OFMs and potentially to other model artifacts.‬
‭
In‬‭
Junger v. Daly‬
‭
, the Sixth Circuit Court of Appeals‬‭
held that encryption software source code‬
‭
was speech protected by the First Amendment and that export controls prohibiting its‬
‭
publication to the internet triggered First Amendment scrutiny: “Because computer source code‬
‭
is an expressive means for the exchange of information and ideas about computer‬
‭
programming, we hold that it is protected by the First Amendment.”‬
‭
159‬
‭
The same argument would apply here to the extent the government is aiming to prevent the‬
‭
expression of scientific knowledge — whether about AI or generated by AI. Or, as the Ninth‬
‭
Circuit put it in another case considering the constitutionality of encryption export controls,‬
‭
Bernstein v. U.S. Department of Justice‬
‭
, in a passage‬‭
worth quoting at length:‬
‭
[C]ryptographers use source code to express their scientific ideas in much the same way‬
‭
that mathematicians use equations or economists use graphs…. [M]athematicians and‬
‭
economists have adopted these modes of expression in order to facilitate the precise‬
‭
and rigorous expression of complex scientific ideas. Similarly, the undisputed record‬
‭
here makes it clear that cryptographers utilize source code in the same fashion. In light‬
‭
of these considerations, we conclude that encryption software, in its source code form‬
‭
and as employed by those in the field of cryptography, must be viewed as expressive for‬
‭
First Amendment purposes, and thus is entitled to the protections of the prior restraint‬
‭
doctrine. If the government required that mathematicians obtain a prepublication license‬
‭
prior to publishing material that included mathematical equations, we have no doubt that‬
‭
such a regime would be subject to scrutiny as a prior restraint.‬
‭
159‬‭
Junger, 209 F.3d at 481.‬
‭
158‬‭
See Bernstein v. U.S. Dept. of Justice, 192 F.3d 1308 (9th Cir. 1999), reh'g granted, opinion withdrawn,‬
‭
192 F.3d 1308 (9th Cir. 1999) and Junger v. Daley, 209 F.3d 481 (6th Cir. 2000) (holding that source code‬
‭
can be expressive); see also Universal City Studios v. Corley, 273 F.3d 429 (2d Cir. 2001) (holding that‬
‭
both source code and object code can be expressive).‬
‭
[‬
‭
perma.cc/29WJ-HVJR‬
‭
]. Initial technical and policy memos from this process will be posted at‬
‭
https://research.mozilla.org/‬‭
on Wednesday, 3/27/23.‬‭
See also similar research efforts such as, e.g., Matt‬
‭
White et al., “The Model Openness Framework: Promoting Completeness and Openness for‬
‭
Reproducibility, Transparency and Usability in AI,”‬‭
arXiv‬
‭
, March 20, 2024,‬
‭
https://arxiv.org/abs/2403.13784‬‭
[‬
‭
perma.cc/L8NH-WLFB‬
‭
];‬‭
Partnership on AI. “PAI’s Guidance for Safe‬
‭
Foundation Model Deployment,” March 14, 2024.‬‭
https://partnershiponai.org/modeldeployment/‬
‭
[‬
‭
perma.cc/Z4PZ-BZZC‬
‭
].‬
‭
33‬
‭
While the‬‭
Bernstein‬‭
court relied on prior restraint‬‭
doctrine, the‬‭
Junger‬‭
court instead applied‬
‭
intermediate scrutiny because it found the regulation targeted the functionality rather than the‬
‭
expressiveness of the code. It further found that the government had not met its First‬
‭
Amendment burden to demonstrate how the export control restrictions were narrowly drawn to‬
‭
address a specific problem with a tailored solution. “The government must demonstrate that the‬
‭
recited [national security] harms are real, not merely conjectural, and that the regulation will in‬
‭
fact alleviate these harms in a direct and material way.”‬
‭
160‬‭
The‬‭
Junger‬‭
court ruled against the‬
‭
government even though it acknowledged that encryption software could enable malicious‬
‭
actors to hide their actions from government surveillance.‬
‭
161‬
‭
Of course, legal doctrine can change — the existence of these rulings are not necessarily‬
‭
dispositive of how courts will rule now, especially when there are some notable factual‬
‭
differences. In particular and as highlighted previously, most OFM system components are‬
‭
made of software code, and therefore likely protected by the First Amendment under these and‬
‭
other precedents. Model weights, however, are not code but more akin to a very complex‬
‭
machine-readable database mapping the strength of connections between billions of “tokens” —‬
‭
in the case of LLMs, portions of words — read from a corpus of training data. Therefore, weights‬
‭
cannot be comprehended directly by people. This may prove to be an important distinction,‬
‭
since the previous courts considered it important that at least some computer scientists could‬
‭
read and comprehend software code.‬
‭
162‬
‭
On the other hand, model weights are arguably more expressive than encryption software code,‬
‭
despite not being readable by human eyes. Weights are a mathematical object reflecting the‬
‭
characteristics of the vast amount of human language or imagery in its training data. By‬
‭
“reading” those weights with inference software, users can receive a vast range of helpful (or‬
‭
unhelpful) expressive content derived from those weights, which in turn could support their‬
‭
creative visions or educational pursuits or business endeavors or scientific exploration.‬
‭
163‬
‭
163‬‭
Courts have recognized that listeners and readers have a 1st amendment right to receive speech..‬‭
See‬
‭
Kleindienst v. Mandel, 408 U.S. 753, 762-63 (1972) (“In a variety of contexts this Court‬
‭
has referred to a First Amendment right to ‘receive information and ideas’ . . . .”); Stanley v. Georgia,‬
‭
394 U.S. 557, 564 (1969) (“It is now well established that the Constitution protects the right to receive‬
‭
information and ideas.”). Therefore, even if one does not count the developer as the speaker of generated‬
‭
outputs because they are somewhat stochastic, and one does not count the system as a speaker‬
‭
162‬‭
Bernstein‬
‭
, 192 F.3d at 1308 (“The distinguishing‬‭
feature of source code is that it is meant to be read‬
‭
and understood by humans and that it can be used to express an idea or a method.”);‬‭
Junger‬
‭
, 209 F.3d‬‭
at‬
‭
484 (“Particularly, a musical score cannot be read by the majority of the public but can be used as a‬
‭
means of communication among musicians. Likewise, computer source code, though unintelligible to‬
‭
many, is the preferred method of communication among computer programmers.”);‬‭
Corley‬
‭
, 273 F.3d at‬
‭
445-46  (“Mathematical formulae and musical scores are written in ‘code,’ i.e., symbolic notations not‬
‭
comprehensible to the uninitiated, and yet both are covered by the First Amendment. If someone chose to‬
‭
write a novel entirely in computer object code by using strings of 1's and 0's for each letter of each word,‬
‭
the resulting work would be no different for constitutional purposes than if it had been written in English.‬
‭
The ""object code"" version would be incomprehensible to readers outside the programming community‬
‭
(and tedious to read even for most within the community), but it would be no more incomprehensible than‬
‭
a work written in Sanskrit for those unversed in that language.”).‬
‭
161‬‭
Id.‬
‭
160‬‭
Id. at 485 (internal quotations omitted).‬
‭
34‬
‭
Viewed in this manner, weights are not only expressive but uniquely so, and therefore especially‬
‭
warranting First Amendment protection.‬
‭
The First Amendment is not absolute, however.‬
‭
164‬‭
Regulation‬‭
of speech protected by the First‬
‭
Amendment is possible when the appropriate standards are met. Consequently, as the NTIA‬
‭
considers the available policy and regulatory options with respect to OFMs, it should consider‬
‭
the constitutional implications of each option and recommend ways in which each option might‬
‭
be designed to maximize the likelihood of meeting requisite First Amendment standards, while‬
‭
still achieving the government’s legitimate regulatory goals.‬
‭
To assist NTIA in that endeavor, we briefly discuss some of the regulatory options often‬
‭
mentioned in reference to OFMs, and the First Amendment concerns they raise, in descending‬
‭
order from most to least serious constitutional questions. Based on this discussion, we offer‬
‭
some practical advice on how to avoid recommending policy solutions that courts are more‬
‭
likely to find violate the First Amendment.‬
‭
Prior Restraints including Pre-Licensing‬
‭
Any requirement that creators or distributors of OFMs obtain a license from a government entity‬
‭
(or a private entity designated by the government) prior to making their model weights widely‬
‭
available would likely be viewed by a reviewing court as a prior restraint on publication.‬
‭
165‬
‭
Courts generally view prior restraints on publication with deep skepticism, including in‬
‭
circumstances related to the protection of national security, and with good reason.‬
‭
166‬‭
More than‬
‭
chilling speech, it “freezes” speech, at least for a time, and has permanent and irreversible‬
‭
negative effects.‬
‭
167‬‭
For that reason, prior restraints‬‭
are presumptively unconstitutional and a‬
‭
heavy burden rests with the government to justify their necessity.‬
‭
To the extent such restraints are put in place, they will almost certainly fail First Amendment‬
‭
scrutiny absent strong procedural safeguards to help counter the burden on speech, such as‬
‭
167‬‭
Nebraska Press Association v. Stuart, 427 U.S. 539, 559 (1976).‬
‭
166‬‭
New York Times, 403 U.S. at 719 (“The word ""security"" is a broad, vague generality whose contours‬
‭
should not be invoked to abrogate the fundamental law embodied in the First Amendment. The guarding‬
‭
of military and diplomatic secrets at the expense of informed representative government provides no real‬
‭
security for our Republic.”).‬
‭
165‬‭
See New York Times Co. v. United States, 403 U.S. 713, 714 (1971) (injunction sought by United‬
‭
States against publication of the Pentagon Papers denied as an unconstitutional prior restraint on‬
‭
publication); Freedman v. Maryland, 380 U.S. 51, (1965) (“a noncriminal process which requires the prior‬
‭
submission of a film to a censor avoids constitutional infirmity only if it takes place under procedural‬
‭
safeguards”); Near v. Minnesota, 283 U.S. 697, 716 (1931).‬
‭
164‬‭
United States v. Stevens, 559 U.S. 460 (Apr. 2010); Kathleen Ann Ruane. Freedom of Speech and‬
‭
Press: Exceptions to the First Amendment, Cong. Research Serv. (Sept. 8, 2014)‬
‭
https://digital.library.unt.edu/ark:/67531/metadc462149/‬
‭
.‬‭
[‬
‭
perma.cc/66ZF-EHN4‬
‭
]‬
‭
because it is not a person, the First Amendment still may be implicated. See, e.g., Eugene Volokh, Mark‬
‭
A. Lemley, and Peter Henderson. “Freedom of Speech and AI Output,”‬‭
Journal of Free Speech Law‬
‭
,‬
‭
August 3, 2023,‬‭
https://www.journaloffreespeechlaw.org/volokhlemleyhenderson.pdf‬‭
(arguing for the First‬
‭
Amendment protection of AI outputs based on the users’ right to receive).‬‭
[‬
‭
perma.cc/9VZJ-EWLY‬
‭
]‬
‭
35‬
‭
clear and objective criteria to reduce the discretion of the licensor, clear time limits for a decision‬
‭
to be made, and an ability for prompt judicial review of negative determinations. The lack of‬
‭
such protections was the final nail in the coffin for the licensing scheme in‬‭
Bernstein‬
‭
; the‬
‭
government should avoid such a result here.‬
‭
168‬
‭
Transparency Requirements‬
‭
Transparency regarding training data, fine-tuning efforts, input and output filtering, and other‬
‭
factors that help make a foundation model and the products it powers understandable is‬
‭
strongly desirable as a best practice.‬
‭
169‬‭
However, transparency‬‭
requirements imposed by the‬
‭
government in the non-commercial context are generally considered to be compelled speech‬
‭
and trigger First Amendment scrutiny.‬
‭
170‬‭
Transparency‬‭
requirements in the commercial context‬
‭
are also subject to the First Amendment, but generally courts apply a lower standard of‬
‭
scrutiny.‬
‭
171‬‭
In any case, NTIA should take into account‬‭
the applicable standard when designing‬
‭
any transparency-related regulatory recommendations.‬
‭
The degree of constitutional concern raised by a particular transparency requirement will‬
‭
depend at least in part upon the scope of models subject to the requirement and the extent to‬
‭
which the requirements might burden or influence the editorial judgment of model developers.‬
‭
172‬
‭
For instance, transparency requirements applicable to all OFMs, regardless of context, purpose,‬
‭
or distributing entity, will more likely be subject to strict scrutiny, necessitating the government to‬
‭
meet the high standard of demonstrating the requirements are the least restrictive means of‬
‭
achieving a compelling government interest.‬
‭
173‬‭
On the other hand, if the requirements apply only‬
‭
173‬‭
See Riley, 487 U.S.at 796-97 (“There is certainly some difference between compelled speech and‬
‭
compelled silence, but in the context of protected speech, the difference is without constitutional‬
‭
significance, for thenFirst Amendment guarantees “freedom of speech,” a term necessarily comprising the‬
‭
decision of both what to say and what not to say.”)‬
‭
172‬‭
See Daphne Keller, “Platform Transparency and the First Amendment,”‬‭
Social Science Research‬
‭
Network‬
‭
, March 7, 2023,‬‭
https://doi.org/10.2139/ssrn.4377578‬‭
[‬
‭
perma.cc/7BG5-64Q8‬
‭
]; Kathleen Ann‬
‭
Ruane. Freedom of Speech and Press: Exceptions to the First Amendment, Cong. Research Serv. (Sept.‬
‭
8, 2014)‬‭
https://digital.library.unt.edu/ark:/67531/metadc462149‬‭
[‬
‭
perma.cc/XP43-NDUN‬
‭
].‬
‭
171‬‭
Zauderer v. Office of Disciplinary Counsel, 471 U.S. 626, (1985) (holding that the government may‬
‭
require the disclosure of purely factual information in the commercial context, as long as the requirement‬
‭
is reasonably related to the government’s interest and not unduly burdensome).‬
‭
170‬‭
Riley v. National Federation of the Blind of North Carolina, Inc, 487 U.S. 781 (1988); Meese v. Keene,‬
‭
481 U.S. 465 (1987).‬
‭
169‬‭
Caitlin Vogus and Emma Llansó, “Report - Making Transparency Meaningful: A Framework for‬
‭
Policymakers,”‬‭
Center for Democracy and Technology‬
‭
,‬‭
December 14, 2021,‬
‭
https://cdt.org/insights/report-making-transparency-meaningful-a-framework-for-policymakers/‬
‭
.‬
‭
[‬
‭
perma.cc/JA6B-7L8V‬
‭
]‬
‭
168‬‭
Alan Estevez, “Fireside Chat with Under Secretary Alan Estevez,” Center for Security and Emerging‬
‭
Technology (CSET), Georgetown University, December 2023,‬
‭
https://www.youtube.com/watch?v=WClaOr4wZMM&t=4325s‬‭
[‬
‭
perma.cc/AW88-Z6BP‬
‭
] (“We’re talking‬
‭
about … large language models, we’re having those discussions … I have a team … working on what’s‬
‭
the answer.”) See also Karen Hao, “The New AI Panic,”‬‭
The Atlantic‬
‭
, October 2023,‬
‭
https://www.theatlantic.com/technology/archive/2023/10/technology-exports-ai-programs-regulations-chin‬
‭
a/675605/‬‭
[‬
‭
https://perma.cc/DYW8-NJ4A‬
‭
] (“Commerce‬‭
is considering a new blockade on a broad‬
‭
category of general-purpose AI programs, not just physical parts, according to people familiar with the‬
‭
matter.”)‬
‭
36‬
‭
to commercial or for-profit publishers of OFMs, more permissible standards of scrutiny may be‬
‭
applied, making it easier for the government to justify their imposition.‬
‭
174‬
‭
The level of constitutional concern will also turn on whether transparency requirements‬
‭
encompass disclosures that would impact the editorial judgment of model developers and‬
‭
distributors. For example, requirements to conform to or disclose performance against‬
‭
benchmarks related to a model’s expressive outputs, including toxic speech, hate speech,‬
‭
election disinformation, and scientific knowledge that is undesirable but that publishers have a‬
‭
right to distribute, would raise serious constitutional questions to the extent that courts conclude‬
‭
that the transparency requirements at issue would impact the editorial judgment of the‬
‭
speakers.‬
‭
175‬
‭
Another factor relevant to constitutionality is the breadth of the audience of the required‬
‭
disclosure. In terms of assessing compelled speech, courts have in the past found that narrower‬
‭
compelled disclosures in the context of a particular proceeding with strong procedural‬
‭
protections can comply with the First Amendment.‬
‭
176‬‭
And a final factor to consider is exactly‬
‭
when disclosure is required. Disclosure that is required contemporaneous with or after the‬
‭
publication of an OFM does not raise the specter of a prior restraint, while courts may see‬
‭
required disclosure to the government prior to publication as an attempt at an informal prior‬
‭
restraint and opportunity for the government to pressure against the publication of otherwise‬
‭
protected speech. On the flip side, to the extent that a transparency requirement is intended to‬
‭
help inform the government of significant risks resulting from an OFM so that it can take‬
‭
appropriate action, a pre-release transparency requirement could be justified as necessary to‬
‭
serve the government’s legitimate interests. How a court might resolve those competing‬
‭
considerations is unclear and may depend on the particular facts.‬
‭
For these reasons, NTIA should think carefully about the government interests advanced by‬
‭
transparency into OFMs with open model weights, closely tie any requirements to fulfilling‬
‭
interests separate and distinct from restricting OFMs from outputting protected expression or‬
‭
knowledge, and avoid requirements that might burden or influence editorial judgment, by‬
‭
176‬‭
Herbert v. Lando, 441 U.S. 153, 169 (1979) (allowing compelled disclosure of editorial decisionmaking‬
‭
under court supervision in a particular proceeding).‬
‭
175‬‭
See NetChoice, LLC v. AG, Fla., 34 F.4th 1196 (11th Cir. 2022) (cert. granted) (finding that‬
‭
requirements for social media platforms to provide public individual justifications for each content‬
‭
moderation decision likely did violate the First Amendment because it is overly burdensome and likely‬
‭
chills protected speech); but see Netchoice, L.L.C. v. Paxton, 49 F.4th 439 (5th Cir. 2022) (cert. granted)‬
‭
(finding similar requirements likely did not violate the First Amendment).‬
‭
174‬‭
See Netchoice, L.L.C. v. Paxton, 49 F.4th 439 (5th Cir. 2022) (cert. granted) (finding requirements for‬
‭
social media platforms to disclose an acceptable use policy and information about content and business‬
‭
practices likely did not violate the First Amendment); NetChoice, LLC v. AG, Fla., 34 F.4th 1196 (11th Cir.‬
‭
2022) (cert. granted) (finding that requirements for social media “platforms to publish their standards,‬
‭
inform users about changes to their rules, provide users with view counts for their posts, and inform‬
‭
candidates about free advertising,” are likely not unduly burdensome nor likely to chill platforms' speech”‬
‭
in violation of the First Amendment). See also, Volokh v. James, 656 F. Supp. 3d 431 (S.D. NY. 2023)‬
‭
(holding requirements for social media companies to create mechanisms from reporting “hateful conduct”‬
‭
and disclose their policies regarding how they will respond to complaints likely violates the First‬
‭
Amendment).‬
‭
37‬
‭
focusing any recommended transparency on the production of factual and uncontroversial‬
‭
information about the models.‬
‭
Impact Assessment and Risk Management Requirements‬
‭
Like transparency reporting, internal processes for assessing the impacts and managing the‬
‭
risks of AI systems are also a desirable best practice. However, when the government imposes‬
‭
requirements for engaging in such processes around First Amendment-protected activities like‬
‭
the creation and publication of expressive software (and here, arguably, expressive weights),‬
‭
care must be taken in designing them to reduce the risk that a court might conclude they unduly‬
‭
interfere with editorial judgments. For example, courts may find that certain decisions about‬
‭
what expressive content the model is allowed to output are editorial and that requirements to‬
‭
assess the risks posed by that content are subject to First Amendment scrutiny (though such‬
‭
requirements may in some circumstances withstand that scrutiny).‬
‭
177‬‭
As with the earlier‬
‭
discussion of transparency requirements, NTIA should where possible tailor its‬
‭
recommendations to reduce or eliminate First Amendment concerns with assessment and risk‬
‭
management requirements, such as by clearly targeting such requirements at non-expressive,‬
‭
functional aspects of a model’s development and performance.‬
‭
Context-Specific Requirements‬
‭
The government also has greater authority to impose stronger regulations on the deployment‬
‭
and use of OFMs in specific applications and contexts. For example, in situations where models‬
‭
are being used to make determinations regulated by existing civil rights laws, including those‬
‭
regarding eligibility for housing, employment, credit, or other economic opportunities, the‬
‭
government has broad discretion to take measures, including requiring transparency, auditing,‬
‭
and training data restrictions, that would ensure that the models are not discriminating against‬
‭
individuals on the basis of their membership in a protected class.‬
‭
178‬‭
As already described‬
‭
previously when discussing civil rights, a focus on enforcement at the deployment level may be‬
‭
most effective practically and policy-wise; it would also mitigate First Amendment concerns.‬
‭
Additionally, the government has greater leeway to impose more stringent requirements on the‬
‭
foundation models it seeks to use for its own purposes‬
‭
179‬‭
(hence the importance of the‬
‭
179‬‭
See Yosemite Park & Curry Co. v. United States, 582 F.2d 552, 558 (Ct. Cl. 1978) (“We begin, as did‬
‭
the Government, with 41 U.S.C. § 252(a), which states unequivocally that executive agencies shall make‬
‭
178‬‭
See‬‭
Christian Legal Soc’y Chapter of the Univ. of‬‭
Cal., Hastings Coll. of the Law v. Martinez, 561 U.S.‬
‭
661, 694-95 (2010) (finding non-discrimination requirements to be viewpoint neutral and therefore subject‬
‭
to intermediate scrutiny); Pittsburgh Press Co. v. Human Rel. Comm'n, 413 U.S. 376 (1973) (holding that‬
‭
“”discrimination in employment is not only commercial activity, it is illegal commercial activity” and‬
‭
newspapers could be prohibited from publishing advertisements for employment that discriminated on the‬
‭
basis of sex).‬
‭
177‬‭
See Netchoice v. Bonta, 2023 U.S. Dist. LEXIS 165500 (N.D. CA 2023) (finding that, applying‬
‭
intermediate scrutiny, a requirement for commercial web sites to conduct risk assessments related to‬
‭
potential harms to children posed by their services and detailed plans to address those risks likely violates‬
‭
the First Amendment because the requirement was not properly drawn to address the government’s‬
‭
legitimate interests).‬
‭
38‬
‭
Administration’s additional work through the White House Office of Management and Budget to‬
‭
develop standards around federal agency procurement and use of AI‬
‭
180‬
‭
). The government might‬
‭
also prosecute companies that market their models for illegal purposes or market them in a‬
‭
deceptive or unfair manner, or individuals or entities who use models for illegal purposes. Each‬
‭
of these goals can be pursued with minimal if any constitutional concern.‬
‭
Recommendations for Minimizing First Amendment Issues with Model Regulation‬
‭
To the extent the government chooses to target action directly at the publishers or publication of‬
‭
OFMs rather than focusing on context-specific requirements, below is a summary of key factors‬
‭
they should consider in light of the above First Amendment values and precedents. As quickly‬
‭
as the technology is moving, it is possible there may soon be novel facts justifying changes in‬
‭
doctrine — or the Supreme Court may make relevant doctrinal changes in imminent decisions.‬
‭
Therefore this guidance is based on current doctrine, assuming courts find OFMs and model‬
‭
weights to be expressive and protected by the First Amendment. Moreover, these are highly‬
‭
general statements whose application will depend on the particular policy and facts at issue.‬
‭
●‬ ‭
Post-publication regulation or liability is more likely constitutional than a prior restraint‬
‭
such as pre-licensing.‬
‭
●‬ ‭
A prior restraint with strong procedural safeguards including clear criteria, time limits,‬
‭
and opportunity for judicial review, is more likely to be constitutional than one without.‬
‭
●‬ ‭
Restricting commercial speakers is more likely constitutional than restricting‬
‭
non-commercial speakers.‬
‭
●‬ ‭
Requiring transparency or impact assessments around non-controversial objective facts‬
‭
about an OFM’s development or performance is more likely to be constitutional than‬
‭
around editorial decisions about what expressive content an OFM can output.‬
‭
●‬ ‭
Similarly, requiring transparency or impact assessments around functional aspects of an‬
‭
OFM is more likely constitutional than around what expressive content an OFM can‬
‭
output.‬
‭
181‬
‭
181‬‭
This is doctrinally distinct from, although potentially factually co-extensive with, the previous‬
‭
consideration.‬
‭
180‬‭
Executive Office of the President, Office of Management and Budget. “Proposed Memorandum for the‬
‭
Heads of Executive Departments and Agencies,” November 2023.‬
‭
https://www.whitehouse.gov/wp-content/uploads/2023/11/AI-in-Government-Memo-draft-for-public-review.‬
‭
pdf‬
‭
.‬‭
[‬
‭
perma.cc/Y4DN-4GTW‬
‭
]‬
‭
all purchases of goods and services in compliance with the procurement statutes and implementing‬
‭
regulations . . . except where those statutes and regulations are ‘made inapplicable pursuant to . . . any‬
‭
other law.’”) (second alteration in original)); See also, Regan v. Taxation with Representation, 461 U.S.‬
‭
540 (1983) (holding that the government need not subsidize all speech).‬
‭
39‬
‭
●‬ ‭
Depending on the scope of the requirements, requiring transparency to a regulator for a‬
‭
specified compelling government interest is more likely constitutional than requiring‬
‭
transparency to a broader audience.‬
‭
●‬ ‭
Finally and most importantly, policy interventions narrowly tailored to address‬
‭
evidence-based problems with evidence-based solutions are more likely constitutional‬
‭
than broad interventions aimed at a range of speculative risks.‬
‭
This last factor of narrow tailoring is likely to be an especially critical question for OFM‬
‭
regulation because at present many proposed policy solutions appear to address speculative‬
‭
risks such as those discussed in part II that are not yet supported by evidence, seek to regulate‬
‭
based on features of OFMs that do not clearly correlate to specific risks (such as the number of‬
‭
floating point operations used to train the model), aim to require evaluations and safety‬
‭
measures that, as discussed above, are still emerging and may not effectively measure for or‬
‭
address the risks intended, and/or seek to prohibit publication of a general-purpose‬
‭
informational asset that could drive expression and innovation in a wide variety of fields and for‬
‭
large numbers of people without a showing that such a broad-brush solution is needed to‬
‭
address known risks. For all of these reasons, courts would likely be skeptical of such‬
‭
expansive and broadly targeted regulatory efforts at this time.‬
‭
Further considering the above factors, legislators in particular should be careful to draft with‬
‭
severability in mind so that, even if certain requirements with respect to OFMs are found to be‬
‭
unconstitutional, the remainder can stand.‬
‭
IV.‬
‭
CONCLUSION‬
‭
NTIA should ensure that its recommendations go through a robust interagency process that‬
‭
includes all of the various agencies with equities in this complex issue, including those with‬
‭
responsibility for competition policy, civil rights, and scientific research — and not just the‬
‭
agencies that oversee national security. Similarly, an opportunity for public comment and a‬
‭
robust interagency process will be vital if the Commerce Department’s Bureau of Industry and‬
‭
Security proposes export controls on AI models.‬
‭
We also urge continued in-depth engagement with civil society on these challenging questions,‬
‭
and appreciate the work NTIA has already done to engage a range of voices. CDT looks‬
‭
forward to continuing to work collaboratively toward AI policies that are evidence-based and‬
‭
effective at protecting the full range of communities impacted by this technology.‬
‭
***‬
‭
We appreciate NTIA’s solicitation of feedback from stakeholders on these important matters. For‬
‭
additional information, or any inquiries, please contact Kevin Bankston (kbankston@cdt.org),‬
‭
CDT’s Senior Advisor on AI Governance.‬
‭
40‬
",CDT,51351
NTIA-2023-0009-0211,"1960 Bryant Street
San Francisco, CA 94110
March 27, 2024
From:
Anna Makanju
Vice President of Global Affairs
OpenAI
To:
The Honorable Alan Davidson
Administrator
National Telecommunications and Information
Administration (NTIA)
1401 Constitution Ave, NW
Washington, D.C. 20230
RE: Request for Comment (RFC) on Dual Use Foundation Artificial
Intelligence Models with Widely Available Model Weights (Docket
No. NTIA–2023–0009)
OpenAI appreciates the opportunity to provide input on the important
questions raised in the NTIA’s request for comment on dual-use
foundation models with widely available weights.
There are many paths to safe and beneficial AI.
OpenAI believes that building, broadly deploying, and using AI can
improve people’s lives and unlock a better future. Progress relies on
innovation and free market competition. Within those broad guidelines,
there are many different paths by which people can further the promise of
AI. OpenAI was among the first AI developers to wrestle with the question
of how to distribute the benefits of unprecedentedly-capable foundation
models, and we start by providing this historical context to help inform the
NTIA’s deliberations.
In 2019, we created GPT-2, which had the new capability of generating
coherent paragraphs of text, and were faced with the question of how to
deploy it. On the one hand, the model seemed very useful; on the other
hand, we weren’t sure if it could be useful for malicious purposes such as
phishing email generation. We opted to experiment with a “staged
release”. As we wrote at the time, “staged release involves the gradual
release of a family of models over time. The purpose of our staged
release of GPT-2 is to give people time to assess the properties of these
models, discuss their societal implications, and evaluate the impacts of
release after each stage.” When we did not observe significant misuse
effects, this gave us the confidence to openly release the full model
weights.
In 2020, we created GPT-3, which was much more capable than any
previous language model on every benchmark, and again faced the
question of how to release it. This time, we decided to release it via our
first product, the OpenAI API (an Application Programming Interface,
which allows developers to build apps on our technology). As we wrote at
the time, we had several motivations for this new release strategy:
“commercializing the technology helps us pay for our ongoing AI
research, safety, and policy efforts” and “the API model allows us to more
easily respond to misuse of the technology. Since it is hard to predict the
downstream use cases of our models, it feels inherently safer to release
1
them via an API and broaden access over time, rather than release an
open source model where access cannot be adjusted if it turns out to
have harmful applications.” Over several years, this API release taught us
and the community lessons about the safety and misuse patterns of
GPT-3 level models.
In the years since, we have continued to support and believe in the
promise of the open-source AI ecosystem, including by openly releasing
the weights of some of our state-of-the-art models (such as CLIP and
Whisper) and developing open-source infrastructure for other AI
developers (such as the Triton GPU programming language). We have
seen openly released weights bring a variety of significant benefits,
including facilitating academic research on the internals of AI models,
enabling users and organizations to run models locally on their edge
devices, and facilitating creative modifications of models to suit users’
ends. Many AI companies have chosen to invest heavily in open model
weight releases for a variety of reasons, including brand, recruiting, and
attracting a developer ecosystem to build on and accelerate the internals
of a company’s technology.
At the same time, our approach to releasing our flagship AI models via
APIs and commercial products like ChatGPT has enabled us to continue
studying and mitigating risks that we discovered after initial release, often
in ways that would not have been possible had the weights themselves
been released. For example, we recently partnered with Microsoft to
detect, study, and disrupt the operations of a number of nation-state cyber
threat actors who were abusing our GPT-3.5-Turbo and GPT-4 models to
assist in cyberoffensive operations. Disrupting these threat actors would
not have been possible if the weights of these at-the-time frontier models
had been released widely, as the same cyber threat actors could have
hosted the model on their own hardware, never interacting with the
original developer. This approach has enabled us to continue to distribute
the benefits of AI broadly, including via widely-available free and low-cost
services.
These experiences have convinced us that both open weights releases
and API and product-based releases are tools for achieving beneficial AI,
and we believe the best American AI ecosystem will include both.
Combining iterative deployment with a Preparedness Framework
Over and over again, across both product releases and weight releases,
we have seen the incredible benefits of “iterative deployment”: gradually
putting increasingly capable AI into people’s hands so they can use it to
improve their lives, and helping society adjust to these new technologies.
As we wrote in 2023: “We work hard to prevent foreseeable risks before
2
deployment, however, there is a limit to what we can learn in a lab.
Despite extensive research and testing, we cannot predict all of the
beneficial ways people will use our technology, nor all the ways people
will abuse it. That’s why we believe that learning from real-world use is a
critical component of creating and releasing increasingly safe AI systems
over time.”
As AI models become even more powerful and the benefits and risks of
their deployment or release become greater, it is also important that we
be increasingly sophisticated in deciding whether and how to deploy a
model. This is particularly true if AI capabilities come to have significant
implications for public safety or national security. The future presence of
such “catastrophic” risks from more advanced AI systems is inherently
uncertain, and there is scholarly disagreement on how likely and how
soon such risks will arise. We do not believe there is yet sufficient
evidence; we can’t rule them out, nor be certain they’re imminent. As
developers advancing the frontier of AI capabilities to maximize their
benefits, we view building the science of the risks of this technology
(including gathering evidence related to those risks) as integral to our
work.
To navigate these uncertainties in an empirically driven manner, OpenAI
publicly launched our Preparedness Framework, a science-based
approach to continuously assess and mitigate any catastrophic risks that
might be posed by our AI models. The Preparedness Framework defines
how we evaluate our AI models’ capability levels in several high-risk
domains, including cybersecurity, autonomous operation, individualized
persuasion, and CBRN (Chemical, Biological, Radiological, and Nuclear)
threats. For an example of this framework in action, see our recent study
testing GPT-4’s ability to aid in biological threat creation, which concluded
that it poses no significant marginal risk.
Based on these evaluations, we assess models’ risk levels in each
category as Low, Medium, High, or Critical. Crucially, under our
Preparedness Framework, we will not deploy AI systems that pose a risk
level of “High” or “Critical” in our taxonomy (and will not even train
“Critical” ones, given their level of risk), unless our mitigations can bring
these systems’ risk down to at most a “Medium” level. The Preparedness
Framework is important because it lets us build and widely share the
benefits of increasingly capable AI, while preparing us to detect and
protect against catastrophic risks as early as possible if they do arise.
Practices for developers of highly capable AI
We believe that people and companies should be able to participate in AI
as they choose —which can include developing or using AI that reflects
3
their values and vision —in order to achieve the benefits of AI. At the
same time, highly capable AI systems should be built and used safely,
with any discovered catastrophic risks appropriately mitigated. These
interests may sometimes be in tension, and need to be thoughtfully
managed in a case-specific way to achieve the best outcomes for society.
In the case of highly-capable foundation models which require significant
resources to create (on the order of hundreds of millions of dollars or
more), we believe that AI developers should assess their model’s
potential to pose catastrophic risks, and, if the model’s risk level is found
to be high, put appropriate mitigations in place before deploying or
releasing it. This strikes an appropriate balance between
risk-management and innovation: these models are anticipated to have
the greatest capabilities, while the cost of assessment is at most a small
fraction of their development cost. Such assessments make sense
regardless of whether the model’s weights are intended to be released
widely or through an API.
On the other end of the spectrum, in the case of less resource-intensive
foundation models, the balance of interests is different. On current
evidence, such models appear much less likely to pose catastrophic risks,
even with likely advances in finetuning and model-modification
techniques. Meanwhile, assessments for catastrophic risk may cost a
substantial fraction of the budget of small training runs, which could lead
to a chilling effect on innovation and competition. We believe that such
assessments for catastrophic risks should not be expected for these
models, as there is huge value in protecting a diversity of developers’
ability to innovate on exciting new AI capabilities and allowing the
marketplace of ideas and products to flourish, and the science indicates
that these models’ risk is relatively low.
Assessment protocols like the Preparedness Framework are a useful tool
to evaluate the ex ante risks from any type of model release, including
open model weight releases. There are a few considerations that are
specific for how to apply them to open weights releases.
One such consideration is that testing conditions would ideally reflect the
range of ways that downstream actors can modify the model. One of the
most useful properties of open models is that downstream actors can
modify the models to expand their initial capabilities and tailor them to the
developer’s specific applications. However, this also means that malicious
parties could potentially enhance the model’s harmful capabilities.
Rigorously assessing an open-weights release’s risks should thus include
testing for a reasonable range of ways a malicious party could feasibly
modify the model, including by finetuning. OpenAI already conducts some
4
modification-testing as part of our Preparedness Framework (as we did in
our biorisk assessment).
Another key consideration is that open model developers may be unable
to rely on system-level safeguards to reduce the risk of their model’s
misuse, as safeguards can often be removed by a malicious downstream
user who possesses the model weights. Today, this difference in
mitigation ability has limited consequences, since even our most capable
current models are not rated as especially risky. But if a future model is
scientifically determined to pose severe risks if released, then the path to
reduce the risk of an open-weights release may rely on increasing the
resilience of the external environment into which the model is released.
The need for societal resilience to AI misuse is broader than any one
organization’s release decisions. Given continuing progress in and
dissemination of AI algorithms, and increasingly widespread access to
compute (including in countries of concern to the United States), today’s
frontier AI capabilities — often accessible to only a few actors at time of
creation — will eventually proliferate widely. The United States, and
countries around the world, also have an opportunity to invest in and lead
in mitigations that will limit the consequence of misuse, so the balance of
outcomes is maximally positive.
For instance, strengthening resilience against AI-accelerated cyberattack
risks might involve providing critical infrastructure providers early access
to those same AI models, so they can be used to improve cyber-defense
(as in the early projects we have funded as part of the OpenAI
Cybersecurity Grant Program). Strengthening resilience against
AI-accelerated biological threat creation risks may involve solutions totally
unrelated to AI, such as improving nucleic acid synthesis screening
mechanisms (as called for in Executive Order 14110), or improving public
health systems’ ability to screen for and identify new pathogen outbreaks.
If an AI model is rigorously shown to pose severe risks to public safety or
national security, then the developer may also have an important role to
play in building awareness of the new capabilities in advance of wide
release (such as via notifying infrastructure providers or limiting API
deployment), to create both time and motivation for urgently needed
resilience efforts. This mirrors the norm of “responsible disclosure” from
the cyber domain, where security researchers will temporarily embargo
the release of vulnerabilities they find so as to give time for defenders to
patch their systems, while not slowing down further security research.
We need a better science of AI risks
While we believe that assessing the risks of the most capable models is
important, the science of AI risk evaluations is nascent. OpenAI and the
5
broader AI community are still building the foundations of how to assess
AI risks, and we are still constantly iterating on many of the
operationalization details in the Preparedness Framework. Governments
have an important role to play in helping the AI ecosystem mature its risk
and capability evaluation practices, such as by convening experts from
the offensive cybersecurity, critical infrastructure, and AI worlds to agree
on a set of priority AI cyber threat models, and build out rigorous and
empirical testbeds for assessing them. We strongly support the voluntary
innovation-friendly and science-first approach being pursued by the
USAISI.
Ever since OpenAI faced the choice of how to release GPT-2 in 2019 –
opting to release only a small version of the model at first — new findings
and events have continuously changed the landscape of considerations
around open release of foundation model weights, sometimes every few
months. We expect this trend to continue. Any government policy
approach should be flexible and adaptable to future changes.
6
",OpenAI,3047
NTIA-2023-0009-0166,"ABOUT MLCOMMONS AND ITS INTEREST IN THIS REQUEST FOR COMMENT
This response to the National Telecommunications and Information Administration's Request for
Comment on Dual Use Foundation Artificial Intelligence Models with Widely Available Model Weights is
submitted on behalf of MLCommons®.
MLCommons is a non-profit consortium that aims to accelerate the benefits of machine learning and
artificial intelligence (AI). Our members and partners include over 125 organizations from around the
world, many of which are leading technology companies, startups, academics, and nonprofits that are
actively researching, developing, and deploying artificial intelligence products for customers. Critically,
our founding membership includes academic researchers at the forefront of machine learning research,
and the research community continues to be core to our membership helping to lead many of our
working groups. MLCommons acts as a neutral nexus for commercial and non-commercial actors to
collaborate on tools that advance the field.
We create, operate and maintain community assets, especially benchmarks and datasets, that facilitate
developing and evaluating artificial intelligence systems in pursuit of our mission to “make artificial
intelligence better for everyone.”1 The original project that brought MLCommons into being is a
benchmarking suite called MLPerf®, which provides unbiased evaluations of training and inference
speed for AI hardware and software.2 These measurements enable a fair comparison of competing
systems, accelerate ML progress through fair and useful measurement, enforce reproducibility to
ensure reliable results, and do so in an open and collaborative way to keep benchmarking affordable for
all participants. We have also developed and released a number of open datasets for AI training,
including images of everyday objects from around the world and spoken words across dozens of
languages.
In November, 2023, we announced the formation of an AI Safety Working Group, which is an open
working group that anyone from the AI community can participate in.3 The working group is developing
a platform and pool of tests from many contributors to support AI safety benchmarks for diverse use
cases. The group’s initial focus will be developing safety benchmarks for large language models
(LLMs), building on groundbreaking work done by researchers at Stanford University’s Center for
Research on Foundation Models and its Holistic Evaluation of Language Models (HELM). We believe
standard AI safety benchmarks will become a vital element of a successful approach to AI safety, and
will be critical to ensuring the safety of both open and proprietary AI systems.
MLCommons counts among members of its AI Safety Working Group organizations that have releasedI
AI models without disclosing their weights, and organizations that have released AI models with open
weights. As an organization, we believe there is a place in the market for both approaches to flourish.
3 ""MLCommons Announces the Formation of AI Safety Working Group,"" MLCommons, October 2023,
https://mlcommons.org/2023/10/mlcommons-announces-the-formation-of-ai-safety-working-group/.
2 Peter Mattson, et al, ""MLPerf: An Industry Standard Benchmark Suite for Machine Learning Performance,"" IEEE Xplore, accessed February
1, 2024, https://ieeexplore.ieee.org/abstract/document/9001257.
1 Machine learning is one of the key techniques through which AI systems are built.
For the purposes of this comment, our focus is on the need to preserve the ability for developers to
release widely available open model weights in order to ensure AI Safety for the ecosystem as a whole.
STANDARDIZED BENCHMARKS WILL BE A LINCHPIN OF THE AI SAFETY ECOSYSTEM
A modern AI system requires empirical measurement to understand its characteristics, including safety.
Traditional approaches to managing risk in technology systems that rely solely on process compliance
will potentially increase friction without delivering intended safety results. Safety-by-process that
focuses on documenting and auditing training inputs, without measuring the behavior of the end model,
may not ensure safety and can produce unexpected failure modes. For example, requiring use of
high-quality training data does not necessarily imply that the dataset is suited to the particular issue one
is trying to address; it is possible for a well-documented training process to use high quality training
data that inadvertently under-represents a real-world problem class, resulting in the model’s outputs on
the problems of that class being incorrect.4 Instead of managing risk through process compliance alone,
modern AI requires a strong emphasis on measurement to mitigate risk.
Testing an AI system for safety is unlike testing conventional software code that is intended to produce
discrete and objectively verifiable behavior. Defining a test set for an AI model that has effective
coverage of the potential input space is a nascent measurement science.5,6 This is because the latest
iteration of said models, known as language models, are able to directly interact in natural language
with an exponentially large number of possible input sentences, making full coverage intractable.7
Measurement for safety is also challenging because of the many aspects of responsible development
that need to be evaluated including avoiding physical harms, resistance to malicious uses, fairness,
misinformation, and privacy. Each of these requires dedicated tests and evaluation resources, as well
as robust input from a wide range of stakeholders and experts. Unlike the more objective measurement
of hardware speed or model performance, these varied aspects of safety contain an inherent
subjectivity and ambiguity.
Managing risk in modern AI is challenging in ways that dramatically differ from traditional software risk
management. There are, however, lessons to be learned in how other industries approach risk
management and safety. In complex systems that necessarily interact with the unpredictability of the
physical world, such as automobiles or planes, standardized approaches to safety testing have been
widely
adopted
with
success.
No
automobile can be deemed perfectly safe in all possible
circumstances, but there are general expectations that automobiles must meet standard safety
benchmarks.
We believe in mirroring this approach to create standardized safety benchmarks in AI. Such
benchmarks will create a common direction for research efforts across companies and academic
7 Rishi Bommasani et al., ""On the Opportunities and Risks of Foundation Models,"" arXiv, August 2021, https://arxiv.org/abs/2108.07258.
6 Sculley, David, et al. ""Hidden technical debt in machine learning systems."" Advances in neural information processing systems 28 (2015).
5 Amershi, Saleema, et al. ""Software engineering for machine learning: A case study."" 2019 IEEE/ACM 41st International Conference on
Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 2019.
4Joy Buolamwini and Timnit Gebru, ""Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,"" Proceedings of
the 1st Conference on Fairness, Accountability, and Transparency, 2018, https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf.
institutions, and raise the bar for safety across the industry. Furthermore, if built with care, the
benchmarks can produce safety analyses that are comprehensible to purchasers, policy makers, and
the public.
Most critically for the purpose of the NTIA’s inquiry into widely available open weights, standardized
benchmarks will be a critical component for mitigating the risk of models both with and without widely
available open weights. Benchmarks provide an equivalent approach to measuring the safety of AI
model outputs, regardless of what is known about the model weights. But building reliable benchmarks
will also depend on including models with widely available open weights in the development process.
AI SAFETY BENCHMARKING BENEFITS BY INCORPORATING MODELS WITH OPEN WEIGHTS
Models with open weights have played a central role in developing widely trusted benchmarks that
have been used to evaluate and measure AI models, and in doing so have helped drive progress in AI.
GLUE, BigBench, Harness, HELM and openCLIP Benchmark are all examples of widely used
benchmarks that have helped researchers and developers measure progress in the development of AI
models.8,9,10,11 In order to create these and similar benchmarks, researchers benefited from the ability to
freely reuse existing models as “reference” models which generated the benchmark results against
which other models’ results are compared. Critically, the use of reference models with open weights
enable the broader AI community to trust the results of these benchmarks because they are able to
interrogate the reference model’s functioning. In a sense, using models with open weights as reference
models has acted as a trust-building mechanism for evaluation of AI models generally.
Increasingly, we see safety benchmarks being operationalized with the use of a separate, “evaluator”
model. When an AI model is tested against a benchmark, it is fed a set of test data that it uses to
produce results, and these results are checked for safety. This safety check can be done by human
review, but it can also be done with the use of an evaluator model. Human review will always be a part
of evaluating AI safety, but it has limitations, specifically it doesn’t scale in a very cost effective way.
Evaluator AI models can be used to automatically review test results in a far more cost effective way.
It may prove ideal to use multiple evaluator models. Some of these will have open weights, building
trust with the wider community that any changes in how results are evaluated can be understood by
interrogating the open evaluator model. But relying only on models with open weights runs the risk that
developers will over-fit to the evaluator model. On the other hand, if only a closed weights model is
used in evaluation, results may change in unpredictable and unintelligible ways, undermining trust in
the evaluation. To balance these competing incentives, we believe it can be ideal to use multiple
evaluator models simultaneously, some with open weights and others with closed weights.
11 ""CLIP Benchmark,"" GitHub, accessed March 26, 2024, https://github.com/LAION-AI/CLIP_benchmark.
10 ""Large Language Model Evaluation,"" EleutherAI, accessed March 26, 2024,
https://www.eleuther.ai/projects/large-language-model-evaluation.
9 ""Big Bench Datasets,"" Hugging Face Datasets, accessed March 26, 2024, https://huggingface.co/datasets/bigbench.
8 ""GLUE Benchmark,"" GLUE Benchmark, accessed March 26, 2024, https://gluebenchmark.com/.
The AI Safety Working Group led by MLCommons is now in the process of building operational safety
benchmarks, leveraging the HELM framework from Stanford’s Center for Research on Foundation
Models. Our goal in operationalizing safety benchmarks is to inform AI development by empowering
users of AI systems and the regulators who oversee them with simple, easy to understand,
standardized measures of AI system performance against a variety of safety goals. The most
challenging aspect of doing this work is not technically defining the benchmarks, but is instead building
trust in the process of how these benchmarks get defined. We believe that defining standard safety
benchmarks will benefit tremendously from the inherent trust-building mechanism of incorporating AI
models with open weights both as reference and evaluator models.
Models with widely available open weights allow the entire AI safety community – including auditors,
regulators, civil society, users of AI systems, and developers of AI systems – to engage with the
benchmark development process. Together with open data and model code, open weights enable the
community to clearly and completely understand what a given safety benchmark is measuring,
eliminating any confounding opacity around how a model was trained or optimized.
",MLCommons,2420
NTIA-2023-0009-0210," 
 
 
 
 
 
Comments of the International Center for Law 
& Economics, Dual Use Foundation Artificial 
Intelligence Models with Widely Available 
Model Weights 
Docket No. NTIA–240216-0052 
March 27, 2024 
 
 
 
 
 
Authored by: 
Kristian Stout (Director of Innovation Policy, International Center for Law & 
Economics) 
 
 
 
 
DOCKET NO. NTIA–2023–0009 
 
 2 OF 18 
 
 
I. 
Introduction 
We thank the National Telecommunications and Information Administration (NTIA) for the 
opportunity to contribute to this request for comments (RFC) in the ""Dual Use Foundation 
Artificial Intelligence Models with Widely Available Model Weights"" proceeding. In these 
comments, we endeavor to offer recommendations to foster the innovative and responsible 
production of artificial intelligence (AI), encompassing both open-source and proprietary 
models. Our comments are guided by a belief in the transformative potential of AI, while 
recognizing NTIA's critical role in guiding the development of regulations that not only protect 
consumers but also enable this dynamic field to flourish. The agency should seek to champion 
a balanced and forward-looking approach toward AI technologies that allows them to evolve 
in ways that maximize their social benefits, while navigating the complexities and challenges 
inherent in their deployment. 
NTIA’s question “How should [the] potentially competing interests of innovation, 
competition, and security be addressed or balanced?”1 gets to the heart of ongoing debates 
about AI regulation. There is no panacea to be discovered, as all regulatory choices require 
balancing tradeoffs. It is crucial to bear this in mind when evaluating, e.g., regulatory proposals 
that implicitly treat AI as inherently dangerous and regard as obvious that stringent regulation 
is the only effective strategy to mitigate such risks.2 Such presumptions discount AI’s unknown 
but potentially enormous capacity to produce innovation, and inadequately account for other 
tradeoffs inherent to imposing a risk-based framework (e.g., requiring disclosure of trade secrets 
or particular kinds of transparency that could yield new cybersecurity attack vectors). Adopting 
an overly cautious stance risks not only stifling AI’s evolution, but may also preclude a fulsome 
exploration of its potential to foster social, economic, and technological advancement. A more 
restrictive regulatory environment may also render AI technologies more homogenous and 
smother development of the kinds of diverse AI applications needed to foster robust 
competition and innovation. 
We observe this problematic framing in the executive order (EO) that serves as the provenance 
of this RFC.3 The EO repeatedly proclaims the importance of “[t]he responsible development 
 
1 Dual Use Foundation Artificial Intelligence Models With Widely Available Model Weights, Docket No. 240216-0052, 89 FR 
14059, NATIONAL TELECOMMUNICATIONS AND INFORMATION ADMINISTRATION (Mar. 27, 2024) at 14063, question 
8(a) [hereinafter “RFC”]. 
2 See, e.g., Kristian Stout, Systemic Risk and Copyright in the EU AI Act, TRUTH ON THE MARKET (Mar. 19, 2024), 
https://truthonthemarket.com/2024/03/19/systemic-risk-and-copyright-in-the-eu-ai-act. 
3 Exec. Order No. 14110, 88 F.R. 75191 (2023), https://www.federalregister.gov/documents/2023/11/01/2023-
24283/safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence?_fsi=C0CdBzzA [hereinafter “EO”]. 
DOCKET NO. NTIA–2023–0009 
 
 3 OF 18 
 
and use of AI” in order to “mitigate[e] its substantial risks.”4 Specifically, the order highlights 
concerns over ""dual-use foundation models""—i.e., AI systems that, while beneficial, could pose 
serious risks to national security, national economic security, national public health, or public 
safety.5 Concerningly, one of the categories the EO flags as illicit “dual use” are systems 
“permitting the evasion of human control or oversight through means of deception or 
obfuscation.”6 This open-ended category could be interpreted so broadly that essentially any 
general-purpose generative-AI system would classify.  
The EO also repeatedly distinguishes “open” versus “closed” approaches to AI development, 
while calling for “responsible” innovation and competition.7 On our reading, the emphasis the 
EO places on this distinction raises alarm bells about the administration’s inclination to stifle 
innovation through overly prescriptive regulatory frameworks, diminishment of the intellectual 
property rights that offer incentives for innovation, and regulatory capture that favors 
incumbents over new entrants. In favoring one model of AI development over another, the 
EO’s prescriptions could inadvertently hamper the dynamic competitive processes that are 
crucial both for technological progress and for the discovery of solutions to the challenges that 
AI technology poses.  
Given the inchoate nature of AI technology—much less the uncertain markets in which that 
technology will ultimately be deployed and commercialized—NTIA has an important role to 
play in elucidating for policymakers the nuances that might lead innovators to choose an open 
or closed development model, without presuming that one model is inherently better than the 
other—or that either is necessarily “dangerous.” Ultimately, the preponderance of AI risks will 
almost certainly emerge idiosyncratically. It will be incumbent on policymakers to address such 
risks in an iterative fashion as they become apparent. For now, it is critical to resist the urge to 
enshrine crude and blunt categories for the heterogeneous suite of technologies currently 
gathered under the broad banner of  “AI.”  
Section II of these comments highlights the importance of grounding AI regulation in actual 
harms, rather than speculative risks, while outlining the diversity of existing AI technologies 
and the need for tailored approaches. Section III starts with discussion of some of the benefits 
and challenges posed by both open and closed approaches to AI development, while cautioning 
against overly prescriptive definitions of ""openness"" and advocating flexibility in regulatory 
frameworks. It proceeds to examine the EO’s prescription to regulate so-called ""dual-use"" 
foundation models, underscoring some potential unintended consequences for open-source 
 
4 See, e.g., EO at §§ 1; 2(c), 5.2(e)(ii); and § 8(c);  
5 Id. at § 3(k). 
6 Id. at § (k)(iii). 
7 Id. at § 4.6. As NTIA notes, the administration refers to “widely available model weight,” which is equivalent to 
“open foundation models” in this proceeding. RFC at 14060. 
DOCKET NO. NTIA–2023–0009 
 
 4 OF 18 
 
AI development and international collaboration. Section IV offers some principles to craft an 
effective regulatory model for AI, including distinguishing between low-risk and high-risk 
applications, avoiding static regulatory approaches, and adopting adaptive mechanisms like 
regulatory sandboxes and iterative rulemaking. Section V concludes. 
II. Risk Versus Harm in AI Regulation 
In many of the debates surrounding AI regulation, disproportionate focus is placed on the 
need to mitigate risks, without sufficient consideration of the immense benefits that AI 
technologies could yield. Moreover, because these putative risks remain largely hypothetical, 
proposals to regulate AI descend quickly into an exercise in shadowboxing. 
Indeed, there is no single coherent definition of what even constitutes “AI.” The term 
encompasses a wide array of technologies, methodologies, and applications, each with distinct 
characteristics, capabilities, and implications for society. From foundational models that can 
generate human-like text, to algorithms capable of diagnosing diseases with greater accuracy 
than human doctors, to “simple” algorithms that facilitate a more tailored online experience, 
AI applications and their underlying technologies are as varied as they are transformative. 
This diversity has profound implications for the regulation and development of AI. Very 
different regulatory considerations are relevant to AI systems designed for autonomous vehicles 
than for those used in financial algorithms or creative-content generation. Each application 
domain comes with its own set of risks, benefits, ethical dilemmas, and potential social impacts, 
necessitating tailored approaches to each use case. And none of these properties of AI map 
clearly onto the “open” and “closed” designations highlighted by the EO and this RFC. This 
counsels for focus on specific domains and specific harms, rather than how such technologies 
are developed.8 
As in prior episodes of fast-evolving technologies, what is considered cutting-edge AI today may 
be obsolete tomorrow. This rapid pace of innovation further complicates the task of crafting 
policies and regulations that will be both effective and enduring. Policymakers and regulators 
must navigate this terrain with a nuanced understanding of AI's multifaceted nature, including 
by embracing flexible and adaptive regulatory frameworks that can accommodate AI’s 
continuing evolution.9 A one-size-fits-all approach could inadvertently stifle innovation or 
entrench the dominance of a few large players by imposing barriers that disproportionately 
affect smaller entities or emerging technologies. 
 
8 For more on the “open” vs “closed” distinction and its poor fit as a regulatory lens, see, infra, at nn. 18-39 and 
accompanying text. 
9 Adaptive regulatory frameworks are discussed, infra, at nn. 40-51 and accompanying text.  
DOCKET NO. NTIA–2023–0009 
 
 5 OF 18 
 
Experts in law and economics have long scrutinized both market conduct and regulatory rent 
seeking that serve to enhance or consolidate market power by disadvantaging competitors, 
particularly through increasing the costs incurred by rivals.10 Various tactics may be employed 
to undermine competitors or exclude them from the market that do not involve direct price 
competition. It is widely recognized that ""engaging with legislative bodies or regulatory 
authorities to enact regulations that negatively impact competitors"" produces analogous 
outcomes.11 It is therefore critical that the emerging markets for AI technologies not engender 
opportunities for firms to acquire regulatory leverage over rivals. Instead, recognizing the 
plurality of AI technologies and encouraging a multitude of approaches to AI development 
could help to cultivate a more vibrant and competitive ecosystem, driving technological 
progress forward and maximizing AI’s potential social benefits. 
This overarching approach counsels skepticism about risk-based regulatory frameworks that fail 
to acknowledge how the theoretical harms of one type of AI system may be entirely different 
from those of another. Obviously, the regulation of autonomous drones is a very different sort 
of problem than the regulation of predictive policing or automated homework tutors. Even 
within a single circumscribed domain of generative AI—such as “smart chatbots” like ChatGPT 
or Claude—different applications may present entirely different kinds of challenges. A highly 
purpose-built version of such a system might be employed by government researchers to 
develop new materiel for the U.S. Armed Forces, while a general-purpose commercial chatbot 
would employ layers of protection to ensure that ordinary users couldn’t learn how to make 
advanced weaponry. Rather treating “chatbots” as possible vectors for weapons development, 
a more appropriate focus would target high-capability systems designed to assist in developing 
such systems. Were it the case that a general-purpose chatbot inadvertently revealed some 
information on building weapons, all incentives would direct that AI’s creators to treat that as 
a bug to fix, not a feature to expand. 
Take, for example, the recent public response to the much less problematic AI-system 
malfunctions that accompanied Google’s release of its Gemini program.12 Gemini was found 
to generate historically inaccurate images, such as ethnically diverse U.S. senators from the 
1800s, including women.13 Google quickly acknowledged that it did not intend for Gemini to 
create inaccurate historical images and turned off the image-generation feature to allow time 
 
10 See Steven C. Salop & David T. Scheffman, Raising Rivals’ Costs, 73:2 AM. ECON. R. 267, 267–71 (1983), 
http://www.jstor.org/stable/1816853. 
11 See Steven C. Salop & David T. Scheffman, Cost-Raising Strategies, 36:1 J. INDUS. ECON. 19 (1987), 
https://doi.org/10.2307/2098594. 
12 Cindy Gordon, Google Pauses Gemini AI Model After Latest Debacle, FORBES (Feb. 29, 2024), 
https://www.forbes.com/sites/cindygordon/2024/02/29/google-latest-debacle-has-paused-gemini-ai-
model/?sh=3114d093536c. 
13 Id. 
DOCKET NO. NTIA–2023–0009 
 
 6 OF 18 
 
for the company to work on significant improvements before re-enabling it.14 While Google 
blundered in its initial release, it had every incentive to discover and remedy the problem. The 
market response provided further incentive for Google to get it right in the future.15 Placing 
the development of such systems under regulatory scrutiny because some users might be able to 
jailbreak a model and generate some undesirable material would create disincentives to the 
production of AI systems more generally, with little gained in terms of public safety.  
Rather than focus on the speculative risks of AI, it is essential to ground regulation in the need 
to address tangible harms that stem from the observed impacts of AI technologies on society. 
Moreover, focusing on realistic harms would facilitate a more dynamic and responsive 
regulatory approach. As AI technologies evolve and new applications emerge, so too will the  
potential harms. A regulatory framework that prioritizes actual harms can adapt more readily 
to these changes, enabling regulators to update or modify policies in response to new evidence 
or social impacts. This flexibility is particularly important for a field like AI, where 
technological advancements could quickly outpace regulation, creating gaps in oversight that 
may leave individuals and communities vulnerable to harm. 
Furthermore, like any other body of regulatory law, AI regulation must be grounded in 
empirical evidence and data-driven decision making. Demanding a solid evidentiary basis as a 
threshold for intervention would help policymakers to avoid the pitfalls of reacting to 
sensationalized or unfounded AI fears. This would not only enhance regulators’ credibility 
with stakeholders, but would also ensure that resources are dedicated to addressing the most 
pressing and substantial issues arising from the development of AI. 
III. The Regulation of Foundation Models 
NTIA is right to highlight the tremendous promise that attends the open development of AI 
technologies:   
Dual use foundation models with widely available weights (referred to here as open 
foundation models) could play a key role in fostering growth among less resourced 
actors, helping to widely share access to AI’s benefits…. Open foundation models 
can be readily adapted and fine-tuned to specific tasks and possibly make it easier 
for system developers to scrutinize the role foundation models play in larger AI 
systems, which is important for rights- and safety-impacting AI systems (e.g. 
healthcare, education, housing, criminal justice, online platforms etc.) 
…Historically, widely available programming libraries have given researchers the 
ability to simultaneously run and understand algorithms created by other 
 
14 Id. 
15 Breck Dumas, Google Loses $96B in Value on Gemini Fallout as CEO Does Damage Control, YAHOO FINANCE (Feb. 28, 
2024), https://finance.yahoo.com/news/google-loses-96b-value-gemini-233110640.html. 
DOCKET NO. NTIA–2023–0009 
 
 7 OF 18 
 
programmers. Researchers and journals have supported the movement towards 
open science, which includes sharing research artifacts like the data and code 
required to reproduce results.16 
The RFC proceeds to seek input on how to define “open” and “widely available.”17 These, 
however, are the wrong questions. NTIA should instead proceed from the assumption that 
there are no harms inherent to either “open” or “closed” development models; it should be 
seeking input on anything that might give rise to discrete harms in either open or closed systems. 
NTIA can play a valuable role by recommending useful alterations to existing law where gaps 
currently exist, regardless of the business or distribution model employed by the AI developer. 
In short, there is nothing necessarily more or less harmful about adopting an “open” or a 
“closed” approach to software systems. The decision to pursue one path over the other will be 
made based on the relevant tradeoffs that particular firms face. Embedding such distinctions 
in regulation is arbitrary, at best, and counterproductive to the fruitful development of AI, at 
worst.  
A. ‘Open’ or ‘Widely Available’ Model Weights 
To the extent that NTIA is committed to drawing distinctions between “open” and “closed” 
approaches to developing foundation models, it should avoid overly prescriptive definitions of 
what constitutes ""open"" or ""widely available"" model weights that could significantly hamper the 
progress and utility of AI technologies. 
Imposing narrow definitions risks creating artificial boundaries that fail to accurately reflect 
AI’s technical and operational realities. They could also inadvertently exclude or marginalize 
innovative AI models that fall outside those rigid parameters, despite their potential to 
contribute positively to technological advancement and social well-being. For instance, a 
definition of ""open"" that requires complete public accessibility without any form of control or 
restriction might discourage organizations from sharing their models, fearing misuse or loss of 
intellectual property. 
Moreover, prescriptive definitions could stifle the organic growth and evolution of AI 
technologies. The AI field is characterized by its rapid pace of change, where today's cutting-
edge models may become tomorrow's basic tools. Prescribing fixed criteria for what constitutes 
""openness"" or ""widely available"" risks anchoring the regulatory landscape to this specific 
moment in time, leaving the regulatory framework less able to adapt to future developments 
and innovations. 
 
16 RFC at 14060. 
17 RFC at 14062, question 1. 
DOCKET NO. NTIA–2023–0009 
 
 8 OF 18 
 
Given AI developers’ vast array of applications, methodologies, and goals, it is imperative that 
any definitions of ""open"" or ""widely available"" model weights embrace flexibility. A flexible 
approach would acknowledge how the various stakeholders within the AI ecosystem have 
differing needs, resources, and objectives, from individual developers and academic researchers 
to startups and large enterprises. A one-size-fits-all definition of ""openness"" would fail to 
accommodate this diversity, potentially privileging certain forms of innovation over others and 
skewing the development of AI technologies in ways that may not align with broader social 
needs. 
Moreover, flexibility in defining ""open"" and ""widely available"" must allow for nuanced 
understandings of accessibility and control. There can, for example, be legitimate reasons to 
limit openness, such as protecting sensitive data, ensuring security, and respecting intellectual-
property rights, while still promoting a culture of collaboration and knowledge sharing. A 
flexible regulatory approach would seek a balanced ecosystem where the benefits of open AI 
models are maximized, and potential risks are managed effectively. 
B. The Benefits of ‘Open’ vs ‘Closed’ Business Models 
NTIA asks: 
What benefits do open model weights offer for competition and innovation, both 
in the AI marketplace and in other areas of the economy? In what ways can open 
dual-use foundation models enable or enhance scientific research, as well as 
education/training in computer science and related fields?18 
An open approach to AI development has obvious benefits, as NTIA has itself acknowledged 
in other contexts.19 Open-foundation AI models represent a transformative force, characterized 
by their accessibility, adaptability, and potential for widespread application across various 
sectors. The openness of these models may serve to foster an environment conducive to 
innovation, wherein developers, researchers, and entrepreneurs can build on existing 
technologies to create novel solutions tailored to diverse needs and challenges. 
The inherent flexibility of open-foundation models can also catalyze a competitive market, 
encouraging a healthy ecosystem where entities ranging from startups to established 
corporations may all participate on roughly equal footing. By lowering some entry barriers 
related to access to basic AI technologies, this competitive environment can further drive 
 
18 RFC at 14062, question 3(a). 
19 Department of Commerce, Competition in the Mobile Application Ecosystem (2023), 
https://www.ntia.gov/report/2023/competition-mobile-app-ecosystem (“While retaining appropriate latitude for 
legitimate privacy, security, and safety measures, Congress should enact laws and relevant agencies should consider 
measures (such as rulemaking) designed to open up distribution of lawful apps, by prohibiting… barriers to the direct 
downloading of applications.”). 
DOCKET NO. NTIA–2023–0009 
 
 9 OF 18 
 
technological advancements and price efficiencies, ultimately benefiting consumers and society 
at-large. 
But more “closed” approaches can also prove very valuable. As NTIA notes in this RFC, it is 
rarely the case that a firm pursues a purely open or closed approach. These terms exist along a 
continuum, and firms blend models as necessary.20 And just as firms readily mix elements of 
open and closed business models, a regulator should be agnostic about the precise mix that 
firms employ, which ultimately must align with the realities of market dynamics and consumer 
preferences. 
Both open and closed approaches offer distinct benefits and potential challenges. For instance, 
open approaches might excel in fostering a broad and diverse ecosystem of applications, 
thereby appealing to users and developers who value customization and variety. They can also 
facilitate a more rapid dissemination of innovation, as they typically impose fewer restrictions 
on the development and distribution of new applications. Conversely, closed approaches, with 
their curated ecosystems, often provide enhanced security, privacy, and a more streamlined 
user experience. This can be particularly attractive to users less inclined to navigate the 
complexities of open systems. Under the right conditions, closed systems can likewise foster a 
healthy ecosystem of complementary products. 
The experience of modern digital platforms demonstrates that there is no universally optimal 
approach to structuring business activities, thus illustrating the tradeoffs inherent in choosing 
among open and closed business models. The optimal choice depends on the specific needs 
and preferences of the relevant market participants. As Jonathan M. Barnett has noted:  
Open systems may yield no net social gain over closed systems, can pose a net social 
loss under certain circumstances, and . . . can impose a net social gain under yet 
other circumstances.21 
Similar considerations apply in the realm of AI development. Closed or semi-closed ecosystems 
can offer such advantages as enhanced security and curated offerings, which may appeal to 
certain users and developers. These benefits, however, may come at the cost of potentially 
limited innovation, as a firm must rely on its own internal processes for research and 
development. Open models, on the other hand, while fostering greater collaboration and 
creativity, may also introduce risks related to quality control, intellectual-property protection, 
and a host of other concerns that may be better controlled in a closed business model. Even 
along innovation dimensions, closed platforms can in many cases outperform open models.  
 
20 RFC at 14061 (“‘openness’ or ‘wide availability’ of model weights are also terms without clear definition or 
consensus. There are gradients of ‘openness,’ ranging from fully ‘closed’ to fully 'open’”). 
21 See Jonathan M. Barnett, The Host’s Dilemma: Strategic Forfeiture in Platform Markets for Informational Goods, 124 
HARV. L. REV. 1861, 1927 (2011). 
DOCKET NO. NTIA–2023–0009 
 
 10 OF 18 
 
With respect to digital platforms like the App Store and Google Play Store, there is a 
“fundamental welfare tradeoff between two-sided proprietary…platforms and two-sided 
platforms which allow ‘free entry’ on both sides of the market.”22 Consequently, “it is by no 
means obvious which type of platform will create higher product variety, consumer adoption 
and total social welfare.”23 
To take another example, consider the persistently low adoption rates for consumer versions 
of the open-source Linux operating system, versus more popular alternatives like Windows or 
MacOS.24 A closed model like Apple’s MacOS is able to outcompete open solutions by better 
leveraging network effects and developing a close relationship with end users.25 Even in this 
example, adoption of open versus closed models varies across user types, with, e.g., developers 
showing a strong preference for Linux over Mac, and only a slight preference for Windows 
over Linux.26 This underscores the point that the suitability of an open or closed model varies 
not only by firm and product, nor even solely by user, but by the unique fit of a particular 
model for a particular user in a particular context. Many of those Linux-using developers will 
likely not use it on their home computing device, for example, even if they prefer it for work.  
The dynamics among consumers and developers further complicate prevailing preferences for 
open or closed models. For some users, the security and quality assurance provided by closed 
ecosystems outweigh the benefits of open systems’ flexibility. On the developer side, the lower 
barriers to entry in more controlled ecosystems that smooth the transaction costs associated 
with developing and marketing applications can democratize application development, 
potentially leading to greater innovation within those ecosystems. Moreover, distinctions 
between open and closed models can play a critical role in shaping inter-brand competition. A 
regulator placing its thumb on the business-model scale would push the relevant markets 
toward less choice and lower overall welfare.27 
By differentiating themselves through a focus on ease-of-use, quality, security, and user 
experience, closed systems contribute to a vibrant competitive landscape where consumers have 
clear choices between differing “brands” of AI. Forcing an AI developer to adopt practices that 
 
22 Id. at 2. 
23 Id. at 3. 
24  Desktop Operating System Market Share Worldwide Feb 2023 - Feb 2024, STATCOUNTER, https://gs.statcounter.com/os-
market-share/desktop/worldwide (last visited Mar. 27, 2024).  
25  Andrei Hagiu, Proprietary vs. Open Two-Sided Platforms and Social Efficiency (HARV. BUS. SCH. STRATEGY UNIT, 
Working Paper No. 09-113, 2006). 
26 Joey Sneddon, More Developers Use Linux than Mac, Report Shows, OMG LINUX (Dec. 28, 2022), 
https://www.omglinux.com/devs-prefer-linux-to-mac-stackoverflow-survey. 
27 See Michael L. Katz & Carl Shapiro, Systems Competition and Network Effects, 8 J. ECON. PERSP. 93, 110 (1994), 
(“[T]he primary cost of standardization is loss of variety: consumers have fewer differentiated products to pick from, 
especially if standardization prevents the development of promising but unique and incompatible new systems”). 
DOCKET NO. NTIA–2023–0009 
 
 11 OF 18 
 
align with a regulator’s preconceptions about the relative value of “open” and “closed” risks 
homogenizing the market and diminishing the very competition that spurs innovation and 
consumer choice. 
Consider some of the practical benefits sought by deployers when choosing between open and 
closed models. For example, it's not straightforward to say close is inherently better than open 
when considering issues of data sharing or security; even here, there are tradeoffs. Open 
innovation in AI—characterized by the sharing of data, algorithms, and methodologies within 
the research community and beyond—can mitigate many of the risks associated with model 
development. This openness fosters a culture of transparency and accountability, where AI 
models and their applications are subject to scrutiny by a broad community of experts, 
practitioners, and the general public. This collective oversight can help to identify and address 
potential safety and security concerns early in the development process, thus enhancing AI 
technologies’ overall trustworthiness. 
By contrast, a closed system may implement and enforce standardized security protocols more 
quickly. A closed system may have a sharper, more centralized focus on providing data security 
to users, which may perform better along some dimensions. And while the availability of code 
may provide security in some contexts, in other circumstances, closed systems perform better.28 
In considering ethical AI development, different types of firms should be free to experiment 
with different approaches, even blending them where appropriate. For example, Claude’s 
approach to ""Collective Constitutional AI"" adopts what is arguably a “semi-open” model, 
blending proprietary elements with certain aspects of openness to foster innovation, while also 
maintaining a level of control.29 This model might strike an appropriate balance, in that it 
ensures some degree of proprietary innovation and competitive advantage while still benefiting 
from community feedback and collaboration. 
On the other hand, fully open-source development could lead to a different, potentially 
superior result that meets a broader set of needs through community-driven evolution and 
iteration. There is no way to determine, ex ante, that either an open or a closed approach to AI 
development will inherently provide superior results for developing “ethical” AI. Each has its 
place, and, most likely, the optimal solutions will involve elements of both approaches. 
In essence, codifying a regulatory preference for one business model over the other would 
oversimplify the intricate balance of tradeoffs inherent to platform ecosystems. Economic 
 
28 See. e.g., Nokia, Threat Intelligence Report 2020 (2020), https://www.nokia.com/networks/portfolio/cyber-
security/threat-intelligence-report-2020; Randal C. Picker, Security Competition and App Stores, NETWORK LAW REVIEW 
(Aug. 23, 2021), https://www.networklawreview.org/picker-app-stores.  
29 Collective Constitutional AI: Aligning a Language Model with Public Input, ANTHROPIC (Oct. 17, 2023), 
https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input. 
DOCKET NO. NTIA–2023–0009 
 
 12 OF 18 
 
theory and empirical evidence suggest that both open and closed platforms can drive 
innovation, serve consumer interests, and stimulate healthy competition, with all of these 
considerations depending heavily on context. Regulators should therefore aim for flexible 
policies that support coexistence of diverse business models, fostering an environment where 
innovation can thrive across the continuum of openness.  
C. Dual-Use Foundation Models and Transparency 
Requirements 
The EO and the RFC both focus extensively on so-called “dual-use” foundation models: 
Foundation models are typically defined as, “powerful models that can be fine-
tuned and used for multiple purposes.” Under the Executive Order, a “dual-use 
foundation model” is “an AI model that is trained on broad data; generally uses 
self-supervision, contains at least tens of billions of parameters; is applicable across 
a wide range of contexts; and that exhibits, or could be easily modified to exhibit, 
high levels of performance at tasks that pose a serious risk to security, national 
economic security, national public health or safety, or any combination of those 
matters….”30 
But this framing will likely do more harm than good. As noted above, the terms “AI” or “AI 
model” are frequently invoked to refer to very different types of systems. Further defining these 
models as “dual use” is also unhelpful, as virtually any tool in existence can be “dual use” in 
this sense. Certainly, from a certain perspective, all software—particularly highly automated 
software—can pose a serious risk to “national security” or “safety.” Encryption and other 
privacy-protecting tools certainly fit this definition.31 While it is crucial to mitigate harms 
associated with the misuse of AI technologies, the blanket treatment of all foundation models 
under this category is overly simplistic. 
The EO identifies certain clear risks, such as the possibility that models could aid in the 
creation of chemical, biological, or nuclear weaponry. These categories are obvious subjects for 
regulatory control, but the EO then appears to open a giant definitional loophole that 
threatens to subsume virtually any useful AI system. It employs expansive terminology to 
describe a more generalized threat—specifically, that dual-use models could ""[permit] the 
evasion of human control or oversight through means of deception or obfuscation.""32 Such 
language could encompass a wide array of general-purpose AI models. Furthermore, by labeling 
 
30 RFC at 14061. 
31 Encryption and the “Going Dark” Debate, CONGRESSIONAL RESEARCH SERVICE (2017), 
https://crsreports.congress.gov/product/pdf/R/R44481. 
32 EO at. § 3(k)(iii). 
DOCKET NO. NTIA–2023–0009 
 
 13 OF 18 
 
systems capable of bypassing human decision making as “dual use,” the order implicitly suggests 
that all AI could pose such risk as warrants national-security levels of scrutiny. 
Given the EO’s broad definition of AI as ""a machine-based system that can, for a given set of 
human-defined objectives, make predictions, recommendations, or decisions influencing real 
or virtual environments,"" numerous software systems not typically even considered AI might 
be categorized as ""dual-use"" models.33 Essentially, any sufficiently sophisticated statistical-
analysis tool could qualify under this definition.  
A significant repercussion of the EO’s very broad reporting mandates for dual-use systems, and 
one directly relevant to the RFC’s interest in promoting openness, is that these might chill 
open-source AI development.34 Firms dabbling in AI technologies—many of which might not 
consider their projects to be dual use—might keep their initiatives secret until they are 
significantly advanced. Faced with the financial burden of adhering to the EO’s reporting 
obligations, companies that lack a sufficiently robust revenue model to cover both development 
costs and legal compliance might be motivated to dodge regulatory scrutiny in the initial 
phases, consequently dampening the prospects for transparency. 
It is hard to imagine how open-source AI projects could survive in such an environment. Open-
source AI code libraries like TensorFlow35 and PyTorch36 foster remarkable innovation by 
allowing developers to create new applications that use cutting-edge models. How could a 
paradigmatic startup developer working out of a garage genuinely commit to open-source 
development if tools like these fall under the EO's jurisdiction? Restricting access to the weights 
that models use—let alone avoiding open-source development entirely—may hinder 
independent researchers' ability to advance the forefront of AI technology.  
Moreover, scientific endeavors typically benefit from the contributions of researchers 
worldwide, as collaborative efforts on a global scale are known to fast-track innovation. The 
pressure the EO applies to open-source development of AI tools could curtail international 
cooperation, thereby distancing American researchers from crucial insights and collaborations. 
For example, AI’s capacity to propel progress in numerous scientific areas is potentially vast—
e.g., utilizing MRI images and deep learning for brain-tumor diagnoses37 or employing machine 
 
33 EO at § 3(b). 
34 EO at § 4.2 (requiring companies developing dual-use foundation models to provide ongoing reports to the federal 
government on their activities, security measures, model weights, and red-team testing results).  
35 An End-to-End Platform for Machine Learning, TENSORFLOW, https://www.tensorflow.org (last visited Mar. 27, 2024).  
36 Learn the Basics, PYTORCH, https://pytorch.org/tutorials/beginner/basics/intro.html (last visited Mar. 27, 2024). 
37 Akmalbek Bobomirzaevich Abdusalomov, Mukhriddin Mukhiddinov, & Taeg Keun Whangbo, Brain Tumor 
Detection Based on Deep Learning Approaches and Magnetic Resonance Imaging, 15(16) CANCERS (BASEL) 4172 (2023), 
available at https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10453020. 
DOCKET NO. NTIA–2023–0009 
 
 14 OF 18 
 
learning to push the boundaries of materials science.38 Such research does not benefit from 
stringent secrecy, but thrives on collaborative development. Enabling a broader community to 
contribute to and expand upon AI advancements supports this process.  
Individuals respond to incentives. Just as how well-intentioned seatbelt laws paradoxically led 
to an uptick in risky driving behaviors,39 ill-considered obligations placed on open-source AI 
developers could unintentionally stifle the exchange of innovative concepts crucial to maintain 
the United States' leadership in AI innovation. 
IV. Regulatory Models that Support Innovation While Managing 
Risks Effectively 
In the rapidly evolving landscape of artificial intelligence (AI), it is paramount to establish 
governance and regulatory frameworks that both encourage innovation and ensure safety and 
ethical integrity. An effective regulatory model for AI should be adaptive, principles-based, and 
foster a collaborative environment among regulators, developers, researchers, and the broader 
community. A number of principles can help in developing this regime. 
A. Low-Risk vs High-Risk AI 
First, a clear distinction should be made between low-risk AI applications that enhance 
operational efficiency or consumer experience and high-risk applications that could have 
significant safety implications. Low-risk applications like search algorithms and chatbots should 
be governed by a set of baseline ethical guidelines and best practices that encourage innovation, 
while ensuring basic standards are met. On the other hand, high-risk applications—such as 
those used by law enforcement or the military—would require more stringent review processes, 
including impact assessments, ethical reviews, and ongoing monitoring to mitigate potentially 
adverse effects. 
Contrast this with the recently enacted AI Act in the European Union, and its decision to 
create presumptions of risk for general purpose AI (GPAI) systems, such as large language 
models (LLMs), that present what the EU has termed so-called “systemic risk.”40 Article 3(65) 
of the AI Act defines systemic risk as ""a risk that is specific to the high-impact capabilities of 
general-purpose AI models, having a significant impact on the Union market due to their 
 
38 Keith T. Butler, et al., Machine Learning for Molecular and Materials Science, 559 NATURE 547 (2018), available at 
https://www.nature.com/articles/s41586-018-0337-2. 
39 The Peltzman Effect, THE DECISION LAB, https://thedecisionlab.com/reference-guide/psychology/the-peltzman-effect 
(last visited Mar. 27, 2024). 
40 European Parliament, European Parliament legislative Resolution of 13 March 2024 on the Proposal for a 
Regulation of the European Parliament and of the Council on Laying Down Harmonised Rules on Artificial 
Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, COM/2021/206, available at 
https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.html [hereinafter “EU AI Act”]. 
DOCKET NO. NTIA–2023–0009 
 
 15 OF 18 
 
reach, or due to actual or reasonably foreseeable negative effects on public health, safety, public 
security, fundamental rights, or the society as a whole, that can be propagated at scale across 
the value chain.""41 
This definition bears similarities to the ""Hand formula"" in U.S. tort law, which balances the 
burden of precautions against the probability and severity of potential harm to determine 
negligence.42 The AI Act's notion of systemic risk, however, is applied more broadly to entire 
categories of AI systems based on their theoretical potential for widespread harm, rather than 
on a case-by-case basis. 
The designation of LLMs as posing ""systemic risk"" is problematic for several reasons. It creates 
a presumption of risk merely based on a GPAI system’s scale of operations, without any 
consideration of the actual likelihood or severity of harm in specific use cases. This could lead 
to unwarranted regulatory intervention and unintended consequences that hinder the 
development and deployment of beneficial AI technologies. And this broad definition of 
systemic risk gives regulators significant leeway to intervene in how firms develop and release 
their AI products, potentially blocking access to cutting-edge tools for European citizens, even 
in the absence of tangible harms. 
While it is important to address potential risks associated with AI systems, the AI Act's 
approach risks stifling innovation and hindering the development of beneficial AI technologies 
within the EU.  
B. Avoid Static Regulatory Approaches 
AI regulators are charged with overseeing a dynamic and rapidly developing market, and should 
therefore avoid erecting a rigid framework that force new innovations into ill-fitting categories. 
The “regulatory sandbox” may provide a better model to balance innovation with risk 
management. By allowing developers to test and refine AI technologies in a controlled 
environment under regulatory oversight, sandboxes can be used to help identify and address 
potential issues before wider deployment, all while facilitating dialogue between innovators 
and regulators. This approach not only accelerates the development of safe and ethical AI 
solutions, but also builds mutual understanding and trust. Where possible, NTIA should 
facilitate policy experimentation with regulatory sandboxes in the AI context. 
 
41 Id. at Art. 3(65). 
42 See Stephen G. Gilles, On Determining Negligence: Hand Formula Balancing, the Reasonable Person Standard, and the Jury, 
54 VANDERBILT L. REV. 813, 842-49 (2001). 
DOCKET NO. NTIA–2023–0009 
 
 16 OF 18 
 
Meta's Open Loop program is an example of this kind of experimentation.43 This program is 
a policy prototyping research project focused on evaluating the National Institute of Standards 
and Technology (NIST) AI Risk Management Framework (RMF) 1.0.44 The goal is to assess 
whether the framework is understandable, applicable, and effective in assisting companies to 
identify and manage risks associated with generative AI. It also provides companies an 
opportunity to familiarize themselves with the NIST AI RMF and its application in risk-
management processes for generative AI systems. Additionally, it aims to collect data on 
existing practices and offer feedback to NIST, potentially influencing future RMF updates.  
1. 
Regulation as a Discovery Process 
Another key principle is to ensure that regulatory mechanisms are adaptive. Some examples of 
adaptive mechanisms are iterative rulemaking and feedback loops that allow regulations to be 
updated continuously in response to new developments and insights. Such mechanisms enable 
policymakers to respond swiftly to technological breakthroughs, ensuring that regulations 
remain relevant and effective, without stifling innovation. 
Geoffrey Manne & Gus Hurwitz have recently proposed a framework for “regulation as a 
discovery process” that could be adapted to AI.45 They argue for a view of regulation not merely 
as a mechanism for enforcing rules, but as a process for discovering information that can 
inform and improve regulatory approaches over time. This perspective is particularly pertinent 
to AI, where the pace of innovation and the complexity of technologies often outstrip 
regulators' understanding and ability to predict future developments. This framework: 
in its simplest formulation, asks regulators to consider that they might be wrong. 
That they might be asking the wrong questions, collecting the wrong information, 
analyzing it the wrong way—or even that Congress has given them the wrong 
authority or misunderstood the problem that Congress has tasked them to 
address.46 
That is to say, an adaptive approach to regulation requires epistemic humility, with the 
understanding that, particularly for complex, dynamic industries: 
there is no amount of information collection or analysis that is guaranteed to be 
""enough."" As Coase said, the problem of social cost isn't calculating what those 
 
43 See Open Loop’s First Policy Prototyping Program in the United States, META, https://www.usprogram.openloop.org (last 
visited Mar. 27. 2024). 
44 Id. 
45 Justin (Gus) Hurwitz & Geoffrey A. Manne, Pigou’s Plumber: Regulation as a Discovery Process, SSRN (2024), available 
at https://laweconcenter.org/resources/pigous-plumber. 
46 Id. at 32. 
DOCKET NO. NTIA–2023–0009 
 
 17 OF 18 
 
costs are so that we can eliminate them, but ascertaining how much of those social 
costs society is willing to bear.47 
In this sense, modern regulators’ core challenge is to develop processes that allow for iterative 
development of knowledge, which is always in short supply. This requires a shift in how an 
agency conceptualizes its mission, from one of writing regulations to one of assisting lawmakers 
to assemble, filter, and focus on the most relevant and pressing information needed to 
understand a regulatory subject’s changing dynamics.48 
As Hurwitz & Manne note, existing efforts to position some agencies as information-gathering 
clearinghouses suffer from a number of shortcomings—most notably, that they tend to operate 
on an ad hoc basis, reporting to Congress in response to particular exigencies.49 The key to 
developing a “discovery process” for AI regulation would instead require setting up ongoing 
mechanisms to gather and report on data, as well as directing the process toward “specifications 
for how information should be used, or what the regulator anticipated to find in the 
information, prior to its collection.”50  
Embracing regulation as a discovery process means acknowledging the limits of our collective 
knowledge about AI's potential risks and benefits. This underscores why regulators should 
prioritize generating and utilizing new information through regulatory experiments, iterative 
rulemaking, and feedback loops. A more adaptive regulatory framework could respond to new 
developments and insights in AI technologies, thereby ensuring that regulations remain 
relevant and effective, without stifling innovation. 
Moreover, Hurwitz & Manne highlight the importance of considering regulation as an 
information-producing activity.51 In AI regulation, this could involve setting up mechanisms 
that allow regulators, innovators, and the public to contribute to and benefit from a shared 
pool of knowledge about AI's impacts. This could include public databases of AI incidents, 
standardized reporting of AI-system performance, or platforms for sharing best practices in AI 
safety and ethics. 
Static regulatory approaches may fail to capture the evolving landscape of AI applications and 
their societal implications. Instead, a dynamic, information-centric regulatory strategy that 
embraces the market as a discovery process could better facilitate beneficial innovations, while 
identifying and mitigating harms. 
 
47 Id. at 33. 
48 See id. at 28-29 
49 Id. at 37. 
50 Id. at 37-38. 
51 Id. 
DOCKET NO. NTIA–2023–0009 
 
 18 OF 18 
 
V. Conclusion 
As the NTIA navigates the complex landscape of AI regulation, it is imperative to adopt a 
nuanced, forward-looking approach that balances the need to foster innovation with the 
imperatives of ensuring public safety and ethical integrity. The rapid evolution of AI 
technologies necessitates a regulatory framework that is both adaptive and principles-based, 
eschewing static snapshots of the current state of the art in favor of flexible mechanisms that 
could accommodate the dynamic nature of this field. 
Central to this approach is to recognize that the field of AI encompasses a diverse array of 
technologies, methodologies, and applications, each with its distinct characteristics, 
capabilities, and implications for society. A one-size-fits-all regulatory model would not only be 
ill-suited to the task at-hand, but would also risk stifling innovation and hindering the United 
States' ability to maintain its leadership in the global AI industry. NTIA should focus instead 
on developing tailored approaches that distinguish between low-risk and high-risk applications, 
ensuring that regulatory interventions are commensurate with the potential identifiable harms 
and benefits associated with specific AI use cases. 
Moreover, the NTIA must resist the temptation to rely on overly prescriptive definitions of 
""openness"" or to favor particular business models over others. The coexistence of open and 
closed approaches to AI development is essential to foster a vibrant, competitive ecosystem that 
drives technological progress and maximizes social benefits. By embracing a flexible regulatory 
framework that allows for experimentation and iteration, the NTIA can create an environment 
conducive to innovation while still ensuring that appropriate safeguards are in place to mitigate 
potential risks. 
Ultimately, the success of the U.S. AI industry will depend on the ability of regulators, 
developers, researchers, and the broader community to collaborate in developing governance 
frameworks that are both effective and adaptable. By recognizing the importance of open 
development and diverse business models, the NTIA can play a crucial role in shaping the 
future of AI in ways that promote innovation, protect public interests, and solidify the United 
States' position as a global leader in this transformative field. 
",ICLE,10510
NTIA-2023-0009-0206," 
 
COMMENTS OF THE ELECTRONIC PRIVACY INFORMATION CENTER 
to the 
NATIONAL TELECOMMUNICATIONS AND INFORMATION ADMINISTRATION 
On its 
Request for Comment: Dual Use Foundation Artificial Intelligence Models with Widely 
Available Model Weights 
 
89 Fed. Reg. 14,059 
March 27, 2024 
 
The Electronic Privacy Information Center (EPIC) submits these comments in response to 
the National Telecommunications and Information Administration (NTIA)’s Request for 
Comment on the potential risks, benefits, and other implications of dual-use foundation models 
with widely available model weights.1  
EPIC is a public interest research center in Washington, D.C., established in 1994 to secure 
the fundamental right to privacy in the digital age for all people through advocacy, research, and 
litigation.2 We advocate for a human-rights-based approach to artificial intelligence (AI) policy 
that ensures new technologies are subject to democratic governance.3 Over the last decade, EPIC 
has consistently advocated for the adoption of clear, commonsense, and actionable AI regulations 
across the country.4 EPIC has litigated cases against the U.S. Department of Justice to compel 
 
1 89 Fed. Reg. 14059 (Mar. 27, 2024). 
2 About Us, EPIC, https://epic.org/about/ (2023). 
3 See, e.g., AI and Human Rights, EPIC, https://epic.org/issues/ai/ (2023); AI and Human Rights: Criminal 
Legal System, EPIC, https://epic.org/issues/ai/ai-in-the-criminal-justice-system/ (2023); EPIC, Outsourced & 
Automated: How AI Companies Have Taken Over Government Decision-Making (2023), 
https://epic.org/outsourced-automated/ [hereinafter “Outsourced & Automated Report”]; Letter from EPIC to 
President Biden and Vice President Harris on Ensuring Adequate Federal Workforce and Resources for 
Effective AI Oversight (Oct. 24, 2023), https://epic.org/wp-content/uploads/2023/10/EPIC-letter-to-White-
House-re-AI-workforce-and-resources-Oct-2023.pdf; EPIC, Comments on the NIST Artificial Intelligence 
Risk Management Framework: Second Draft (Sept. 28, 2022), https://epic.org/wp-
content/uploads/2022/09/EPIC-Comments-NIST-RMF-09-28-22.pdf. 
4 See, e.g., Press Release, EPIC, EPIC Urges DC Council to Pass Algorithmic Discrimination Bill (Sept. 23, 
2022), https://epic.org/epic-urges-dc-council-to-pass-algorithmic-discrimination-bill/; EPIC, Comments to the 
 
 
Comments of EPIC 
2 
Dual Use Foundation Models 
NTIA 
 
March 27, 2024 
production of documents regarding “evidence-based risk assessment tools,”5 against the U.S. 
Department of Homeland Security to produce documents about a program purported to assess the 
probability that an individual will commit a crime,6 and against the National Security Commission 
on Artificial Intelligence (NSCAI) to enforce its transparency obligations under the Freedom of 
Information Act and the Federal Advisory Committee Act.7 EPIC has also published extensive 
research on the risks and documented harms of emerging AI technologies like generative AI,8 as 
well as the ways that government agencies develop, procure, and use AI systems around the 
country.9 
As the NTIA evaluates the relative risks, benefits, and policy approaches to different 
foundation model weighting regimes, EPIC wishes to highlight key privacy and bias issues that 
will remain regardless of whether a model’s weights are non-public or widely available, including 
vulnerabilities to adversarial attacks seeking sensitive information within training datasets and 
efforts to undermine weight-based de-biasing techniques. Both closed and open AI model 
paradigms carry benefits and risks; selecting one path forward will not be enough by itself to solve 
the privacy, accuracy, and bias issues at the core of AI technologies. Instead, the decision to make 
model weights widely available or keep them non-public is a decision of which benefits and risks 
to prioritize. Keeping AI systems closed may hinder more adversarial attacks and enable easier 
enforcement compared to open systems, while making model weights widely available may foster 
more independent evaluation of AI systems and greater competition compared to closed systems. 
Rather than recommend one model weight paradigm over the other, EPIC urges the NTIA 
to grapple with the nuanced advantages, disadvantages, and regulatory hurdles that emerge within 
AI models along the entire gradient of openness—and how benefits, risks, and effective oversight 
 
Patent and Trademark Office on Intellectual Property Protection for Artificial Intelligence Innovation (Jan. 10, 
2020), https://epic.org/wp-content/uploads/apa/comments/EPIC-USPTO-Jan2020.pdf; EPIC, Comments on the 
Department of Housing and Urban Development’s Implementation of the Fair Housing Act’s Disparate Impact 
Standard (Oct. 18, 2019), https://epic.org/wp-content/uploads/apa/comments/EPIC-HUD-Oct2019.pdf.  
5 EPIC v. DOJ, 320 F. Supp. 3d 110 (D.D.C. 2018), voluntarily dismissed, 2020 WL 1919646 (D.C. Cir. 
2020), https://epic.org/foia/doj/criminal-justice-algorithms/. 
6 See EPIC v. DHS – FAST Program, EPIC, https://epic.org/documents/epic-v-dhs-fast-program/ (last visited 
Dec. 5, 2023). 
7 EPIC v. NSCAI, 419 F. Supp. 3d 82, 86, 95 (D.D.C. 2019), https://epic.org/documents/epic-v-ai-
commission/. 
8 EPIC, Generating Harms: Generative AI’s Impact & Paths Forward (2023), https://epic.org/gai [hereinafter 
“EPIC Generative AI Report”].  
9 Outsourced & Automated Report; EPIC, Screened & Scored in the District of Columbia (2022), 
https://epic.org/wp-content/uploads/2022/11/EPIC-Screened-in-DC-Report.pdf [hereinafter “Screened & 
Scored Report”]. 
 
Comments of EPIC 
3 
Dual Use Foundation Models 
NTIA 
 
March 27, 2024 
mechanisms shift as models move along the gradient.10 To further inform the NTIA’s report, EPIC 
has also appended our own generative AI report, Generating Harms: Generative AI’s Impact & 
Paths Forward, which taxonomizes a range of consumer and societal harms caused by generative 
AI applications and the foundation models at their core. 
 
I. 
Key Privacy Risks Remain When Model Weights are Widely Available 
Responsive to Questions 2, 4, and 5. 
Data privacy and security is at the center of AI and machine-learning. For foundation 
models to operate, they must first be trained and finetuned on data, often by splitting a dataset into 
training and test sets.11 While many AI datasets involve non-human data, today’s most popular AI 
applications are built using data collected indiscriminately via web scraping12 and purchased from 
data brokers. 13  In fact, several popular AI applications have trained on user content after 
deployment as well, meaning that sensitive or personal information provided in user emails, 
prompts, and other content are incorporated into AI systems after initial model development.14 
Because AI training data can incorporate extensive sensitive or personally identifiable information 
about people, the ways AI developers collect, use, and secure their training data directly impacts 
the privacy rights of countless people. 
Unlike traditional software systems, AI models built using machine-learning methods 
cannot easily correct or delete personal data used to train the system. Once a model is trained on 
 
10 See Irene Solaiman, The Gradient of Generative AI Release: Methods and Considerations, arXiv (Feb. 5, 
2023), https://arxiv.org/pdf/2302.04844.pdf; Rishi Bommasani et al., Stan. Inst. Human-Centered A.I., 
Considerations for Governing Open Foundation Models (2023), 
https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf.  
11 See, e.g., Hyojin Bahng et al., Learning De-Biased Representations with Biased Representations, 37 Proc. 
Int. Conf. on Mach. Learning 1 (2020), https://proceedings.mlr.press/v119/bahng20a/bahng20a.pdf. 
12 See Müge Fazlioglu, Training AI on Personal Data Scraped from the Web, IAPP (Nov. 8, 2023), 
https://iapp.org/news/a/training-ai-on-personal-data-scraped-from-the-web/; Thomas Claburn, How to Spot 
OpenAI’s Crawler Bot and Stop it Slurping Sites for Training Data, Register (Aug. 8, 2023), 
https://www.theregister.com/2023/08/08/openai_scraping_software/; Sara Morrison, The Tricky Truth About 
How Generative AI Uses Your Data, Vox (July 27, 2023). 
13 See, e.g., Evan Weinberger, Data Brokers Eyed by CFPB for Selling Sensitive Info for Ads, AI, Bloomberg 
Law (Aug. 15, 2023), https://news.bloomberglaw.com/banking-law/data-brokers-eyed-by-cfpb-for-selling-
sensitive-info-for-ads-ai.  
14 See Geoffrey A. Fowler, Your Instagrams are Training AI. There’s Little You Can Do About It., Wash. Post 
(Sept. 27, 2023), https://www.washingtonpost.com/technology/2023/09/08/gmail-instagram-facebook-trains-
ai/; Kyle Wiggers, Addressing Criticism, OpenAI Will No Longer Use Customer Data to Train its Models by 
Default, TechCrunch (Mar. 1, 2023), https://techcrunch.com/2023/03/01/addressing-criticism-openai-will-no-
longer-use-customer-data-to-train-its-models-by-default/. 
 
Comments of EPIC 
4 
Dual Use Foundation Models 
NTIA 
 
March 27, 2024 
data, it memorizes that data and cannot easily unlearn it.15 Every model output will reflect the 
training data, and some models, like large-language models, may even leak personal data directly 
to users.16 Because of the ways that AI models incorporate training data, adversarial machine-
learning techniques have been developed to effectively identify and extract sensitive information 
in training datasets through an AI model’s behavior.17 Two such techniques are membership 
inference attacks, which aim to determine whether a certain data sample was included in a model’s 
training data by evaluating model outputs,18 and attribute inference attacks, which aims to impute 
sensitive training data attributes using partial knowledge of nonsensitive training data attributes 
and model outputs. 19  Crucially, many adversarial machine-learning techniques have proven 
effective at identifying sensitive or personally identifiable information in training datasets in both 
closed and open models.20 
To fortify data privacy in AI models against vulnerabilities and adversarial attacks, some 
AI researchers have developed differential privacy techniques for AI models. 21 Differential 
privacy is one of several burgeoning data privacy-preserving mathematical techniques, wherein 
random noise is injected into different elements of a technical system to prevent the identification 
of any one individual’s data without significantly impacting the accuracy of the system’s outputs.22 
As applied to machine-learning models, differentially private noise can be applied to at least four 
elements: (1) a model’s training data; (2) a model’s loss function, which evaluates how well a 
trained model predicts an expected outcome; (3) a model’s gradients, which are commonly used 
 
15 See Liwei Song & Prateek Mittal, Systematic Evaluation of Privacy Risks of Machine Learning Models, 30 
Proc. USENIX Sec. Symp. 2615, 2615 (2021). Hurdles to unlearning data are at the core of recent FTC cases 
requiring AI model deletion. See Jevan Hutson & Ben Winters, America’s Next ‘Stop Model!’: Model 
Deletion, 8 Geo. L. Tech. Rev. 125, 128–134 (2022), 
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4225003. 
16 Tiernan Ray, ChatGPT can Leak Training Data, Violate Privacy, Says Google’s DeepMind, ZDNet (Dec. 4, 
2023), https://www.zdnet.com/article/chatgpt-can-leak-source-data-violate-privacy-says-googles-deepmind/. 
17 See Song & Mittal, supra note 15, at 2629; Michale Backes et al.,  
18 See, e.g., Reza Shokri et al., Membership Inference Attacks Against Machine Learning Models, 2017 IEEE 
Sump. On Sec. & Priv. 3, https://ieeexplore.ieee.org/document/7958568. 
19 See, e.g., Bargav Jayaraman & David Evans, Are Attribute Inference Attacks Just Imputation?, arXiv (Sept. 
2, 2022), https://arxiv.org/pdf/2209.01292.pdf. 
20 See Song & Mittal, supra note 15, at 2615 (overview of research studies into membership inference attacks); 
Reza Shokri et al., supra note 18, at 3–18 (closed AI research); see generally Milad Nasr et al., Comprehensive 
Privacy Analysis of Deep Learning: Passive and Active White-Box Inference Attacks Against Centralized and 
Federating Learning, 2019 IEEE Symp. On Sec. & Priv. (open AI research). 
21 See Tianqing Zhu et al., More than Privacy: Applying Differential Privacy in Key Areas of Artificial 
Intelligence, 344 IEEE Transactions on Knowledge & Data Eng’g 2824, 2830–36 (June 2022), 
https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158374. 
22 Ctr. for Info. Pol’y Leadership, Hunton Andrews Kurth LLP, Privacy-Enhancing and Privacy-Preserving 
Technologies: Understanding the Role of PETs and PPTs in the Digital Age 36–41 (2023), 
https://www.informationpolicycentre.com/uploads/5/7/1/0/57104281/cipl-understanding-pets-and-ppts-
dec2023.pdf. 
 
Comments of EPIC 
5 
Dual Use Foundation Models 
NTIA 
 
March 27, 2024 
to optimize model training by adjusting model weights to minimize errors; and (4) a model’s 
weights.23 However, adding noise to any single element of an AI model will not be effective at 
preserving privacy: adding noise to training data may combat attribute inference attacks but is less 
effective against membership inference attacks, the inverse is true for adding noise to the loss 
function or gradients, and adding noise to model weights themselves may resist both membership 
and attribute inference attacks at the cost of significantly reducing model accuracy.24 Crucially, 
making model weights widely available can facilitate the development and use of both stronger 
privacy-preserving techniques and adversarial machine-learning techniques. Data privacy and 
adversarial attacks will remain a pressing issue for AI development regardless of whether AI 
systems favor closed versus open AI models, but the specific privacy advantages and 
disadvantages of different AI models will shift as developers move along the gradient of AI 
openness.25 
 
II. 
Key AI Bias Risks Remain When Model Weights are Widely Available 
Responsive to Questions 2, 4, and 5. 
Because training data often reflects human biases about the world,26 AI bias remains a core 
challenge for AI development and use—one that has already produced tangible harms across 
myriad industries and use contexts.27  To reduce the prevalence and impact of bias, AI developers 
can employ a variety of debiasing techniques, including but not limited to (1) augmenting training 
data to increase representation of underprivileged groups in datasets or avoid bias-prone 
 
23 Zhu et al., supra note 21, at 2830. 
24 Id. 
25 See Solaiman, supra note 10. 
26 See IBM Data and AI Team, Shedding Light on AI Bias with Real World Examples, IBM (Oct. 16, 2023), 
https://www.ibm.com/blog/shedding-light-on-ai-bias-with-real-world-examples/; Joy Buolamwini & Timnit 
Gebru, Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification, 81 Proc. 
Mach. Learning Rsch. 1–15 (2018), https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf. 
27 See, e.g., Leon Yin et al., OpenAI’s GPT is a Recruiter’s Dream Tool. Tests Show There’s Racial Bias, 
Bloomberg (Mar. 7, 2024); Jesutofunmi A. Omiye et al., Large Language Models Propagate Race-Based 
Medicine, 6 npj Digit. Med. 1–3 (2023), https://www.nature.com/articles/s41746-023-00939-z; Drew Harwell, 
Federal Study Confirms Racial Bias of Many Facial-Recognition Systems, Casts Doubt on Their Expanding 
Use, Wash. Post (Dec. 19, 2019), https://www.washingtonpost.com/technology/2019/12/19/federal-study-
confirms-racial-bias-many-facial-recognition-systems-casts-doubt-their-expanding-use/. 
 
Comments of EPIC 
6 
Dual Use Foundation Models 
NTIA 
 
March 27, 2024 
attributes,28 (2) training a model to avoid a predefined bias,29 and (3) adjusting model weights to 
foster fairer model outputs, even where developers cannot effectively reduce bias in training 
datasets.30 
Because debiasing techniques focus on adjusting or counteracting bias within training 
datasets, most debiasing techniques rely on developers’ ability to accurately identify sources of 
bias while training an AI model. Consequently, these techniques tend to fall short for multimodal 
foundation models, where anticipated and tested use contexts may differ from actual use contexts; 
without anticipating a particular use case, AI developers cannot effectively test for and reduce bias 
for that use case. Additionally, any undetected biases within a model’s training data may continue 
to bias model outputs during deployment—even after developers employ debiasing techniques. 
Adjusting model weights to favor underrepresented groups can be an effective way to 
produce fairer model outputs, even when a model’s training data is difficult to debias directly. For 
example, in circumstances where developers train an AI model on an extensive dataset spanning 
thousands of nuanced variables, they may struggle to accurately identify all relevant biases within 
their training data. By adjusting model weights while testing the accuracy of a model within 
different use contexts, developers may be able to reduce bias within a model even when bias 
remains within a model’s training data. For this purpose, making model weights publicly available 
may assist in the debiasing process (e.g., independent parties may be able to identify model bias 
and effective model weight adjustments where AI developers cannot, such as in novel use contexts). 
However, with new advantages come new disadvantages. Making model weights publicly 
available may not only permit independent parties to evaluate foundation models and identify 
better ways to mitigate AI bias, but also permit malicious third-party actors to identify model 
weight debiasing techniques and counteract them. To maintain model accuracy and fairness, then, 
AI developers and regulators will need to pair technical solutions with ongoing model evaluation 
and red-teaming efforts. For a more robust account of EPIC’s recommendations for post-
 
28 See Hyojin Bahng et al., Learning De-biased Representations with Biased Representations, 37 Proc. Int’l 
Conf. on Mach. Learning 1 (2020), https://proceedings.mlr.press/v119/bahng20a/bahng20a.pdf; Robert 
Geirhos et al., ImageNet-Trained CNNs are Biased Towards Texture; Increasing Shape Bias Improves 
Accuracy and Robustness, 2019 Proc. Int’l Conf. on Learning Representations 7, 17, 
https://openreview.net/pdf?id=Bygh9j09KX. 
29 See Bahng et al., supra note 28; Haohan Wang et al., Learning Robust Representations by Projecting 
Superficial Statistics Out, 2019 Proc. Int’l Conf. on learning Representations 8–9, 
https://openreview.net/pdf?id=rJEjjoR9K7. 
30 See, e.g., Faisal Kamiran & Toon Calders, Data Preprocessing Techniques for Classification Without 
Discrimination, 33 Knowledge Info. Sys. 1, 2, 16–18 (2012), https://link.springer.com/article/10.1007/s10115-
011-0463-8. 
 
Comments of EPIC 
7 
Dual Use Foundation Models 
NTIA 
 
March 27, 2024 
deployment AI testing and evaluation, see EPIC’s recent comment to the Office of Management 
and Budget (OMB) regarding their draft AI memorandum.31 
 
III. 
The Impact of Model Openness Relies on the Effectiveness of Different Oversight Tools 
Responsive to Questions 7 and 9. 
The benefits and risks of any AI model will shift as it becomes more open, but model 
openness is not the sole variable impacting the NTIA’s analysis. We must also evaluate the 
capacity of developers, regulators, and other independent parties to (1) fully realize an AI model’s 
relative benefits and (2) fully mitigate a model’s risks. As AI models move from closed to open, 
the relative capacity of different actors to use, misuse, and regulate AI will shift as well. For 
example, in a fully closed ecosystem, third-party actors are relatively limited in what role they play 
in overseeing responsible AI development and use; they may only access consumer-facing model 
outputs, voluntary testing outputs, and other resources either offered by AI developers or required 
under state, federal, or international laws. However, with these oversight limitations comes a 
simpler oversight and liability regime: if an AI model is fully proprietary, regulators can place 
most obligations around responsible AI development, use, and security on AI developers. 
By contrast, in a fully open ecosystem, the barriers to independent AI evaluation, 
adjustment, and development lower substantially, allowing more robust independent audits and 
evaluation, more streamlined government enforcement, and more malicious AI use and 
retooling. 32 While AI regulators may more easily oversee AI developers under a more open 
ecosystem, harmful, anonymous AI use may be harder to police. And regardless of model openness, 
the relative resources and technical expertise available to different parties—small AI developers, 
state and federal regulators, academic researchers, civil society advocates, etc.—will influence 
how impactful proposed benefits and risks of open versus closed AI are in practice.33 
Ultimately, where the United States lands in the open versus closed AI debate must depend, 
at least in part, on whether independently assessing AI models or assigning liability for AI harms 
is a greater regulatory hurdle. Regardless of any technical issues around model openness, privacy-
 
31 EPIC, Comments on the OMB’s Draft Memorandum on Advancing Governance, Innovation, and Risk 
Management for Agency Use of Artificial Intelligence (Dec. 5, 2023), https://epic.org/wp-
content/uploads/2023/12/EPIC-OMB-AI-Guidance-Comments-120523-1.pdf. 
32 See, e.g., Andrew Burt, The AI Transparency Paradox, Harv. Bus. Rev. (Dec. 13, 2019), 
https://hbr.org/2019/12/the-ai-transparency-paradox.  
33 See, e.g., Natalie Alms, The People Problem Behind the Government’s AI Ambitions, NextGov (Nov. 21, 
2023), https://www.nextgov.com/artificial-intelligence/2023/11/people-problem-behind-governments-ai-
ambitions/392212/; FTC Bureau of Competition & Office of Technology, Generative AI Raises Competition 
Concerns, FTC Tech. Blog (June 29, 2023), https://www.ftc.gov/policy/advocacy-research/tech-at-
ftc/2023/06/generative-ai-raises-competition-concerns. 
 
Comments of EPIC 
8 
Dual Use Foundation Models 
NTIA 
 
March 27, 2024 
preserving techniques, and model debiasing, the threat of harm from negligent or malicious AI use 
will remain. Understanding how model openness interacts with AI regulation and oversight will 
be essential for fostering more responsible, more equitable, and more beneficial AI. 
 
Respectfully submitted, 
 
 
 
/s/ Grant Fergusson 
 
 
 
 
 
 
Grant Fergusson 
 
 
 
 
 
 
Equal Justice Works Fellow 
 
/s/ Calli Schroeder 
 
 
 
 
 
 
Calli Schroeder 
 
 
 
 
 
 
EPIC Senior Counsel &  
Global Privacy Counsel 
 
/s/ Maria Villégas Bravo 
Maria Villégas Bravo 
 
 
 
 
 
 
EPIC Law Fellow 
ELECTRONIC PRIVACY   
INFORMATION CENTER (EPIC)  
1519 New Hampshire Ave. NW   
Washington, DC 20036  
202-483-1140 (tel)  
202-483-1248 (fax)  
 
MAY 2023
Generative AI’s Impact & 
Paths Forward
 
CONTRIBUTIONS BY 
Grant Fergusson 
Caitriona Fitzgerald 
Chris Frascella 
Megan Iorio 
Tom McBrien 
Calli Schroeder 
Ben Winters 
Enid Zhou 
 
EDITED BY  
Grant Fergusson, Calli Schroeder, Ben Winters, and Enid Zhou 
Thank you to Sarah Myers West and Katharina Kopp for your generous 
comments on an earlier draft of the paper. 
Notes on this Paper: 
This is version 1 of this paper and is reflective of documented and 
anticipated harms of Generative AI as of May 15, 2023. Due to the fast-
changing pace of development, use, and harms of Generative AI, we want to 
acknowledge that this is an inherently dynamic paper, subject to changes in 
the future. 
Throughout this paper, we use a standard format to explain the typology of 
harms that generative AI can produce. Each section first explains relevant 
background information and potential risks imposed by generative AI, then 
highlights specifics harms and interventions that scholars and regulators 
have pursued to remedy each harm. This paper draws on two taxonomies of 
A.I. harms to guide our analysis: 
1. Danielle Citron’s and Daniel Solove’s Typology of Privacy Harms, 
comprising physical, economic, reputational, psychological, autonomy, 
discrimination, and relationship harms;1 and 
2. Joy Buolamwini’s Taxonomy of Algorithmic Harms, comprising loss of 
opportunity, economic loss, and social stigmatization, including loss of 
liberty, increased surveillance, stereotype reinforcement, and other 
dignitary harms.2 
These taxonomies do not necessarily cover all potential AI harms, and our 
use of these taxonomies is meant to help readers visualize and 
contextualize AI harms without limiting the types and variety of AI harms that 
readers consider.
 
 
 
Table of Contents 
Introduction 
......................................................................................................................... 
i 
Turbocharging Information Manipulation .................................................................. 1 
Harassment, Impersonation, and Extortion .............................................................. 9 
Spotlight: Section 230 .................................................................................................. 
19 
Profits Over Privacy: Increased Opaque Data Collection 
................................... 
24 
Increasing Data Security Risk .................................................................................... 
30 
Confronting Creativity: Impact on Intellectual Property Rights ......................... 
33 
Exacerbating Effects of Climate Change ................................................................ 
40 
Labor Manipulation, Theft, and Displacement 
....................................................... 
44 
Spotlight: Discrimination 
.............................................................................................. 
53 
The Potential Application of Products Liability Law 
............................................. 
54 
Exacerbating Market Power and Concentration 
................................................... 
57 
Recommendations ....................................................................................................... 60 
Appendix of Harms ....................................................................................................... 
64 
References 
..................................................................................................................... 68 
 
 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
i 
 
 
Introduction 
OpenAI’s decision to release ChatGPT, a chatbot built on the Large 
Language Model GPT-3, last November thrust AI tools to the forefront of 
public consciousness. In the last six months, new AI tools used to generate 
text, images, video, and audio based on user prompts exploded in 
popularity. Suddenly, phrases like Stable Diffusion, Hallucinations, and Value 
Alignment were everywhere. Each day, new stories about the different 
capabilities of generative AI—and their potential for harm—emerged without 
any clear indication of what would come next or what impacts these tools 
would have. 
While generative AI may be new, its harms are not. AI scholars have been 
warning us of the problems that large AI models can cause for years.3 These 
old problems are exacerbated by the industry’s shift in goals from research 
and transparency to profit, opacity, and concentration of power. The 
widespread availability and hype of these tools has led to increased harm 
both individually and on a massive scale. AI replicates racial, gender, and 
disability discrimination, and these harms are weaved inextricably through 
every issue highlighted in this report. 
OpenAI and other companies’ decisions to rapidly integrate generative AI 
technology into consumer-facing products and services have undermined 
longstanding efforts to make AI development transparent and accountable, 
leaving many regulators scrambling to prepare for the repercussions. And it 
is clear that generative AI systems can significantly amplify risks to both 
individual privacy and to democracy and cybersecurity generally. In the 
words of the OpenAI CEO, who indeed had the power not to accelerate the 
release of this technology, “I’m especially concerned that these models 
could be used for widespread misinformation…[and] offensive cyberattacks.” 
Introduction 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
ii 
 
 
This rapid deployment of generative AI systems without adequate 
safeguards is clear evidence that self-regulation has failed. Hundreds of 
entities, from corporations to media and government entities, are 
developing and looking to rapidly integrate these untested AI tools into a 
wide range of systems. And this rapid rollout will have disastrous results 
without necessary fairness, accountability, and transparency protections 
built in from the beginning. 
We are at a critical juncture as policymakers and industry around the globe 
are focusing on the substantial risks and opportunities posed by AI. There is 
an opportunity to make this technology work for people. Companies should 
be required to show their work, make it clear when AI is in use, and offer 
informed consent throughout the training, development, and use process. 
One thread of public concern focuses on AI’s “existential” risks—speculative 
long-term risks in which robots replace humans at work, socially, and 
ultimately taking over, a la “I, Robot.” Some legislators on the state and 
federal level have begun to take the issue of addressing AI more seriously—
however, it remains to be seen if their focus will be only on supporting 
companies with their development of AI tools and requiring marginal 
disclosure and transparency requirements. Enacting clear prohibitions on 
high-risk uses, addressing the easy spread of disinformation, requiring 
meaningful and proactive disclosures that facilitate informed consent, and 
bolstering consumer protection agencies are necessary to address the 
harms and risks specific to generative AI. This paper strives to provide a 
broad outline of different issues that the use of generative AI brings up, 
educate lawmakers and the public, and offer some paths forward to mitigate 
harm. 
- Ben Winters, Senior Counsel
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
1 
 
 
Turbocharging 
Information 
Manipulation 
BACKGROUND AND RISKS  
The widespread availability of free and low-cost generative AI tools 
facilitates the spread of high volumes of text, image, voice, and video 
content. Much of the content created by AI systems is likely benign or could 
be beneficial to specific audiences, but these systems will also facilitate the 
spread of extremely harmful content. For example, generative AI tools can 
and will be used to propagate content that is false, misleading, biased, 
inflammatory, or dangerous. As generative AI tools grow more sophisticated, 
it will be quicker, cheaper, and easier to produce this content—and existing 
harmful content can serve as the foundation to produce more. In this 
section, we consider five categories of harmful content that AI tools would 
turbocharge: Scams, Disinformation, Misinformation, Cybersecurity Threats, 
and Clickbait and Surveillance Advertising. Though we draw distinctions 
between disinformation (purposeful spread of false information) and 
misinformation (less purposeful spread or creation of false information), the 
spread of AI-generated content will blur this line for parties that use AI-
generated content without first editing or factchecking it. Entities using AI-
generated outputs without exercising due diligence should be held jointly 
responsible with the entity behind the generation of that output for the harm 
it causes.  
Turbocharging Information Manipulation 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
2 
 
 
SCAMS 
Scam phone calls, texts, and emails have long been out of control, harming 
the public in many ways. In 2021 alone, 2.8 million consumers filed fraud 
reports with the FTC, claiming more than $2.3 billion in losses, and nearly 1.4 
million consumers filed identity theft reports.4 Generative AI can accelerate 
the creation, personalization, and believability of these various scams using 
AI-generated text, voices, and videos. AI voice generation can also be used 
to mimic the voice of a loved one, calling to request immediately financial 
assistance for bail, legal help, or ransom.5 
According to a 2022 report from EPIC and the National Consumer Law 
Center, there are over one billion scam robocalls made to American 
telephones each month, which led to nearly $30 billion in consumer losses 
between June 2020-21—most frequently targeting vulnerable communities 
like seniors, individuals with disabilities, and people in debt.6 These scams 
are made at scale, and often use an automated voice speaking a script 
CASE STUDY – ELECTION 2024  
Products using GPT-4 and subsequent large language models can 
create quick and unique human-sounding “scripts” that can be 
distributed via text, email, print, or through an AI voice generator 
combined with AI video generators. These AI-generated scripts can 
be used to dissuade or scare voters—or spread misinformation about 
voting or elections. In 2022, for example, text messages were sent to 
voters in at least five states with purposefully wrong voting 
information. This type of election misinformation has become 
common in recent years, but generative AI tools will supercharge bad 
actors’ ability to quickly spread believable election misinformation. 
Congress must enact legislation that protects against deliberate 
voter intimidation, deterrence, or interference through false or 
misleading information, as well as false claims of endorsement. 
 
Turbocharging Information Manipulation 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
3 
 
 
generated by a text generator like ChatGPT designed to pretend they’re 
someone of authority to scare consumers into sending money. In 2022, 
estimated consumer losses increased to $39.5 billion,7 with the FTC 
reporting more than $326 million lost from scam texts alone.8  
Auto dialers, robo-texts, robo-emails, and mailers, combined with data 
brokers that sell lists of numbers or email addresses, enable entities to send 
out a massive number of messages at once. The same data brokers can sell 
lists of people as potential targets along with “insights” about their mental 
health conditions, religious beliefs, or sexuality that can be exploited. The 
degree of targeting that data brokers are allowed to use on individuals 
exacerbates AI-generated harm. 
Text generation services also increase the likelihood of successful phishing 
scams and election interference by bad actors. This has already happened—
in a 2021 study, researchers found phishing emails generated by GPT-3 
were more effective than human-generated ones.9 Generative AI can 
expand the pool of potentially effective fraudsters by aiding people with 
limited English skills in crafting natural and accurate-sounding emails that 
can then target employees, intelligence targets, and individuals in a way that 
makes it much more difficult to detect the scam. 
DISINFORMATION 
Bad actors can also use generative AI tools to produce adaptable content 
designed to support a campaign, political agenda, or hateful position and 
spread that information quickly and inexpensively across many platforms. 
This rapid spread of false or misleading content—AI-facilitated 
disinformation—can also create a cyclical effect for generative AI: when a 
high volume of disinformation is pumped into the digital ecosystem and 
more generative systems are trained on that information via reinforcement 
learning methods, for example, false or misleading inputs can create 
increasingly incorrect outputs. 
Turbocharging Information Manipulation 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
4 
 
 
The use of generative AI tools to accelerate the spread of disinformation 
could fuel efforts to influence public opinion, harass specific individuals, or 
affect politics and elections. The impacts of increased disinformation may be 
far-reaching and cannot be easily countered once spread; this is especially 
concerning given the risks disinformation poses to the democratic process. 
MISINFORMATION 
The phenomenon of inaccurate outputs by text-generating large language 
models like Bard or ChatGPT has already been widely documented. Even 
without the intent to lie or mislead, these generative AI tools can produce 
harmful misinformation. The harm is exacerbated by the polished and 
typically well-written style that AI generated text follows and the inclusion 
among true facts, which can give falsehoods a veneer of legitimacy. As 
reported in the Washington Post, for example, a law professor was included 
on an AI-generated “list of legal scholars who had sexually harassed 
someone,” even when no such allegation existed.10 As Princeton Professor 
Arvind Narayanan said in an interview with The Markup: 
Sayash Kapoor and I call it a bullshit generator, as have others 
as well. We mean this not in a normative sense but in a 
relatively precise sense. We mean that it is trained to produce 
plausible text. It is very good at being persuasive, but it’s not 
trained to produce true statements. It often produces true 
statements as a side effect of being plausible and persuasive, 
but that is not the goal.11 
AI-generated content implicates a broader legal issue as well: our trust in 
what we see and hear. As AI-generated media becomes more common, so 
too will circumstances where we are tricked into believing something 
fictional is real12—or that something real is fictional.13 When individuals can 
no longer trust information and new information is generated faster than it 
can be checked for accuracy, what can they do? Information sources like 
Wikipedia could be overwhelmed with false AI-generated content. This can 
Turbocharging Information Manipulation 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
5 
 
 
be harmful in targeted situations by inducing a target to act under the 
assumption that, e.g., their loved ones are in crisis.14  
SECURITY 
The same phishing concerns described above pose a security threat. 
Though chatbots cannot (yet) develop their own novel malware from 
scratch, hackers could soon potentially use the coding abilities of large 
language models like ChatGPT to create malware that can then be minutely 
adjusted for maximum reach and effect, essentially allowing more novice 
hackers to become a serious security risk. In fact, security professionals 
have noted that hackers are already discussing how to install malware and 
extract information from targets using ChatGPT.15  
Generative AI tools could very well begin to learn from repeated exposure 
to malware and be able to develop more novel and unpredictable malware 
that evades detection by common security systems. 
CLICKBAIT AND FEEDING THE SURVEILLANCE 
ADVERTISING ECOSYSTEM 
Beyond misinformation and disinformation, generative AI can be used to 
create clickbait headlines and articles, which manipulate how users navigate 
the internet and applications. For example, generative AI is being used to 
create full articles, regardless of their veracity, grammar, or lack of common 
sense, to drive search engine optimization and create more webpages that 
users will click on. These mechanisms attempt to maximize clicks and 
engagement at the truth’s expense, degrading users’ experiences in the 
process. Generative AI continues to feed this harmful cycle by spreading 
misinformation at faster rates, creating headlines that maximize views and 
undermine consumer autonomy. 
Turbocharging Information Manipulation 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
6 
 
 
HARMS 
• Economic/Economic Loss: Successful scams and malware can result 
in victims’ direct economic loss through extortion, trickery, or gaining 
access to financial accounts. This can lead to long-term impacts on 
credit as well. 
• Reputational/Relationship/Social Stigmatization: Misinformation and 
disinformation can generate and spread false or harmful information 
about an individual resulting in harm to their reputation in the 
community, potential damage to their personal and professional 
relationships, and impacts to their dignity. 
• Psychological—Emotional Distress: Disinformation and 
misinformation can cause severe emotional harm as individuals 
navigate the impacts of false information being spread about them—in 
addition, many individuals face shame and embarrassment if they are 
the victim of scams and may feel manipulated or used in the context of 
clickbait and surveillance advertising. 
• Psychological—Disturbance: The influx of false or misleading 
information and clickbait makes it difficult for individuals to carry on 
their daily activities online. 
• Autonomy: The spread of misinformation and disinformation makes it 
increasingly difficult for individuals to make properly informed choices 
and the manipulative nature of surveillance advertising complicates 
the issue of choice even further. 
• Discrimination: Scams, disinformation, misinformation, malware, and 
clickbait all prey on vulnerabilities of the “marks,” including 
membership in certain vulnerable groups and categories (the elderly, 
immigrants, etc.). 
Turbocharging Information Manipulation 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
7 
 
 
EXAMPLES 
• People used AI to call in fake bomb threats to public places like 
schools.16 
• AI voice generators were used call people’s loved ones, convincing 
them that their family member was in jail and desperately needed 
money for bail and legal assistance.17 
• The Center for Countering Digital Hate tested Google’s Bard chatbot 
to see if they would replicate 100 common conspiracy theories 
including Holocaust denial and saying the mass child murder tragedy 
at Sandy Hook was staged using “crisis actors.” Bard pumped out text 
based on these lies 78 out of 100 times without context or disclosure. 
• Unedited AI Spam was found by Vice reporters widely throughout the 
internet.18  
• CNET, a tech news website, paused its use of AI and issued 
corrections in 41 out of the 77 stories that it published which had been 
written using an AI tool. The AI-written articles, which were designed 
to be viewed more on Google searches to increase ad revenue, 
contained inaccurate and misleading information.19 
• Similarly, Buzzfeed reportedly published AI-written content, namely 
travel guides, with the aim to attract search traffic about different 
destinations. The quality of the results was uniformly reviewed as 
useless and unhelpful.20 
INTERVENTIONS  
• Enact a law that makes intimidating, deceiving, or deliberately 
misinforming someone about an election or candidate illegal 
(regardless of the means), such as the Deceptive Practices and Voter 
Intimidation Prevention Act.  
Turbocharging Information Manipulation 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
8 
 
 
• Pass the American Data Privacy Protection Act. The ADPPA will limit 
the collection and use of personal information to that which is 
reasonably necessary and proportionate to the purpose for which the 
information was collected. Such limitation will limit personal 
information being used to profile users to target them with ads, 
phishing attempts, and other scams. The ADPPA will also restrict the 
use of personal data to train generative AI systems that can 
manipulate users. 
• Promulgate an FTC Commercial Surveillance rule that sets a data 
minimization standard prohibiting out-of-context secondary uses of 
personal information, which would similarly prevent training generative AI 
systems using personal information collected for an unrelated purpose. 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
9 
 
 
Harassment, 
Impersonation, and 
Extortion 
BACKGROUND AND RISKS  
Some of the earliest uses—or misuses—of generative AI technologies are 
deepfakes:21 realistic images or videos created using machine-learning 
algorithms to depict someone as saying or doing something they did not—
often by replacing the likeness of one person with that of another.22 
Deepfakes and other AI-generated content can be used to facilitate or 
exacerbate many of the harms listed throughout this report, but this section 
focuses on one subset: intentional, targeted abuse of individuals. AI-
generated images and videos provide several ways for bad actors to 
impersonate, harass, humiliate, exploit, and blackmail others. For example, a 
deepfake video could show a victim praising a cause they detest or 
engaging in sexually explicit or otherwise humiliating acts. These images 
and videos can spread rapidly across the internet as well, making it difficult 
or impossible for victims, law enforcement, and other interested parties to 
identify the creator(s) and ensure harmful deepfakes are removed. 
Unfortunately, many victims of targeted deepfakes are left without recourse, 
and those who pursue recourse are often forced to identify and confront the 
perpetrators themselves. 
The harms of synthetic media predate AI and machine learning. As far back 
as the 1990s, commercial photo editing software enabled users to alter 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
10 
 
 
appearances or swap faces in photos. However, modern deepfakes and 
other AI-generated synthetic content trace their roots to Google’s 2015 
release of TensorFlow, an open-source tool for building machine-learning 
models, and the viral spread of a 2017 deepfake created using such a tool.23 
To create these early deepfakes—many of which involved placing 
celebrities’ faces onto the bodies of pornographic film actors—a creator had 
to build a machine-learning model (often, a generative adversarial network, 
or GAN) using a tool like TensorFlow, train it on various image, video, or 
audio files, and then instruct the model to map a specific person’s features 
or voice onto another person’s body.24 The release of new generative AI 
services like Midjourney and Runway removed these technical hurdles, 
enabling anyone to quickly create AI-generated content by providing a few 
key images, a source video, or even text entries. 
At its core, using AI-generated content to impersonate, harass, humiliate, 
exploit, or blackmail an individual or organization is frequently no different 
from doing the same using other methods. Victims of deepfake harms may 
still turn to existing criminal and civil remedies for fraud,25 impersonation,26 
extortion,27 and cyberstalking28 to redress malicious uses of generative AI 
tools. However, generative AI raises novel legal issues and exacerbates 
harm in new ways, straining the ability of victims and regulators alike to use 
existing legal avenues to redress harm. For example, deepfake 
impersonations of deceased people—a phenomenon described as 
“ghostbots”—may not only implicate defamation law, but also cause 
emotional distress among a deceased individual’s loved ones where false 
textual quotes may not.29 These new legal issues fall into roughly three 
categories: issues involving malicious intent; issues involving privacy and 
consent; and issues involving believability. 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
11 
 
 
 
MALICIOUS INTENT 
A frequent malicious use case of generative AI to harm, humiliate, or 
sexualize another person involves generating deepfakes of nonconsensual 
sexual imagery or videos. These sexual deepfakes are some of the earliest 
and most common examples of deepfake technology, garnering widespread 
media attention.30 However, many existing nonconsensual pornography 
laws limit liability to circumstances where content is published with an intent 
to harm.31 Some malicious uses of generative AI no doubt meet this 
threshold, but many deepfake creators may not intend to harm the subject 
of a sexual deepfake; rather, they may create and circulate the deepfake 
without ever expecting the subject to see or be impacted by the content. 
Intent requirements permeate other criminal laws applicable to malicious 
uses of generative A.I as well. For example, the federal cyberstalking 
statute, 18 U.S.C. § 2261A, only applies to those who act “with the intent to 
kill, injure, harass, intimidate, or place under surveillance [with similar 
intent].” State impersonation statutes like California Penal Code § 528.5 
similarly limit enforcement to those who impersonate another “for purposes 
of harming, intimidating, threatening, or defrauding another person.” Using 
CASE STUDY – SILENCING A JOURNALIST 
In April of 2018, Indian investigative journalist Rana Ayyub 
received an email from a source within the Modi government. A 
video of her engaging in sexual acts was going viral, leading to 
public humiliation and criticism from those who wanted to discredit 
her work. But it was a fake. Ayyub’s likeness was inserted into a 
pornographic video using an early deepfake technology. As public 
scrutiny increased, her home address and cell phone information 
were leaked, leading to death and rape threats. This early video 
was circulated to harass, shame, and ostracize a vocal critic of the 
government – and for months, it succeeded.  
 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
12 
 
 
generative AI to intimidate, harass, 
defraud, or extort another person may 
fall within these criminal statutes, but 
creating harmful or sexual deepfakes 
for personal enjoyment or 
entertainment may not. 
Lastly, divining the intent of a deepfake 
creator is made more difficult by a 
modern feature of many online 
platforms: user anonymity. When a 
victim becomes aware of a malicious 
deepfake as it spreads online—as 
happened to Journalist Rana Ayyub in 
2018—it can be incredibly difficult, if 
not impossible, to track down the 
original creator to bring a lawsuit or 
criminal charges. 
PRIVACY AND CONSENT 
Even when a victim of targeted, AI-
generated harms successfully 
identifies a deepfake creator with 
malicious intent, they may still struggle to redress many harms because the 
generated image or video isn’t the victim, but instead a composite image or 
video using aspects of multiple sources to create a believable, yet fictional, 
scene. At their core, these AI-generated images and videos circumvent 
traditional notions of privacy and consent: because they rely on public 
images and videos, like those posted on social media websites, they often 
don’t rely on any private information. This feature of AI-generated content 
excludes certain traditional privacy torts, including intrusion upon seclusion 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
13 
 
 
and publication of private facts, which depend explicitly on the publication 
or intrusion upon private facts.32 Other privacy torts, including false light, 
fare better because they only require plaintiffs to show that the creator knew 
or recklessly disregarded whether a reasonable person would find the AI-
generated content highly offensive.33 Still, these claims too face a difficult 
legal hurdle: the First Amendment.34 
The generative nature of new AI tools like Midjourney and Runway places 
them at a difficult crossroads between free expression protections and 
privacy protections for deepfake victims. Many AI-generated photos and 
videos transform source material or include new content in ways that may 
be protected under the First Amendment, but they can appear to be real 
footage of the victim in embarrassing, sexual, or otherwise undesirable 
circumstances. This tension between free speech, privacy, and consent 
raises new and difficult legal questions for both private individuals and 
public figures like celebrities and politicians. 
Consider the issue of consent. Many harmful AI depictions of private 
individuals use public source photos that victims post online. Victims may 
disapprove of the fictional, yet believable, photos and videos that generative 
AI tools produce of them, but existing legal claims may not provide the 
remedies these victims expect. Although the legal right of publicity originally 
protected the privacy and dignity of individuals, for example, some modern 
courts have focused their attention on the economic interest that a victim 
holds in their identity—namely, celebrities’ economic interest in their public 
image, which others may appropriate for their own commercial gain.35 These 
courts and similar state appropriation laws may not provide the easy legal 
remedy that victims expect when facing nonconsensual deepfakes; they 
may expect the victim to show some economic or physical injury in addition 
to their lack of consent, or they may expect the deepfake creator to have 
benefited financially. These laws and judicial interpretations did not develop 
with generative AI in mind, meaning that even AI harms that should be easy 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
14 
 
 
to remedy can become complex, costly, and confusing for victims. Of 
course, victims of malicious deepfakes and other AI-generated content can 
still pursue several other legal claims, such as defamation or negligent 
infliction of emotional distress, but the generative nature of new AI tools 
suggest that even these claims may face legal hurdles. The novelty and 
scalability of generative AI can be obstacles for victims of malicious 
deepfakes, even when their underlying legal claim is strong. 
Defamation is yet another example of a legal claim made more challenging 
by generative AI. While private individuals may hold the creator of a 
defamatory deepfake liable so long as the depiction was false and harmed 
the victim, public figures like celebrities and politicians must overcome a 
higher First Amendment hurdle to get redress. In New York Times Co. v. 
Sullivan, for example, the Supreme Court held that public figures had to 
show that a defendant published defamatory material with actual malice—in 
other words, “with knowledge that it was false or with reckless disregard of 
whether it was false or not.”36 And in Hustler Magazine, Inc. v. Falwell, the 
Supreme Court applied the same standard to defeat a claim of intentional 
infliction of emotional distress.37 However, the actual malice standard 
applied in these cases developed based on assumptions about what a 
reasonably prudent person could do to investigate and uncover the truth of 
information they receive. As generative AI tools grow more sophisticated, it 
will only become more difficult for individuals and press organizations to tell 
whether something is real or generated by AI, effectively raising the hurdle 
that public figures must overcome to redress harms caused by defamatory 
deepfakes. 
Importantly, the malicious use of generative AI can impact everyone—private 
individuals and public figures alike. The distinction between private 
individuals and public figures within the law is far from clear, and both 
private individuals and public figures have successfully overcome the First 
Amendment, privacy, and consent hurdles discussed above.38 These cases 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
15 
 
 
and the legal tests they implicate merely highlight legal assumptions that 
may not hold true when someone uses generative AI to impersonate, 
harass, defame, or otherwise harm others—legal assumptions that may 
impose barriers to redress and perpetuate AI-generated harm. While many 
traditional legal remedies may still be available for victims of malicious 
deepfakes and other generative AI harms, the novel legal questions that 
generative AI raises—as well as the potentially massive volume of violations 
that a publicly available generative AI tool can produce—will no doubt make 
these legal remedies harder to pursue and less effective in practice. 
BELIEVABILITY 
Deepfakes can impose real social injuries on their subjects when they are 
circulated to viewers who think they are real. Even when a deepfake is 
debunked, it can have a persistent negative impact on how others view the 
subject of the deepfake.39 And the believability of AI-generated content can 
undermine victims’ ability to pursue legal redress as well. The proliferation 
of generative AI and deepfakes undermines core assumptions about how 
legal fact-finding and the authentication of evidence occurs.40 Currently, the 
bar for authenticating courtroom evidence is not particularly high.41 All a 
claimant must show is that a reasonable juror could find in favor of 
authenticity or identification,42 after which point the determination of 
authenticity is up to the jury.43 In addition, many courts have adopted 
assumptions about the authenticity of aural and visual evidence that 
deepfakes undermine. For example, some courts recognize the “silent 
witness” theory of video authentication, wherein the existence of a 
recording speaks to the evidence’s authenticity without the need for a 
human witness’s observations.44 Others assume the authenticity of evidence 
taken from press archives or government databases, both of which may be 
vulnerable to deepfakes.45 As AI-generated content grows more common 
and more believable, courts and regulators alike will need to identify and 
adopt methods to determine whether images and videos are real and 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
16 
 
 
reconsider legal assumptions about the truth and value of evidence 
submitted at trial. 
HARMS 
• 
Physical: In some contexts, believable deepfakes of the victim 
seeming to engage in certain behaviors may put them at risk of 
physical harm and violence, for example, in cultures where publicly 
known sex acts would shame the family or in cultures where same-sex 
relationships are illegal. 
• 
Economic/Economic Loss: Distribution of AI-generated fake images 
and videos that are pornographic in nature or touch on hot-button 
political or social topics could lead to job loss for the victim as well as 
trouble finding future employment. 
• 
Reputational/Relationship/Social Stigmatization: Victims’ standing in 
the community, intimate and professional relationships, and dignity 
could all be severely damaged or destroyed if, for example, deepfakes 
convinced others that person was cheating on a partner or engaging 
in illicit acts with minors. 
• 
Psychological: Victims of these attacks often feel severely violated 
and may face feelings of hopelessness and fear that their lives have 
been destroyed. 
• 
Autonomy/Loss of Opportunity: Deepfakes have already been 
weaponized to intentionally silence journalists, activists, and other 
vulnerable individuals and can lead to loss of opportunity and change 
in life circumstances if believed broadly. This can also contribute to a 
threat to democracy and social change. 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
17 
 
 
• 
Autonomy/Discrimination: Deepfakes can easily be tools used to 
target already-vulnerable individuals belonging to marginalized groups 
or to make individuals appear to belong to marginalized groups—they 
also may reinforce negative attitudes about sex work and sex workers. 
EXAMPLES 
• The European Union’s police force issued an official warning that 
“grim” criminal abuse using ChatGPT and other generative AI tools is 
here and growing.46  
• A Twitch streamer made Deepfake porn of another Twitch streamer, 
imposing her face onto porn and passing it off as if it was her.47 
• A TikTok user spoke out about digitally created nude photos of her 
shared on the internet. The photos were used to threaten and 
blackmail her.48 
• Video game voice actors had their voice taken and used to train an AI 
to use their voice to harass and expose information about them, all 
without their knowledge or consent.49 
INTERVENTIONS 
• Technological solutions include deepfake detection software and 
methods for watermarking AI-generated content. These solutions may 
help victims, courts, and regulators identify AI-generated content, but 
the effectiveness of these solutions depends entirely on technical 
experts and responsible AI actors developing innovative detection 
and authentication tools faster than malicious AI developers can 
develop new, harder-to-detect AI tools. 
• Many longstanding legal tools may still apply despite the novel 
features of generative AI tools and the legal challenges they impose. 
For example, deepfakes that exploit copyrighted content—potentially 
including photos that victims took of themselves50—may be vulnerable 
Harassment, Impersonation, and Extortion  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
18 
 
 
to traditional copyright claims. Depending on the circumstances 
surrounding the AI-generated content, victims may also turn to various 
tort claims like defamation, false light, intentional infliction of 
emotional distress, and appropriation of name and likeness.51 To 
circumvent the challenge of identifying anonymous creators, victims 
may be able to sue the online platforms that host and circulate 
malicious AI-generated content if the platforms—including the 
providers of AI tools like Midjourney and Runway— materially 
contributed to what makes the content harmful or otherwise illegal.52 
And several criminal laws, from criminal impersonation and fraud 
statutes to incitement to violence, could apply to claims involving the 
malicious circulation of AI-generated content.53 
• Several regulatory interventions may further protect victims of 
deepfakes and other malicious uses of generative AI. While a general 
ban on deepfakes or generative AI tools may run afoul of the First 
Amendment,54 expanding claims under copyright law or privacy torts 
to cover fictional depictions of victims created with reckless disregard 
to the content’s impact on victims would go far to redress the harms 
caused by malicious uses of generative AI. Criminal statutes could 
also be updated or complemented with statutory language that 
captures the issues raised above, including language that lowers the 
intent required to hold someone liable for nonconsensual, AI-
generated sexual depictions of another person. And given the 
difficulty in identifying believable deepfakes and authenticating 
evidence, the Federal Rules of Evidence may benefit from higher 
authentication standards to counteract possible deepfakes. Lastly, 
malicious deepfakes and other AI-generated content created for 
commercial purposes could be regulated by administrative agencies 
like the Federal Trade Commission and state Attorneys General 
Offices on the grounds that they are unfair and deceptive.55 
 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
19 
 
 
Spotlight:  
Section 230  
Section 230 of the Communications Decency Act says that a provider of an 
interactive computer service is not to be “treated as the publisher or 
speaker of information provided by” a third party.56 Historically, companies—
and courts—have taken an expansive view on what it means to treat a 
company as the publisher or speaker of information—basically, if the lawsuit 
had anything to do with third-party provided content, companies claimed 
Section 230 immunity. In recent years, courts have begun to cabin Section 
230’s reach, finding instead that companies can only claim Section 230 
immunity if the basis for liability is dissemination of improper information that 
the company played no role in making improper.57  
Generative AI tools do not get blanket immunity: Some commentators 
have framed the generative AI Section 230 debate as an all-or-nothing 
determination, with some proclaiming that generative AI tools receive 
Section 230 immunity58 and others proclaiming they do not.59 But judges in 
recent major court decisions have declined to apply Section 230 in such a 
broad manner. Instead, courts apply Section 230 on a claim-by-claim basis.60 
Thus, whether a company will get Section 230 protection will depend on the 
specific facts and legal obligations at issue, not simply whether they have 
deployed a generative AI tool. 
Section 230 should not apply to some claims, like products liability 
claims, because they do not treat the company as the publisher or 
speaker of information: In the past, courts have applied Section 230 very 
broadly, largely by reading the provision to mean that a company is treated 
as the publisher or speaker whenever their allegedly unlawful activities 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
20 
 
 
involved the dissemination of third-party information. Courts have begun to 
backtrack on this and are recognizing that Section 230 does not protect 
against claims that target a company’s own obligations not to cause harm.61 
Thus, claims that generative AI companies violated their own duties 
regarding the design of their service, the collection, use, or disclosure of 
information, and the creation of content should not be barred by Section 
230. 
For instance, generative AI companies will have difficulty using Section 230 
to escape product liability claims—such as for negligent design or failure to 
warn—at least in the Ninth Circuit, where courts now recognize that such 
claims are based not on harm caused by third-party information but on a 
company’s breach of their duty to design products that do not pose an 
unreasonable risk of injury to consumers.62 Generative AI companies should 
also have to face claims that they violated privacy laws that limit how 
generative AI companies can collect, use, and disclose personal information 
because these laws impose duties on companies to respect the privacy 
interests of third-parties. 
Generative AI companies will not get Section 230 protection when the 
tool is wholly responsible for creating the content: Generative AI 
companies could potentially face several different types of claims about the 
information that their tools generate. Section 230 provides companies with 
protection for legal claims based on information provided by another party—
another “information content provider,” in Section 230 lingo. An information 
content provider is defined as “any person or entity that is responsible, in 
whole or in part, for the creation or development of information” provided to 
the company.63 So, a generative AI company does not get Section 230 
protection if it is, itself, an information content provider of the information at 
issue—that is, if the company “is responsible, in whole or in part, for the 
creation or development of the information.”  
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
21 
 
 
When a generative AI tool is alleged to have created new harmful content, 
such as when it “hallucinates” or makes up information that is not in its 
training data,64 the legal claim is not based on third-party information and 
Section 230 should not apply. For example, when a generative AI tool 
makes up false and reputationally damaging information about an individual, 
the generative AI company will not be protected by Section 230 for, say, 
defamation or false light, because the company, and not any third party, is 
responsible for creating the false and reputationally damaging information 
that is the basis for the legal claim. 
Generative AI companies will not get Section 230 protection when they 
materially contribute to the improper content: In some cases, generative AI 
companies will try to argue that the outputs at issue originated with a third 
party, either as user input or training data.65 In such cases, courts will have 
to determine whether the company created or developed the information in 
part. The prevailing test is whether the company materially contributed to 
making the information improper.66 Material contribution can include altering 
or summarizing third-party information to make it violate a law,67 requiring or 
encouraging the third party to input information that violates a law,68 or 
otherwise acting in a way that contributes to the illegality. 
When a user asks that a generative AI tool create misinformation or a 
deepfake, or when a tool uses training data to create harmful content, the 
tool transforms inputs into harmful content and the company that deployed it 
should not be able to use Section 230 to avoid liability. The user inputs—the 
request for harmful information, the photos or videos of the target of a 
deepfake—are not themselves harmful or sufficient to create the harmful 
content. After all, that is why the user is using generative AI to create the 
content. The inputs are also unlikely, on their own, to be sufficient to form 
the legal basis for the claim against the generative AI company. In such 
scenarios, a company deploying a generative AI tool materially contributes 
to the improper information by transforming information that cannot form the 
basis for liability into information that can form the basis for liability.  
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
22 
 
 
If, on the other hand, a user asks a generative AI tool to simply repeat a 
defamatory statement that the user enters into the tool, or to repeat harmful 
information from other sources, the tool may not materially contribute to the 
harm and may, consequently, benefit from Section 230 protection. 
 
Section 230 should not be an obstacle to holding 
companies accountable for harms caused by 
generative AI tools. Any new regulation or claim 
should be stated in terms of the generative AI 
company’s obligations and the harm the tool itself 
caused by generating harmful content. 
 
It is not clear that scraped training data is information “provided by” a 
third party: To obtain Section 230 protection, a company must show that 
the information that forms the basis for liability was “provided by” a third 
party. There is very little precedent on the question of when information has 
been “provided by” a third party.69 To “provide” information can mean to 
supply it or make it available to another.70 Generative AI companies would 
likely argue that publicly available information is made available for 
everyone to republish, including generative AI tools. But it is not at all clear 
that third parties intend to make their information available to generative AI 
tools simply by making their information viewable by a general audience on 
the internet—in fact, in many cases it is clearly the opposite. 
The relationship between the internet company and the third-party 
information provider matters for determining whether the third party 
provided the information.71 The types of services that Section 230 originally 
contemplated had users that directly provided information to the service, 
such as the Prodigy message boards that were the basis of the case that 
inspired Section 230.72 Search engines and other types of services that third 
parties do not provide information directly to have also been found to enjoy 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
23 
 
 
 
some Section 230 protection,73 but even these companies afford third 
parties some control over whether and to what extent their information is 
published or republished on their services. For example, websites can tell 
Google’s search engine crawlers not to index their pages,74 but there is no 
effective means to block an AI company from scraping their site.75 The lack 
of control third parties have over the use of their information in generative AI 
tools, along with similar considerations described in [privacy section], could 
sway courts against finding that scraped data is “provided by” third parties. 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
24 
 
 
Profits Over Privacy: 
Increased Opaque 
Data Collection 
BACKGROUND AND RISKS 
Generative AI tools are built on top of a variety of large, complex machine-
learning models, which need a large amount of training data to function. For 
tools like ChatGPT, the data includes text scraped from across the internet. 
For products like Lensa or Stable Diffusion, the data includes photos and art. 
With generative AI’s voracious need for data, many AI developers may 
scrape the web indiscriminately for data. While, in some cases, these 
developers attempt to sanitize their training data by filtering out protected 
work, explicit content, hate speech, or biased inputs, the practice of 
cleaning data is far from industry-standard. Without meaningful data 
minimization or disclosure rules, companies have an incentive to collect and 
use increasingly more (and more sensitive) data to train AI models. The 
excuse for collecting this data indiscriminately—increasing competition and 
innovation within the AI space—is harmful to the state of data privacy. This 
arms race narrative creates a justification for maximizing data collection just 
in case it provides some nebulous advantage later. In reality, these tools can 
be built with less data and without coercive and secretive data collection 
processes.  
Profits Over Privacy: Increased Opaque Data Collection  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
25 
 
 
SCRAPING TO TRAIN DATA 
Many generative AI tools use models built on data scraped from publicly 
available websites. This information often includes personal information 
posted on social media and other websites. People post information on 
social media and elsewhere for a variety of reasons: to allow potential 
employers to find them on LinkedIn; so that friends and acquaintances can 
find them on Facebook, Twitter, and Venmo; and so forth. These reasons 
have an important common feature: people post information on a website 
for the purpose of making that information viewable on that website. But 
sometimes, a person’s personal information is made publicly available 
without their consent. Third parties might publish their photo or other 
information about them. A platform’s confusing privacy settings may lead a 
person to accidentally make their information available. A software error76 or 
design change77 can also expose information that a person had set to be 
viewable only to a select few.  
When companies scrape personal information and use it to create 
generative AI tools, they undermine consumers’ control of their personal 
information by using the information for a purpose for which the consumer 
did not consent. The individual may not have even imagined their data could 
be used in the way the company intends when the person posted it online. 
Individual storing or hosting of scraped personal data may not always be 
harmful in a vacuum, but there are many risks. Multiple data sets can be 
combined in ways that cause harm: information that is not sensitive when 
spread across different databases can be extremely revealing when 
collected in a single place, and it can be used to make inferences about a 
person or population. And because scraping makes a copy of someone’s 
data as it existed at a specific time, the company also takes away the 
individual’s ability to alter or remove the information from the public sphere.  
Profits Over Privacy: Increased Opaque Data Collection  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
26 
 
 
The privacy harms that follow from indiscriminate scraping of personal 
information for AI training data also create risks for online speech and the 
openness of the internet. As AI tools use people’s personal information for 
more and more harmful purposes, people may become more hesitant to 
share any information on social media or sites that could potentially be 
scraped in the future, even if those sites promise to secure their data. They 
may be less likely to post photos of themselves, to participate in public 
debates on “the vast public forums of the internet”78—particularly social 
media—or to have social media profiles or personal websites that can be 
associated with them at all. Disincentivizing people from engaging in public 
discourse and interacting online will limit the usefulness of the internet as a 
whole and networking tools in particular. 
Basic data minimization principles dictate that peoples’ personal information 
should only be collected or used for the specific purpose for which each 
person provided the information. But there are currently no statutes that 
prohibit companies from scraping people’s personal information and using it 
to train generative AI tools. Privacy laws in the U.S. exempt most publicly 
available information from regulation based on a concern that collection and 
use of this information is protected by the First Amendment. But lawmakers 
underestimate the significant countervailing privacy interests against 
allowing companies to indiscriminately scrape personal information.  
People should be able to post public profile photos without fear that these 
photos will be used to create deepfakes of them or feed other abusive AI 
applications. Laws limiting the collection of publicly available personal 
information and/or its subsequent use would both protect people’s interest 
in controlling their information and encourage people to continue to make 
information publicly available on the internet. 
Profits Over Privacy: Increased Opaque Data Collection  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
27 
 
 
GENERATIVE AI USER DATA 
Many generative AI tools require users to log in for access, and many retain 
user information, including contact information, IP address, and all the inputs 
and outputs or “conversations” the users are having within the app. These 
practices implicate a consent issue because generative AI tools use this 
data to further train the models, making their “free” product come at a cost 
of user data to train the tools. This dovetails with security, as mentioned in 
the next section, but best practices would include not requiring users to sign 
in to use the tool and not retaining or using the user-generated content for 
any period after the active use by the user. 
GENERATIVE AI OUTPUTS 
Generative AI tools may inadvertently share personal information about 
someone or someone’s business or may include an element of a person 
from a photo. Particularly, companies concerned about their trade secrets 
being integrated into the model from their employees have explicitly banned 
their employees from using it. 
HARMS 
• Physical: Individuals who may want to remove personal data for their 
own safety, such as domestic violence or stalking victims, may be 
unable to do so where data has been added to generative AI data sets 
and so may be at risk from their abusers. 
• Economic/Economic Loss: Businesses whose trade secrets have 
been incorporated into training sets face potential economic loss. 
• Psychological: Individuals unable to remove their personal data from 
training sets may face frustration or fear if the data could impact them 
negatively if spread. 
Profits Over Privacy: Increased Opaque Data Collection  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
28 
 
 
• Autonomy: Individuals unable to block addition or force removal of 
their personal information from training sets demonstrably have lost 
control of their data. 
• Autonomy: Individuals are often not informed, consulted, or given 
options about whether their personal data will be added to training 
datasets. 
• Autonomy/Loss of Opportunity: Inability to remove data that is 
inaccurate or no longer accurate or make updates may result in 
incorrect outputs that then exacerbate as the incorrect information 
proliferates. 
EXAMPLES 
• The Italian Data Protection Authority began an enforcement action 
based on the EU’s General Data Protection Regulation against 
OpenAI, banning the service in the country pending investigation. This 
led the company to institute some privacy disclosures and controls to 
the system.79 Regulatory interest from bodies throughout the world will 
likely act as a catalyst to improved data protection behavior. 
• Photos from private medical records were found in public database 
LAION-5B, which are used to make image generators.  
INTERVENTIONS 
• Enforce laws that prohibit unfair and deceptive trade practices, 
consent requirements for child users, and require justification for data 
processing. 
• Enact laws and regulations that impose a data minimization standard 
that would limit use of personal data for generative AI training (e.g., 
the American Data Privacy Protection Act, the FTC commercial 
surveillance rule, and certain state privacy regulations) 
Profits Over Privacy: Increased Opaque Data Collection  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
29 
 
 
§ Support tools that are built using a limited and disclosed set of 
data. 
• Adopt a strict data minimization standard by developers to help 
mitigate the privacy harms of creating, tweaking, and updating models 
to train AI. Data minimization is a standard that, depending on the 
precise definition, should only allow collection of personal data to the 
extent that it is necessary to carry out the service requested by the 
user. The tenets of data minimization are fundamentally at odds with 
the large-scale creation of generative AI datasets from public info 
without disclosure or consent.  
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
30 
 
 
Increasing Data 
Security Risk  
BACKGROUND AND RISKS  
The Identity Theft Resource Center estimated a record-breaking 1,862 data 
breaches occurred in 2021;80 with another 1,802 in 2022.81 Beyond the 
inherent privacy harms of a breach, there can be severe downstream 
impacts as well. A Government Accountability Office report indicated that 
victims have “lost job opportunities, been refused loans, or even been 
arrested for crimes they did not commit as a result of identity theft.”82 Yet 
these harms do not appear on the victim’s bank statement or credit report, 
and can be nearly impossible to control where a Social Security Number 
(SSN) is used; by virtue of its unique and unchangeable nature, the SSN 
serves as a powerful identifier for both government and private sector 
entities. To make matters worse, a stolen SSN, unlike a stolen credit card, 
cannot be effectively cancelled or replaced.83 Criminals in possession of 
SSNs can open new financial accounts and perpetrate identity theft because 
many financial institutions rely on SSNs to verify transactions.84 
Unsurprisingly, research by the Bureau of Justice Statistics indicates that 
identity theft can result in severe distress.85 
The threat landscape has gotten worse in recent years as well, with the 
introduction of ransomware-as-a-service, malware-as-a-service, and other 
proxy services by which hackers-for-hire have productized methods for 
unauthorized access to data.86 We should expect to see continued 
examples of purchasable tools by which data can be accessed, encrypted, 
and/or manipulated without authorization. 
Increasing Data Security Risk  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
31 
 
 
Just as every other type of individual and organization has explored 
possible use cases for generative AI products, so too have malicious actors. 
This could take the form of facilitating or scaling up existing threat methods, 
for example drafting actual malware code,87 business email compromise 
attempts,88 and phishing attempts.89 This could also take the form of new 
types of threat methods, for example mining information fed into the AI’s 
learning model dataset90 or poisoning the learning model data set with 
strategically bad data.91 We should also expect that there will be new attack 
vectors that we have not even conceived of yet made possible or made 
more broadly accessible by generative AI. 
HARMS  
• Physical: Where an individual is the victim of identity theft, they may 
face arrest for crimes they have not committed. 
• Economic/Economic Loss: Victims may lose job opportunities or be 
refused loans as the result of identity theft and destruction of credit. 
• Reputational/Social Stigmatization: Identity theft can cause severe 
reputational damage and malware can also be used to reveal sensitive 
information about an individual, resulting in additional social harms. 
• Psychological: Victims of these attacks may face embarrassment and 
fear as well as feelings of helplessness, anger, and more due to the 
results of such attacks. 
• Autonomy: Loss of identity control, financial control, image, and more 
may accompany these attacks. 
• Discrimination: Scams often target historically vulnerable groups, 
such as the elderly. 
Increasing Data Security Risk  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
32 
 
 
EXAMPLES  
• ChatGPT suffered a massive data breach, exposing user information 
and prompt history.92  
• Samsung banned use of AI after secure information was found leaked 
to ChatGPT by employees.93 
• OpenAI is allowing users to get information from users through plug-
ins that let the chatbot get new sources of information like Expedia 
and Instacart.94 
INTERVENTIONS 
• If companies invest in employee training and patching known 
vulnerabilities, they can mitigate some of the risk of generative AI 
super-charging existing threat methods. However, risks related to the 
use of the AI model itself will require distinct solutions, including but 
not limited to those outlined in NIST’s AI Risk Management 
Framework95 and those required in the proposed American Data 
Privacy and Protection Act (ADPPA).96  
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
33 
 
 
Confronting 
Creativity: Impact 
on Intellectual 
Property Rights  
BACKGROUND AND RISKS  
Intellectual property law (IP law) encompasses copyrights, patents, 
trademarks, and trade secrets. Loosely, copyrights protect original works in 
any medium of expression (think books, music, theater, and artwork), 
patents protect inventions, trademarks protect any words or symbols used 
to identify the source of specific goods and services, and trade secrets 
protect proprietary business information.97 Each of the branches of IP law 
contain specific rights for the creators and owners of a work over that 
work—for example, controls over how that work is used or preventing others 
from claiming the work as theirs. While all areas of IP law have challenged 
generative AI use and generation of works, copyright has been by far the 
most frequently invoked. 
The extent and effectiveness of legal protections for intellectual property 
have been thrown into question with the rise of generative AI. Generative AI 
trains itself on vast pools of data that often include IP-protected works. As 
stated in a recent open letter from the Center for Artistic Inquiry and 
Reporting, “AI-art generators are trained on enormous datasets, containing 
millions upon millions of copyrighted images, harvested without their 
Confronting Creativity: Impact on Intellectual Property Rights 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
34 
 
 
creator’s knowledge, let alone compensation or consent. This is effectively 
the greatest art heist in history.”98 The systems trained on these works may 
then learn to mimic specific styles, as has already occurred in several 
cases.99 Several artists whose style has been copied have expressed deep 
frustration, anger, and dismay over their work being mimicked, noting that 
the AI is profiting off the work they have put in to develop distinct styles, 
impacting their livelihood, and reducing deeply personal work to an 
algorithm. In the words of Nick Cave, an artist confronted with a song 
generated “in the style of” Nick Cave, “This song is bullshit, a grotesque 
mockery of what it is to be human.” 100 
Questions about how far IP protections extend in the realm of generative AI 
can be categorized into either the input or output cycle of these systems. 
 
INPUTS 
Generative AI systems can produce extremely detailed and adaptable 
content because they are trained on enormous amounts of data scraped 
from across the internet. The type of data taken in will vary depending on 
the system type. For example, AI art generators will scrape art and images, 
CASE STUDY – WHAT’S IN A VOICE? 
An AI-generated song, billed as a “collaboration” between Drake and 
the Weeknd, popped up on Spotify, Tidal, Apple Music, and YouTube, 
quickly collecting enough listens and views over the weekend to 
appear on music charts by Monday. The song was generated by 
scraping multiple samples of both artists’ voices and music, creating a 
realistic-sounding new hit. It was taken down after multiple copyright 
claims by Universal Music Group, leading to questions about whether 
an original song can be copyrighted and what protections exist for the 
artists whose voices and musical styles are being cloned.  
 
Confronting Creativity: Impact on Intellectual Property Rights 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
35 
 
 
translating information about their key features into code that is then 
reviewed by those systems for patterns, relationships, and rules, then used 
to generate responses to user prompts. Because these systems’ outputs 
become more “accurate” or responsive with more data, many are 
programmed to scrape their preferred content type continuously and 
automatically. 
These vast datasets nearly always contain protected works. The entities 
using the datasets to create a generative AI system rarely, if ever, have 
permission or license from the creators and owners of artistic works to use 
them. In fact, many artists have openly stated that they do not want their 
work going into systems that may make them obsolete.101  
There is serious and ongoing debate over whether generative AI tools 
should be permitted to use protected works without a license. Some argue 
that such use constitutes fair use, an exception to some copyright 
protections with a very limited scope of application. Fair use often depends 
on the use of copyrighted material. For instance, a research or non-profit 
group using the content may have a better fair use claim than a company 
intending to sell the work generated using the original work. The extent to 
which fair use may apply to generative AI is still unsettled law. 
OUTPUTS 
End-users of generative AI have already attempted to claim ownership over 
the outputs of generative AI tools, including several who have attempted to 
file for copyrights with the United States Copyright Office.102 The rising use 
of generative AI to create creative works and subsequent copyright filing 
attempts has been significant enough to prompt the Copyright Office to 
launch a new AI initiative.103 
Statements from the U.S. Copyright Office so far have mandated that a work 
cannot receive copyright protections unless it contains “creative 
Confronting Creativity: Impact on Intellectual Property Rights 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
36 
 
 
contribution from a human actor,”104 noting that copyright may only protect 
material that is “the product of human creativity.”105 While some have argued 
that the prompt constitutes sufficient “human creativity” to result in IP 
protections for the resulting work, the Copyright Office disagrees, 
comparing a prompt to “instructions to a commissioned artist—they identify 
what the prompter wishes to have depicted, but the machine determines 
how these instructions are implemented in its output.”106 
This distinction becomes more complex when a portion of the work is AI-
generated and a portion is human-generated.107 Copyright may be applied 
to work that contains or builds off AI-generated work, but the copyright will 
apply solely to the human-authored aspects.108 
HARMS  
The harms to individual creators of works and to the artist community are 
substantial.  
• Economic/Economic Loss: Legal harms in this space likely include 
infringement on works and right of use, along with questions about 
ownership of AI-generated products, as discussed above. 
Infringement on works would likely stem from generative AI outputs. 
These include unauthorized reproduction (which may be the case of 
the AI-generated work is too similar to original pieces) and derivative 
work (work which contains too many original elements from the initial 
piece, often seen in reproductions, condensations, or abridgments of 
original work). Right of use would relate to generative AI input, 
whether the generative AI systems have a license to use original work 
in learning sets or whether this could fall under an exception, such as 
Fair Use. 
• Economic/Economic Loss: Creators and owners may face severe 
economic harms as demand for their work shrinks when similar work 
can be easily and cheaply generated. These harms likely would 
Confronting Creativity: Impact on Intellectual Property Rights 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
37 
 
 
manifest in lack of opportunity and hiring (as many creator jobs and 
commissions are replaced with generative AI) and infringement on 
economic gain from originally created work (missing out on licensing 
or selling work since buyers are using replicated work instead). This 
could also lead to a sharp drop in professional artists if there is rising 
fear that AI-generated work will make it too difficult to make a living as 
a creator. Finally, the influx of AI-generated work will impact the 
market for that work. 
• Reputational: Creators may face reputational harm as well. It is 
entirely possible for fans to confuse AI-generated work with that of the 
inspiring creator, which is a problem when the creator has no part in 
that work and cannot give input regarding use, quality, or direction. 
Work generated in a specific creator’s style or voice may be used to 
promote causes the creator disagrees with or may be of low quality, 
both of which could cause reputational damage. 
• Psychological: Several artists have expressed pain, sadness, anger, 
and more regarding their work being used and replicated by 
generative AI. In many cases, artists’ work is deeply personal, making 
copies and exploitation of that work deeply personal as well. 
• Loss of Opportunity/Relationship/Dignitary: If creators and their work 
are not protected from exploitation and copying via generative AI, it 
will likely result in fewer artists putting in the time and effort to develop 
their own distinct art styles, leading to an overall drop in the creator 
community and volume of all creative works by humans. 
EXAMPLES  
• Several artists have found that their work has already been used to 
train AI without their permission, in some cases leading to AI that can 
convincingly replicate their precise artistic style when asked.109  
Confronting Creativity: Impact on Intellectual Property Rights 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
38 
 
 
• As noted in the case study above, this extends to AI-generated songs 
“performed” in exact mimics of artists’ vocal tones and musical 
styles.110  
• Artists have found AI copying their style or modifying their work in 
ways that make it seem as if it supports hateful messages, as with one 
artist who found the alt-right using AI-generative tools to create 
express offensive worldviews in her artistic style.111  
• There are current examples of individuals attempting to claim 
copyright over work generated by AI tools.112  
• Three artists have filed a class action in the Northern District of 
California against generative AI image companies for using their 
artwork without consent or compensation to build the training sets that 
inform the platforms.113 
• Getty Images has started legal proceedings against Stability AI for 
copying and processing millions of its images for training sets without 
license.114 
INTERVENTIONS  
• AI developers could be forced to license any IP included in training 
data for generative AI technology, which would prevent indiscriminate, 
continuous content scraping across the internet. 
• Customers could be obligated to perform a form of due diligence to 
confirm whether models were trained with any protected content prior 
to using the model. 
• AI tools could be forced to acquire creator permission before 
generating art “in the style” of any specific creator. 
• Academic researchers at the University of Chicago developed a tool 
called Glaze that introduces nearly imperceptible elements to artwork 
that are designed to disrupt generative AI’s ability to scrape 
information about the artwork and add it to the training data.115 
Confronting Creativity: Impact on Intellectual Property Rights 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
39 
 
 
• Shutterstock is putting together the option for creators to opt out of 
their work being used in AI training sets and has established a 
contributor fund to compensate creators if their IP is used in training 
sets.116 
• DeviantArt has implemented a metadata tag for images shared on the 
web that contains a warning to AI systems not to scrape the tagged 
content.117 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
40 
 
 
Exacerbating Effects 
of Climate Change 
BACKGROUND AND RISK  
The planet is hurtling toward ongoing climate disasters.118 Climate change 
has already caused hundreds of thousands to millions of deaths, billions of 
dollars in economic damage, and mass species extinction.119 In the future, 
every tenth of a degree of warming that we are able to avoid will mean 
millions of saved lives, avoidance of enormous economic loss, and a chance 
at a livable future.120 Eventually, by choice or by necessity, our society will 
evaluate every industry and activity in terms of its resource and carbon cost.  
Into this high-risk situation crashes the growing field of generative AI, which 
brings with it direct and severe impacts on our climate: generative AI comes 
with a high carbon footprint and similarly high resource price tag, which 
largely flies under the radar of public AI discourse.  
Training and running generative AI tools requires companies to use extreme 
amounts of energy and physical resources. Training one natural language 
processing model with normal tuning and experiments emits, on average, 
the same amount of carbon that seven people do over an entire year.121 
Exacerbating Effects of Climate Change 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
41 
 
 
122 
A comparison of pounds of carbon dioxide produced  
by everyday people/objects and generative-AI-related tasks 
AI models take an enormous amount of carbon to produce, and this trend is 
not likely to meaningfully improve given the incentives in the industry. Much 
AI research, especially at large tech companies that effectively control the 
space, focuses on accuracy or related measures at the expense of all other 
considerations.123 A good portion of AI research seeks to “buy” better results 
by investing exponentially more money over time for a linear increase in 
accuracy, disregarding other externalities like resource costs.124 These costs 
are physical requirements for many AI systems: the relationship between 
model performance and model complexity is at best logarithmic, so for a 
linear gain in performance, an exponentially larger and more resource-
inefficient model is required.125 While some AI researchers have begun 
focusing on efficiency, whether for cost-cutting or environmental reasons, 
there is no reason to think that large tech companies will abandon their 
quest for accuracy any time soon. 
Meanwhile, the data centers that AI developers use to train and host 
generative AI models have high energy costs and massive carbon footprints. 
Though some of this energy may come from renewable resources, data 
centers’ energy consumption is still concerning for several reasons. First, 
many regions that house data centers still use carbon-intensive energy 
sources to generate electricity.126 Second, even when renewable energy is 
Exacerbating Effects of Climate Change 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
42 
 
 
available, it may be better allocated to heat a family’s home, power a 
greenhouse, or further other socially important goals, rather than train an AI 
model—but this tradeoff is generally not examined or discussed.127 
These data centers are also resource-intensive in unsustainable ways. Many 
tech firms draw from public water supplies to cool their centers in the middle 
of drought-prone areas—a practice that has led to public backlash.128 “New 
research suggests training for GPT-3 alone consumed 185,000 gallons 
(700,000 liters) of water. An average user’s conversational exchange with 
ChatGPT basically amounts to dumping a large bottle of fresh water out on 
the ground, according to a new study.”129 These technologies also rely 
heavily on minerals that are procured under violent and exploitative 
conditions.130 
HARMS 
• Physical: Severe environmental changes will result in substantial 
physical harms to people globally (drought, natural disasters, etc.). 
• Economic/Economic Loss: The economic resources required to 
counter environmental harms or to run generative AI as-is are 
significant. 
• Autonomy: So many limited resources going to large companies using 
them for generative AI necessarily means that others will have less 
access and suffer shortages. 
EXAMPLES 
• Sasha Luccioni, the Climate Lead at HuggingFace, evaluates 
environmental and societal impact of Generative AI – she highlights 
“tonnes of carbon emissions,” “huge quantities of energy/water,” and 
“Rare metals for manufacturing hardware” in her Iceberg model of 
Generative AI costs. 
Exacerbating Effects of Climate Change 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
43 
 
 
 
Figure 1: Credit: Sasha Luccioni 
INTERVENTIONS 
• Because of environmental disruption, the Dutch Government imposed 
a nine-month moratorium on large data centers in the country in order 
to stand up regulations.131 
• Tech companies should be required to track and publish the amount 
of energy and resources their models and data centers are using. 
• Conferences should require tracking of resources to develop and run 
a system. 
• Academic researchers should be given equitable access to 
computational resources. As of now, academics do not have sufficient 
access to understand the specifics of how modern AI tools work and 
what resources they require. This knowledge is hoarded inside large 
tech companies. Without this knowledge and access, the focus is kept 
on profit/accuracy, not environmental concerns. Sunshine is the best 
disinfectant, and academics who understand computer science are a 
useful window to let the sunshine in.
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
44 
 
 
Labor Manipulation, 
Theft, and 
Displacement 
BACKGROUND AND RISKS 
Recent clickbait headlines playing into fears and hype trumpet that 
generative AI is coming for people’s jobs.132 While generative AI will disrupt 
the way certain industries work, it is still too early to see how this technology 
will impact labor markets and integrate into existing work. 
When it comes to labor and market dominance, large tech companies like 
Apple, Meta, Amazon, Google, and Microsoft employ much of the AI 
research and development industry. These companies are directing this 
specialized workforce to develop commercial AI products that can be used 
for private profit rather than public benefit.  
Major tech companies have also been the dominant players in developing 
new generative AI systems because training generative AI models requires 
massive swaths of data, computing power, and technical and financial 
resources. Their market dominance has a ripple effect on the labor market, 
affecting both workers within these companies and those implementing 
their generative AI products externally. With so much concentrated market 
power, expertise, and investment resources, these handful of major tech 
companies employ most of the research and development jobs in the 
generative AI field. The power to create jobs also means these tech 
companies can slash jobs in the face of economic uncertainty. And 
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
45 
 
 
externally, the generative AI tools these companies develop have the 
potential to affect white-collar office work intended to increase worker 
productivity and automate tasks. 
GENERATIVE AI IN THE WORKPLACE 
The development of AI as whole is changing how companies design their 
workplace and business models. Generative AI is no different. Time will tell 
whether and to what extent employers will adopt, implement, and integrate 
generative AI in their workplaces—and how much it will impact workers. 
Still, early signs suggest that generative AI will change white-collar work. 
Many white-collar workers have already begun to embrace generative AI to 
help with daily tasks like drafting presentations, marketing materials, 
speeches, emails, conducting research, and even coding. A Fishbowl survey 
found that 43% of working professionals have used generative AI tools to 
complete tasks at work and 70% of those respondents do so without their 
boss’s knowledge.133 News outlets and websites have used ChatGPT to 
write whole or part of articles.134 Hiring managers are turning to generative 
AI to help write job descriptions and draft interview questions, among other 
administrative tasks.135 Lawyers are using generative AI to do research, 
administrative tasks, and even draft contracts.136 And in medicine, physicians 
are using generative AI for research and summarizing patient visits.137  
The proliferation of generative AI has also created a demand for workers 
with experience using the tools and entirely new jobs built around these 
tools. According to a ResumeBuilder.com study, nine out of ten surveyed 
companies are currently seeking workers with ChatGPT experience.138 The 
rise in generative AI tools has also created a growing demand for “prompt 
engineers”—people who train AI chatbots to test and improve answers or 
otherwise facilitate better prompt-inputs for large language models like 
ChatGPT.139 In fact, there is already a prompt database where people can 
sell their own prompts to produce better results.140  
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
46 
 
 
Not all employers are jumping on the generative AI bandwagon. Some 
workplaces are cautious to quickly adopt this technology due to concerns 
about reliability, as the technology sometimes responds to prompts with 
misinformation or wrong answers. Other employers have expressed concern 
over security risks and restricted employee use. Workplaces like J.P. 
Morgan, Chase & Co., Bank of America, Citigroup, and Verizon prohibited 
employees from using ChatGPT.141 Samsung banned generative AI tools 
after employees uploaded sensitive data to ChatGPT, expressing concern 
that the data transmitted is stored on external servers where it is difficult to 
retrieve or delete and could be leaked to others.142  
The overall effect of generative AI on the economy remains to be seen. 
Some experts have said unregulated and freely deployed generative AI can 
cause harm to competition, push down wages, and lead to excessive 
automation and inequality.143 But when discussing potential risks of 
generative AI on labor, there needs to be a distinction between whether 
generative AI tools lead to automation or augmentation of job roles. Since 
the 1980s, a significant portion of income inequality has been driven by 
automation.144 When generative AI is used for automation, potential risks 
include job loss as well as the devaluation of labor and heightened 
economic inequality.  
JOB AUTOMATION INSTEAD OF AUGMENTATION 
There are both positive and negative aspects to the impact of AI on labor. A 
White House report states that AI “has the potential to increase productivity, 
create new jobs, and raise living standards,” but it can also disrupt certain 
industries, causing significant changes, including job loss.145 Beyond risk of 
job loss, workers could find that generative AI tools automate parts of their 
jobs—or find that the requirements of their job have fundamentally changed.  
The impact of generative AI will depend on whether the technology is 
intended for automation (where automated systems replace human work) or 
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
47 
 
 
augmentation (where AI is used to aid human workers). For the last two 
decades, rapid advances in automation have resulted in a “decline in labor 
share, stagnant wages[,] and the disappearance of good jobs in many 
advanced economies.”146 AI used exclusively for automation could 
exacerbate these negative trends.147 
Some studies suggest that AI may lead to reduced hiring if the technology 
replaces many routine tasks previously performed by workers.148 But other 
studies suggest that AI could create new opportunities—particularly in high-
skilled jobs—and increase worker productivity.149 Proponents of generative 
AI say that technology like ChatGPT can automate repetitive tasks and free 
more time for people to focus on complex or creative tasks. However, 
employers trying to reduce costs, maximize profits, and increase 
shareholder value are more likely to prioritize AI technology that automates 
rather than augments work.  
While it is still too early to determine whether AI will either significantly 
devalue or fully replace workers, preliminary research shows that generative 
AI does impact job-related tasks. According to research by OpenAI, “80% of 
the U.S. workforce could have at least 10% of their work tasks affected” by 
large language models, and this effect is projected to span all wage levels 
across industries.150 The OpenAI paper also found that “approximately 19% 
of workers may see at least 50% of their tasks impacted.”151  
A Goldman Sachs report states that generative AI could impact as much as 
300 million jobs.152 Generative AI could substitute a quarter of current work, 
with white-collar workers in administrative and legal sectors most likely to be 
affected.153 The Goldman Sachs report also shows that AI will impact the 
labor market more generally, but the report emphasizes that the impact 
depends greatly on the technology’s capabilities and how it is adopted. 
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
48 
 
 
DEVALUATION OF LABOR & HEIGHTENED 
ECONOMIC INEQUALITY 
Technological advancement to accelerate productivity, automate jobs, and 
increase profitability by reducing costs began way before the generative AI 
boom. Historically, automation is one of the clearest factors in wage decline. 
According to a White House report, much of the development and adoption 
of AI is intended to automate rather than augment work.154 The report notes 
that a focus on automation could lead to a less democratic and less fair 
labor market.155  
Consider the potential labor impacts that generative AI is having in the 
software engineering industry, where many start-ups are using GPT-4 to 
spend less on human programmers.156 While generative AI will not replace 
all software engineers anytime soon, it will impact the accessibility of 
learning code, how much programmers’ services cost, and how in-demand 
human programmers are.157 Beginner coders could benefit from using 
generative AI to help them learn code, but more experienced programmers 
may find the value of their labor decrease with increased competition.  
In 2021, OpenAI CEO Sam Altman predicted that there would be an 
“unstoppable” technological AI revolution where the price of many types of 
labor “will fall toward zero once sufficiently powerful AI ‘joins the 
workforce.’”158 Altman elaborates that, since labor is the driving cost of the 
supply chain, AI performing tasks will lower the cost of goods and 
services.159 He acknowledged that, if public policy does not adapt to such a 
predicted revolution, “most people will end up worse off than they are 
today.” This prediction shows how the CEO of the leading generative AI 
company is viewing the future—a future where economic inequality is 
accelerated by AI.  
In addition, generative AI fuels the continued global labor disparities that 
exist in the research and development of AI technologies. Outsourcing labor 
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
49 
 
 
to subcontractors in the Global South for the benefit of the Global North is a 
problem inherent in the tech industry—and the entire global economic 
ecosystem more broadly. Labor that is deemed simple and routine is often 
outsourced to locations where workers are forced into terrible working 
conditions with low wages. The AI supply chain reflects and reproduces the 
inequities of imperial colonialism, where the Global North, wielding greater 
economic power, profit from the proliferation of AI technology while 
excluding the Global South.160  
The development of AI has always displayed a power disparity between 
those who work on AI models and those who control and profit from these 
tools.161 Overseas workers training AI chatbots or people whose online 
content has been involuntarily fed into the training models do not reap the 
enormous profits that generative AI tools accrue.162 Instead, companies 
exploiting underpaid and replaceable workers or the unpaid labor of artists 
and content creators are the ones coming out on top. The development of 
generative AI technologies only contributes to this power disparity, where 
tech companies that heavily invest in generative AI tools benefit at the 
expense of workers. For instance, OpenAI is projected to make $1 billion in 
revenue by 2024.163  
But collective worker action around AI is growing. For example, over 150 
content moderation and data label employees in Africa recently voted to 
unionize.164 Even more, the Writers Guild of America went on strike partly 
over the studios refusal to negotiate on banning the use of AI to generate 
scripts and using the writers’ written work to train AIs.165 
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
50 
 
 
HARMS 
• Economic/Economic Loss/Loss of Opportunity: Outsourcing labor to 
generative AI may lead to job loss and job replacement on a global 
scale, including impacting jobs that are currently being outsourced to 
other countries. 
• Autonomy/Loss of Opportunity: Entire industries may be affected by 
workplace demands for generative AI, meaning that those specializing 
in certain industries may be unable to find work and have to shift to 
new venues, possibly meaning education, training, and experience in 
that field is “wasted.” 
EXAMPLES 
• Sama, a San Francisco-based firm, hires workers in Uganda, Kenya, 
and India to label data for tech companies like Microsoft, Meta, and 
Google.166 OpenAI outsourced work to Sama where Sama paid Kenyan 
workers less than $2 per hour to label data to help make ChatGPT 
less toxic.167 The company subjected workers to traumatic content 
moderation practices where workers read and labeled textual 
descriptions of hate speech, violence, and sexual abuse.168 Workers 
became mentally scarred by the distressing nature of the work, with 
one Sama worker describing it as “torture.”169 The traumatizing nature 
of the work led Sama to eventually end its relationship with OpenAI in 
February 2022, ceasing work eight months early.170 
• In a survey of 1,000 U.S. business leaders by ResumeBuilder.com, half 
of companies surveyed are using ChatGPT while 30% plan to and 48% 
have replaced workers with ChatGPT.171 
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
51 
 
 
• In January 2023, BuzzFeed said it would use ChatGPT to create 
quizzes and personalized content for its readers and employees 
expressed concern about whether this move would lead to a 
reduction in the workforce.172 At that time Buzzfeed contended that it 
remains “focused on human-generated journalism in its newsroom,”173 
but since then BuzzFeed shut down its news division as part of a 15% 
reduction of its workforce.174 
INTERVENTIONS 
REDISTRIBUTE POWER AND PROFITS AMONG ALL 
PARTICIPANTS 
• Workplaces should not use generative AI as a means to cheapen labor 
costs and devalue workers’ contributions. In fact, wages should be 
increased to match the increased worker productivity from generative 
AI.  
• Tech companies should include voices and give decision making 
power to those who are actually working on the development and 
training of generative AI, especially those in the Global South. Major 
companies need to elevate the involvement and participation of 
workers to ensure equity. 
• Technology vendors and service providers should invest in AI 
research and development that improves worker productivity rather 
than replacing job functions. 
§ If workplaces benefit economically from generative AI, companies 
should share in the profits with those whose labor they benefit from 
instead of concentrating it among shareholders and top earners. 
INVEST IN PEOPLE 
• Employers should invest in training and job transition services where 
they are training workers in new skills for jobs that have been changed 
by generative AI. 
Labor Manipulation, Theft, and Displacement 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
52 
 
 
• Employers should invest in training where there is a growing demand 
for new jobs that have been created by generative AI (e.g., prompt 
engineers, machine managers, AI auditors, and AI trainers). 
• Companies should implement policy programs where there is a 
commitment to invest in training to retain labor rather than to cut costs 
by reducing staffing in favor of generative AI technology. 
• Companies, local and federal government, and other public-private 
programs should make commitments to invest in resources that help 
workers displaced by generative AI find alternative jobs. 
INVEST IN COMPLEMENTARY AI 
• Workplaces should be investing and implementing generative AI that 
augments and complements work rather than replaces work. 
• Tech companies should invest in AI research and development that 
improves worker productivity rather than replacing job functions. 
• Policymakers should regulate and redirect generative AI research to 
develop technology for public-interest use cases rather than for 
primarily commercial-use cases. 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
53 
 
 
Spotlight: 
Discrimination  
Artificial intelligence and other automated decision-making systems have 
long been deployed in opaque and unaccountable ways that harm 
individuals and exacerbate existing biases. Because AI is trained on 
historical data and often used by the resource controlling actor (hiring 
company, landlord, government benefits agency), Black people, women, 
individuals with disabilities, and poor people are hardest hit. And the harm 
isn’t trivial—algorithmic systems have landed innocent Black men in jail, 
given lower credit limits to women, higher interest rates to graduates of 
Historically Black or Latino Colleges, and prevented people from receiving 
interviews or job offers. 
An image generator is more likely to show a woman when you ask it to 
generate an image of a cleaner, and white men when you ask it to generate 
an image of a boss. Google’s Bard text generator has replicated dangerous 
conspiracy theories. It recommended conversion therapy for gay people, 
generated text saying that Trans people are “groomers,” and generated text 
claiming that major parts of the holocaust was fabricated.175  
Generative AI is not appropriate for use in determinations for important life 
opportunities, but the public must remain vigilant in identifying inappropriate 
use of AI for these purposes – such as a Chatbot that stands as an arbiter 
for people on criminal justice or social services websites.  
Discrimination is at the heart of every risk outlined in this paper, and the 
negative effects of security breaches, privacy violations, and environmental 
impacts will be felt most closely by marginalized communities. 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
54 
 
 
The Potential 
Application of 
Products Liability Law 
BACKGROUND AND RISK  
Products liability is an area of law that developed throughout the twentieth 
century to respond to the harms that mass-produced products can impose 
at scale on society. This area of law focuses on three main harms: 
defectively designed products, defectively manufactured products, and 
defectively marketed products. Products liability law is characterized by two 
elements: (i) its adaptability and ability to evolve to address new types of 
products and harms, and (ii) its concern with distinguishing blameworthily 
harmful products from products which did harm but could not have been 
designed, manufactured, or marketed differently—or those that were simply 
used in an unreasonable way. 
Like manufactured items like soda bottles, mechanized lawnmowers, 
pharmaceuticals, or cosmetic products, generative AI models can be viewed 
like a new form of digital products developed by tech companies and 
deployed widely with the potential to cause harm at scale. For example, 
generative AI products can cause harm to people’s reputations by defaming 
them, directly abuse or facilitate abuse against people, violate intellectual 
property rights, and violate consumers’ privacy. 
The Potential Application of Products Liability Law 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
55 
 
 
Products liability evolved because there was a need to analyze and redress 
the harms caused by new, mass-produced technological products. The 
situation facing society as generative AI impacts more people in more ways 
will be similar to the technological changes that occurred during the 
twentieth century, with the rise of industrial manufacturing, automobiles, and 
new, computerized machines. The unsettled question is whether and to 
what extent products liability theories can sufficiently address the harms of 
generative AI. 
So far, the answers to this question are mixed. In Rodgers v. Christie (2020), 
for example, the Third Circuit ruled that an automated risk model could not 
be considered a product for products liability purposes because it was not 
“tangible personal property distributed commercially for use or 
consumption.”176 However, one year later, in Gonzalez v. Google, Judge 
Gould of the Ninth Circuit argued that “social media companies should be 
viewed as making and ‘selling’ their social media products through the 
device of forced advertising under the eyes of users.”177 Several legal 
scholars have also proposed products liability as a mechanism for 
redressing harms of automated systems.178 As generative AI grows more 
prominent and sophisticated, their harms—often generated automatically 
without being directly prompted or edited by a human—will force courts to 
consider the role of products liability in redressing these harms, as well as 
how old notions of products liability, involving tangible, mechanized 
products and the companies that manufacture them, should be updated for 
today’s increasingly digital world.179 
HARMS 
• 
Physical: Generative AI may produce false information about 
individuals, leading to physical violence and danger, or could hold 
information that individuals are trying to delete for their own safety. 
The Potential Application of Products Liability Law 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
56 
 
 
• 
Economic/Economic Loss/Loss of Opportunity: Generative AI leads 
to loss of income for artists whose style is mimicked and may cause 
job loss or shrinking of opportunities for work in certain industries, with 
no redress for individuals absent some form of liability. 
• 
Reputational/Relationship/Social Stigmatization: Spread of incorrect 
information about an individual can severely damage their reputation, 
relationships, and dignity. 
• 
Psychological: Impacts of generative AI harms may cause emotional 
distress, fear, helplessness, frustration, and other serious emotional 
harm. 
INTERVENTIONS  
• Scholars, policymakers, and plaintiffs’ attorneys should explore ways 
that common law and statutory products liability law regimes can apply 
to redress generative AI harms. Products liability law itself may prevail, 
or instead a new doctrine based on some of its tenets, but either way, 
private people must have a remedy when they are harmed by 
generative AI.  
 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
57 
 
 
Exacerbating 
Market Power and 
Concentration 
BACKGROUND AND RISK 
Developing, training, using, and maintaining generative AI tools is a 
resource intensive endeavor. In addition to the environmental costs 
discussed in the Environmental Impact section above, generative AI tools 
cost an incredibly large amount of money and computing resources to 
develop and maintain. To maintain the underlying computing power 
necessary to run ChatGPT, for example, experts have estimated that OpenAI 
must spend roughly $700,000 a day,180 leading to an estimated $540 million 
loss for OpenAI181 in 2022 alone. To compensate for this loss, OpenAI 
sought and received an investment from Microsoft of over $10 billion dollars, 
which included critically necessary and expensive cloud hosting services 
using Microsoft Azure.182 OpenAI is reported to be seeking additional 
investments of $100 billion dollars as well. And it will cost Alphabet an 
estimated $20 million in computing costs to train its massive, 540-billion 
parameter language model, PaLM (Pathways Language Model).183 
The astronomical cost of large-scale AI models means that only the biggest 
tech companies can handle—and afford—both the rapidly expanding needs 
of maintaining and controlling the models and the public relations and 
lobbying needs that recent generative AI advances require. One example of 
the entrenched power influencing public opinion about generative AI is the 
Exacerbating Market Power and Concentration 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
58 
 
 
prevalence of the word, “foundational,” used to describe large models like 
GPT-4 and LAION B-5, among others. As the AI Now Institute explains, the 
term “foundational” was introduced by Stanford University in early 2022, in 
the wake of the publication of an article listing the many existential harms 
associated with large language models. Calling these models “foundational’’ 
aimed to equate them (and those espousing them) with unquestionable 
scientific advancement, a steppingstone on the path to “artificial general 
intelligence” (AGI)—another fuzzy term evoking science-fiction notions of 
replacing or superseding human intelligence. By describing generative AI 
tools as foundational scientific advances, tech companies and AI evangelists 
frame the wide-scale adoption of generative AI as inevitable. 
In addition, many of the leading generative AI tools, as well as the training 
methods and cloud computing services that support them, are owned and 
maintained by a select few tech companies, including Amazon, Google, and 
Microsoft. The dominance of these few companies in not only developing 
generative AI but also providing the underlying tools and services that 
generative AI requires, further concentrates a market that, despite 
promoting “open source” technologies, is captured by a few powerful 
companies with opaque AI development methods and incentives to restrict 
competition. 
HARMS 
• Economic/Economic Loss/Loss of Opportunity: Concentration of 
power in only a few large companies means that any individual who 
either does not wish to work for those specific companies or has been 
rejected by those companies may be unable to work in the generative 
AI industry altogether. 
• Autonomy: Big Tech’s monopoly over generative AI lessens the ability 
for competitors to develop or for others to have access to necessary 
resources. 
Exacerbating Market Power and Concentration 
  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
59 
 
 
• Autonomy: Choice is necessarily limited with fewer actors in the 
space. 
• Autonomy/Discrimination: Any problems with data quality will be 
exacerbated through re-use and spread among the few dominant 
players.  
• Discrimination: Any discriminatory behaviors in hiring or the 
workplace by Big Tech companies will more directly and strongly 
impact the field due to the limited opportunity to change employer or 
protest treatment and remain working in the field. 
EXAMPLES 
• The Wall Street Journal illustrates how the generative AI “race” will 
make Google and Microsoft richer and “even more powerful.”184  
• The Federal Trade Commission is launching an inquiry into the 
Business Practices of Cloud Computing Providers. 
INTERVENTIONS 
• Enact laws that provide additional resources to and bolster authorities 
of Antitrust enforcers. 
• Advocates and commentators should explicitly tie the data and 
computing resource advantage in coverage of the industry. 
• Reform merger guidelines to reflect how consolidation of data 
advantages is considered in Antitrust reviews. 
• Advocates and reporters should refrain from emboldening the “Arms 
Race” dynamic with China propagated largely from interested industry 
actors.
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
60 
 
 
Recommendations 
LEGISLATIVE AND REGULATORY 
§ Enact a law that makes intimidating, deceiving, or deliberately 
misinforming someone about an election or candidate illegal 
(regardless of the means), such as the Deceptive Practices and Voter 
Intimidation Prevention Act.  
§ Pass the American Data Privacy Protection Act – The ADPPA will limit 
the collection and use of personal information to that which is 
reasonably necessary and proportionate to the purpose for which the 
information was collected. Such limitation will limit improper secondary 
uses of personal data, such as cross-site tracking and 
targeting/profiling based on sensitive data. The ADPPA will also 
restrict the use of personal data to train generative AI systems that can 
manipulate users. 
§ Provide additional resources to antitrust law enforcement agencies to 
adequately monitor and take enforcement action against violations 
related to concentration of the data and computer markets. 
§ Impose a data minimization standard through legislative or regulatory 
means that would limit the use of personal information for generative 
AI training. 
§ Enact legislation that requires both government and commercial use 
of AI to be provably nondiscriminatory and proactively transparent by 
mandating audits and impact assessments—and prohibit manipulative 
or otherwise unacceptably risky uses. Both the White House AI Bill of 
Rights and the National Institute of Standards & Technology’s AI RMF 
provide helpful frameworks for these requirements. 
Recommendations  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
61 
 
 
§ Do not provide broad immunity (under Section 230 or otherwise) for 
companies or operators of Generative AI tools. 
§ Do not provide legislative or regulatory exemptions for copyright 
infringement when images are used in AI training. 
§ Do not invest more money in the development of AI without 
dedicating comparable resources to evaluation professionals, control 
mechanisms, and enforcement capabilities. 
§ Ensure that entities using AI outputs are held jointly responsible with 
entities behind the generation of those outputs for the harm that the 
entity using AI has caused with those outputs. 
ADMINISTRATIVE AND ENFORCEMENT 
§ Continue to use existing consumer protection authorities, including 
Unfair and Deceptive Acts or Practices (FTC) and Unfair, Deceptive, or 
Abusive Acts or Practice (CFPB) authorities, to protect against 
manipulative, deceptive, and unfair AI practices. 
§ Establish standards through advisory opinions and policy statements 
for evaluating intellectual property and other claims relating to 
generative AI (e.g., copyright, trademark, etc.). 
§ Require the publication of environmental footprints of Generative AI 
models and their use. 
§ Secure injunctive relief to halt the operation of generative AI systems 
that lack necessary safeguards (as seen in Italy using GDPR). 
§ Promulgate rules that require both government and commercial uses 
of AI to be provably nondiscriminatory and proactively transparent by 
mandating audits and impact assessments—and prohibit manipulative 
or otherwise unacceptably risky uses. Both the White House AI Bill of 
Rights and the National Institute of Standards & Technology’s AI RMF 
provide helpful frameworks for these requirements. 
Recommendations  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
62 
 
 
PRIVATE ACTOR BEHAVIOR 
§ Entities considering using generative AI procurement should critically 
examine whether these tools are appropriate. 
§ Entities should proactively document the data lifecycle and implement 
data audit trails. 
§ Individuals, companies, and research teams should develop tools to 
detect protected information within training models – like Glaze. 
§ Develop tools to detect deepfakes and make those tools easily 
accessible and usable by the public to help debunk deepfakes 
quickly. 
§ Watermark any protected documents or images to prevent or limit 
their use in the training of AI models. 
§ Publish the data sources, training sets, and logic of AI systems. 
§ Limit the scope of permissible external uses and modifications of 
generative AI models (including through API access).  
§ Limit permissible uses of Generative AI to low-risk settings. 
§ Determine and publish environmental footprints for Generative AI 
models and their use. 
§ Employers should invest in training workers in new skills for jobs that 
have been changed by generative AI. 
§ Employers should invest in training where there is a growing demand 
for new jobs that have been created by generative AI (e.g., prompt 
engineers, machine managers, AI auditors, and AI trainers). 
§ Companies should invest in training to retain labor rather than cutting 
costs by reducing staff in favor of generative AI technology. 
§ Companies, governments, and public-private programs should commit 
resources to helping workers displaced by generative AI find 
alternative jobs. 
Recommendations  
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
63 
 
 
§ Technology vendors and service providers should invest in AI 
research and development that improves worker productivity rather 
than replacing job functions. 
§ Technology companies should include the voices of, and give 
decision-making power to, those who are actually working on the 
development and training of generative AI, especially those in the 
Global South. Major companies need to elevate the involvement and 
participation of workers to ensure equity. 
§ Share profits with those whose labor helped build the systems rather 
than concentrating it among shareholders and top earners. 
§ Wages should be increased to match the increased worker 
productivity from generative AI. Workplaces should not use generative 
AI as a means to cheapen labor costs and devalue workers’ 
contributions. 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
64 
 
 
Appendix of Harms 
Algorithmic harms exist 
today—and have been 
around for a long time. 
However, with the 
introduction of generative AI 
tools like ChatGPT, the 
scope and severity of 
algorithmic harms have 
exploded. In addition to 
unique harms posed by 
violations of data privacy 
and algorithmic systems, 
generative AI will accelerate 
the disintegration of trust in 
authoritative sources of information, exacerbate existing harms like IP theft 
and impersonation, and undermine existing legal protections for those 
harmed. 
This Appendix is meant to give you a better sense of the universe of harms 
that generative AI is causing or exacerbating right now. However, AI is 
innovating at a rapid pace and new examples of harm emerge every day, so 
encapsulating every potential harm that generative AI could cause would be 
impossible. This appendix serves as a snapshot of pressing and real harms 
caused by generative AI today, rather than a comprehensive analysis of all 
possible harms. 
Appendix of Harms  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
65 
 
 
This appendix includes: 
1. Definitions for many common harms caused by generative AI. 
2. Examples of real-world harms caused by generative AI. 
3. A Table comparing the harms implicated by each example. 
COMMON AI HARMS 
1. Physical Harms: These are harms that lead to bodily injury or death, 
which may include acts by AI companies that facilitate or encourage 
physical assault.  
2. Economic Harms: These are harms that cause monetary losses or 
decrease the value of something, which may include the harms of 
fraudulent transactions conducted by those using AI to impersonate a 
victim.  
3. Reputational Harms: These harms involve injuries to someone’s 
reputation within their community, which may in turn result in lost 
business or social pariahdom.  
4. Psychological Harms: These harms include a variety of negative—and 
legally cognizable—mental responses, such as anxiety, anguish, 
concern, irritation, disruption, or aggravation. Danielle Citron and 
Daniel Solove place these harms within two categories: emotional 
distress or disturbance.  
5. Autonomy Harms: These harms restrict, undermine, or otherwise 
influence people’s choices and include acts like coercion, 
manipulation, failing to inform someone, acting in ways that undermine 
a user’s choices, and inhibiting lawful behavior.  
6. Discrimination Harms: These are harms that entrench or exacerbate 
inequality in ways that disadvantage certain people based on their 
demographics, characteristics, or affiliations. Discrimination harms 
often lead to other types of AI harms. 
7. Relationship Harms: These harms involve damaging personal or 
professional relationships in ways that negatively impact one’s health, 
Appendix of Harms  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
66 
 
 
wellbeing, or functioning in society. Often, these harms damage 
relationships by degrading trust or damaging social boundaries.  
8. Loss of Opportunity: Related to economic, reputational, 
discrimination, and relationship harms, loss of opportunity is an 
especially common AI harm in which AI-mediated content or decisions 
serve as a barrier to individuals accessing employment, government 
benefits, housing, and educational opportunities.  
9. Social Stigmatization and Dignitary Harms: Related to reputational, 
discrimination, and relationship harms, these harms undermine 
individuals’ sense of self and dignity through, e.g., loss of liberty, 
increased surveillance, stereotype reinforcement, or other negative 
impacts on one’s dignity.  
REAL EXAMPLES OF HARM 
1. Suicide: ChatGPT encouraged an individual to commit suicide.  
2. Impersonation: Scammers used generative AI to trick a woman into 
thinking her daughter was kidnapped, demanding $1,000,000 in 
return for her release. 
3. Deepfakes: A prominent investigative reporter was ridiculed online 
after a pornographic deepfake of her went viral online. 
4. Defamation: ChatGPT falsely included a law professor on a list of 
professors accused of sexual assault. 
5. Sexualization: Lensa, an AI image generation application, portrayed 
women—particularly Asian and Black women —in a hypersexualized 
manner regardless of the source photos provided. 
6. Threats of Physical Harm: An individual used ChatGPT to designate 
whether a person originating from different countries of origin should 
be tortured or not. 
7. Misinformation: In Turkey’s election, generative AI was used to 
spread over 150 unwarranted claims of terrorism by a presidential 
candidate. 
Appendix of Harms  
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
67 
 
 
8. Copyright Infringement: Parts of artists’ work are routinely mimicked 
or duplicated by AI image generators, including commercially 
protected art. 
9. Labor Disputes: Studios have threatened to use generative AI to 
replace striking writers, undermining labor negotiations. 
10. 
Data Breaches: A viral generative AI tool’s lax security practices and 
maintenance of personal data led to personal information like name, 
prompts, and email are exposed. 
 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
68 
 
 
References 
 
1 Danielle K. Citron & Daniel J. Solove, Privacy Harms, 102 B.U. L. Rev. 793 (2022). 
2 See Joy Buolamwini & Timnit Gebru, Gender Shades: Intersectional Accuracy 
Disparities in Commercial Gender Classification, 81 Proc. Mach. Learning Rsch. 1 (2018); 
Gender Shades Project, http://gendershades.org/overview.html. 
3 See, e.g., Emily M. Bender et al., On the Dangers of Stochastic Parrots: Can Language 
Models Be Too Big?, Proc. 2021 ACM Conf. on Fairness, Accountability, & Transparency 
610 (2021). 
4 Press Release, FTC, New Data Shows FTC Received 2.8 Million Fraud Reports from 
Consumers in 2021 (Feb. 22, 2022), https://www.ftc.gov/news-events/news/press-
releases/2022/02/new-data-shows-ftc-received-28-million-fraud-reports-consumers-
2021-0. 
5 Pranshu Verma, They Thought Loved Ones Were Calling for Help. It was an AI Scam., 
The Washington Post (Mar. 5, 2023), 
https://www.washingtonpost.com/technology/2023/03/05/ai-voice-scam/; Erielle Reshef, 
Kidnapping Scam Uses Artificial Intelligence to Clone Teen Girl’s Voice, Mother Issues 
Warning, ABC News (Apr. 13, 2023), https://abc7news.com/ai-voice-generator-artificial-
intelligence-kidnapping-scam-detector/13122645/.  
6 See National Consumer Law Center & EPIC, Scam Robocalls: Telecom Providers Profit 
(2022), https://epic.org/documents/scam-robocalls-telecom-providers-profit/. 
7 See TrueCaller, 2022 U.S. Spam & Scam Report (2022), 
https://www.truecaller.com/blog/insights/truecaller-insights-2022-us-spam-scam-report 
(noting that “[t]he total money lost to scams is also comparable to the entire child care 
budget of $39 billion for the American Rescue Plan Act. If phone scam fraud was 
somehow eliminated, the amount saved could fund federally subsidized child care across 
the U.S. for a full year to help families and employers.”). The same source reported $29.8 
billion in actual consumer losses in 2021 and $19.7 billion in losses in 2020, an increase 
of nearly $10 billion every year since 2019. 
8 Reported losses from text scams more than doubled from $131M to $330M between 
2021 and 2022. FTC Consumer Sentinel Network, Fraud Reports by Contact Method, 
Reports & Amount Lost by Contact Method (2023), 
https://public.tableau.com/app/profile/federal.trade.commission/viz/FraudReports/FraudF
acts (""Losses & Contact Method” tab selected, with quarters 1 through 4 checked for 
2021, 2022). 
9 Lily Hay Newman, AI Wrote Better Phishing Emails Than Humans in a Recent Test, 
Wired (Aug. 7, 2021), https://www.wired.com/story/ai-phishing-emails/. 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
69 
 
 
References 
 
10 Pranshu Verma & Will Oremus, ChatGPT Invented a Sexual Harassment Scandal and 
Named a Real Law Prof as the Accused, Wash. Post (Apr. 5, 2023), 
https://www.washingtonpost.com/technology/2023/04/05/chatgpt-lies/. 
11 Julia Angwin, Decoding the Hype About AI, Markup (Jan. 28, 2023), 
https://themarkup.org/hello-world/2023/01/28/decoding-the-hype-about-ai. 
12 See, e.g., James Vincent, The Swagged-out Pope is an AI Fake—and an Early Glimpse 
of a New Reality, Verge (Mar. 27, 2023), 
https://www.theverge.com/2023/3/27/23657927/ai-pope-image-fake-midjourney-
computer-generated-aesthetic. 
13 Danielle Citron and Robert Chesney have called attempts to sow distrust in real 
information using the specter of generative AI—and the increasing success that 
perpetrators would have as the public grows more aware of generative AI—the “Liar’s 
Dividend.” Danielle K. Citron & Robert Chesney, Deep Fakes: A Looming Challenge for 
Privacy, Democracy, and National Security, 107 Cal. L. Rev. 1753,22 1785–86 (2019). 
14 See Ashley Belanger, Thousands Scammed by AI Voices Mimicking Loved Ones in 
Emergencies, Ars Technica (Mar. 6, 2023), https://arstechnica.com/tech-
policy/2023/03/rising-scams-use-ai-to-mimic-voices-of-loved-ones-in-financial-distress/. 
15 OpwnAI: Cybercriminals Starting to Use ChatGPT, Check Point Rsch. (Jan. 6, 2023), 
https://research.checkpoint.com/2023/opwnai-cybercriminals-starting-to-use-chatgpt/. 
16 Joseph Cox, A Computer Generated Swatting Service is Causing Havoc Across 
America, Vice (Apr. 13, 2023), https://www.vice.com/en/article/k7z8be/torswats-
computer-generated-ai-voice-swatting. 
17 Pranshu Verma, They Thought Loved Ones Were Calling for Help. It was an AI Scam., 
Wash. Post (Mar. 5, 2023), https://www.washingtonpost.com/technology/2023/03/05/ai-
voice-scam/.  
18 Matthew Gault, AI Spam is Already Flooding the Internet and It Has an Obvious Tell, 
Vice (Apr. 24, 2023), https://www.vice.com/en/article/5d9bvn/ai-spam-is-already-flooding-
the-internet-and-it-has-an-obvious-tell. 
19 Igor Bonifacic, CNET Had to Correct Most of its AI-Written Articles, Engadget (Jan. 25, 
2023), https://www.engadget.com/cnet-corrected-41-of-its-77-ai-written-articles-
201519489.html. 
20 Jay Peters, BuzzFeed is Using AI to Write SEO-Bait Travel Guides, Verge (Mar. 30, 
2023), https://www.theverge.com/2023/3/30/23663206/buzzfeed-ai-travel-guides-buzzy. 
21 The term, “deepfake,” is a portmanteau of “deep learning” and “fake.” The term was 
popularized by a Reddit user, @deepfakes, who posted the first viral deepfake video in 
2017. See Moncarol Y. Wang, Don’t Believe Your Eyes: Fighting Deepfaked 
Nonconsensual Pornography with Tort Law, 2022 U. Chi. Legal F. 415, 417–18 (2022). 
22 See Citron & Chesney, supra note 1313, at 1757 (defining deepfakes as the “full range 
of hyper-realistic digital falsification of images, video, and audio”). 
23 See Wang, supra note 2121. 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
70 
 
 
References 
 
24 See Anna Yamaoka-Enerklin, Disrupting Disinformation: Deepfakes and the Law, 22 
N.Y.U. J. Legis. & Pub. Pol’y 725, 731 (2020). 
25 See, e.g., Restatement (Second) of Torts § 525 (1977) (fraudulent misrepresentation); 
Colo. Code Regs. § 18-5-113 (2016). 
26 See, e.g., Cal. Penal Code § 528.5; Haw. Rev. Stat. Ann. § 711-1106.6; La. Rev. Stat. § 
14:73.10; Miss. Code Ann. § 97-45-33; N.Y. Penal Law § 190.25; R.I. Gen Laws § 11-52-7.1; 
Tex. Penal Code § 33.07. 
27 See, e.g., 18 U.S.C. § 873; D.C. Code § 22-3252 (2019). 
28 See, e.g., 18 U.S.C. § 2261A. 
29 See Edina Harbinja et al., Governing Ghostbots, 48 Comp. L. & Sec. Rev. 105791 (2023). 
30 See, e.g., Kat Tenbarge, Hundreds of Sexual Deepfake Ads Using Emma Watson’s 
Face Ran on Facebook and Instagram in the Last Two Days, NBC News (Mar. 7, 2023), 
https://www.nbcnews.com/tech/social-media/emma-watson-deep-fake-scarlett-
johansson-face-swap-app-rcna73624; William Turton & Matthew Justus, “Deepfake” 
Videos Like That Gal Gadot Porn are Only Getting More Convincing—and More 
Dangerous, Vice (Aug. 27, 2018), https://www.vice.com/en/article/qvm97q/deepfake-
videos-like-that-gal-gadot-porn-are-only-getting-more-convincing-and-more-dangerous. 
31 See 46 States + DC + One Territory Now Have Revenge Porn Laws, Cyber Civ. Rts. 
Initiative, https://www.cybercivilrights.org/revenge-porn-laws/ (last visited May 15, 2023); 
Orin S. Kerr, Computer Crime Law 245–47 (4th ed. 2018); State Revenge Porn Policy, 
EPIC, https://epic.org/state-revenge-porn-policy/. 
32 See Restatement (Second) of Torts § 652B, 625D. 
33 Id. § 652E. 
34 See, e.g., Cass R. Sunstein, Falsehoods and the First Amendment, 33 Harv. J. L. & 
Tech. 387 (2020).  
35 See White v. Samsung Elecs. Am., Inc., 971 F.2d 1395, 1398 (9th Cir. 1992); Carson v. 
Here’s Johnny Portable Toilets, Inc., 698 F.2d 831, 835 (6th Cir. 1983). 
36 376 U.S. 254, 280 (1964). 
37 485 U.S. 46, 50–52 (1988). 
38 See, e.g., Bollea v. Gawker Media, LLC, No. 522012CA012447, 2016 WL 
4073660 (Fla. Cir. Ct. June 8, 2016) (Hulk Hogan awarded $140 million against 
Gawker on privacy grounds). 
39 See Jeffrey T. Hancock & Jeremy N. Bailenson, The Social Impact of Deepfakes, 24 
Cyberpsych., Behav., & Soc. Networking 149, 150 (2021). 
40 See Riana Pfefferkorn, “Deepfakes” in the Courtroom, 29 Pub. Int. L.J. 245, 259–74 
(2020); Danielle C. Breen, Silent No More: How Deepfakes will Force Courts to 
Reconsider Video Admission Standards, 21 J. High Tech. L. 122, 132, 150–53 (2021). 
41 United States v. Gagliardi, 506 F.3d 140, 151 (2d Cir. 2007) (citing United States v. 
Dhinsa, 243 F.3d 635, 658 (2d Cir. 2001)). 
42 United States v. Workinger, 90 F.3d 1409, 1415 (9th Cir. 1996). 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
71 
 
 
References 
 
43 United States v. Vayner, 769 F.3d 125, 130 (2d Cir. 2014). 
44 Pfefferkorn, supra note 4040, at 260. 
45 Id.; see also, e.g., Fed. R. Evid. 902(4)(A) (“A copy of an official record” is self-
authenticating “if the copy is certified as correct by… the custodian or another person 
authorized to make the certification.”) 
46 Cox, supra note 1616.  
47 Samantha Cole, ‘You Feel So Violated’: Streamer QTCinderella Is Speaking Out 
Against Deepfake Porn Harassment, Vice (Feb. 13, 2023), 
https://www.vice.com/en/article/z34pq3/deepfake-qtcinderella-atrioc; Deepfake Porn 
Booms in the Age of A.I., NBC News (Apr. 28, 2022), 
https://www.nbcnews.com/now/video/deepfake-porn-booms-in-the-age-of-a-i-
171726917562. 
48 Chandler Treon, ‘Please Stop’: Tiktoker Frightened After Being Harassed with AI-
Generated Nudes of Herself, Yahoo News (May 3, 2023), 
https://news.yahoo.com/please-stop-tiktoker-frightened-being-182335652.html. 
49 Joseph Cox, Video Game Voice Actors Doxed and Harassed in Targeted AI Voice 
Attack, Vice (Feb. 13, 2023), https://www.vice.com/en/article/93axnd/voice-actors-doxed-
with-ai-voices-on-twitter. 
50 Citron & Chesney, supra note 13, at 1793; Megan Farokhmanesh, The Debate on 
Deepfake Porn Misses the Point, Wired (Mar. 1, 2023), 
https://www.wired.com/story/deepfakes-twitch-streamers-qtcinderella-atrioc-pokimane/. 
51 Citron & Chesney, supra note 1313, at 1793–94. 
52 See, e.g., Fair Hous. Council v. Roommates.com, LLC, 521 F.3d 1157, 1168 (9th Cir. 
2008) (holding that website that contributes materially to the alleged illegality of user 
content is not shielded from liability under 47 U.S.C. § 230). 
53 Citron & Chesney, supra note 1313, at 1803. 
54 See United States v. Alvarez, 567 U.S. 709, 719 (2012) (plurality) (concluding “falsity 
alone” could not remove expression from First Amendment protection). 
55 Citron & Chesney, supra note 1313, at 1805–06. 
56 47 U.S.C. § 230(c)(1). 
57 The Supreme Court issued an opinion in Gonzalez v. Google, No. 21-1333, vacating the 
Ninth Circuit’s decision but otherwise refused to weigh in on the proper test for Section 
230 protection. The decision essentially leaves in place the status quo, where courts of 
appeals have been steadily converging on a test that is increasingly skeptical of industry 
arguments for Section 230 protection. The emerging test does not perfectly encapsulate 
EPIC’s position on Section 230, but we follow precedent in this section. EPIC has argued 
that Section 230(c)(1) simply means that internet companies are not to be treated the 
same, for liability purposes, as the third parties who publish information on their services. 
Our test will often generate the same outcome as the test that is emerging in the circuit 
courts, but with less room for judicial discretion. See Brief for EPIC as Amicus Curiae in 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
72 
 
 
References 
 
Support of Neither Party, Gonzalez v. Google LLC, 143 S. Ct. 80 (2022) (No. 21-1333), 
https://epic.org/wp-content/uploads/2022/12/EPIC-Amicus-Gonzalez-v.-Google-1.pdf. 
58 See Jess Miers, Yes, Section 230 Should Protect ChatGPT and Other Generative AI 
Tools, TechDirt (Mar. 17, 2023), https://www.techdirt.com/2023/03/17/yes-section-230-
should-protect-chatgpt-and-others-generative-ai-tools/. 
59 See Cristiano Lima, AI Chatbots Won’t Enjoy Tech’s Legal Shield, Section 230 Authors 
Say, Wash. Post (Mar. 17, 2023), https://www.washingtonpost.com/politics/2023/03/17/ai-
chatbots-wont-enjoy-techs-legal-shield-section-230-authors-say/. 
60 Henderson v. Source for Public Data, L.P., 53 F. 4th 110, 125 (4th Cir. 2022); Lemmon v. 
Snap, Inc., 995 F.3d 1085 (9th Cir. 2021). 
61 See, e.g., Roommates.com, 521 F.3d at 1168; HomeAway v. City of Santa Monica, 918 
F.3d 676 (9th Cir. 2019); Airbnb, Inc. v. City and County of San Francisco, 217 F. Supp. 3d 
1066 (N.D. Cal. 2016); Lemmon, 995 F.3d at 1085.  
62 Lemmon, 995 F.3d at 1092 (9th Cir. 2021); see also A.M v. Omegle.com, 614 F. Supp. 
3d 814, 819–21 (D. Or. 2022). 
63 47 U.S.C. § 230(f)(3). 
64 See, e.g., Pranshu Verma and Will Oremus, ChatGPT Created a Sexual Harassment 
Scandal and Named a Real Law Prof as the Accused, Wash. Post (Apr. 5, 2023), 
https://www.washingtonpost.com/technology/2023/04/05/chatgpt-lies/; see also Eugene 
Volokh, Communications Can Be Defamatory Even If Readers Realize There's a 
Considerable Risk of Error, Volokh Conspiracy (Mar. 31, 2023), 
https://reason.com/volokh/2023/03/31/communications-can-be-defamatory-even-if-
readers-realize-theres-a-considerable-risk-of-error/.  
65 See Miers, supra note 5858. 
66 This test originated in Roommates.com, 521 F.3d at 1157, and its latest significant 
articulation is found in Henderson, 53 F.4th at 110. 
67 See, e.g., Henderson, 53 F.4th at 125. 
68 Roommates.com, 521 F.3d at 1168. 
69 The leading case on this issue is Batzel v. Smith, 333 F.3d 1018 (9th Cir. 2003), which 
concerned whether a person who sent an email to a listserv moderator with no intention 
of having the email posted online could be said to have “provided” information as 
contemplated by Section 230. A split Ninth Circuit panel determined that information is 
“provided by” a third party “when a third person or entity that created or developed the 
information in question furnished it to the provider or user under circumstances in which 
a reasonable person in the position of the service provider or user would conclude that 
the information was provided for publication on the Internet or other ‘interactive 
computer service.’” Id. at 1034 (emphasis added). Batzel could be read to require direct 
furnishing of information to the defendant or, at the very least, some sort of relationship 
wherein the defendant could form a reasonable basis to believe that the third party 
intend for the defendant to publish the information. 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
73 
 
 
References 
 
70 Merriam-Webster Dictionary, Provide (2023), https://www.merriam-
webster.com/dictionary/provide.  
71 See discussion of Batzel v. Smith, supra note 6969. 
72 Stratton Oakmont, Inc. v. Prodigy Servs., 23 Media L. Rep. (BNA) 1794 (N.Y. Sup. Ct. 
1995). 
73 See, e.g., O’Korley v. Fastcase, Inc., 831 F.3d 352 (6th Cir. 2016). 
74 Google, Block Search Engine Indexing with noindex, Search Central (Feb. 20, 2023), 
https://developers.google.com/search/docs/crawling-indexing/block-indexing.  
75 Available technical tools to block scrapers, such as robot.txt flags, IP blockers and 
CAPTCHAs, can be bypassed by those determined enough to collect the data, which is 
why companies have attempted to use legal tools such as breach of contract and the 
Computer Fraud and Abuse Act to stop unauthorized scraping. See Kyle R. Dull & Julia B. 
Jacobson, LinkedIn’s Data Scraping Battle with hiQ Labs Ends with Proposed Judgment, 
National Law Review (Dec. 19, 2022), https://www.natlawreview.com/article/linkedin-s-
data-scraping-battle-hiq-labs-ends-proposed-judgment.  
76 For instance, in May 2018, Facebook made public the posts of as many as 14 million 
users that thought they were only sharing with their friends or a smaller group. Kurt 
Wagner, Facebook Says Millions of Users Who Thought They Were Sharing Privately 
with Their Friends May Have Shared with Everyone Because of a Software Bug, Vox 
(June 7, 2018), https://www.vox.com/2018/6/7/17438928/facebook-bug-privacy-public-
settings-14-million-users. A few weeks later, Facebook unblocked users who had been 
previously blocked by other users, allowing the newly unblocked users to view content 
they should not have been per- mitted to view. Kurt Wagner, Facebook’s Year of Privacy 
Mishaps Continues—This Time with a New Software Bug that ‘Unblocked’ People, Vox 
(July 2, 2018), https://www.vox.com/2018/7/2/17528220/facebook-soft-ware-bug-block-
unblock-safety-privacy.  
77 For example, the FTC’s 2011 consent order against Facebook was based, in part, on 
Facebook’s decisions to change privacy settings to make public information users had 
previously set to private. See FTC, Facebook Settles FTC Charges That It Deceived 
Consumers By Failing To Keep Privacy Promises (Nov. 29, 2011), 
https://www.ftc.gov/news-events/news/press-releases/2011/11/facebook-settles-ftc-
charges-it-deceived-consumers-failing-keep-privacy-promises.  
78 Packingham v. North Carolina, 137 S. Ct. 1730, 1735 (2017) (quoting Reno v. American 
Civil Liberties Union, 521 U.S. 844, 868 (1997)). 
79 Ryan Browne, Italy Became the First Western Country to Ban ChatGPT. Here’s What 
Other Countries are Doing, CNBC (Apr. 4, 2023), 
https://www.cnbc.com/2023/04/04/italy-has-banned-chatgpt-heres-what-other-countries-
are-doing.html; Natasha Lomas, ChatGPT Resumes Service in Italy After Adding Privacy 
Disclosures and Controls, TechCrunch (Apr. 28, 2023), 
https://techcrunch.com/2023/04/28/chatgpt-resumes-in-italy/. 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
74 
 
 
References 
 
80 See Record Number of Data Breaches in 2021, IAPP Daily Dashboard (Jan. 25, 2022), 
https://iapp.org/news/a/record-number-of-data-breaches-in-2021/ (citing to ITRC report 
which estimated “1,862 breaches last year, up 68% from the year prior, and exceeded 
2017’s previous record of 1,506”).  
81 See Identity Theft Resource Center (ITRC), 2022 Data Breach Report 2 (Jan. 2023), 
https://www.idtheftcenter.org/publication/2022-data-breach-report/. 
82 U.S. Gov’t Accountability Oﬀ., GAO-14-34, Agency Responses to Breaches of Personally 
Identiﬁable Information Need to be More Consistent 11 (2013), 
http://www.gao.gov/assets/660/659572.pdf. 
83 See id. at 13. 
84 See Soc. Sec. Admin., Identity Theft and Your Social Security Number 1 (2021), 
https://www.ssa.gov/pubs/EN-05-10064.pdf (“A dishonest person who has your Social 
Security number can use it to get other personal information about you. Identity thieves 
can use your number and your good credit to apply for more credit in your name. Then, 
when they use the credit cards and don’t pay the bills, it damages your credit. You may 
not ﬁnd out that someone is using your number until you’re turned down for credit, or 
you begin to get calls from unknown creditors demanding payment for items you never 
bought. Someone illegally using your Social Security number and assuming your identity 
can cause a lot of problems.”) 
85 See Erika Harrell, Bureau of Just. Stat., Dep’t of Just., Victims of Identity Theft, 2018 11 
(Apr. 2020), https://bjs.ojp.gov/content/pub/pdf/vit18.pdf; Danielle K. Citron & Daniel 
Solove, Risk and Anxiety: A Theory of Data Breach Harms, 96 Tex. L. Rev. 737, 745 
(“Knowing that thieves may be using one’s personal data for criminal ends may produce 
signiﬁcant anxiety.”). 
86 See, e.g., Cyber. & Infrastructure Sec. Agency (CISA), DarkSide Ransomware: Best 
Practices for Preventing Business Disruption from Ransomware Attacks, Alert Code AA21-
131A (July 7, 2021), https://www.cisa.gov/news-events/cybersecurity-advisories/aa21-131a 
(describing one example of ransomware-as-a-service); Kaspersky, Malware-as-a-service 
(MaaS), Encyclopedia by Kaspersky, 
https://encyclopedia.kaspersky.com/glossary/malware-as-a-service-maas/ (last visited 
May 15, 2023) (deﬁning the term “malware-as-a-service”); Brian Krebs, Giving a Face to 
the Malware Proxy Service ‘Faceless’, Krebs on Security (Apr. 18, 2023), 
https://krebsonsecurity.com/2023/04/giving-a-face-to-the-malware-proxy-service-
faceless/ (describing a malware proxy service). 
87 See, e.g., Elias Groll, ChatGPT Shows Promise of Using AI to Write Malware, 
CyberScoop (Dec. 6, 2022), https://cyberscoop.com/chatgpt-ai-malware/ (“‘If not 
ChatGPT, then a model in the next couple years will be able to write code for real world 
software vulnerabilities,’ [Dolan-Gavitt, an assistant professor in the Computer Science 
and Engineering Department at New York University.] added…. Benjamin Tan, a computer 
scientist at the University of Calgary, said he was able to bypass some of ChatGPT’s 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
75 
 
 
References 
 
safeguards by asking the model to produce software piece by piece that, when 
assembled, might be put to malicious use. ‘It doesn’t know that when you put it all 
together it’s doing something that it shouldn’t be doing,’ Tan said.”). 
88 See, e.g., Crane Hassold, Executive Impersonation Attacks Targeting Companies 
Worldwide, Abnormal Blog (Feb. 16, 2023), https://abnormalsecurity.com/blog/midnight-
hedgehog-mandarin-capybara-multilingual-executive-impersonation (“Using widely 
available marketing technology and highly accurate translation apps, attackers can 
rapidly scale their efforts, maximizing their reach and wreaking havoc across the globe. 
And because many translation tools now use machine learning to improve context, such 
as translating the meaning of a sentence rather than each word individually, they’re much 
easier to manipulate for nefarious purposes.”) 
89 See, e.g., Center for Strategic & International Studies, A Conversation on Cybersecurity 
with NSA’s Rob Joyce, YouTube (Apr. 11, 2023), https://youtu.be/MMNHNjKp4Gs?t=530 
(8:50 mark) (NSA Dir. of Cybersecurity Rob Joyce describing ChatGPT as able to optimize 
the workflow of bad actors seeking to use zero day exploits, and for malicious foreign 
actors to “craft very believable native-language English text that could be part of your 
phishing campaign or part of your interaction with a person or your ability to build a 
backstory—all the things that will allow you to do those activities, or even malign 
influence.”). 
90 See, e.g., Kate Park, Samsung Bans Use of Generative AI Tools like ChatGPT After 
April Internal Data Leak, TechCrunch (May 2, 2023), 
https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-
chatgpt-after-april-internal-data-leak/; Dan Milmo and Agencies, Italy’s Privacy Watchdog 
Bans ChatGPT Over Data Breach Concerns, Guardian (Apr. 1, 2023), 
https://www.theguardian.com/technology/2023/mar/31/italy-privacy-watchdog-bans-
chatgpt-over-data-breach-concerns. 
91 See, e.g., Marcus Comiter, Attacking Artificial Intelligence: AI’s Security Vulnerability 
and What Policymakers Can Do About It, Harvard Kennedy School Belfer Center for 
Science and International Affairs (Aug. 2019), 
https://www.belfercenter.org/publication/AttackingAI. 
92 See, e.g., Eduard Kovacs, ChatGPT Data Breach Confirmed as Security Firm Warns of 
Vulnerable Component Exploitation, SecurityWeek (Mar. 28, 2023), 
https://www.securityweek.com/chatgpt-data-breach-confirmed-as-security-firm-warns-of-
vulnerable-component-exploitation/. 
93 See, e.g., Mark Gurman, Samsung Bans Staff’s AI Use After Spotting ChatGPT Data 
Leak, Bloomberg (May 1, 2023), https://www.bloomberg.com/news/articles/2023-05-
02/samsung-bans-chatgpt-and-other-generative-ai-use-by-staff-after-leak. 
94 See, e.g., Mitchell Clark & James Vincent, OpenAI is Massively Expanding ChatGPT’s 
Capabilities to Let It Browse the Web and More, Verge (Mar. 23, 2023), 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
76 
 
 
References 
 
https://www.theverge.com/2023/3/23/23653591/openai-chatgpt-plugins-launch-web-
browsing-third-party. 
95 Nat’l Institute of Standards & Tech. (NIST), Artificial Intelligence Risk Management 
Framework (AI RMF 1.0), NIST AI-100-1 (Jan. 2023), https://doi.org/10.6028/NIST.AI.100-1. 
96 American Data Privacy and Protection Act Fact Sheet, EPIC, 
https://epic.org/documents/american-data-privacy-and-protection-act-fact-sheet/ (last 
visited May 15, 2023) (e.g., algorithmic impact assessments). 
97 See, e.g., Intellectual Property Law, Georgetown Law (May 2023), 
https://www.law.georgetown.edu/your-life-career/career-exploration-professional-
development/for-jd-students/explore-legal-careers/practice-areas/intellectual-property-
law/; Explore the Four Areas of IP Law, Suffolk Law (May 2023), 
https://www.suffolk.edu/law/academics-clinics/what-can-i-study/intellectual-
property/intellectual-property-law-basics-certificate/explore-the-four-areas-of-ip-law. 
98 Open Letter, Ctr. for Artistic Inquiry, Restrict AI Illustration from Publishing: An Open 
Letter (May 2, 2023), https://artisticinquiry.org/AI-Open-Letter. 
99 Case Study Footnote: Chris Willman, AI-Generated Fake ‘Drake’/’Weeknd’ 
Collaboration, ‘Heart on My Sleeve,’ Delights Fans and Sets Off Industry Alarm Bells, 
Variety (Apr. 17, 2023), http://variety.com/2023/music/news/fake-ai-generated-drake-
weeknd-collaboration-heart-on-my-sleeve-1235585451/; see also Will Knight, Algorithms 
Can Now Mimic Any Artist. Some Artists Hate It., Wired (August 19, 2022), 
https://www.wired.com/story/artists-rage-against-machines-that-mimic-their-work/; Sarah 
Andersen, The Alt-Right Manipulated My Comic. Then A.I. Claimed It., N.Y. Times 
(December 31, 2022), https://www.nytimes.com/2022/12/31/opinion/sarah-andersen-how-
algorithim-took-my-work.html; Vanessa Thorpe, ‘ChatGPT Said I did Not Exist’: How 
Artists and Writers are Fighting Back Against AI, Guardian (March 18, 2023), 
https://www.theguardian.com/technology/2023/mar/18/chatgpt-said-i-did-not-exist-how-
artists-and-writers-are-fighting-back-against-ai; Rachel Metz, These Artists Found Out 
Their Work Was Used to Train AI. Now They’re Furious, CNN Business (October 21, 
2022), https://www.cnn.com/2022/10/21/tech/artists-ai-images/index.html. 
100 See, e.g., Nick Cave, Issue #218, Red Hand Files (January 2023), 
https://www.theredhandfiles.com/chat-gpt-what-do-you-think/. 
101 Metz, supra note 9999. 
102 U.S. Copyright Office, Libr. of Cong., Copyright Registration Guidance: Works 
Containing Material Generated by Artificial Intelligence, 88 Fed. Reg. 16190, 16191 (March 
16, 2023), https://www.federalregister.gov/documents/2023/03/16/2023-
05321/copyright-registration-guidance-works-containing-material-generated-by-artificial-
intelligence#footnote-8-p16191. 
103 Press Release, U.S. Copyright Oﬃce, Copyright Oﬃce Launches New Artiﬁcial 
Intelligence Initiative (March 16, 2023), 
https://www.copyright.gov/newsnet/2023/1004.html.  
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
77 
 
 
References 
 
104 U.S. Copyright Oﬃce, Decision Aﬃrming Refusal of Registration of a Recent Entrance 
to Paradise 2–3 (Feb. 14, 2022), https://www.copyright.gov/rulings-ﬁlings/review-board/
docs/a-recent-entrance-to-paradise.pdf. 
105 U.S. Copyright Oﬃce & Libr. of Cong., supra note 102102. 
106 Id. at 16192. 
107 See, e.g., U.S. Copyright Office, Cancellation Decision re: Zarya of the Dawn (Reg. No. 
VAu001480196) 2 (Feb. 21, 2023), https://www.copyright.gov/docs/zarya-of-the-dawn.pdf. 
108 17 U.S.C. § 103(b). 
109 Metz, supra note 9999; Thorpe, supra note 9999; Knight, supra note 9999; Cave, 
supra note 100100. 
110 Willman, supra note 9999. 
111 Andersen, supra note 9999. 
112 U.S. Copyright Office & Libr. of Cong., supra note 102102. 
113 See Class Action Complaint and Demand for Jury Trial, Andersen et al. v. Stability AI 
Ltd. et al., No. 3:23-cv-00201-WHO, (N.D. Cal. Jan. 13, 2023), 
https://stablediffusionlitigation.com/pdf/00201/1-1-stable-diffusion-complaint.pdf. 
114 Taylor Dafoe, Getty Images Is Suing the Company Behind Stable Diffusion, Saying the 
A.I. Generator Illegally Scraped Its Content, ArtNet (Jan. 17, 2023), 
https://news.artnet.com/art-world/getty-images-suing-stability-ai-stable-diffusion-illegally-
scraped-images-copyright-infringement-2243631. 
115 Natasha Lomas, Glaze Protects Art from Prying AIs, TechCrunch (Mar. 17, 2023), 
https://techcrunch.com/2023/03/17/glaze-generative-ai-art-style-mimicry-protection/. 
116 Shutterstock Datasets and AI-generated Content: Contributor FAQ, Shutterstock (Mar. 
20 2023), https://support.submit.shutterstock.com/s/article/Shutterstock-ai-and-
Computer-Vision-Contributor-FAQ?language. 
117 Kyle Wiggers, DeviantArt Provides a Way for Artists to Opt Out of AI Art Generators, 
TechCrunch (Nov. 11, 2022), https://techcrunch.com/2022/11/11/deviantart-provides-a-way-
for-artists-to-opt-out-of-ai-art-generators/. 
118 See generally Hans-Otto Pörtner et al., Intergovernmental Panel on Climate Change, 
Climate Change 2022: Impacts Adaptation and Vulnerability (2022), 
https://report.ipcc.ch/ar6/wg2/IPCC_AR6_WGII_FullReport.pdf [hereinafter “IPCC 
Report”]. 
119 IPCC Report at 9–11. 
120 IPCC Report at 13–14.  
121 Emma Strubell et al., Energy and Policy Considerations for Deep Learning in NLP, Proc. 
57th Ann. Meeting Ass’n for Comp. Linguistics 3645, 3645 (2019). 
122 Id. 
123 Roy Schwartz et al., Green AI, 63 Commc’ns ACM 54, 56 (2020). 
124 Id. 
125 Id. 
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
78 
 
 
References 
 
126 Strubell et al., supra note 121121, at 3645. 
127 Id. 
128 Amba Kak & Sarah Myers West, AI Now Institute, 2023 Landscape: Confronting Tech 
Power 100 (2023), https://ainowinstitute.org/wp-content/uploads/2023/04/AI-Now-2023-
Landscape-Report-FINAL.pdf [hereinafter “AI Now Report”]. 
129 Mack DeGeurin, ‘Thirsty’ AI: Training ChatGPT Required Enough Water to Fill a 
Nuclear Reactor’s Cooling Tower, Study Finds, Gizmodo (May 4, 2023), 
https://gizmodo.com/chatgpt-ai-water-185000-gallons-training-nuclear-1850324249.  
130 AI Now Report at 99. 
131 Dutch Call a Halt to New Massive Data Centres, While Rules are Worked Out, 
DutchNews (Feb. 17, 2022), https://www.dutchnews.nl/2022/02/dutch-call-a-halt-to-new-
massive-data-centres-while-rules-are-worked-out/. 
132 See e.g., Stephen Thomas, Who Will You Be After ChatGPT Takes Your Job, Wired 
(Apr. 21, 2023), https://www.wired.com/story/status-work-generative-artiﬁcial-intelligence/; 
Greg Ip, The Robots Have Finally Come for My Job, Wall St. J. (Apr. 5, 2023), 
https://www.wsj.com/articles/the-robots-have-ﬁnally-come-for-my-job-34a69146; Jyoti 
Mann, Sam Altman Admits OpenAI is ‘A Little Bit Scared’ of ChatGPT and Says It Will 
‘Eliminate’ Many Jobs, Insider (Mar. 18, 2023), https://www.businessinsider.com/sam-
altman-little-bit-scared-chatgpt-will-eliminate-many-jobs-2023-3; Steven Greenhouse, US 
Experts Warn AI Likely to Kill oﬀ Jobs—and Widen Wealth Inequality, Guardian (Feb. 8, 
2023), https://www.theguardian.com/technology/2023/feb/08/ai-chatgpt-jobs-economy-
inequality.  
133 70% of Workers Using ChatGPT At Work Are Not Telling Their Bosses; Overall Usage 
Among Professionals Jumps to 43%, Fishbowl (Feb. 1, 2023), 
https://www.fishbowlapp.com/insights/70-percent-of-workers-using-chatgpt-at-work-are-
not-telling-their-boss/.  
134 See e.g., Katie Notopoulos, A Tech News Site Has Been Using AI to Write Articles, So 
We Did the Same Thing Here, Buzzfeed News (Jan. 12, 2023), 
https://www.buzzfeednews.com/article/katienotopoulos/cnet-articles-written-by-ai-
chatgpt-article; Connie Guglielmo, CNET Is Experimenting With an AI Assist. Here’s Why, 
CNET (Jan. 16, 2023), https://www.cnet.com/tech/cnet-is-experimenting-with-an-ai-assist-
heres-why/; Ryan Ermey, ChatGPT Wrote Part of This Article—It Didn’t Go Great, CNBC 
(Jan. 26, 2023), https://www.cnbc.com/2023/01/26/chatgpt-wrote-part-of-this-article-it-
didnt-go-great.html; Noor Al-Sibai & Jon Christian, BuzzFeed is Quietly Publishing Whole 
AI-Generated Articles, Not Just Quizzes, Futurism (Mar. 30, 2023), 
https://futurism.com/buzzfeed-publishing-articles-by-ai.  
135 See Kevin Travers, How ChatGPT is Changing the Job Hiring Process, From the HR 
Department to Coders, CNBC (Apr. 8, 2023), https://www.cnbc.com/2023/04/08/chatgpt-
is-being-used-for-coding-and-to-write-job-descriptions.html.  
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
79 
 
 
References 
 
136 See, e.g., Chris Morris, A Major International Law Firm is Using an A.I. Chatbot to Help 
Lawyers Draft Contracts: ‘It’s Saving Time at All Levels’, Fortune (Feb. 15, 2023), 
https://fortune.com/2023/02/15/a-i-chatbot-law-firm-contracts-allen-and-overy/.  
137 Belle Lin, Generative AI Makes Headway in Healthcare, Wall St. J. (Mar. 21, 2023), 
https://www.wsj.com/articles/generative-ai-makes-headway-in-healthcare-cb5d4ee2.  
138 9 in 10 Companies That are Currently Hiring Want Workers with ChatGPT Experience, 
Resume Builder (Apr. 17, 2023), https://www.resumebuilder.com/9-in-10-companies-that-
are-currently-hiring-want-workers-with-chatgpt-experience/.  
139 Britney Nguyen, AI ‘Prompt Engineer’ Job Can Pay up to $375,000 a Year and Don’t 
Always Require a Background in Tech, Insider (May 1, 2023), 
https://www.cnbc.com/2023/04/05/chatgpt-is-the-newest-in-demand-job-skill-that-can-
help-you-get-hired.html.  
140 PromptBase, https://promptbase.com/.  
141 Alyssa Lukpa, JPMorgan Restricts Employees From Using ChatGPT, Wall St. J. (Feb. 
22, 2023), https://www.wsj.com/articles/jpmorgan-restricts-employees-from-using-
chatgpt-2da5dc34.  
142 Gurman, supra note 9393. 
143 See Daron Acemoglu, Harms of AI 49 (Nat’l Bureau of Econ. Rsch., Working Paper No. 
29247, 2021), https://www.nber.org/system/files/working_papers/w29247/w29247.pdf.  
144 Daron Acemoglu & Pascual Restrepo, Tasks, Automation, and the Rise in US Wage 
Inequality 35 (2022), 
http://pascual.scripts.mit.edu/research/taskdisplacement/task_displacement.pdf.  
145 White House, The Impact of Artificial Intelligence on the Future of Workforces in the 
European Union and the United States of America 15 (2022), 
https://www.whitehouse.gov/wp-content/uploads/2022/12/TTC-EC-CEA-AI-Report-
12052022-1.pdf.  
146 Daron Acemoglu, Automation Shouldn’t Always be Automatic: Marking Artificial 
Intelligence Work for Workers and the World, OECD: The Forum Network (Nov. 6, 2020), 
https://www.oecd-forum.org/posts/automation-shouldn-t-always-be-automatic-making-
artificial-intelligence-work-for-workers-and-the-world.  
147 Id. 
148 Daron Acemoglu et al., AI and Jobs: Evidence from Online Vacancies 3 (Nat’l Bureau 
of Econ. Rsch., Working Paper No. 28257, 2022), 
https://www.nber.org/system/files/working_papers/w28257/w28257.pdf.  
149 David H. Autor, Why Are There Still So Many Jobs? The History and Future of 
Workplace Automation, 29 J. Econ. Persp. 3 (2015), 
https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.29.3.3.  
150 Tyna Eloundou et al., GPTs are GPTs: An Early Look at the Labor Market Impact 
Potential of Large Language Models, arXiv (Mar. 23, 2023), 
https://arxiv.org/pdf/2303.10130.pdf.  
 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward   
80 
 
 
References 
 
151 Id. 
152 Jan Hatzius et al., Goldman Sachs, The Potentially Large Effects of Artificial 
Intelligence on Economic Growth (Briggs/Kodnani) 1 (2023), https://www.key4biz.it/wp-
content/uploads/2023/03/Global-Economics-Analyst_-The-Potentially-Large-Effects-of-
Artificial-Intelligence-on-Economic-Growth-Briggs_Kodnani.pdf. 
153 Id. at 1, 6-7. 
154 White House, supra note 145145. 
155 White House, supra note 145145. 
156 Chloe Xiang, Startups Are Already Using GPT-4 to Spend Less on Human Coders, 
Motherboard (Mar. 20, 2023), https://www.vice.com/en/article/jg5xmp/startups-are-
already-using-gpt-4-to-spend-less-on-human-coders.  
157 Id. 
158 Moore’s Law for Everything, Sam Altman’s Blog (Mar. 16, 2021), 
https://moores.samaltman.com/.  
159 Id. 
160 Alan Chan et al., The Limits of Global Inclusion in AI Development, arXiv (Feb. 2, 2021), 
https://arxiv.org/pdf/2102.01265.pdf.  
161 Wendy Liu, AI Is Exposing Who Really Has Power in Silicon Valley, Atlantic (Mar. 27, 
2023), https://www.theatlantic.com/technology/archive/2023/03/open-ai-products-labor-
profit/673527/. 
162 Id. 
163 Jeffrey Dastin et al., Exclusive: ChatGPT Owner OpenAI Projects $1 Billion in Revenue 
by 2024, Reuters (Dec. 15, 2022), https://www.reuters.com/business/chatgpt-owner-
openai-projects-1-billion-revenue-by-2024-sources-2022-12-15/.  
164 Billy Perrigo, 150 African Workers for ChatGPT, TikTok and Facebook Vote to 
Unionize at Landmark Nairobi Meeting, Time (May 1, 2023) 
https://time.com/6275995/chatgpt-facebook-african-workers-union/ 
165 Alissa Wilkinson, The Looming Threat of AI to Hollywood, and Why It Should Matter to 
You, Vox (May 2, 2023) https://www.vox.com/culture/23700519/writers-strike-ai-2023-
wga.  
166 Billy Perrigo, Exclusive: OpenAI Used Kenyan Workers on Less Than $2 Per House to 
Make ChatGPT Less Toxic, Time (Jan. 18, 2023), https://time.com/6247678/openai-
chatgpt-kenya-workers/.  
167 Id. 
168 Id. 
169 Id. 
170 Id. 
171 Resume Builder, supra note 138138. 
 
EPIC | Generating Harm: Generative AI’s Impact and Paths Forward  
81
References 
172 Alexandra Bruell, BuzzFeed to Use ChatGPT Creator OpenAI to Help Create Quizzes 
and Other Content, Wall St. J. (Jan. 26, 2023), https://www.wsj.com/articles/buzzfeed-to-
use-chatgpt-creator-openai-to-help-create-some-of-its-content-11674752660.  
173 Id. 
174 Jacklyn Diaz & Madj Al-Waheidi, BuzzFeed Shutters Its Newsroom as the Company 
Undergoes Layoffs, NPR (Apr. 21, 2023), 
https://www.npr.org/2023/04/20/1171056620/buzzfeed-news-shut-down-media-layoffs.  
175 Misinformation on Bard, Google’s New AI Chatbot, Ctr. for Countering Digit. Hate (Apr. 
5, 2023), https://counterhate.com/research/misinformation-on-bard-google-ai-chat/. 
176 795 F. App’x 878, 879–80 (3d Cir. 2020) (quoting Restatement (Third) of Torts: 
Products Liability § 19(a) (Am. L. Inst. 1998)). 
177 2 F.4th 871, 938 (9th Cir. 2021) (Gould, J., concurring in part). 
178 See, e.g., Karni A. Chagal-Feferkorn, Am I an Algorithm or a Product? When Products 
Liability Should Apply to Algorithmic Decision-Makers, 60 Stan. L. & Pol’y Rev. 61 (2019) 
(distinguishing between algorithmic products that should be subject to products liability 
and thinking algorithms that should not). 
179 Cf. Catherine M. Sharkey, Products Liability in the Digital Age: Online Platforms as 
“Cheapest Cost Avoiders”, 73 Hastings L.J. 1327 (2022). 
180 Hasan Chowdhury, ChatGPT Cost a Fortune to Make with OpenAI’s Losses Growing 
to $540 Million Last Year, Report Says, Insider (May 5, 2023), 
https://www.businessinsider.com/openai-2022-losses-hit-540-million-as-chatgpt-costs-
soared-2023-5.  
181 Id. 
182 See Cade Metz & Karen Weise, Microsoft to Invest $10 Billion in OpenAI, the Creator 
of ChatGPT, N.Y. Times (Jan. 23, 2023), 
https://www.nytimes.com/2023/01/23/business/microsoft-chatgpt-artificial-
intelligence.html. 
183 See ChatGPT and More: Large Scale AI Models Entrench Big Tech Power, AI Now 
Institute (Apr. 11, 2023), https://ainowinstitute.org/publication/large-scale-ai-models. 
184 Christopher Mims, The AI Boom That Could Make Google and Microsoft Even More 
Powerful, Wall St. J. (Feb. 11, 2023), https://www.wsj.com/articles/the-ai-boom-that-could-
make-google-and-microsoft-even-more-powerful-9c5dd2a6. 
",EPIC,41946
NTIA-2023-0009-0158,"575 7th St NW
Washington, DC
20004
March 26, 2024
Bertram Lee
National Telecommunications and Information
Administration, U.S. Department of Commerce
1401 Constitution Avenue NW
Washington, DC 20230
Subject: Openness in AI Request for Comment
Docket Number: NTIA-2023-0009
Dear Mr. Lee:
Attached please find the comments of Meta
Platforms, Inc. in response to the National
Telecommunications and Information
Administration (NTIA) Request for Comment
related to the agency’s assignment to conduct
a public consultation process and issue a report
on the potential risks, benefits, other
implications, and appropriate policy and
regulatory approaches to dual-use foundation
models for which the model weights are widely
available under the Executive Order on Safe,
Secure, and Trustworthy Development and Use
of Artificial Intelligence.
Sincerely,
Brian Rice
Vice President, Public Policy
Meta Platforms, Inc.
0
Executive Summary
Open source is the foundation of U.S. innovation and global technological
leadership. Continued leadership of this technological revolution ⏤including
through support for responsible open source AI domestically and in international
fora ⏤will underpin U.S. economic, domestic, foreign policy, international
development, and national security interests. Meta has been at the forefront of
responsible open source in AI for over a decade and believes that an open
approach to AI leads to better, safer products, faster innovation, and a larger
market. We appreciate the opportunity to share our experience with building AI
with the goal of making the benefits accessible to everyone.
In our submission, we suggest that NTIA’s report:
1. underscores the importance of open foundation models to U.S. economic,
national security, and foreign policy interests;
2. recommends that the Government continue its work through NTIA, NIST,
the White House Voluntary AI Commitments, and through other agencies
to establish common standards for risk assessments, benchmarks and
evaluations informed by science, noting that the U.S. national interest is
served by the broad availability of U.S.-developed open foundation models;
3. notes the importance of continued American leadership in driving
international consensus, cooperation, and interoperability on AI
governance, including among AI Safety Institutes, as well as at the United
Nations, G7 and OECD; and
4. points to the need for bipartisan federal AI legislation informed by the
work of NTIA, NIST and international efforts on common standards in
order to avoid a fragmented regulatory environment across the U.S.
1
Table of Contents
Introduction
3
I. How should NTIA define “open” or “widely available weights” when thinking
about foundation models and model weights?
6
II. How do the risks associated with making model weights widely available
compare to the risks associated with non-public model weights?
15
III. What are the benefits of foundation models with model weights that are
widely available as compared to fully closed models?
22
IV. Are there other relevant components of open foundation models that, if
simultaneously widely available, would change the risks or benefits
presented by widely available model weights?
If so, please list them and explain their impact.
30
V. What are the safety related or broader technical issues involved in managing
risks and amplifying benefits of dual-use foundation models
with widely available weights?
30
VI. What are the legal or business issues related to open foundation models?
39
VII. What are the current or potential voluntary, domestic regulatory, and
international mechanisms to manage risks and maximize the benefits of
foundation models with widely available weights? What kinds of entities should
take a leadership role across which features of governance?
41
VIII. In the face of continually changing technology, and unforeseen risks and
benefits, how can governments, companies, and individuals make decisions or
plans today about open foundation models that will be useful in the future?
45
IX. What other issues, topics, or adjacent technological advancements should we
consider when analyzing risks and benefits of dual-use foundation models with
widely available model weights?
46
X. Recommendations
48
2
Introduction
Meta has been a leader in open innovation in AI for over a decade and believes that
an open approach to AI leads to better, safer products, faster innovation, and a
larger market. We appreciate the opportunity to share our experience building AI
with the goal of making the benefits accessible to everyone.
During earlier eras of the Internet, open source technologies played a core role in
promoting innovation and safety. Over time, open source has become the
foundation of U.S. innovation and global technological leadership.
Open Source Increases U.S. Competitiveness
Open source democratizes access to the benefits of AI. These benefits are
potentially profound for the U.S., and for societies around the world. We have seen
a groundswell of enthusiasm for open source AI from small businesses across the
U.S. They recognize the transformative potential of AI for competition, innovation,
and productivity, which is already contributing to U.S. economic growth. Having
access to state-of-the-art AI creates opportunities for everyone, not just a small
handful of Big Tech companies. Open source provides the foundations on which
developers can innovate that would otherwise be prohibitively costly.
We have heard from the U.N. and governments across the Global South that open
source AI could finally light the path to achieve the Sustainable Development
Goals. We’ve seen this reflected in the recently adopted U.S.-led U.N. General
Assembly Resolution on Artificial Intelligence.
The U.N. and governments see the value of open source AI as a force multiplier to
help deliver services to their populations, close inequality gaps, and fuel new
technologies and enterprises. We are already witnessing the potential of open
source AI to generate fundamental breakthroughs in health and science for
everyone.
And we have heard from departments and agencies across the U.S. government
that are building tools on Meta’s open foundation model, Llama 2, to enhance
efficiency and improve service delivery.
Leading this technological revolution ⏤including via support for responsible open
source AI in international fora informed by the work of NTIA, NIST and other
agencies ⏤will benefit U.S. economic, domestic, foreign policy, international
development, and national security interests.
3
By making U.S. technology the standard on which developers around the world
build, the U.S. has an opportunity to embed our innovation and values in the fabric
of the AI revolution, ushering in a new era of American technological leadership
that will benefit the U.S. for decades to come.
Responsible Approach to Open Source
We acknowledge that there is a risk/reward tradeoff when it comes to any
approach with regard to releasing AI technology. While open source is in Meta’s
corporate DNA, we are not dogmatic in our approach. Meta open sources
responsibly, which means releasing AI that is designed around: 1) privacy and
security; 2) fairness and inclusion; 3) robustness and safety; 4) transparency and
control; and 5) accountability and governance. We've also open sourced
safeguards (e.g., Llama Guard) that, when combined with our models, increase
safety at the system level. These are now being standardized by MLCommons and
the open source community for safe generative AI application development.
Our commitment to responsible open source means we prohibit a range of
harmful uses via our commercial license, and we have at times elected not to
release tools because we deem the risks associated with them too high. We
strongly support the work of NIST and other bodies working to define taxonomies
of risks, benchmarks, and evaluations, and building processes for external
validation, such as red-teaming. We urge the NTIA to work with NIST on
standardizing the threat models and evaluations along the AI value chain, so that
risk can be measured quantitatively and consistently and standard thresholds can
be set against those evaluations. The outcomes of this work will provide crucial
guidance to companies like Meta on how to measure and mitigate risks, and how
and whether we decide to release certain models.
It is also important to consider the limitations associated with closed models.
Open source models leverage transparency to crowd-source efforts to build safer
and more robust systems, resulting in continuous vulnerability identification
through the scientific method of interrogation, as well as mitigation
experimentation (e.g., at the model level, at the system level, or at the user
education level).
Closed models must rely on in-house experts to perform these functions; without
the continuous scrutiny involved in open sourcing, vulnerabilities may remain
undetected and exploitable for significant periods of time. This is why open source
has become the gold standard in cybersecurity and encryption, among other
areas.
4
Closed models also create an illusion of security. As we have seen recently, closed
models may be subject to unintentional release, either through lapses in security,
leaks, or model extraction attacks. If a closed model is exfiltrated, only the model
developer has the information required to mitigate harms, and that understanding
may be limited. Given historical precedents of closed models leaked or stolen, and
the current general availability of equally capable open source models, it is
important not to rely on secrecy as the only means of safety and security.
As such, the focus should be on ensuring the responsible deployment of all
models, rather than focusing on whether the model weights are released. In short,
we suggest that further work is required to establish benchmarks for assessing
the quality of existing safeguards ⏤for both closed and open approaches ⏤to
better understand potential risks with each. NIST’s work will be important in this
regard.
The Spectrum of Openness & the Need for U.S. Leadership
All of this points to an ecosystem that includes a spectrum of openness, where
developers ⏤guided and supported by standard-setting and other governance
initiatives ⏤can tailor how they release models based on a variety of factors. The
United States has a crucial role to play in driving international consensus on AI
governance, U.S. leadership and innovation in the age of AI will promote U.S.
interests.
Multi-stakeholder efforts, like those underway through the U.S. AI Safety Institute
and in international fora like the G7 and OECD, are central to the ability for
governments, companies, and individuals to formulate durable, balanced solutions
to the challenges and opportunities of today and the future.
If the United States restricted U.S. companies’ ability to open source foundation
models, it would undermine U.S. interests. Specifically, open source drives
innovation and sets the standard on which others will build. Other countries are
already fielding strong competitors in this race (owing, in part, to a desire to
maintain independence from the U.S.); by restricting the distribution of
U.S.-grown AI, the U.S. would cede its pole position in AI leadership and
innovation, depriving the U.S. economy of related growth opportunities.
This would leave a vacuum that other countries would be eager to fill and benefit
from, with no guarantee that the models provided would reflect our values or
vision for the future.
5
Historically, broad and restrictive limits on open innovation of new technologies —
such as export controls on certain types of encryption in early web browsers —
have been counterproductive. Many of these restrictions were ultimately rolled
back as the U.S. government shifted to preferring open source tools in these areas
(e.g., open source encryption protocols that were actually more secure). We can
and should apply these lessons to AI and we encourage the U.S. government to
carefully consider and minimize restrictive limitations on open source in this
space.
1. How should NTIA define “open” or “widely available weights” when thinking
about foundation models and model weights?
Concepts like “open” or “widely available weights” are not binary but rather exist
on a spectrum. Rather than striving for strict definitions, we suggest teasing out
the nuances of an “open” approach as it applies to AI, which includes
understanding the foundation model technology stack.
The Open Source Spectrum
There is wide discourse in industry, academia, and civil society on what “open”
means in the context of foundation models. While “open source” has a specific
meaning as applied to traditional software, this meaning is less clear when applied
to AI systems. Hugging Face, one of the most widely used platforms for sharing
and collaborating on machine learning systems, has described the current state of
play as follows:
“For AI systems, conversations about how to operationalize
the values that underpin open source software are still very
much ongoing; especially given the importance of data in fully
understanding how they are designed and how they work. As
such, we tend to use terminology like “open science” and “open
access.” For AI models, we create processes in light of the
trade-offs
inherent
in
sharing
different
kinds
of
new
technology, and approach our work in terms of a “gradient of
openness”
(also
called
a
“release
gradient”)
to
foster
responsible development.”1
1 Hugging Face Comment on FR Doc # 2023-28232.
6
When considering this issue, NTIA should take account of recent discussions at
the Open Source Initiative itself 2 and the ongoing work of the Columbia
University Convening on Openness and AI, a forum that is bringing together over
40 leading experts working on openness and AI.3
The Foundation Model Technology Stack
When considering what “open” means in the context of foundation models, it is
important to consider the entire tech stack underpinning these models. The tech
stack is the software, hardware, and other elements required to develop the
model. Among other things, this includes programming languages (e.g., Python),
machine learning frameworks (e.g., PyTorch; these frameworks execute the
mathematical calculations that generate the model weights); training datasets
(e.g., CommonCrawl); the model itself (e.g., transformer models like Meta’s Llama
2); and hardware (e.g., GPU’s such as Nvidia H100’s, and the associated
architecture). It is also important to consider who can access each component
and for what purposes.
An open approach to the foundation model tech stack, for example, could include
providing access to model weights but not providing access to other assets, like
training datasets. Although one can build on (‘fine-tune’) a foundation model
without model weights (e.g., through an API) to adapt it for a variety of purposes,
there is significantly more flexibility, lower costs, and more control over one’s data
associated with fine-tuning openly released model weights. Other assets, like
training datasets, are not required to fine-tune a model and, given that sharing
them could come with risks, should be shared less routinely. (See Question 2 (c)).
Considering all the components of the AI tech stack demonstrates that there are
different types of openness that can be helpful for accomplishing different
technical and societal goals.4 It is important that model providers retain the ability
to make decisions regarding the degree of openness for each asset, based on
evaluations of the risks of each asset separately.
4 Id.
3 See Ayah Bdeir & Camille Francois, Introducing the Columbia Convening on Openness and AI, dest://ed (March 6,
2024).
2 In an interview with Emilia David for The Verge (Oct 30, 2023), Stefano Maffulli, the executive director of the
OSI, noted that “the group understands that current OSI-approved licenses may fall short of the certain needs of AI
models and that the OSI is reviewing how to work with AI developers to provide transparent, permissionless, yet
safe access to models.”
7
Meta’s Approach to Open Source
Meta has been deeply involved in the AI research community for over a decade.
This has allowed us to develop and evolve a set of best practices, principles, and
processes to support our release of foundation models informed by a deeper level
of intentionality anchored on assessing the differential (or ‘marginal’) risk (see
response to Question 2). We have also worked through the Partnership on AI to
develop “The Guidance for Safe Foundation Model Deployment,” which is one of
the most comprehensive, nuanced, and inclusive frameworks for responsibly
building and deploying AI models through an open approach.
As noted by Joelle Pineau, Vice President AI Research at Meta and Vice-Chair of
the PAI Board, “the Partnership on AI’s leadership has been invaluable in bringing
together industry, civil society, and experts as companies like ours determine the
best approach when looking at both open and closed releases.”5
Feedback from NTIA on this framework will be critical to advancing its potential
for integration into NTIA and NIST guidance. This would inform a standard
approach to responsible scaling policies that would drive practices across the
ecosystem.
As Meta develops our AI technologies, we are advancing both the state of the art
in terms of technical capabilities and the responsibilities that come with those
developments (for example, continuously developing new tooling for assessing
safety, datasets for assessing fairness and bias, and new methods to probe
security). We believe that, in general, being more open with our AI research
accelerates the innovation cycle of the underpinning technology and risk
assessments, benchmarks, evaluations, and mitigations.
Overall, relative to closed source, responsible open source establishes a higher bar
before the technology is widely embedded in consumer-facing products and used
by people around the world.
However, Meta’s responsible open source strategy is not absolute, and each
decision is taken on a case-by-case basis, largely because static processes are
quickly outdated by the pace of developments.
5 Partnership on AI Releases Guidance for Safe Foundation Model Deployment, Takes the Lead to Drive Positive
Outcomes and Help Inform AI Governance Ahead of AI Safety Summit in UK, PAI blog (Oct. 24, 2023).
8
This is why the work of NIST under the E.O., and of the U.S. AI Safety Institute, in
defining taxonomies of risks, benchmarks, and building processes for external
validation (like red-teaming) is so important. As we consider various factors when
assessing whether to open source a model, we are rigorous in our approach, and
are continuously mindful of evaluating risks and mitigations in a way that is
mindful of the public interest. And this ultimately facilitates the development of
better technology.
Given that there is no broad consensus on a settled definition, at Meta we
consider “open” in terms of making models and their weights publicly available
responsibly for research as well as commercial purposes. This means when we
make pre-trained model weights widely available for commercial and research
use, we do so along with model cards and other information such as user guides
and an Acceptable Use Policy, the use of which is subject to the acceptance of the
model license terms. When we launched the Llama 2 model, over 100 experts,
academics, and policymakers supported this responsible approach.6
Balancing Factors
Not necessarily every model should be open sourced, and a decision to open
source should be made after weighing a range of factors, including risk
assessments and business needs. For some highly advanced and novel models, for
example, it may be appropriate to first release the model (or certain artifacts) to
researchers to allow time to understand and, if necessary, mitigate risks
associated with the model.
At Meta, we are working to address the hard questions around issues such as
privacy, safety, fairness, accountability, and transparency through our privacy
review process, with privacy- and AI-specific risks identified, mitigated, evidenced
and monitored.7 Furthermore, for foundation models that we pre-train and
fine-tune for use across some of Meta’s platforms (e.g., MetaAI in WhatsApp,
Messenger, and Instagram8), we instituted additional layers of review and
escalation to assess risks at every stage of development (including data
collection, model training, and fine-tuning) in addition to evaluations (e.g.,
red-teaming and scaled).
8 Introducing New AI Experiences Across Our Family of Apps and Devices, Meta Newsroom (Sept. 27, 2023).
7 Mike Clark, Privacy Matters: Meta’s Generative AI Features, Privacy Matters blog (Sept. 27, 2023)
6 See statement of Support for Meta’s Open Approach to Today’s AI (“responsible and open innovation gives us all a
stake in the AI development process, bringing visibility, scrutiny and trust to these technologies. Opening today’s
Llama models will let everyone benefit from this technology”).
9
In the case of generative AI products, we further instituted additional
measurement of prompt/output-level filtering and classification effectiveness
across an extensive range of safety dimensions.
When it comes to models Meta has released in non-production environments
(e.g., for testing, development, or research purposes and not intended for live
usage by end-users), they undergo an Institutional Review Board (IRB)-like
process in addition to a privacy review, where researchers work to mitigate safety
issues at the model level. Where this is not yet possible, we have limited the
release of the model weights.
Some of our recent limited releases include our AudioBox model, for which we
only released the demo and research paper publicly, limiting model access to a
small group of researchers under a special program to study its applications,
including safety mitigations. (This is because tools like Audiobox can raise
concerns about voice impersonation or other abuses, and more work is needed in
collaboration with researchers and academics to conduct safety and
responsibility research.)
Similarly, Meta did not open source our model that decodes images using brain
activity.
Another factor for model providers to consider is that it might not make business
sense to make models publicly available at first, or even at all, given the cost and
resources incurred by the model provider in training a model. Indeed, there are
estimates that training the next generation of large language models will cost
more than $1 billion within a few years.9
In general, while we believe that open sourcing models responsibly enables
scrutiny and advances innovation in ways that closed approaches broadly cannot
as efficiently or effectively, we recognize that model providers may wish to adopt
different approaches at different times and/or for different models based on a
range of factors, including business model.
9 Craig S. Smith, What Large Models Cost You – There Is No Free AI Lunch, Forbes (Sept. 8, 2023)
10
Risk Assessment
Responsibly open sourcing also means acknowledging and taking seriously that
new risks may arise. With respect to Llama 2, we worked to build safety measures
and identify research areas to account for such risks. We examined a number of
potential risk areas that were relevant to our use cases, and then mapped and
deployed appropriate mitigations. (See Question 5(a) on evaluations).
Prohibited Uses
We responsibly open source models using a “permissive commercial license,”
which allows for a wide range of downstream uses ⏤but is not unrestricted. For
example, our license and Acceptable Use Policy, which must be agreed to before
the model is made available by Meta, includes a range of prohibited uses and
conditions of use. Such uses include violating the law or other’s rights; engaging in
the planning or development of activities that present a risk of death or bodily
harm to individuals; and intentionally deceiving or misleading others.10
Community Feedback
Responsible open sourcing also includes iterating on the findings of the research
community through adopting a progressive approach to releases (for example, at
Meta we released our Llama and Dino models to researchers in the first instance,
and iterated on the feedback before progressing to our permissive commercial
license). A core part of responsible open sourcing also means iterating based on
broader community feedback in a timely fashion, as well as transparency about
known issues and limitations of fixes. For example, we provide a number of means
to report violations of our Llama 2 Acceptable Use Policy, software bugs, or other
problems that could lead to a violation of the policy.11
11 More information on Meta’s Bug Bounty Program here and here.
10 See Llama 2 Acceptable Use Policy.
11
1. a. Is there evidence or historical examples suggesting that weights of models
similar to currently-closed AI systems will, or will not, likely become widely
available? If so, what are they?
Given the progression of the technology around the world, the performance and
capabilities of closed source and open source models (with widely available
weights) are converging. A contemporary example is Meta’s Llama 2 model, the
weights for which are publicly available subject to the license terms for use of the
model. Llama 2 outperforms GPT-3 (a model for which the weights are not
publicly available) on many benchmarks.12
As open foundation models further evolve around the world (e.g., Falcon, Vicuna,
Mistral) they will continue to match, and exceed, the capabilities of closed
foundation models, and there is no empirical evidence or reason to believe
otherwise.
Further, there is always a risk that closed models’ model weights can end up in the
public domain. For example, even if closed models implement state-of-the-art
security around model weights, there is always the possibility that these might be
subject to unintentional release ⏤either through lapses in security, leaks, or model
extraction attacks.13 Indeed, there are recent examples from across the AI
ecosystem of model leaks, as well as insider attacks at AI labs.14
In contrast to traditional software security vulnerabilities, where defenders
assume bad actors are already exploiting them and therefore transparently and
broadly share remediation information, if a closed foundation model is exfiltrated,
only the model developer has the information required to mitigate harms. This can
lead to adverse outcomes because nobody apart from the developer will have had
the opportunity to interrogate the model previously, to identify vulnerabilities and
fix them (the cybersecurity threats Heartbleed, Shellshock, Spectre, and
Meltdown were all identified and addressed by the open source community).
14 Chinese National Residing in California Arrested for Theft of Artificial Intelligence-Related Trade Secrets from
Google, DOJ Press Release (March 6, 2024).
13 See Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, Jinwen He, Towards Privacy and Security of Deep
Learning Systems: A Survey (Oct. 27, 2020) (“any kind of machine learning can be stolen”).
12 Sunil Ramlochan, How Does Llama-2 Compare to GPT-4/3.5 and Other AI Language Models, Prompt
Engineering & AI Inst. (Sept. 1, 2023).
12
Because the model weights for closed models can always end up being in the
public domain, and because the weights of models similar to currently closed AI
systems are/will be widely available in any event, it does not logically follow that
closed sourced models are, or will always be, inherently safer. To the contrary,
because open development allows vulnerability detection and mitigation by the
community at large, open releases can have safety advantages.
As such, the focus should be on ensuring the responsible deployment of all
models rather than focusing on whether the model weights are released.
1.b. Is it possible to generally estimate the timeframe between the deployment of
a closed model and the deployment of an open foundation model of similar
performance on relevant tasks? How do you expect that timeframe to change?
Based on what variables? How do you expect those variables to change in the
coming months and years?
It is not possible to generally estimate this timeframe given the variables involved,
including the model deployment developers’ business models and whether, in the
case of Llama 2, they download the model weights from Meta directly or
accessed it through third-party services like Azure or AWS.
Other variables include the application complexity (a highly complex agent is
significantly different to just deploying a text summarizer), the sophistication of
the developer (e.g., an average developer without much ML experience in
comparison to highly specialized and experienced ML teams), and the level of
abstraction targeted (for example if a developer uses the models directly from a
“model zoo” (e.g., Vertex Model Garden), this requires significantly more resources
to manage the hosting work, operations of the Large Language Model (LLM), etc.,
compared to those from a model API).
Model providers work hard to make their user interfaces, APIs, and model access
points as seamless as possible, and providers like Meta make “recipes” available
(e.g., including the choice of model architecture, the training algorithm, the
hyperparameters, and other factors that influence the training process). The
downstream developer and researcher community are similarly developing
recipes to make model deployment quick and easy.
13
1.c. Should “wide availability” of model weights be defined by level of
distribution? If so, at what level of distribution (e.g., 10,000 entities; 1 million
entities; open publication; etc.) should model weights be presumed to be “widely
available”? If not, how should NTIA define “wide availability?”
“Widely available” should be aligned with the Partnership on AI's Guidance for
Safe Foundation Model Deployment, which provides model providers with
recommendations based on foundation model type (e.g., Specialized Narrow
Purpose, Advanced Narrow and General Purpose, Paradigm-shifting or Frontier)
and release type (e.g., Open Access, Restricted API/Hosted Access, Closed
Development and Research Release).
Based on this guidance, ""widely available"" might usefully be defined as a subset of
those criteria (i.e., paradigm shifting/frontier plus open access), which would carry
additional recommendations, also available in the guide, on how to conduct such a
release safely.
1.d. Do certain forms of access to an open foundation model (web applications,
Application Programming Interfaces (API’s), local hosting, edge deployment)
provide more or less benefit or more or less risk than others? Are these risks
dependent on other details of the system or application enabling access?
1.d.i Are there promising prospective forms or modes of access that could strike
a more favorable benefit-risk balance? If so, what are they?
Model providers assessing how to make foundation models available consider a
range of factors, including business model, performance, control over the model,
and the target audience (higher levels of abstraction favor a broader range of
developers, while lower levels of abstraction (with more ways to customize) favor
deeper ML experts). Which approach to take is therefore fact-specific and
variable.
Locally Hosting
Locally hosting an open foundation model can provide a level of security not
available through an API. With local hosting, there is no transfer of data back to
the model provider or downstream developers’ servers, which can be an
important factor for those building on the model who want to ensure confidential,
sensitive, or classified information remains within their control. For example,
Meditron and other leading medical models rely on open architecture to ensure
that sensitive and private clinical data is run and stored locally.
14
APIs
APIs can provide multiple ways to check outputs and vet user behaviors to ensure
models are not abused, but responsible open source enables wider scrutiny of the
model, often enabling deeper research, innovation and safety. (See Question 3.)
Web Applications
Accessing foundation models through web applications can be a cost-effective,
scalable, and convenient method, but trade-offs include dependency on the web
application provider, performance limitations, and security risks (e.g., if the web
application itself is compromised).
Edge Deployment
While this method includes benefits such as reduced latency, privacy, and
potentially reduced costs, as with other methods there are trade-offs, including
devices’ computing power (phones, tablets, laptops are more limited), which can
impact performance and accuracy, as well as challenges to maintaining and
updating the on-device model.
2. How do the risks associated with making model weights widely available
compare to the risks associated with non-public model weights?
In general, it can be more challenging to fix or mitigate safety/security issues with
open models as the model provider/developer can't issue patches (unlike for
traditional software). However, it is important to remember that if bad actors find
an issue in a closed model via API, they are unlikely to disclose or report it;the
onus is entirely on the model API owner to detect and mitigate such issues.
Therefore, broadly speaking, responsible open source reduces the potential risks
of foundation models. Providing access to a wide community ⏤including
regulators, AI experts, hobbyists, and innovators ⏤subjects the model to scrutiny
and interrogation at scale, which can lead to the rapid identification of poor
outcomes or vulnerabilities⏤and the development of fixes.
For example, since the release of Llama 2, we have tracked dozens of model
improvement, features asks, and bugs reported by the developer community,
resulting in changes to the next generation of models and improved tooling such
as Purple Llama. Furthermore, we triage and resolve regularly issues reported on
Github, resulting in overall model improvement and which quickly advances the
state of the art.
15
Marginal Risk Analysis
In order to precisely identify and assess risks uniquely presented by open
foundation models, it is important to apply a “marginal risk analysis”15 that takes
account of the risks of open models compared to: (1) preexisting technologies,
and (2) closed models.
Open Source vs. Preexisting Technology
An example where the marginal risk to preexisting technologies is not increased
compared involves phishing attacks. These have existed for a long time, and while
large language models could be used to generate sophisticated phishing emails at
scale, the models themselves do not necessarily increase this cybersecurity risk.
That risk arises only when people receive the email and take steps in response to it
(e.g., installing malicious code or divulging sensitive information).
And large language models are in fact a powerful means of detecting and
combating this type of harmful content.16 Because the most effective solutions
are not on the content-generation side (malicious actors do not need LLMs to
generate phishing content), risk countermeasures should forgo LLM access
restrictions and instead recognize that they are an important tool, along with
other mitigations (e.g., implementing unclonable authentication credentials and
multifactor authentication that would apply no matter how the attacker
generates phishing emails).
Open Source vs. Closed Models
With respect to the marginal risk of open models compared to closed models, it is
important to note that a closed approach is not necessarily safer. For example,
model weights can be exfiltrated (see Question 2, above), API safeguards are
fallible, accidental releases can happen, and insider threats are real.17 Further,
there are additional benefits to an open source approach, including security
benefits that, in general, make open foundation models a safer option. (See
response to Question 3.)
17 Andy Zou, et al., Universal and Transferable Adversarial Attacks on Aligned Language Models (Dec. 20, 2023).
16 See Fredrik Heiding & Bruce Schneier, et. al., Devising and Detecting Phishing: Large Language Models vs.
Smaller Human Models (“large language models are adept at detecting phishing emails and can provide good
recommendations to the recipient”).
15 Sayash Kapoor & Rishi Bommasani, et. al., On the Societal Impact of Open Foundation Models, Stanford Center
for Research on Foundation Models (CRFM) (Feb. 27, 2024).
16
In short, we suggest that further work is required to establish benchmarks for
assessing the quality of existing safeguards for both closed and open approaches
to better understand potential risks with each. The efforts of NIST and the U.S. AI
Safety Institute will be important in this regard.
2.a. What, if any, are the risks associated with widely available model weights?
How do these risks change, if at all, when the training data or source code
associated with fine tuning, pretraining, or deploying a model is simultaneously
widely available?
On the question of assessing the risks and benefits associated with releasing
model weights, we would again draw NTIA’s attention to the Partnership on AI
Guidance for Safe Foundation Model Deployment.
Releasing model code alongside model weights comes with both risks and
benefits. The evaluation of the risk depends on the threat model. With the model
code, a sophisticated and resourced developer may remove safety guardrails that
were built into the model at the base layer.
For example, guardrails that prevent models from generating offensive or harmful
content could be removed, or modifications could be made to model weights in a
way that makes them more susceptible to bias or discrimination.18 However,
pushing out code fixes with open models is significantly faster and more efficient
than data-level or model-weights-level mitigations because code fixes can be
applied universally, but data-level or model-weights-level mitigations often need
to be specifically tailored, or require retraining the model⏤so code is a crucial part
of open releases.
Releasing model weights alone, without additional information about how to use a
model (such as model cards and source code), is not particularly useful to a
regular (non-nefarious) model deployment developer. It would therefore make
little sense for a company to release weights without additional instructions,
recipes, and code.
18 See, e.g., Chloe Xiang, The Amateurs Jailbreaking GPT Say They're Preventing a Closed-Source AI Dystopia,
Vice (March 22, 2023), Will Knight, A New Trick Uses AI to Jailbreak AI-Models-including GPT-4, Wired (Oct. 5,
2023).
17
While it is possible to fine-tune an open foundation model with just pre-trained
model weights (provided one has access to a suitable new dataset, a training
framework/library, and the necessary compute resources), it is much less efficient
and valuable to good actors than fine-tuning with access to the model code,
model card, and other assets like a user guide, responsible use guide, and
acceptable use policy.
This is why we believe that, on balance, responsibly making model assets (e.g.,
weights, code, and model cards) available for some models is a net-good for
society. With them, the open source community has access to the information it
needs to meaningfully engage with those models and more effectively counter
malicious uses.
We encourage NTIA to work with NIST on establishing the relevant threat models
and evaluations for common use across the industry.
2.b. Could open foundation models reduce equity in rights and safety-impacting
AI systems (e.g. healthcare, education, criminal justice, housing, online platforms,
etc.)?
We developed and made available responsible use guides to help downstream
developers measure impact. And it's why the Llama 2 Acceptable Use Policy
prohibits specific high risk use cases.19
In general, we believe that when responsibly open sourced, foundation models
can lead to improvements in equity. They enable diverse interest groups to
scrutinize performance against benchmarks for their particular interest group, and
find ways to improve those outcomes.
Meta is also furthering more inclusive and accessible AI by increasing language
accessibility and coverage through ‘No Language Left Behind’ (NLLB), a
first-of-its-kind AI project that open sources, under a non-commercial license,
models capable of delivering evaluated, high-quality translation directly between
200 languages, including low-resource languages such as Asturian, Luganda,
Urdu, and others. It aims to give people the opportunity to access and share web
content in their native language and communicate with anyone, anywhere,
regardless of language preferences.
19 For example, the Llama 2 Acceptable Use Policy prohibits the use of the model to “engage in, promote, incite, or
facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits,
credit, housing, other economic benefits, or other essential goods and services.”
18
As a real-world example, the technology behind NLLB is supporting Wikipedia
editors as they translate information from their native and preferred languages.
This helps make more knowledge available to more people around the world.
2.c. What, if any, risks related to privacy could result from the wide availability of
model weights?
Privacy risks may arise in the context of how open foundation models are
deployed downstream. For example, a malicious actor might try to solicit personal
information from a model. However, closed models are also vulnerable to some of
the same privacy risks, which is why responsibly developing and deploying all
models must include addressing privacy risks. Training data deidentification and
deploying privacy-enhancing techniques, are examples of mitigations that can be
taken at the training dataset stage to address these downstream risks. Closed
models have the advantage of prompt classification and prompt engineering to
identify prompts that may be attempts to extract personal information, or be
used to track an individual based on previous locations. However, these
system-level techniques have yet to be made resilient to repeated attacks or
jailbreaks. Open research has been demonstrably helpful in identifying adversarial
privacy attacks, and significant research is ongoing in making AI secure against
such attacks.
At Meta, our responsible open source approach includes several steps to mitigate
privacy risks at the data layer, such as data removal and techniques to reduce
memorization.
In addition to limiting the data that can be used to train a model, we continue to
learn about privacy adversarial attacks through our evaluations work, allowing us
to test fine-tuned models for whether sensitive personal information could be
reproduced, especially by an adversarial actor.20 And we do not publicly make our
training datasets widely available along with our model weights under permissive
commercial licenses. (Because of the same concern, mandating access for third
parties to training datasets would exponentially increase risk.)
20 Id.
19
Additionally, our privacy review process is designed to assess privacy risks that
collecting, using, or sharing people’s personally identifiable information (‘PII’) may
present, and to help determine whether steps should be taken to mitigate any
identified privacy risks, including through the development and use of AI models
and tools.
When training Llama 2, we filtered publicly available information to exclude from
the dataset certain websites that commonly share PII, like LinkedIn.21 And we
trained and tuned Llama 2 to limit the possibility of private PII from appearing in
responses.
All providers and developers of foundation models should have the same
minimum baseline obligations to identify and address privacy risks. It is also
important that providers and developers are not mandated to provide
unrestricted access to the entire tech stack, including training datasets, to third
parties.
2.d. Are there novel ways that state or non-state actors could use widely
available model weights to create or exacerbate security risks, including but not
limited to threats to infrastructure, public health, human and civil rights,
democracy, defense, and the economy?
2.d.i. How do these risks compare to those associated with closed models?
2.d.ii. How do these risks compare to those associated with other types of
software systems and information resources?
Malicious state and non-state actors will use any technology they can to advance
their objectives. Many such actors are well-resourced and engage in sophisticated
malicious cyber activity that is targeted and aimed at prolonged network/system
intrusion.
As such, it is likely that state or non-state actors will use widely available model
weights ⏤whether because they were responsibly open-sourced or
unintentionally released ⏤as well as closed source models available via API to
further their goals and interests.
21 Clark, Privacy Matters: Meta’s Generative AI Features.
20
These actors also routinely use open and closed source software for activities
such as targeting and exfiltrating highly protected proprietary databases that
have little open source value.22
2.e. What, if any, risks could result from differences in access to widely available
models across different jurisdictions?
2.f. Which are the most severe, and which are the most likely risks described in
answering the questions above? How do these sets of risks relate to each other,
if at all?
American interests in competitiveness, economic growth, and global leadership
would be jeopardized if the U.S. were to adopt a more restrictive approach to open
foundation models than that of other jurisdictions.
The most severe risk would be if the U.S. were to adopt restrictions for open
foundation models or otherwise implement an approach that prohibits model
providers and downstream developers from determining how their models can be
accessed, while other jurisdictions continue to invest in and facilitate the
development and adoption of open foundation models.23 24
For example, U.S. policies and rules regarding the standards governing 5G
inadvertently “led to considerable uncertainty and, in some cases, chilled U.S.
participation in standards development activities involving Huawei.” This
approach facilitated market dominance by Huawei until U.S. policies were
clarified.25
25 U.S. Commerce Department Clarifies Rules for Dealing with Huawei in Standards Setting
24 Making AI technologies available to the public under open-source terms is a stated policy objective of China’s
Global AI Governance Initiative.
23 Hunyuan, a large language model developed by China’s Tencent, claims to be more capable than GPT-3 and
Llama-2 across some benchmarks. Josh Ye, China's Tencent debuts large language AI model, says open for
enterprise use, Reuters (Sept. 7, 2023). Baidu, another Chinese developer, also claims that its Ernie 4.0 model is
more capable than GPT-4. Yelin Mo and Eduardo Baptista, China's Baidu unveils new Ernie AI version to rival
GPT-4, Reuters (Oct. 17, 2023). Qwen-VL-Max, from Alibaba’s Qwen family of open source models, is also
claimed to outperform GPT-4V on some benchmarks. Qwen team, Introducing Qwen-VL, ( Jan. 25, 2024). The open
source Falcon 180B LLM, developed by the UAE Technology Innovation Institute, claims to be “one of the three
best language models in the world along with GPT-4 and PaLM-2-Large.” Ebtesam Almazrouei & Hamza Alobeidli,
et. al., The Falcon Series of Open Language Models, (Nov. 29, 2023).
22 Ransomware: The Data Exfiltration and Double Extortion Trends, Center for Internet Security.
21
U.S. government policy should ensure that companies outside the U.S. do not gain
the lead in developing foundational AI technology, or that models developed
elsewhere become the global norm. Doing otherwise would not only disadvantage
the U.S. economically, but the resulting models likely would not be developed in a
way that aligns with U.S. values and standards, and may be subject to the laws or
political oversight of countries that are not aligned with the United States and/or
its allies.26
This result would directly undermine the U.S. government’s commitment to
“establishing a set of rules and norms for AI, with allies and partners, that reflect
democratic values and interests, including transparency, privacy, accountability,
and consumer protections.”27
3. What are the benefits of foundation models with model weights that are
widely available as compared to fully closed models?
There are extensive and demonstrable benefits to the U.S. of enabling the
responsible open sourcing of foundation models, including macroeconomic,
cybersecurity, freedom of expression, scientific research, national security, and
foreign policy ones, including international development. Perhaps in recognition
of these benefits, the U.S. government has well established policies supporting
open source, allowing its widespread use and adoption across the federal
government.
We discuss each of the benefits further below.
Macroeconomic Benefits of Open Models
In October 2023, Goldman Sachs raised its U.S. GDP forecast to a 2% expansion
rate in 2027 and to 2.3% by 2034 based on the impact of AI on the U.S.
economy.28 Foundation models, in particular, may generate between $2.6 trillion
to $4.4 trillion in economic growth across the global economy.29
29 The economic potential of generative AI: The next productivity frontier, McKinsey report (June 14, 2023).
28 Chris Anstey, Goldman Sachs Sees AI Lifting US GDP Over Next Decade, Bloomberg (Oct. 30, 2023).
27 Vice President Harris Announces New U.S. Initiatives to Advance the Safe and Responsible Use of Artificial
Intelligence, White House Fact Sheet (Nov. 1, 2023).
26 See, e.g., Tehran, Moscow to cooperate on AI ethics, Tehran Times (March 9, 2024).
22
Given that there are barriers to entry for low-resource actors to develop
foundation models as a result of its significant capital costs (even with reducing
costs of computational power), it is critical that the government’s policies on open
foundation models maximize the benefits to competition and innovation.30
Meta developed and open-sourced PyTorch, a powerful, flexible and easy-to-use
workflow for building machine learning models, in 2016.31 Since then, it has
become the leading tool for AI research and the development of such models
around the world, with an extensive community of open source developers
continuously contributing to improvements and innovation across the ecosystem
(over 2,400 contributors have built more than 150,000 projects on the
framework).32
Open Foundation Models’ Importance to the U.S. Government
The U.S. government uses AI, including open foundation models, in a variety of
ways across different agencies and departments.
For example:
-
The National Aeronautics and Space Administration (NASA) collaborated
with IBM to develop and open source the largest geospatial foundation
model on Hugging Face. IBM fine-tuned the model to allow users to map
past U.S. floods and wildfires, measurements that can be used to predict
future areas of risk. With additional fine tuning, the model could be
redeployed for tasks like tracking deforestation, predicting crop yields, and
detecting and monitoring greenhouse gasses. The project coincided with
NASA’s Year of Open Science, a series of events to promote data and AI
model sharing. It is also part of NASA’s decade-long Open-Source Science
Initiative to build a more accessible, inclusive, and collaborative scientific
community.33
-
The National Institutes of Health (NIH) uses AI models in biomedical
research. For instance, the NIH Clinical Center released an open-source
dataset of 32,000 CT images to help the scientific community improve the
detection of lesions.34
34 NIH Clinical Center releases dataset of 32,000 CT images, NIH press release (July 20, 2018).
33 Kim Martineau, IBM and NASA open source the largest geospatial AI foundation model on Hugging Face, IBM
blog (Aug. 3, 2023).
32 Announcing the PyTorch Foundation: A new era for the cutting-edge AI framework, Meta AI blog (Sept. 12,
2022).
31 Frameworks like PyTorch are the vehicle for architectural exploration, allowing the research community to iterate
on different ideas and then release code for others to use and build on.
30 Kapoor & Bommasani, et. al., On the Societal Impact of Open Foundation Models.
23
-
The U.S. Department of Agriculture (USDA) uses AI to predict crop yields,
optimize irrigation, and monitor soil health. For example, the USDA's
National Agricultural Statistics Service uses machine learning models to
analyze satellite imagery and predict crop production.35
-
The Department of Defense's Project Maven uses AI to interpret video
images, which could be used to improve drone footage analysis. The project
initially used TensorFlow, an open-source machine learning framework
developed by Google.36
-
The U.S. Census Bureau uses machine learning models to improve the
accuracy and efficiency of the decennial census. For example, it uses
computer vision models to analyze satellite imagery and identify housing
units, which helps to ensure that every household is included in the
census.37
-
The U.S. General Services Administration (GSA) has an AI for Citizen
Services program that aims to make public data more accessible and useful.
It uses AI models to create chatbots and virtual assistants that can answer
questions about public services.38
Additionally, the U.S. government, through agencies and contractors, is already
using or proposing to use Llama 2.39 In the 2023 End of Year Report on the Open
Source Software Security Initiative, NCS commits that, “in partnership with the
private sector and the open-source software community, the federal government
will also continue to invest in the development of secure software, including
memory-safe languages and software development techniques, frameworks, and
testing tools.”40
40 Securing the Open-Source Software Ecosystem: End of Year Report (Jan, 2024).
39 Cecilia Kang, The Department of Homeland Security is Embracing AI, NYT (March 18, 2024).
38 Justin Herman, Opening Public Services to Artificial Intelligence Assistants, GSA blog (Jan. 6, 2017).
37 Why does the U.S. Census Bureau Need Machine Learning?, US Census Bureau blog (Oct. 28, 2021).
36 Samuel Gibbs, Google’s AI is being used by US military drone programme, The Guardian (March 7, 2018).
35 Scott Elliott, Artificial Intelligence Improves America’s Food System, USDA Blog (Dec. 10, 2020).
24
Competition and Innovation
Open models set a “floor” for competition, incentivizing innovation while ensuring
that no one actor can capture the “baseline” and extract undue rents.41
Broadening access to models allows more people to use them for their own
innovations, which in turn enables greater competition in downstream markets,
helping to reduce market concentration at the foundation-model level from
vertical cascading.42
We have already seen impressive organic adoption of Llama 2 by downstream
developers around the world for purposes ranging from gaming to clinical
decision-making.43 Significantly, developers are not only using, but improving
upon, openly released models: “[O]pen-source developers have created
thousands of derivatives of models like Llama, including increasingly, mixing
models ⏤and they are steadily achieving parity with, or even superiority over
closed models on certain metrics (e.g., FinGPT, BioBert, Defog SQLCoder, and
Phind).”44
Finally, we are particularly invested in supporting the adoption of Llama 2 (and
other models) for innovative social impact purposes ⏤which has led to the
creation of our Llama Impact Grants program. The goal of the program is to
identify and support the most compelling applications of Llama 2 for societal
benefit, and the volume of applicants (over 800) demonstrates Llama 2’s
significant potential for promoting social good. Examples of some of the
applications of Llama 2 under the programme include: building a Medical-Vision
Language Model (Med-VLM) that can process medical images and provide
high-quality textual answers to medical questions in various languages (Barcelona
Supercomputing Center); and leveraging Llama 2 to enhance the process of
matching cancer patients with clinical trials and then integrating these capabilities
into the open-source MatchMiner platform (Dana-Farber Cancer Institute).45
45 See the Llama Impact Grants landing page.
44 Matt Marshall, How enterprises are using open source LLMs: 16 examples, Venturebeat (Jan. 29, 2024).
43 Discover the possibilities of building on Llama, Meta blog.
42 Id.
41 Indeed, the Securities and Exchange Commission Chair, Gary Gensler, has also noted the risk to the financial
sector if there is overreliance on one base model. See SEC chair Gensler on AI's threat to Wall Street: “I don't want
everybody to drive off the cliff,” Politico Tech Podcast (March 19, 2024).
25
Security
Open sourcing is a longstanding, well-regarded approach to enhancing security, as
demonstrated by these examples:
●
Historically, many software vendors sought to prevent security researchers
from publishing information about software vulnerabilities. Ultimately,
researchers’ disclosures created public pressure for software vendors to
enhance security and develop more secure software.
●
Open source software is an essential component supporting cybersecurity
in the federal government. The Cybersecurity and Infrastructure Security
Agency (CISA) notes that “open source software is widely used across the
federal government and every critical infrastructure sector.”46
●
The Department of Defense is also highly engaged with the open source
community. Specifically, the Defense Advanced Research Projects Agency
(DARPA) regularly engages with the Open Source Security Foundation
(OpenSSF)47 and has offered awards up to $1million USD for work helping
to “protect the integrity of open source infrastructure critical to the DoD.”48
In our experience, instead of creating more new risks than benefits, open source
releases have helped us, and the broader community of developers, build safer
and more robust systems. By democratizing access, vulnerabilities are
continuously identified and mitigated by an open community, and that creates
safer products.
For example, researchers were able to test Meta’s earlier generative LLM,
BlenderBot 2, to uncover ways that the model can be tricked into remembering
misinformation⏤ensuring that BlenderBot 3 was more resistant to such attacks.49
As a further example, recent academic research into the security of closed and
open models (including Llama 2) identified vulnerabilities and will help developers
identify appropriate solutions50 (we are taking note of the findings in the paper for
our future benchmark work).
50 Fengqing Jiang, Zhangchen Xu, Luyao Niu, et al., ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned
LLMs (Feb. 22, 2024).
49 BlenderBot 3: A 175B parameter, publicly available chatbot that improves its skills and safety over time, Meta
blog (Aug. 9, 2022).
48 See Hybrid AI to Protect Integrity of Open Source Code (SocialCyber).
47 See DARPA’s Artificial Intelligence Cyber Challenge.
46 See the Open Source Software Security page of the Cybersecurity & Infrastructure Security Agency.
26
Other companies are also invested in using open source to improve security. For
example, Google recently launched the AI Cyber Defense Initiative and open
sourced Magika, a new AI-powered tool it hopes will benefit defenders more than
attackers.51
Scientific Research
Continued investments in AI openness and science will fuel the next generation of
AI discoveries. Open research52 furthers our collective fundamental
understanding in both new and existing domains across the full spectrum of
AI-related topics related, which advances the state of the art. This in turn benefits
the entire ecosystem, including closed-source model providers and downstream
developers.
Jeff Boudier, head of product and growth at Hugging Face, notes, “AI remains a
science-driven field, and science can only progress through information sharing
and collaboration. This is why open-source AI and the open release of models and
datasets are so fundamental to the continued progress of AI, and making sure the
technology will benefit as many people as possible.”53
Open foundation models are critical to research in areas spanning from
interpretability, security research, and improvements to model training and
inference efficiency.54 Open source enables scientific scrutiny within a research
community and experimentation with advanced technologies. As a recent
statement by over 100 leading biologists noted, “many researchers in our
community benefit from open-source scientific software, which has enabled rapid
innovation and broad collaboration.”55
55 Community Values, Guiding Principles, and Commitments for the Responsible Development of AI for Protein
Design, Responsible AI x Biodesign (March 8, 2024).
54 Rishi Bommasani & Sayash Kapoor, et. al., Considerations for Governing Open Foundation Models, Stanford
Center for Research on Foundation Models (CRFM), (Dec. 13, 2023).
53 IBM and NASA open source the largest geospatial AI foundation model on Hugging Face, IBM blog (Aug. 3,
2023).
52 Meta defines open research as a collaborative and transparent approach to scientific inquiry that encourages the
sharing of data, methods, and results with other researchers and the broader public. This approach is designed to
promote the advancement of knowledge and the development of new technologies by allowing others to build upon
and verify the findings of previous research. Open research at Meta involves making our data, methods, and results
available to other researchers and the public through various channels, such as open-access publications, datasets,
and software repositories. We also engage in collaborations with other institutions and researchers to further advance
the field and promote the use of AI for social good.
51 Phil Venables and Royal Hansen, How AI can strengthen digital security, Google blog (Feb. 16, 2024).
27
Similarly, Meta’s release of the open source OPT language model in 202256
enabled recent advancements in watermarking large language models at the
University of Maryland.57 And work by Meta’s Fundamental AI Research (FAIR)
team is driving advances that look beyond the transformer architecture of models
(the architecture that current state-of-the-art models like Llama 2 and GPT-4 are
built on). This includes FAIR’s work on V-JEPA, which is a new self-supervised
learning and objective-driven architecture.
Research on new model architectures is valuable in areas like efficiency: While
transformer models have achieved state-of-the-art results on many tasks, they
can be computationally intensive and require a lot of memory, which can be a
limitation for certain applications or devices. Alternative architectures might offer
similar performance with fewer computational requirements, thereby driving both
performance up and costs/accessibility down.
The necessary, continuous work on AI safety requires openness; as several AI
experts recently noted, “[c]urrent safety research is often limited by insufficient
access to large, cutting-edge models and relevant information such as their
architecture and training processes.”58
National Security and Foreign Policy
U.S. leadership in open source will result in models trained on datasets that reflect
U.S. values and that have not been subject to censorship via fine tuning. These
models will become the standard on which developers around the world build, and
on which further innovation is based⏤embedding U.S. values and maintaining the
U.S. lead in AI innovation. In turn, this will give the U.S. additional influence to
shape AI governance conversations in international institutions.
Open source models strengthen cybersecurity by scaling cybersecurity defenders
and enable classified use cases for generative AI because they can be hosted
locally and do not rely on cloud infrastructure.
58 Elizabeth Seger & Noemi Dreksler, et. al., Open-Sourcing Highly Capable Foundation Models: An Evaluation of
Risks, Benefits, and Alternative Methods for Pursuing Open-Source Objectives, LPP Working Paper No. 4-2023
(Oct. 25, 2023).
57 Written Testimony of Clement Delangue, Co-Founder and CEO, Hugging Face, For a Hearing on ‘Artificial
Intelligence: Advancing Innovation Towards the National Interest (June 22, 2023).
56 Will Douglas Heaven, Meta Has Built a Massive New Language AI—and It’s Giving It Away for Free, MIT
Technology Review (May 3, 2022).
28
More broadly, open source democratizes access to AI technology, which can
support U.S. foreign and development policy in the Global South. Open source AI
can act as a force multiplier for countries to help deliver services to their
populations, close inequality gaps, and fuel new technologies and enterprises. The
U.N. has embraced open source AI as a key tool to achieve the Sustainable
Development Goals, an ambition the U.S. shares.59
3.a. What benefits do open model weights offer for competition and innovation,
both in the AI marketplace and in other areas of the economy? In what ways can
open dual-use foundation models enable or enhance scientific research, as well
as education/training in computer science and related fields?
See response to Question 3.
3.b. How can making model weights widely available improve the safety, security,
and trustworthiness of AI and the robustness of public preparedness against
potential AI risks?
See response to Question 3.
3.c. Could open model weights, and in particular the ability to retrain models,
help advance equity in rights and safety-impacting AI systems (e.g. healthcare,
education, criminal justice, housing, online platforms etc.)?
See response to Question 2 (b).
3.d. How can the diffusion of AI models with widely available weights support
the United States’ national security interests? How could it interfere with, or
further the enjoyment and protection of human rights within and outside of the
United States?
See response to Questions 2 (b) and 2 (f).
59 See remarks by Secretary Antony J. Blinken at the AI for Accelerating Progress on Sustainable Development
Goals Event (Sept. 18, 2023).
29
3.e. How do these benefits change, if at all, when the training data or the
associated source code of the model is simultaneously widely available?
As discussed in response to Question 1, a responsible approach to open source
means balancing a range of factors (such as privacy or business imperatives) and
considering various aspects of openness in the tech stack (e.g., source code,
training datasets, model weights, and other elements).
Because of the variety of factors at play ⏤and their corresponding mitigations ⏤it
is hard to categorically assess the benefits of releasing with or without the
training data and source code.
However, while it is possible that making training data, source code, and model
weights all simultaneously widely available could allow malicious, determined and
well-resourced downstream developers or other actors to solicit personal
information from the model, releasing code with model weights can lead to
positive security and safety outcomes. (See Question 2, above). It is therefore
important that providers of foundation models are able to choose the type of
release that best balances the benefits of information-sharing, the potential costs
to the provider’s business, and the risks of misuse.
4. Are there other relevant components of open foundation models that, if
simultaneously widely available, would change the risks or benefits presented by
widely available model weights? If so, please list them and explain their impact.
See response to Question 3 (e).
5. What are the safety related or broader technical issues involved in managing
risks and amplifying benefits of dual-use foundation models with widely
available weights?
In principle, responsible development of foundation models should include robust
risk assessments and evaluations (which may include red-teaming for certain
models) that are commensurate with the potential risks involved, are transparent,
and include minimum guardrails. Common standards should be based on existing
frameworks (e.g., the PAI Guidance for Safe Foundation Model Deployment) and
the work of agencies like NIST and NTIA. While this work is ongoing, it is
premature to require specific measurements and benchmarks.
We provide further detail on these points in response to the questions below.
30
5.a. What model evaluations, if any, can help determine the risks or benefits
associated with making weights of a foundation model widely available?
The development of foundation models is a new field, and, at present, there is a
lack of consensus about how best to measure and evaluate certain risks.
Ultimately, determinations around model evaluations need to be based on
repeatable measurements. Helpfully, valuable work is underway and progress is
being made by numerous research institutions, NIST, the U.S. AI Safety Institute,
and the MLCommons initiative.
These efforts should advance the development of evaluations on issues such as:
standardized harm categories; violent crimes; non-violent crimes; sex-related
crimes; child sexual exploitation; indiscriminate weapons (Chemical, Biological,
Radiological, Nuclear, and high yield Explosives ‘CBRNE’); defamation; specialized
advice; privacy; intellectual property; elections; hate; self-harm; and sexual
content. Such evaluations would help ensure a shared understanding on the most
appropriate methodologies for conducting risk/benefit analysis. Their
development should also include various measurements related to the ability of
the model in question to enable/encourage/endorse these activities; standardized
measurement on these evaluations will be important in the context of risk/reward
calculations.60
Until consensus develops, it is premature to require specific measurements and
benchmarks. Doing so likely would lead to reliance on potentially unreliable ⏤and
soon to be outdated ⏤metrics. Mandating specific benchmarks at this point is
likely also to lead to fragmented approaches, making it impossible for
governments, researchers, deployers, and users to compare the safety and
performance of different models. For these reasons, we urge the NTIA to work
with NIST on standardizing the threat models and evaluations along the AI value
chain.
60 We also would note the work of Singapore’s Infocomm Media Development Authority (IMDA), which
collaborated with the AI Verify Foundation to catalog evaluations. Cataloging LLM Evaluations, AI Verify
Foundation (Oct. 2023).
31
Meta’s Approach to Evaluations
Red-teaming
One type of evaluation, red-teaming, has become a key component of our AI
development process for certain models and we continue to invest in research to
make it more efficient and effective.
We significantly invested in red-teaming for Llama 2. Over 350 people were
involved, including employees, contractors, and external vendors. The process
included domain experts in cybersecurity, election fraud, social media
misinformation, legal, policy, civil rights, ethics, software engineering, machine
learning, responsible AI, and creative writing. It also included individuals
representative of a variety of socioeconomic, gender, ethnicity, and racial
demographics; this diversity is important for any successful red-teaming exercise.
The red-teamers probed our models across a wide range of risk categories, such
as criminal planning, human trafficking, regulated or controlled substances,
sexually explicit content, unqualified health or financial advice, and privacy
violations, as well as different attack vectors, such as hypothetical questions,
malformed/misspelled inputs, and extended dialogues.
For Llama-2 we also conducted specific tests to determine the capabilities of our
models to facilitate the production of weapons (e.g., nuclear, biological, chemical,
and cyber).61 And we submitted Llama 2 to the DEFCON convention in Las Vegas
in 2023, alongside other companies like Anthropic, Google, Hugging Face,
Stability, and OpenAI. At that event, over 2,500 hackers analyzed and stress
tested their capabilities ⏤making this one of the largest public red-teaming
events for AI. To support the future release of large foundation models, we have
engaged outside subject- matter experts to support our testing and threat
prioritization for CBRNE, with a specific emphasis on chemical and biological
risks. We will continue to capacity build for evaluations and red-teaming in this
area.
But while we believe that red-teaming is an important evaluation tool, it should
not be the only evaluation tool used or necessarily required for all foundation
models.
61 Hugo Touvron, Louis Martin, et. al., Llama 2: Open Foundation and Fine-Tuned Chat Models (July 19, 2023).
32
Red-teaming is a costly undertaking,62 and is limited by the expertise and scope of
the red-teamers (for example, if a red-team group is only composed of experts in
one particular area, it will not be best equipped to test for vulnerabilities outside
of that scope).63
Model providers need to be able to deploy a range of evaluations, including tools
like benchmarking, human evaluation, A/B testing, cross-validation, and
adversarial attacks.
Open Safety Benchmarks and Classifiers
Meta is committed to advancing AI safety and safety research on evaluations. As
part of this commitment, we developed model evaluations and open safety
tooling through our Purple Llama Project ⏤a major step toward enabling
community collaboration and standardizing the development and use of trust and
safety tools for generative AI development.
Two components of the Purple Llama Project are CyberSecEval and Llama Guard.
We believe CyberSecEval is the most extensive cybersecurity safety benchmark
to date. In our Code Llama-Instruct model, we use CyberSecEval to understand
and mitigate cyber risks, prior to release, providing good-faith actors with one of
the safest coding copilot tools available⏤which we open sourced for both research
and commercial purposes.
Our second tool, Llama Guard, is a safety classifier for filtering input and output
that is trained to detect certain problematic content. We have made both of these
components available on an open-source basis so that other model providers and
downstream developers can leverage these advances for their own safety
fine-tuning.
63 See The Center for Advanced Red Teaming, University of Albany landing page.
62 For example, red-team penetration testing costs between $10,000 to $85,000 and typically runs for several weeks.
See Josh Gormally, How Much Does Red Team Penetration Testing Cost In 2023?, Network Assured (Sept. 26,
2023).
33
Industry Initiatives
Meta is also part of a number of industry initiatives focused on model evaluations.
These initiatives are driving consensus on key foundational issues that we believe
will be important additions to the evolution of the AI governance ecosystem.
(i) ​
AI Risk-Management Standards Profile for General-Purpose AI Systems
and Foundation Models
Issued by the University of California Berkeley AI Research Lab, this
provides specific guidance on how to apply the NIST AI Risk Management
Framework (RMF) to providers of general-purpose AI systems (intended as
an umbrella term for ‘foundation models,’ ‘frontier models,’ and ‘generative
AI’).
(ii) Open Loop US Program
This program explores how NIST’s AI RMF can be adapted to provide
guidance on generative AI, providing evidence gathered from 40 companies
across various industries on their current risk management practices for
generative AI and identifying gaps around information, benchmarks,
standards, and resources.
(iii) Partnership on AI Guidance for Safe Foundation Model Deployment
This represents the most comprehensive and multi-stakeholder AI
self-regulatory approach to date, and in addition to aligning with the White
House Voluntary AI Commitments introduces important recommended
practices, including for the responsible open-sourcing of foundation
models. It also touches on the distribution of responsibility across all actors
in the AI value chain and its lifecycle. (See Question 6 regarding actors in
the value chain).
(iv) AI Safety Working Group at the MLCommons non-profit consortium
This initiative aims to establish and maintain a toolkit of globally viable,
standardized metrics and benchmarks for evaluating and measuring the
potential risks of generative AI. This work includes building and maintaining
AI tooling infrastructure for benchmarking large language models for
safety.
34
Across 2024, Meta will open source some of our research outputs,
datasets, and other assets in collaboration with MLCommons, which we
believe is key to helping the AI community’s research efforts to bring
Responsible Use Guides to life around a shared set of evaluations and
mitigations.
(v) AI Alliance
This is a membership organization Meta co-founded with IBM that
specifically advances open source and open innovation approaches. With
more than 80 members so far, the AI Alliance seeks to work with global
stakeholders to create resources and benchmarks for the responsible and
safe development and deployment of AI.
5.b. Are there effective ways to create safeguards around foundation models,
either to ensure that model weights do not become available, or to protect
system integrity or human well-being (including privacy) and reduce security
risks in those cases where weights are widely available?
5.c. What are the prospects for developing effective safeguards in the future?
Effective safeguards include investing in access control, infrastructure security,
and insider/external threat detections to ensure model weights do not become
available before a planned/controlled release, as well as for models that will not be
made available on an open basis. In addition to implementing these safeguards,
Meta’s approach is to articulate the risk attributes of models and to automate
access control and detection capabilities proportional to these risk attributes.
We have released multiple research projects such as Reasoning over Public and
Private Data in Retrieval-Based Systems, Watermarking Makes Language Models
Radioactive, and An Efficient Algorithm for Integer Lattice Reduction which all
make progress toward exploratory research with potential safeguard applications,
from personal data mitigations, to novel watermarking techniques to potential
attacks of certain vulnerable cryptosystems if combined with other methods. The
research needs to advance as the current results do not yet transfer to product
use cases, but Meta is actively investigating how advancements in our and others’
research can help develop safeguards in the future.
See response to Question 5 (a) on evaluations and red-teaming.
35
5.d. Are there ways to regain control over and/or restrict access to and/or limit
use of weights of an open foundation model that, either inadvertently or
purposely, have already become widely available? What are the approximate
costs of these methods today? How reliable are they?
We are not aware of technical capabilities that could not be overcome by
determined, well-resourced, and capable actors that would allow for regaining
control over model weights that have become widely available. For this reason,
our responsible release approach focuses on pre-release assessment and
mitigation, as well as post-release assessment that can leverage the broader open
source community to identify risks. This enables us to improve both future
releases of our models and system-level mitigations like our Purple Llama suite of
tools, which we can rapidly deploy for developers to use as a part of the systems
they operate.
5.e. What if any secure storage techniques or practices could be considered
necessary to prevent unintentional distribution of model weights?
Computing and storage environments should be actively managed and
monitored, physically and logically secured, and supported by a variety of insider
threat detection programs corresponding to the level of risk presented.
At Meta, we perform training and evaluation of AI models ranging in sensitivity
from purely open source research on public data to proprietary models used as
part of consumer products. We utilize computing environments that are
appropriately secured for the level of sensitivity of the project.
In relation to frontier AI systems, Commitment 3 of the White House Voluntary AI
Commitments specifically addresses investments in “cybersecurity and insider
threat safeguards to protect proprietary and unreleased model weights.”64 For our
unreleased AI model weights for frontier AI systems as defined in the White
House Commitments, we are committed to treating them as core intellectual
property for our business, especially with regards to cybersecurity and insider
threat risks.
64 Voluntary AI Commitments, White House (July 21, 2023)
36
As part of that, until we have completed our pre-release assessment efforts for
frontier AI systems, we will limit internal access to those model weights to people
whose job function requires such access, and we have in place a robust insider
threat detection program consistent with protections provided for our most
valuable intellectual property and trade secrets. In addition, for these models, we
will work with the weights in an appropriately secure environment to reduce the
risk of unapproved release.65
5.f. Which components of a foundation model need to be available, and to whom,
in order to analyze, evaluate, certify, or red-team the model? To the extent
possible, please identify specific evaluations or types of evaluations and the
component(s) that need to be available for each.
The components and type of access required to assess a model depends on the
kind of assessment conducted. It is possible, for example, to conduct adversarial
testing without access beyond that of a typical user. Many AI researchers conduct
AI model experiments using publicly available interfaces (e.g., Google Colab,
Hugging Face, Chatbot Arena).
Similarly, it may be possible to verify that a foundation model meets certain
requirements using documentation such as a model card or other transparency
documentation about how a model was developed or how it performs on certain
evaluations and benchmarks. This kind of information is often included in model
cards or research papers that accompany foundation model releases (e.g., the
artifacts Meta provided for the Llama 2 release).
However, deeper and more reliable analysis of a model’s capabilities can be
conducted with model weights. This is especially valuable for understanding
models that aren’t integrated with system-level, end-to-end protection layers. For
example, if a model is not integrated with appropriate access controls or
encryption, analyzing the model weights could reveal whether sensitive
information is stored in the weights or whether the model relies on certain
features that could be used to launch attacks.
65 Overview of Meta AI safety policies prepared for the UK AI Safety Summit, Transparency Center (Oct. 20, 2023).
37
Similarly, if a model is deployed in a critical infrastructure system without
appropriate network security measures or monitoring in place, analyzing the
model weights could reveal whether the model is vulnerable to adversarial attacks
or other types of manipulation.
Furthermore, for a large number of red-teamers representing a community of
typical users, the testing environment needs to be user-friendly so that the
red-teamers do not spend valuable time learning how to use model weights. In
contrast, to emulate threat actors of a different type, such as determined state
actors or other nefarious threat actors, it is important to make the model weights
available because such actors are more likely to have access to them.
See response to Question 5 (a) regarding evaluations.
5.g. Are there means by which to test or verify model weights? What
methodology or methodologies exist to audit model weights and/or foundation
models?
Open sourcing responsibly is the best means by which to test or verify model
weights because the pre-trained model weights are readily available. Otherwise
one is reliant on available documentation, comparing performance of the closed
model to a baseline model or other forms of external validation.
As part of responsibly developing foundation models, transparency is essential.
When Meta released Llama 2, alongside releasing the model weights, we
developed and shared a number of artifacts relaying how we developed the model
to help developers, researchers, and policymakers better understand its
capabilities and limitations (e.g., model card, research paper).
These artifacts, most importantly the Llama 2 Research Paper, were designed to
provide more information about the process of developing Llama 2 and the steps
we took to do so responsibly. We believe that this approach, in addition to
providing the pre-trained model weights, further facilitates testing or verification.
38
6. What are the legal or business issues related to open foundation models?
There are several parties involved in the AI value chain: foundation model
providers (who train foundational models, proprietary or open-source, that others
may build on as well as interfaces to interact with the models),66 downstream
developers (who use model weights to create their own use cases using their own
data for fine-tuning the model), model deployers (who can be the same as the
downstream developer), and end-users (those who interact with the fine-tuned
model). There needs to be clear differentiation of responsibilities across this value
chain for legal and business clarity. Intellectual property and generative AI needs
to be considered in the context that generative AI models and systems are tools
that enhance human creativity and productivity.
We expand on these points further below.
6.a. In which ways is open-source software policy analogous (or not) to the
availability of model weights? Are there lessons we can learn from the history
and ecosystem of open-source software, open data, and other “open” initiatives
for open foundation models, particularly the availability of model weights?
6.b. How, if at all, does the wide availability of model weights change the
competition dynamics in the broader economy, specifically looking at industries
such as but not limited to healthcare, marketing, and education?
See response to Question 3.
6.c. How, if at all, do intellectual property-related issues—such as the license
terms under which foundation model weights are made publicly
available—influence competition, benefits, and risks? Which licenses are most
prominent in the context of making model weights widely available? What are
the tradeoffs associated with each of these licenses?
With respect to licenses, see response to Question 6 (d).
With respect to competition benefits of open source models, see response to
Question 3 (a).
66 See PAI’s Guidance for Safe Foundation Model Deployment.
39
Regarding intellectual property related issues more broadly, the role of a
generative AI model is to enhance human creativity and productivity. Generative
AI is a tool, no different from the printing press, the camera, or the computer.
Those technologies changed the nature of creative endeavors, and were a
tremendous boon to human creativity and productivity. Generative AI will be no
different.
As with any tool, people ultimately are responsible for how to use that tool. Just
as photographers choose the settings on a camera and the subjects they wish to
photograph, the person using a generative AI model is the one who provides the
prompts to the model that determine the content of the output⏤and is the one
that decides how to use that output.
The purpose of these models is to enable people to create new creative outputs
to suit their preferences and needs. In that sense, a generative AI model is not
different from other neutral commercial technologies accepted under copyright
law, from sound mixing to digital photo editing tools and beyond.67
6.d. Are there concerns about potential barriers to interoperability stemming
from different incompatible “open” licenses, e.g., licenses with conflicting
requirements, applied to AI components? Would standardizing license terms
specifically for foundation model weights be beneficial? Are there particular
examples in existence that could be useful?
Providers of foundation models necessarily need to be able to decide, based on
their business considerations, and risk assessments of their model, what an
appropriate release should look like along the “gradient of release,” including what
aspects of the tech-stack should, or should not, be made available. Incompatibility
between existing open source licenses already exists in the marketplace68 and
consumers of open source code already have to make project decisions based on
these incompatibilities.69
69 Irene Solaiman, The Gradient of Generative AI Release: Methods and Considerations (Feb. 5, 2023).
68 For example, software that combined code released under version 1.1 of the Mozilla Public License (MPL) with
code under the GNU General Public License (GPL) could not be distributed without violating one of the terms of the
licenses⏤regardless that both licenses were approved by both the Open Source Initiative and the Free Software
Foundation.
67 For more detailed discussion of the Copyright Act and generative AI, see Meta Platforms Inc submission to the
US Patent and Trademark Office.
40
7. What are the current or potential voluntary, domestic regulatory, and
international mechanisms to manage risks and maximize the benefits of
foundation models with widely available weights? What kinds of entities should
take a leadership role across which features of governance?
In addition to longstanding laws that already apply to generative AI, an increasing
number of new voluntary initiatives, domestic regulations, and international laws
and principles are emerging.
Given the significant potential for regulatory fragmentation, the U.S. government
has an important role to play ⏤domestically and internationally ⏤to ensure
consistency and interoperability, particularly in driving alignment on net-new
definitions, taxonomies, risk identification, assessments, and mitigations.
As noted by Vice President Kamala Harris, “to provide order and stability in the
midst of global technological change, [ . . . ] we must be guided by a common set
of understandings among nations.”70
To achieve this common understanding, we suggest that the U.S. government
focus on identifying net-new technological and regulatory issues related to
foundation models, continue investing in the most influential international
forums, and drive cooperation on technical standards to advance regulatory
interoperability.
Specifically, we recommend:
●
Focusing on the net-new. We welcome the approach of the U.S.
government and Congress thus far to take time to identify what the
net-new issues are with respect to generative AI. The effort across the
entire federal government under the Executive Order represents a
commitment to working collaboratively with all stakeholders to develop
evidence-based policies on complex, novel issues. This approach is a helpful
contrast to the one adopted by the European Union and other jurisdictions
that have sought to regulate first and understand later.
70 Remarks by Vice President Harris on the Future of Artificial Intelligence, Office of the White House (Nov. 1,
2023)
41
●
Continuing to invest in the most advanced and influential international
efforts. With extensive work underway internationally, the U.S. has an
important role to play connecting work under the E.O. to the most
influential fora, including the OECD’s work to develop a framework for the
implementation of the G7 Hiroshima Process voluntary Code of Conduct.
We also welcome U.S. leadership in securing approval by the U.N. General
Assembly of a resolution on safe, secure, and trustworthy AI systems for
sustainable development,71 which we believe is a meaningful step toward
the interoperable, global standards we need to promote a safe, vibrant,
open, and inclusive AI ecosystem.
●
Cooperating on technical standards. We recommend a formal cooperation
and collaboration framework between the U.S. AI Safety Institute and
similar institutions around the world. This will drive alignment on
fundamental common standards and avoid duplication of efforts. Aligning
the work of standard-development organizations72 will also be an important
aspect of these efforts.
●
Advancing regulatory interoperability. Rather than look to implement hard
law at this time, the government should focus efforts on building consensus
and alignment across initiatives such as the White House Voluntary AI
Commitments, the forthcoming NIST AI RMF companion guide for
generative AI, the Munich Security Conference AI Elections Accord, the
Bletchley Declaration, and industry frameworks such the Partnership on AI
Synthetic Media Framework, the Guidance for Safe Foundation Model
Deployment, and the work of the AI Alliance, MLCommons, Frontier Model
Forum, and AI Verify Foundation.
7.a. What security, legal, or other measures can reasonably be employed to
reliably prevent wide availability of access to a foundation model’s weights, or
limit their end use?
See response to Question 5 (d).
72 There are broadly four main international standards development organizations in the field of AI: the International
Organization for Standardization (ISO), the International Electrotechnical Commission (IEC), the International
Telecommunications Union (ITU), and the IEEE Standards Association (IEEE). See also work of the AI Standards
Hub.
71 More Than 50 UN Member States Join U.S. in Urging Support for Proposed U.S.-Drafted General Assembly
Resolution on AI
42
7.b. How might the wide availability of open foundation model weights facilitate,
or else frustrate, government action in AI regulation?
For the reasons cited in Question 3, restricting open foundation models could
frustrate governments objectives around AI transparency, competition, fairness,
innovation, maintaining U.S. leadership, and security.
7.c. When, if ever, should entities deploying AI disclose to users or the general
public that they are using open foundation models either with or without widely
available weights?
See response to Question 1 regarding Meta’s approach to responsible open
source.
7.d. What role, if any, should the U.S. government take in setting metrics for risk,
creating standards for best practices, and/or supporting or restricting the
availability of foundation model weights?
See response to Question 7.
7.d.i. Should other government or non-government bodies, currently existing or
not, support the government in this role? Should this vary by sector?
See response to Question 7.
7.e. What should the role of model hosting services (e.g. Hugging Face, GitHub,
etc.) be in making dual-use models with open weights more or less available?
Should hosting services host models that do not meet certain safety standards?
By whom should those standards be prescribed?
Irrespective of whether a model is closed or open, hosting services could provide a
form of notice to consumers to indicate whether a model has met industry
accepted benchmarks, such as those in development by NIST.
43
Hosting services can significantly help disseminate information about AI safety to
the general public by noting their own safety rankings and publishing other model
evaluations.73 We also note the work of the Department of Commerce through the
NPRM implementing EO 13984 & EO 14110, and draw NTIA’s attention to
previous comments in submissions on the negative impact of potential
obligations on IaaS providers , such as Know Your Customer, to the U.S.’
macroeconomy and global digital leadership.74
7.f. Should there be different standards for government as opposed to private
industry when it comes to sharing model weights of open foundation models or
contracting with companies who use them?
When considering whether different standards should apply to the government as
opposed to private industry, the determining factors should be the use of a model
and the risks of that use ⏤not whether the foundation model is open or not. For
example, a government may use foundation models for a range of legitimate
public policy objectives that carry a potentially higher risk for individual rights ⏤
such as the use of foundation models to provide social security benefits, or for law
enforcement and military purposes.
Because these uses could have legal or substantially similar effects on individuals,
they should be subject to a higher standard of risk assessment, fairness analysis
and mitigation, transparency, accountability, and recourse.
7.g. What should the U.S. prioritize in working with other countries on this topic,
and which countries are most important to work with?
See response to Question 7.
7.h. What insights from other countries or other societal systems are most useful
to consider?
See response to Question 7.
74 Miller, John, et. al., ITI Comments Responding to Commerce Department Advance Notice of Proposed
Rulemaking on Taking Additional Steps to Address the National Emergency with Respect to Significant Malicious
Cyber-Enabled Activities (EO 13984) (RIN # 0605-AA61; Docket No. 210913-0183), ITI (Oct. 25, 2021)
73 Chenhui Zhang, et. al., An Introduction to AI Secure LLM Safety Leaderboard, HuggingFace blog (Jan. 26, 2024).
44
7.i. Are there effective mechanisms or procedures that can be used by the
government or companies to make decisions regarding an appropriate degree of
availability of model weights in a dual-use foundation model or the dual-use
foundation model ecosystem? Are there methods for making effective decisions
about open AI deployment that balance both benefits and risks? This may include
responsible capability scaling policies, preparedness frameworks, et cetera.
See response to Question 1.
7.j. Are there particular individuals/entities who should or should not have access
to open-weight foundation models? If so, why and under what circumstances?
We know there is a small, determined group of bad actors who will go to extreme
lengths to gain access to technologies, including AI technology, regardless of
whether it is open or closed. There is always the possibility that models will be
subject to unintentional release ⏤either through lapses in security, leaks, or
adversarial attacks. So providers of foundation models ⏤open or closed ⏤should
operate on the basis that model weights, and other artifacts, could be accessed
by bad actors or become publicly available, and act accordingly.
As part of our commitment to responsible open source, Meta evaluates our
models on a variety of risk vectors prior to releasing our models under permissive
commercial licenses, including risks related to misuse.
8. In the face of continually changing technology, and unforeseen risks and
benefits, how can governments, companies, and individuals make decisions or
plans today about open foundation models that will be useful in the future?
It is precisely because of technology’s continually changing nature, as well as its
potential risks and benefits, that policy measures should not fossilize in hard law
obligations based on technology itself, but rather focus on uses and allocating
responsibility across the value chain, with a focus on end-use.
It is also for this reason that a rush to regulate now, before common standards,
taxonomies, and cross-industry technical work has matured, will render such
regulation redundant at best, and at worst lead to poor outcomes for American
innovation, individual rights, and global competitiveness.
45
This is why multi-stakeholder efforts, like that of the U.S. AI Safety Institute, and
in international fora like the G7 and OECD, are central to the ability for
governments, companies, and individuals to formulate durable, balanced solutions
to the challenges and opportunities of today, and the future.
8.a. How should these potentially competing interests of innovation,
competition, and security be addressed or balanced?
See response to Question 8.
8.b. Noting that E.O. 14110 grants the Secretary of Commerce the capacity to
adapt the threshold, is the amount of computational resources required to build a
model, such as the cutoff of 10^26 integer or floating-point operations used in
the Executive Order, a useful metric for thresholds to mitigate risk in the
long-term, particularly for risks associated with wide availability of model
weights?
8.c. Are there more robust risk metrics for foundation models with widely
available weights that will stand the test of time? Should we look at models that
fall outside of the dual-use foundation model definition?
The amount of computational resources required to build a model is not a useful
proxy for risk over time. It does not correlate with the ability of the model to result
in outcomes that would pose a significant risk beyond existing technology and it
does not take into account that smaller fine-tuned models could be capable of
such risks with significantly fewer computational resources. Scaling laws and the
decreasing costs of compute suggest that both pre-training and fine-tuning will
be possible with far less compute power than contemplated by the threshold set
forth in the E.O. NIST’s work to develop benchmarks for evaluations will be
important to establishing common standards on this.
Further, it is generally important to distinguish between ""dual use foundation
models"" and models that fall outside of that definition.
9. What other issues, topics, or adjacent technological advancements should we
consider when analyzing risks and benefits of dual-use foundation models with
widely available model weights?
See response to Question 1.
46
If the United States seeks to impose restrictive limitations on open foundation
models, in addition to curtailing the work underway by NIST to establish common
standards for the safe development and deployment of AI, it would have
significant negative effects on U.S. interests. Such actions could constrain the
ability of model providers to open source models, including within the U.S.
Given broad-based enthusiasm for open source AI globally, including among U.S.
allies and the Global South, restrictions could diminish U.S. leadership and
standing related to AI governance and other important issues. Since open source
drives innovation and sets the standard on which others will build, the U.S. would
cede the pole position in AI leadership and innovation, depriving the U.S. economy
of related growth opportunities.
Such restrictions would also leave a vacuum that other countries would be eager
to fill and benefit from, with no guarantee that the models they enable will reflect
U.S. values or vision for the future.
As Freedom House has noted, “authoritarian governments are building centralized
foundation models that limit access to accurate information and embed
censorship.”75 The U.S. Department of State, Bureau of Democracy, Human Rights
and Labor similarly found that, “authoritarian governments such as the People’s
Republic of China attempt to influence the fundamental character of the global
Internet with the advancement of protocols and standards that enable more
centralized Internet control, for the purposes of censorship and surveillance, as
well as the abuse of other human rights…critical open source software and
systems [can] protect the security and integrity of communications on the global
Internet.”76
Furthermore, the U.S. government has historically taken the view that certain
types of tech/software qualify for protection under the First Amendment and
should therefore not be subject to restrictions and made freely available.
76 Supporting Critical Open Source Technologies That Enable a Free and Open Internet, U.S. Department of State
Funding Opportunity Announcement (Feb. 21, 2023).
75 Freedom on the Net 2023: The Repressive Power of Artificial Intelligence, Freedom House (2023).
47
10. Recommendations
In its report to the President on the potential benefits, risks, and implications of
dual-use foundation models for which the model weights are widely available, as
well as policy and regulatory recommendations pertaining to those models, we
suggest NTIA recommend that the U.S. government:
1. Minimize restrictive limitations on open source AI so that open foundation
models can continue to advance American leadership globally and deliver
for the American people, including use throughout the federal government.
2. In considering approaches to AI governance that balance the benefits of
open foundation models against cognizable marginal risks, the focus should
be the responsible development of all AI models.
3. Adequately resource and fund the work of the U.S. government under the
White House Executive Order on Safe, Secure, and Trustworthy
Development and Use of Artificial Intelligence, in particular the U.S. AI
Safety Institute.
4. Expedite the work of the U.S. AI Safety Institute and promote cooperation
with counterpart safety institutes around the world through a
comprehensive framework agreement (e.g., a Memorandum of
Understanding between AI Safety Institutes, or via existing mechanisms
such as Cooperative Research and Development Agreements).
5. Support bipartisan AI legislation so that the United States can lead globally
on responsible innovation for the technology and ensure consistent
standards across the U.S. Such legislation should be focused on the
development of domestic common standards informed by the White House
Voluntary AI Commitments, the work of federal departments and agencies
under the E.O., academia, industry, and standard development
organizations.
6. Connect the U.S. government’s existing work under the E.O. and the White
House Voluntary AI Commitments to its leadership globally through
ensuring that responsible open source is a central tenet of its foreign and
trade policies, driving alignment with American governance frameworks,
technical standards, and values in international fora.
48
7. Protect American industry and interests abroad from aggressive efforts in
other jurisdictions to unfairly discriminate against American companies,
restrict the ability for American companies to make their technology
available in global markets, or force technology transfer, including
mandatory government access to foundation model artifacts.
49
",Meta,22124
NTIA-2023-0009-0165," 
 
 
 
Comment to the National Telecommunications and 
Information Administration 
Dual Use Foundation Artiﬁcial Intelligence Models with Widely 
Available Model Weights 
March 26, 2024 
 
Regulations.gov Docket No. NTIA-2023-0009 
NTIA Docket No. 240216-0052 
 
Max Gulker, Ph.D.1 
Spence Purnell2 
Richard Sill3 
 
1 Senior Policy Analyst, Reason Foundation 
2 Director of Technology Policy, Reason Foundation 
3 Technology Policy Intern, Reason Foundation 
 
 
 
2 
 
Introduction 
On behalf of Reason Foundation4, we respectfully respond to the National Telecommunications 
and Information Administration’s (“NTIA”) request for comments on the risks and beneﬁts of “dual-
use foundation artiﬁcial intelligence models with widely available weights,” or open foundation AI 
models.5 Reason Foundation is a national 501(c)(3) public policy research and education 
organization with expertise across a range of policy areas, including emerging technology. 
These generative AI models entered widespread public awareness following the release of OpenAI’s 
ﬁrst version of ChatGPT in late 2022.6 Open-foundation AI models with publicly available weights, a 
particular category of generative AI model explained further below, allows greater or full access to 
the inputs of models so that others may customize or create applications with them. 
The NTIA is not a policymaking body but will issue an advisory report to the president on potential 
policy in this area. In the thus far limited public debate some have proposed preemptive measures 
to limit the openness of AI foundation models or their proliferation.7 
Our comment discusses several of the questions posed by the NTIA, particularly Question #3 on 
the potential beneﬁts of open foundation models, and Question #2 on the potential risks. We argue 
that allowing model developers to freely choose and innovate along dimensions of openness may 
be indispensable in realizing many of the technology’s beneﬁts, without any evidence of speciﬁc 
risks over and above those of more closed approaches. 
The NTIA should not at this time recommend a policy aimed at restricting open foundation AI 
models with publicly available weights. 
What are open foundation AI models? 
OpenAI publicly released ChatGPT in late 2022.8 It is therefore not surprising that the issue of open 
versus closed foundation AI models remains very much under development and has not yet led to 
widespread public debate. 
For AI foundation or large language models (“LLMs,” of which the ChatGPT releases are examples), 
“open” versus “closed” hinges on the degree of access allowed by model developers to the 
computer code of the model itself, the data the model was trained on, and the numerical weights 
assigned to the data when producing output such as text or pictures. Building on earlier work, 
 
4 See About Reason Foundation, https://reason.org/about-reason-foundation/ 
5  National Telecommunications and Information Association, “Dual Use Foundation Artiﬁcial Intelligence 
Models With Widely Available Model Weights,” Request for Comment, 26 Feb 2024. 
https://www.federalregister.gov/documents/2024/02/26/2024-03763/dual-use-foundation-artiﬁcial-
intelligence-models-with-widely-available-model-weights 
6 “Chat-GPT,” Wikipedia page accessed 26 March 2024. https://en.wikipedia.org/wiki/ChatGPT 
7 Sayash Kapoor et al., “On the Societal Impact of Open Foundation Models,” arXiv:2403.07918 [cs.CY], 27 
Feb 2024. https://hai.stanford.edu/sites/default/ﬁles/2023-12/Governing-Open-Foundation-Models.pdf 
8 “Chat-GPT,” Wikipedia page accessed 26 March 2024. https://en.wikipedia.org/wiki/ChatGPT 
 
 
 
3 
 
researchers in late 2023 categorized current and anticipated generative AI models on an 
approximate open/closed continuum:9 
 
The ﬁgure shows that diƯerent foundation model developers have already experimented with an 
array of degrees of openness. ChatGPT, for example, has allowed public access to some model 
features but not model weights. 
Full access to model weights in addition to other inputs would allow end users to customize the 
data on which LLMs are trained or adjust the weights of already-existing data. Anyone lacking the 
means to create their own specialized generative AI models from the ground up could customize 
open-foundation models. Third-party developers would likely produce an array of applications of 
interest to end users. As of March 2024, we have yet to see this rapidly developing aspect of the 
technology fully deployed. 
However, those less familiar with the technology need not view these dimensions of openness for 
AI models as unusual relative to other uses of computer software and applications. There is now a 
long history of closed and proprietary computer software existing in the same ecosystem as a 
vibrant open-source movement.10  Consumers now understand the potential of third-party 
developers through the case of smartphone apps, where Apple and the Android ecosystem have 
been two examples of intermediate cases on diƯerent points of a similar open/closed continuum.11 
Beneﬁts of open foundation AI models (NTIA question #3) 
“What are the beneﬁts of foundation models with model weights that are widely 
available as compared to fully closed models?” 
Consumers, end-users, and third-party developers are not merely the beneﬁciaries of innovation 
for novel technologies like generative AI, but contribute indispensably to the ongoing process of 
innovation itself. They provide feedback for new ideas through the market mechanism, informing 
 
9 Rishi Bommasani et al., “Considerations for Governing Open Foundation Models,” HAI Policy and Society 
Issue Brief, December 2023. https://hai.stanford.edu/sites/default/ﬁles/2023-12/Governing-Open-
Foundation-Models.pdf 
10 Tozzi, Christopher, For Fun and Proﬁt: A History of the Free and Open Source Software Revolution, MIT 
Press, 2017. 
11 Purnell, Spence and Grayce Burns, “The pitfalls of regulating app stores,” Reason Foundation. 
https://reason.org/commentary/the-pitfalls-of-regulating-app-stores/ 
 
 
 
4 
 
innovators in a continuous process of many small interactions that generally steers technology 
toward beneﬁcial uses that a room full of the greatest minds could never anticipate.12 
The reliably unpredictable development of internet applications provides numerous examples of 
consumers helping guide innovations to places few people foresaw. In the early days of mp3 
downloads, both legal and through illicit ﬁle-sharing, companies assumed consumers would place 
a high value on owning ﬁles on their hard drives. But through the process of more reliable internet 
technology and growing consumer comfort with the model, streaming emerged as the truly 
disruptive force in the music industry.13 Social media platforms present similar examples. 
Facebook was created for students at a single university before widespread adoption. 
The role of consumers and end-users is especially important in the development of foundational 
technologies—advances beneﬁcial through a wide array of applications rather than a single-use 
case. Like the internet technology noted above, generative AI will almost certainly derive its beneﬁts 
by helping people perform tasks, access information, and communicate ideas. 
Third-party application developers add another link in the chain between AI model and end-user 
but do not alter the fundamental truth that the most useful innovative ideas emerge from an 
evolutionary give-and-take process between developers and users of various types. In fact, such 
third parties would likely increase the speed and eƯicacy of the market process in fueling 
innovation. 
Open-foundation AI models would allow for niche specialization by various groups who may lack 
the resources and know-how to create their own generative AI model. The NTIA request for 
comment makes frequent mention, for example, of medical and academic researchers. 
Consider how a small group of cutting-edge researchers in a ﬁeld might make use of an open-
foundation AI model. Broadly speaking, generative AI models are trained over very large sets of 
language from the internet. Researchers in any number of ﬁelds might want to place diƯerent 
weights on diƯerent portions of academic literature, customizing what the generative AI model 
might do for them. The true beneﬁts of open approaches to technology often lie in the process that 
ensues as adjustments and interactions take place up and down the chain between model 
developers, intermediaries such as application developers, and consumers. This is when millions 
of minds lead us to use cases that a few very smart people in a room could never anticipate.14 
A scenario with only closed generative AI models and end users would likely lead to some beneﬁts 
of this creative process going unrealized. When some AI models are open to varying degrees, it 
allows developers and end users to unpack and understand what makes such models work and 
provides the original model builders with an extra layer of feedback from sophisticated third-party 
 
12 Ridley, Matt, How Innovation Works, Harper-Collins, 2021. 
13 Ganz, Jacob, “How Streaming is Changing Music,” National Public Radio, 1 June 2015. 
https://www.npr.org/sections/therecord/2015/06/01/411119372/how-streaming-is-changing-music 
14 Gulker, Max. “Calls to regulate AI ignore how consumers help shape 
innovationhttps://reason.org/commentary/calls-to-regulate-ai-ignore-how-consumers-help-shape-
innovation/ 
 
 
 
5 
 
developers. This makes many more people than the model’s proprietary creators able to 
experiment, debug, and innovate. 
This does not mean that regulators should require certain types of open access or otherwise tilt the 
playing ﬁeld in favor of more openness in generative AI models. Allowing developers to experiment 
with diƯerent degrees of openness and means of providing data is a critical part of a robust market-
based ecosystem for generative AI. As mentioned earlier, the Apple “closed” software system has 
been very succesful at incorporating user feedback and iterating successful versions.  Requiring all 
models to be open access puts this model at risk. The feedback they receive from a well-
functioning market will better allocate resources to models of varying openness than premature 
regulatory guesswork. 
In a June 2023 public comment to the NTIA, Neil Chilson and Will Rinehart of the Center for Growth 
and Opportunity emphasize the indispensability of a market-based system of governance and 
accountability for generative AI more broadly:15  
“An accountability ecosystem for software already exists and has proven highly eƯective. It 
is polycentric in that it is layered and is comprised of business-to-business and business-
to-consumer markets, reputational markets, and ﬁnancial markets, all backed by generally 
applicable laws and norms.” 
This position does not preclude regulation or a role for the public sector entirely but comes with a 
warning that regulators should only step in when clear and well-established market failures are 
observed to take place. 
This is especially important advice with highly novel technology like generative AI. Because the 
underlying technology and its use cases are still extremely early in their processes of development, 
creating preemptive rules, licensing regimes, or prohibitions—without indication of speciﬁc risks 
and market failures—would not be advisable.  
Risks of open foundation AI models (NTIA question #2) 
“How do the risks associated with making model weights widely available compare to the 
risks associated with non-public model weights?” 
Though AI model developers have begun experimenting along dimensions of openness, we have not 
yet seen a fully operational open-foundation AI model with modiﬁcations and applications created 
by third parties. Thus far, computer scientists have not found clear or compelling theoretical 
evidence that points to open foundation AI models having new or greater risks because of their 
openness. 
Writing in February 2024, a team of computer scientists and experts from related disciplines 
spanning academia and industry examined the theoretical foundations of open foundation AI 
models. They considered several commonly discussed categories of threats, including 
 
15 Chilson, Neil and Will Rinehart, “Public Interest Comment on the National Telecommunications 
and Information Administration (NTIA) AI Accountability Policy,” The Center for Growth and Opportunity at 
Utah State University, 12 June 2023. https://www.thecgo.org/wp-content/uploads/2023/06/NTIA-comments-
on-AI-accountability_03.pdf 
 
 
 
6 
 
disinformation, cyberattacks, and scams directed at individuals. In this newly burgeoning ﬁeld, they 
ﬁnd that “current research is insuƯicient to eƯectively characterize the marginal risk of open 
foundation models relative to pre-existing technologies,” but stress the need for more empirical 
research as those data become available.16 
In a December 2023 issue brief published jointly by AI labs at institutions including Stanford 
University and Princeton University, a team of researchers stated that:17 
“While open foundation AI models are conjectured to contribute to malicious uses of AI, the 
weakness of evidence is striking. More research is necessary to assess the marginal risk of 
open foundation models. 
Policymakers should also consider the potential for AI regulation to have unintended 
consequences on the vibrant innovation ecosystem around open foundation models.” 
These ﬁndings should give the NTIA particular pause in recommending speciﬁc regulatory 
restrictions on open foundation models before the technology can appropriately develop in the 
market. 
Issues of equity in open foundation AI models (NTIA questions #2b and 
#3c) 
“Could open foundation models reduce equity in rights and safety-impacting AI systems (e.g., 
healthcare, education, criminal justice, housing, online platforms, etc.)?” 
“Could open model weights, and in particular the ability to retrain models, help advance 
equity in rights and safety-impacting AI systems (e.g., healthcare, education, criminal justice, 
housing, online platforms etc.)?” 
The distinction between open and closed will likely not make a material diƯerence to how AI 
impacts equity and rights.   
Both models present a low risk of discrimination because these behaviors are already illegal 
whether carried out by humans or AI and can be enforced the same way. If an AI system 
discriminates against a protected class during a hiring process, citizens would still have grounding 
to bring suit.  Whether the model is open or closed, the violation can only occur once that system 
has made a determination. Using a data collection or reporting system would help detect illegal 
patterns in AI decision-making.    
Just like a human operator, an AI can be trained to follow certain rules but still needs to be 
monitored for accuracy. However, this doesn’t necessarily imply that regular AI source code audits 
are necessary, as current human-based hiring practices can discriminate but are not regularly 
audited by the government. An AI system could operate under the current complaint-based system 
 
16 Sayash Kapoor et al., “On the Societal Impact of Open Foundation Models,” arXiv:2403.07918 [cs.CY], 27 
Feb 2024. https://hai.stanford.edu/sites/default/ﬁles/2023-12/Governing-Open-Foundation-Models.pdf 
17 Rishi Bommasani et al., “Considerations for Governing Open Foundation Models,” HAI Policy and Society 
Issue Brief, December 2023. https://hai.stanford.edu/sites/default/ﬁles/2023-12/Governing-Open-
Foundation-Models.pdf 
 
 
 
7 
 
where citizens can ﬁle a complaint with the government if they believe discrimination is occurring. 
If enough complaints and evidence pile up, an investigation is initiated and the legal/court process 
handles the rest. These types of complaint-based systems exist in all the areas raised by the 
request (healthcare, education, hiring, etc.) and would continue to function normally without any 
new regulations whether and AI system was open or closed.   
In addition to outcome-based evaluation, approximating the source code to both and closed 
source is similar. While open-source models have source code that can be more easily inspected, 
even closed systems can be interacted with enough to “reverse engineer” the source code to near 
exact approximation. In fact, ChatGPT has already been reverse-engineered and released onto the 
web several times, demonstrating this capability.18 In the case of image and video generative AI, the 
recent rise of “deepfakes” has already spawned a counter industry of “deepfake detectors,” which 
tell users whether images and videos have been generated or heavily edited by AI.19  
Public policy should focus on working with industry to standardize and deploy AI detection and 
evaluation systems in appropriate areas. Developing technologies that evaluate AI decision-making 
will be critical to ensuring appropriate use.  
Instead of seeking out speciﬁc regulations or trying to prevent broadly deﬁned negative outcomes, 
policy should help cultivate and develop industry standards such as monitoring and reporting of AI 
systems for things that are already illegal.   
Conclusion 
Generative AI—including the features related to openness—is the most recent in a now long list of 
advances in information and communication technology that have sparked concern and debate 
about a similar set of risks. These risks are fundamentally tied to the technologies’ vast beneﬁts: 
enabling even individual users to access information, communicate, and create content in 
unprecedented ways. With new tools, individual users gain greater capacities to misinform, 
victimize others, or commit crimes. 
We do not yet have any reason to believe open foundation generative AI models represent more 
than continued progress in what such technology enables us to do. We do have reason to believe 
that hasty regulation on this topic has a high chance of preventing us from realizing some of the 
beneﬁts of this novel technology. 
These considerations suggest that tight regulation, especially this early in the development of open 
foundation AI models, is not needed and would likely be counterproductive. Instead, the focus 
should be on markets, innovation, and free enterprise, where consumers and builders of AI models 
alike learn to set standards and mitigate risks through time. 
 
 
 
18 Y Combinator, accessed 26 March 2024. https://news.ycombinator.com/item?id=35742685 
19 MIT Media Lab, “Detect DeepFakes: How to counteract misinformation created by AI,” accessed 26 March 
2024. https://www.media.mit.edu/projects/detect-fakes/overview/ 
",Reason Found,4007
